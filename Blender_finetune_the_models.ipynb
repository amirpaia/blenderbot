{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirpaia/blenderbot/blob/main/Blender_finetune_the_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIjKC2PuaiN8"
      },
      "source": [
        "# 0.Installing prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwE6XWn6-SKi",
        "outputId": "c81626f4-0a24-4734-bb98-99a3f023e5e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 11 09:01:01 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifdiYyBz-fcX",
        "outputId": "4d680cb0-62fc-496a-c997-acd77946f3de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99GiNSrdj4TY",
        "outputId": "4d62833b-9b11-45f4-b55d-82206449c087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF-1Vn2JGcsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0895a453-4e9d-4434-a4ae-a70f689111d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 38.7 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 38.5 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 133 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 143 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 163 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 174 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 194 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 204 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 215 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 225 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 235 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 245 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 256 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 266 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 276 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 286 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 296 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 307 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 317 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 327 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 337 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 348 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 358 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 368 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 378 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 389 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 399 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 409 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 419 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 430 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 440 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 450 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 460 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 471 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 481 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 491 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 501 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 512 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 522 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 532 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 542 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 552 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 563 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 573 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 583 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 593 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 604 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 614 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 624 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 634 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 645 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 655 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 665 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 675 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 686 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 696 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 706 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 716 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 727 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 737 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 747 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 757 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 768 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 778 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 788 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 798 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 808 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 819 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 829 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 839 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 849 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 860 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 870 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 880 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 890 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 901 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 911 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 921 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 931 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 942 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 952 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 962 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 972 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 983 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 993 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.0 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.0 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.0 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.1 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.1 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.1 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.3 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.4 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.4 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.4 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.4 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.4 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.5 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.5 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.5 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.5 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.6 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.6 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.6 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.6 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.6 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.7 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.7 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.7 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 9.5 MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 49.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20 kB 57.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 66.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 40 kB 69.9 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 51 kB 72.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 61 kB 75.6 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 71 kB 77.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 81 kB 78.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 92 kB 81.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 102 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 112 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 122 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 133 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 143 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 153 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 163 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 174 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 184 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 194 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 204 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 215 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 225 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 235 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 245 kB 82.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 248 kB 82.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 125 kB 71.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 61.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 52.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 342 kB 75.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 96.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 78.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 58.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 92.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 61.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 71.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 175 kB 80.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 547 kB 83.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 69.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 95 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 208 kB 82.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 235 kB 72.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 749 kB 69.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 65.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 95.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 98.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 77.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 81.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 110 kB 75.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 12.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 100 kB 12.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 121 kB 96.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 93.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 94.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 963 kB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25h  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 61.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.4.24)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.2.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=bde1f6d97270711fbc232b5012599a5e3cd9ebc8d164964c1b0649610abf0145\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "mydrive_path = '/content/drive/MyDrive/colabs/blender-models/'\n",
        "# !pip uninstall -q parlai\n",
        "!pip install -q parlai\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmFp5DT2Nk7M"
      },
      "source": [
        "# 1.Chatting with a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQlB2lBPOWX-"
      },
      "source": [
        "zoo models: https://parl.ai/docs/zoo.html \\\\\n",
        "Model files that begin with \"zoo:\" are special files distributed by the ParlAI team.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xREs2-yPBuf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uXMJLG8_Nnp-",
        "outputId": "a2f0ccae-279a-4c77-9b80-c0c9a97f0b19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:05:28 | \u001b[33mOverriding opt[\"model_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model (previously: /checkpoint/ems/2020_antiscaling/sweeps/s2020_11_19__productionizing/01_blenderbot/005/b1ff/model)\u001b[0m\n",
            "09:05:28 | Using CUDA\n",
            "09:05:28 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict\n",
            "09:05:28 | num words = 8008\n",
            "09:05:28 | TransformerGenerator: full interactive mode on.\n",
            "09:05:35 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "09:05:35 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model\n",
            "09:05:51 | Opt:\n",
            "09:05:51 |     activation: gelu\n",
            "09:05:51 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "09:05:51 |     adam_eps: 1e-08\n",
            "09:05:51 |     add_p1_after_newln: False\n",
            "09:05:51 |     aggregate_micro: False\n",
            "09:05:51 |     allow_missing_init_opts: True\n",
            "09:05:51 |     attention_dropout: 0.0\n",
            "09:05:51 |     batchsize: 8\n",
            "09:05:51 |     beam_block_full_context: True\n",
            "09:05:51 |     beam_block_list_filename: None\n",
            "09:05:51 |     beam_block_ngram: 3\n",
            "09:05:51 |     beam_context_block_ngram: 3\n",
            "09:05:51 |     beam_delay: 30\n",
            "09:05:51 |     beam_length_penalty: 0.65\n",
            "09:05:51 |     beam_min_length: 20\n",
            "09:05:51 |     beam_size: 10\n",
            "09:05:51 |     betas: '[0.9, 0.999]'\n",
            "09:05:51 |     bpe_add_prefix_space: True\n",
            "09:05:51 |     bpe_debug: False\n",
            "09:05:51 |     bpe_dropout: None\n",
            "09:05:51 |     bpe_merge: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict-merges.txt\n",
            "09:05:51 |     bpe_vocab: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict-vocab.json\n",
            "09:05:51 |     checkpoint_activations: False\n",
            "09:05:51 |     chosen_topic_delimiter: '\\n'\n",
            "09:05:51 |     compute_tokenized_bleu: False\n",
            "09:05:51 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "09:05:51 |     datatype: train\n",
            "09:05:51 |     delimiter: '  '\n",
            "09:05:51 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "09:05:51 |     dict_endtoken: __end__\n",
            "09:05:51 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict\n",
            "09:05:51 |     dict_include_test: False\n",
            "09:05:51 |     dict_include_valid: False\n",
            "09:05:51 |     dict_initpath: None\n",
            "09:05:51 |     dict_language: english\n",
            "09:05:51 |     dict_loaded: True\n",
            "09:05:51 |     dict_lower: False\n",
            "09:05:51 |     dict_max_ngram_size: -1\n",
            "09:05:51 |     dict_maxexs: -1\n",
            "09:05:51 |     dict_maxtokens: -1\n",
            "09:05:51 |     dict_minfreq: 0\n",
            "09:05:51 |     dict_nulltoken: __null__\n",
            "09:05:51 |     dict_starttoken: __start__\n",
            "09:05:51 |     dict_textfields: text,labels\n",
            "09:05:51 |     dict_tokenizer: bytelevelbpe\n",
            "09:05:51 |     dict_unktoken: __unk__\n",
            "09:05:51 |     display_add_fields: \n",
            "09:05:51 |     display_examples: False\n",
            "09:05:51 |     display_prettify: False\n",
            "09:05:51 |     distributed_world_size: 8\n",
            "09:05:51 |     download_path: None\n",
            "09:05:51 |     dropout: 0.1\n",
            "09:05:51 |     dynamic_batching: full\n",
            "09:05:51 |     embedding_loss_coeff: 0.35\n",
            "09:05:51 |     embedding_projection: random\n",
            "09:05:51 |     embedding_size: 1280\n",
            "09:05:51 |     embedding_type: random\n",
            "09:05:51 |     embeddings_scale: True\n",
            "09:05:51 |     enc_dec_attn_loss_coeff: 3.0\n",
            "09:05:51 |     encoder_loss_coeff: 24.0\n",
            "09:05:51 |     eval_batchsize: 8\n",
            "09:05:51 |     evaltask: None\n",
            "09:05:51 |     ffn_size: 5120\n",
            "09:05:51 |     force_fp16_tokens: True\n",
            "09:05:51 |     fp16: True\n",
            "09:05:51 |     fp16_impl: mem_efficient\n",
            "09:05:51 |     gpu: 0\n",
            "09:05:51 |     gradient_clip: 0.1\n",
            "09:05:51 |     hidden_loss_coeff: 5.0\n",
            "09:05:51 |     hide_labels: False\n",
            "09:05:51 |     history_add_global_end_token: end\n",
            "09:05:51 |     history_reversed: False\n",
            "09:05:51 |     history_size: -1\n",
            "09:05:51 |     image_cropsize: 224\n",
            "09:05:51 |     image_mode: raw\n",
            "09:05:51 |     image_size: 256\n",
            "09:05:51 |     include_checked_sentence: True\n",
            "09:05:51 |     include_knowledge: True\n",
            "09:05:51 |     include_knowledge_separator: False\n",
            "09:05:51 |     inference: beam\n",
            "09:05:51 |     init_model: None\n",
            "09:05:51 |     init_opt: /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt\n",
            "09:05:51 |     interactive_mode: True\n",
            "09:05:51 |     interactive_task: True\n",
            "09:05:51 |     invsqrt_lr_decay_gamma: -1\n",
            "09:05:51 |     is_debug: False\n",
            "09:05:51 |     label_truncate: 128\n",
            "09:05:51 |     label_type: response\n",
            "09:05:51 |     learn_positional_embeddings: False\n",
            "09:05:51 |     learningrate: 0.0004\n",
            "09:05:51 |     local_human_candidates_file: None\n",
            "09:05:51 |     log_every_n_secs: 10.0\n",
            "09:05:51 |     log_keep_fields: all\n",
            "09:05:51 |     loglevel: info\n",
            "09:05:51 |     lr_scheduler: reduceonplateau\n",
            "09:05:51 |     lr_scheduler_decay: 0.5\n",
            "09:05:51 |     lr_scheduler_patience: 3\n",
            "09:05:51 |     max_lr_steps: -1\n",
            "09:05:51 |     max_train_time: -1.0\n",
            "09:05:51 |     metrics: default\n",
            "09:05:51 |     model: transformer/generator\n",
            "09:05:51 |     model_file: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model\n",
            "09:05:51 |     model_parallel: False\n",
            "09:05:51 |     momentum: 0\n",
            "09:05:51 |     multitask_weights: '[1.0, 3.0, 3.0, 3.0]'\n",
            "09:05:51 |     n_decoder_layers: 12\n",
            "09:05:51 |     n_encoder_layers: 2\n",
            "09:05:51 |     n_heads: 32\n",
            "09:05:51 |     n_layers: 2\n",
            "09:05:51 |     n_positions: 128\n",
            "09:05:51 |     n_segments: 0\n",
            "09:05:51 |     nesterov: True\n",
            "09:05:51 |     no_cuda: False\n",
            "09:05:51 |     num_epochs: -1\n",
            "09:05:51 |     num_topics: 5\n",
            "09:05:51 |     numthreads: 1\n",
            "09:05:51 |     nus: [0.7]\n",
            "09:05:51 |     optimizer: mem_eff_adam\n",
            "09:05:51 |     outfile: \n",
            "09:05:51 |     output_scaling: 1.0\n",
            "09:05:51 |     override: \"{'model_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model'}\"\n",
            "09:05:51 |     parlai_home: /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI\n",
            "09:05:51 |     person_tokens: False\n",
            "09:05:51 |     port: 61337\n",
            "09:05:51 |     pred_loss_coeff: 8.0\n",
            "09:05:51 |     rank: 0\n",
            "09:05:51 |     rank_candidates: False\n",
            "09:05:51 |     relu_dropout: 0.0\n",
            "09:05:51 |     remove_political_convos: False\n",
            "09:05:51 |     save_after_valid: True\n",
            "09:05:51 |     save_every_n_secs: -1\n",
            "09:05:51 |     save_format: conversations\n",
            "09:05:51 |     self_attn_loss_coeff: 0.6\n",
            "09:05:51 |     share_word_embeddings: True\n",
            "09:05:51 |     short_final_eval: False\n",
            "09:05:51 |     show_advanced_args: False\n",
            "09:05:51 |     single_turn: False\n",
            "09:05:51 |     skip_generation: True\n",
            "09:05:51 |     special_tok_lst: None\n",
            "09:05:51 |     split_lines: False\n",
            "09:05:51 |     starttime: Dec05_09-33\n",
            "09:05:51 |     task: blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues\n",
            "09:05:51 |     task_loss_coeff: 1.0\n",
            "09:05:51 |     teacher_model: /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model\n",
            "09:05:51 |     temperature: 1.0\n",
            "09:05:51 |     tensorboard_log: False\n",
            "09:05:51 |     tensorboard_logdir: None\n",
            "09:05:51 |     text_truncate: 128\n",
            "09:05:51 |     topk: 10\n",
            "09:05:51 |     topp: 0.9\n",
            "09:05:51 |     train_experiencer_only: False\n",
            "09:05:51 |     truncate: 128\n",
            "09:05:51 |     update_freq: 2\n",
            "09:05:51 |     use_reply: label\n",
            "09:05:51 |     validation_cutoff: 1.0\n",
            "09:05:51 |     validation_every_n_epochs: -1.0\n",
            "09:05:51 |     validation_every_n_secs: 900.0\n",
            "09:05:51 |     validation_max_exs: -1\n",
            "09:05:51 |     validation_metric: ppl\n",
            "09:05:51 |     validation_metric_mode: min\n",
            "09:05:51 |     validation_patience: 20\n",
            "09:05:51 |     validation_share_agent: False\n",
            "09:05:51 |     variant: prelayernorm\n",
            "09:05:51 |     verbose: False\n",
            "09:05:51 |     warmup_rate: 0.0001\n",
            "09:05:51 |     warmup_updates: 100\n",
            "09:05:51 |     weight_decay: None\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "09:05:52 | creating task(s): interactive\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e2b6b8f02e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Model files that begin with \"zoo:\" are special files distributed by the ParlAI team.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# They'll be automatically downloaded when you ask to use them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zoo:blender/blender_400Mdistill/model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/interactive.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/interactive.py\u001b[0m in \u001b[0;36minteractive\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Show some example dialogs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparley\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_total_parleys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# chat was reset with [DONE], [EXIT] or EOF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/tasks/interactive/worlds.py\u001b[0m in \u001b[0;36mparley\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/agents/local_human/local_human.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mreply_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter Your Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Import the Interactive script\n",
        "from parlai.scripts.interactive import Interactive\n",
        "\n",
        "# call it with particular args\n",
        "Interactive.main(\n",
        "    # the model_file is a filename path pointing to a particular model dump.\n",
        "    # Model files that begin with \"zoo:\" are special files distributed by the ParlAI team.\n",
        "    # They'll be automatically downloaded when you ask to use them.\n",
        "    model_file='zoo:blender/blender_400Mdistill/model'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRfDXAOxQX-8"
      },
      "source": [
        "# 2.Taking a look at some data\n",
        "\n",
        "You can take a full look in our [task list](https://parl.ai/docs/tasks.html).\n",
        "\n",
        "The black, unindented text is the _prompt_, while the blue text is the _label_. That is, the label is what we will be training the model to mimic.\n",
        "\n",
        "We can also ask to see fewer examples, and get them from the validation set instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a8ebMstQfWu",
        "outputId": "794fae2a-e8c4-4bd1-bdad-ece81ade8611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "08:14:15 | Opt:\n",
            "08:14:15 |     allow_missing_init_opts: False\n",
            "08:14:15 |     batchsize: 1\n",
            "08:14:15 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "08:14:15 |     datatype: train:ordered\n",
            "08:14:15 |     dict_class: None\n",
            "08:14:15 |     display_add_fields: \n",
            "08:14:15 |     download_path: None\n",
            "08:14:15 |     dynamic_batching: None\n",
            "08:14:15 |     hide_labels: False\n",
            "08:14:15 |     ignore_agent_reply: True\n",
            "08:14:15 |     image_cropsize: 224\n",
            "08:14:15 |     image_mode: raw\n",
            "08:14:15 |     image_size: 256\n",
            "08:14:15 |     init_model: None\n",
            "08:14:15 |     init_opt: None\n",
            "08:14:15 |     is_debug: False\n",
            "08:14:15 |     loglevel: info\n",
            "08:14:15 |     max_display_len: 1000\n",
            "08:14:15 |     model: None\n",
            "08:14:15 |     model_file: None\n",
            "08:14:15 |     multitask_weights: [1]\n",
            "08:14:15 |     mutators: None\n",
            "08:14:15 |     num_examples: 10\n",
            "08:14:15 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 10}\"\n",
            "08:14:15 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "08:14:15 |     remove_political_convos: False\n",
            "08:14:15 |     starttime: Mar02_08-14\n",
            "08:14:15 |     task: empathetic_dialogues\n",
            "08:14:15 |     train_experiencer_only: False\n",
            "08:14:15 |     verbose: False\n",
            "08:14:15 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered\n",
            "[building data: /usr/local/lib/python3.7/dist-packages/data/empatheticdialogues]\n",
            "08:14:15 | Downloading http://parl.ai/downloads/empatheticdialogues/empatheticdialogues.tar.gz to /usr/local/lib/python3.7/dist-packages/data/empatheticdialogues/empatheticdialogues.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading empatheticdialogues.tar.gz: 100%|██████████| 28.0M/28.0M [00:03<00:00, 7.76MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mI remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\u001b[0;0m\n",
            "   \u001b[1;94mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "\u001b[0mThis was a best friend. I miss her.\u001b[0;0m\n",
            "   \u001b[1;94mWhere has she gone?\u001b[0;0m\n",
            "\u001b[0mWe no longer talk.\u001b[0;0m\n",
            "   \u001b[1;94mOh was this something that happened because of an argument?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "   \u001b[1;94mThis was a best friend. I miss her.\u001b[0;0m\n",
            "\u001b[0mWhere has she gone?\u001b[0;0m\n",
            "   \u001b[1;94mWe no longer talk.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0m it feels like hitting to blank wall when i see the darkness\u001b[0;0m\n",
            "   \u001b[1;94mOh ya? I don't really see how\u001b[0;0m\n",
            "\u001b[0mdont you feel so.. its a wonder \u001b[0;0m\n",
            "   \u001b[1;94mI do actually hit blank walls a lot of times but i get by\u001b[0;0m\n",
            "\u001b[0m i virtually thought so.. and i used to get sweatings\u001b[0;0m\n",
            "   \u001b[1;94mWait what are sweatings\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mOh ya? I don't really see how\u001b[0;0m\n",
            "   \u001b[1;94mdont you feel so.. its a wonder \u001b[0;0m\n",
            "\u001b[0mI do actually hit blank walls a lot of times but i get by\u001b[0;0m\n",
            "   \u001b[1;94m i virtually thought so.. and i used to get sweatings\u001b[0;0m\n",
            "08:14:21 | loaded 39057 episodes with a total of 64636 examples\n"
          ]
        }
      ],
      "source": [
        "# The display_data script is used to show the contents of a particular task.\n",
        "# By default, we show the train\n",
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=10)\n",
        "\n",
        "# DisplayData.main(task='empathetic_dialogues', num_examples=3, datatype='valid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpIjlOegR-tX"
      },
      "source": [
        "On the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.display_data --task empathetic_dialogues\n",
        "```\n",
        "or a bit shorter\n",
        "```\n",
        "python -m parlai.scripts.display_data -t empathetic_dialogues\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsHr3FTnSP90"
      },
      "source": [
        "# 3.Training model from scratch\n",
        "\n",
        "Well it's one thing looking at data, but what if we want to train our own model (from scratch)? \n",
        "\n",
        "Let's train a very simple seq2seq LSTM with attention, to respond to empathetic dialogues.\n",
        "\n",
        "To get some extra performance, we'll initialize using GloVe embeddings, but we will cap the training time to 2 minutes for this tutorial. It won't perform very well, but that's okay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK66Exf3SmCK",
        "outputId": "5e37191f-eb75-4a5b-e76a-d635cb5c66a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14:32:18 | building dictionary first...\n",
            "14:32:18 | Opt:\n",
            "14:32:18 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "14:32:18 |     adam_eps: 1e-08\n",
            "14:32:18 |     add_p1_after_newln: False\n",
            "14:32:18 |     aggregate_micro: False\n",
            "14:32:18 |     allow_missing_init_opts: False\n",
            "14:32:18 |     attention: dot\n",
            "14:32:18 |     attention_length: 48\n",
            "14:32:18 |     attention_time: post\n",
            "14:32:18 |     batchsize: 1\n",
            "14:32:18 |     beam_block_full_context: True\n",
            "14:32:18 |     beam_block_list_filename: None\n",
            "14:32:18 |     beam_block_ngram: -1\n",
            "14:32:18 |     beam_context_block_ngram: -1\n",
            "14:32:18 |     beam_delay: 30\n",
            "14:32:18 |     beam_length_penalty: 0.65\n",
            "14:32:18 |     beam_min_length: 1\n",
            "14:32:18 |     beam_size: 1\n",
            "14:32:18 |     betas: '(0.9, 0.999)'\n",
            "14:32:18 |     bidirectional: False\n",
            "14:32:18 |     bpe_add_prefix_space: None\n",
            "14:32:18 |     bpe_debug: False\n",
            "14:32:18 |     bpe_dropout: None\n",
            "14:32:18 |     bpe_merge: None\n",
            "14:32:18 |     bpe_vocab: None\n",
            "14:32:18 |     compute_tokenized_bleu: False\n",
            "14:32:18 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "14:32:18 |     datatype: train\n",
            "14:32:18 |     decoder: same\n",
            "14:32:18 |     delimiter: '\\n'\n",
            "14:32:18 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "14:32:18 |     dict_endtoken: __end__\n",
            "14:32:18 |     dict_file: /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model.dict\n",
            "14:32:18 |     dict_include_test: False\n",
            "14:32:18 |     dict_include_valid: False\n",
            "14:32:18 |     dict_initpath: None\n",
            "14:32:18 |     dict_language: english\n",
            "14:32:18 |     dict_loaded: False\n",
            "14:32:18 |     dict_lower: False\n",
            "14:32:18 |     dict_max_ngram_size: -1\n",
            "14:32:18 |     dict_maxexs: -1\n",
            "14:32:18 |     dict_maxtokens: -1\n",
            "14:32:18 |     dict_minfreq: 0\n",
            "14:32:18 |     dict_nulltoken: __null__\n",
            "14:32:18 |     dict_starttoken: __start__\n",
            "14:32:18 |     dict_textfields: text,labels\n",
            "14:32:18 |     dict_tokenizer: re\n",
            "14:32:18 |     dict_unktoken: __unk__\n",
            "14:32:18 |     display_examples: False\n",
            "14:32:18 |     download_path: None\n",
            "14:32:18 |     dropout: 0.1\n",
            "14:32:18 |     dynamic_batching: None\n",
            "14:32:18 |     embedding_projection: random\n",
            "14:32:18 |     embedding_type: random\n",
            "14:32:18 |     embeddingsize: 128\n",
            "14:32:18 |     eval_batchsize: None\n",
            "14:32:18 |     eval_dynamic_batching: None\n",
            "14:32:18 |     evaltask: None\n",
            "14:32:18 |     final_extra_opt: \n",
            "14:32:18 |     force_fp16_tokens: False\n",
            "14:32:18 |     fp16: False\n",
            "14:32:18 |     fp16_impl: safe\n",
            "14:32:18 |     gpu: -1\n",
            "14:32:18 |     gradient_clip: 0.1\n",
            "14:32:18 |     hiddensize: 128\n",
            "14:32:18 |     hide_labels: False\n",
            "14:32:18 |     history_add_global_end_token: None\n",
            "14:32:18 |     history_reversed: False\n",
            "14:32:18 |     history_size: -1\n",
            "14:32:18 |     image_cropsize: 224\n",
            "14:32:18 |     image_mode: no_image_model\n",
            "14:32:18 |     image_size: 256\n",
            "14:32:18 |     inference: greedy\n",
            "14:32:18 |     init_model: None\n",
            "14:32:18 |     init_opt: None\n",
            "14:32:18 |     input_dropout: 0.0\n",
            "14:32:18 |     interactive_mode: False\n",
            "14:32:18 |     invsqrt_lr_decay_gamma: -1\n",
            "14:32:18 |     is_debug: False\n",
            "14:32:18 |     label_truncate: None\n",
            "14:32:18 |     learningrate: 1\n",
            "14:32:18 |     load_from_checkpoint: True\n",
            "14:32:18 |     log_every_n_secs: -1\n",
            "14:32:18 |     log_every_n_steps: 50\n",
            "14:32:18 |     loglevel: info\n",
            "14:32:18 |     lookuptable: all\n",
            "14:32:18 |     lr_scheduler: reduceonplateau\n",
            "14:32:18 |     lr_scheduler_decay: 0.5\n",
            "14:32:18 |     lr_scheduler_patience: 3\n",
            "14:32:18 |     max_train_steps: -1\n",
            "14:32:18 |     max_train_time: -1\n",
            "14:32:18 |     metrics: default\n",
            "14:32:18 |     model: seq2seq\n",
            "14:32:18 |     model_file: /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model\n",
            "14:32:18 |     momentum: 0\n",
            "14:32:18 |     multitask_weights: [1]\n",
            "14:32:18 |     mutators: None\n",
            "14:32:18 |     nesterov: True\n",
            "14:32:18 |     no_cuda: False\n",
            "14:32:18 |     num_epochs: 1.0\n",
            "14:32:18 |     num_workers: 0\n",
            "14:32:18 |     numlayers: 2\n",
            "14:32:18 |     numsoftmax: 1\n",
            "14:32:18 |     nus: (0.7,)\n",
            "14:32:18 |     optimizer: sgd\n",
            "14:32:18 |     override: \"{'model_file': '/content/drive/MyDrive/colabs/blender-models/from_scratch_model/model', 'task': 'empathetic_dialogues', 'num_epochs': 1.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
            "14:32:18 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "14:32:18 |     person_tokens: False\n",
            "14:32:18 |     rank_candidates: False\n",
            "14:32:18 |     remove_political_convos: False\n",
            "14:32:18 |     rnn_class: lstm\n",
            "14:32:18 |     save_after_valid: False\n",
            "14:32:18 |     save_every_n_secs: -1\n",
            "14:32:18 |     short_final_eval: False\n",
            "14:32:18 |     skip_generation: False\n",
            "14:32:18 |     special_tok_lst: None\n",
            "14:32:18 |     split_lines: False\n",
            "14:32:18 |     starttime: Mar01_14-32\n",
            "14:32:18 |     task: empathetic_dialogues\n",
            "14:32:18 |     temperature: 1.0\n",
            "14:32:18 |     tensorboard_log: False\n",
            "14:32:18 |     tensorboard_logdir: None\n",
            "14:32:18 |     text_truncate: None\n",
            "14:32:18 |     topk: 10\n",
            "14:32:18 |     topp: 0.9\n",
            "14:32:18 |     train_experiencer_only: False\n",
            "14:32:18 |     truncate: 64\n",
            "14:32:18 |     update_freq: 1\n",
            "14:32:18 |     use_reply: label\n",
            "14:32:18 |     validation_cutoff: 1.0\n",
            "14:32:18 |     validation_every_n_epochs: -1\n",
            "14:32:18 |     validation_every_n_secs: -1\n",
            "14:32:18 |     validation_every_n_steps: -1\n",
            "14:32:18 |     validation_max_exs: -1\n",
            "14:32:18 |     validation_metric: accuracy\n",
            "14:32:18 |     validation_metric_mode: None\n",
            "14:32:18 |     validation_patience: 10\n",
            "14:32:18 |     validation_share_agent: False\n",
            "14:32:18 |     verbose: False\n",
            "14:32:18 |     wandb_entity: None\n",
            "14:32:18 |     wandb_log: False\n",
            "14:32:18 |     wandb_name: None\n",
            "14:32:18 |     wandb_project: None\n",
            "14:32:18 |     warmup_rate: 0.0001\n",
            "14:32:18 |     warmup_updates: -1\n",
            "14:32:18 |     weight_decay: None\n",
            "14:32:19 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered:stream\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building dictionary: 100%|██████████| 64.6k/64.6k [00:08<00:00, 7.28kex/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14:32:30 | Saving dictionary to /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model.dict\n",
            "14:32:30 | dictionary built with 22419 tokens in 0.0s\n",
            "14:32:30 | No model with opt yet at: /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model(.opt)\n",
            "14:32:30 | Using CUDA\n",
            "14:32:30 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model.dict\n",
            "14:32:30 | num words = 22419\n",
            "14:32:30 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "14:32:30 | Opt:\n",
            "14:32:30 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "14:32:30 |     adam_eps: 1e-08\n",
            "14:32:30 |     add_p1_after_newln: False\n",
            "14:32:30 |     aggregate_micro: False\n",
            "14:32:30 |     allow_missing_init_opts: False\n",
            "14:32:30 |     attention: dot\n",
            "14:32:30 |     attention_length: 48\n",
            "14:32:30 |     attention_time: post\n",
            "14:32:30 |     batchsize: 16\n",
            "14:32:30 |     beam_block_full_context: True\n",
            "14:32:30 |     beam_block_list_filename: None\n",
            "14:32:30 |     beam_block_ngram: -1\n",
            "14:32:30 |     beam_context_block_ngram: -1\n",
            "14:32:30 |     beam_delay: 30\n",
            "14:32:30 |     beam_length_penalty: 0.65\n",
            "14:32:30 |     beam_min_length: 1\n",
            "14:32:30 |     beam_size: 1\n",
            "14:32:30 |     betas: '(0.9, 0.999)'\n",
            "14:32:30 |     bidirectional: False\n",
            "14:32:30 |     bpe_add_prefix_space: None\n",
            "14:32:30 |     bpe_debug: False\n",
            "14:32:30 |     bpe_dropout: None\n",
            "14:32:30 |     bpe_merge: None\n",
            "14:32:30 |     bpe_vocab: None\n",
            "14:32:30 |     compute_tokenized_bleu: False\n",
            "14:32:30 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "14:32:30 |     datatype: train\n",
            "14:32:30 |     decoder: same\n",
            "14:32:30 |     delimiter: '\\n'\n",
            "14:32:30 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "14:32:30 |     dict_endtoken: __end__\n",
            "14:32:30 |     dict_file: /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model.dict\n",
            "14:32:30 |     dict_include_test: False\n",
            "14:32:30 |     dict_include_valid: False\n",
            "14:32:30 |     dict_initpath: None\n",
            "14:32:30 |     dict_language: english\n",
            "14:32:30 |     dict_loaded: True\n",
            "14:32:30 |     dict_lower: False\n",
            "14:32:30 |     dict_max_ngram_size: -1\n",
            "14:32:30 |     dict_maxexs: -1\n",
            "14:32:30 |     dict_maxtokens: -1\n",
            "14:32:30 |     dict_minfreq: 0\n",
            "14:32:30 |     dict_nulltoken: __null__\n",
            "14:32:30 |     dict_starttoken: __start__\n",
            "14:32:30 |     dict_textfields: text,labels\n",
            "14:32:30 |     dict_tokenizer: re\n",
            "14:32:30 |     dict_unktoken: __unk__\n",
            "14:32:30 |     display_examples: False\n",
            "14:32:30 |     download_path: None\n",
            "14:32:30 |     dropout: 0.1\n",
            "14:32:30 |     dynamic_batching: None\n",
            "14:32:30 |     embedding_projection: random\n",
            "14:32:30 |     embedding_type: random\n",
            "14:32:30 |     embeddingsize: 128\n",
            "14:32:30 |     eval_batchsize: None\n",
            "14:32:30 |     eval_dynamic_batching: None\n",
            "14:32:30 |     evaltask: None\n",
            "14:32:30 |     final_extra_opt: \n",
            "14:32:30 |     force_fp16_tokens: False\n",
            "14:32:30 |     fp16: False\n",
            "14:32:30 |     fp16_impl: safe\n",
            "14:32:30 |     gpu: -1\n",
            "14:32:30 |     gradient_clip: 0.1\n",
            "14:32:30 |     hiddensize: 128\n",
            "14:32:30 |     hide_labels: False\n",
            "14:32:30 |     history_add_global_end_token: None\n",
            "14:32:30 |     history_reversed: False\n",
            "14:32:30 |     history_size: -1\n",
            "14:32:30 |     image_cropsize: 224\n",
            "14:32:30 |     image_mode: raw\n",
            "14:32:30 |     image_size: 256\n",
            "14:32:30 |     inference: greedy\n",
            "14:32:30 |     init_model: None\n",
            "14:32:30 |     init_opt: None\n",
            "14:32:30 |     input_dropout: 0.0\n",
            "14:32:30 |     interactive_mode: False\n",
            "14:32:30 |     invsqrt_lr_decay_gamma: -1\n",
            "14:32:30 |     is_debug: False\n",
            "14:32:30 |     label_truncate: None\n",
            "14:32:30 |     learningrate: 1\n",
            "14:32:30 |     load_from_checkpoint: True\n",
            "14:32:30 |     log_every_n_secs: -1\n",
            "14:32:30 |     log_every_n_steps: 50\n",
            "14:32:30 |     loglevel: info\n",
            "14:32:30 |     lookuptable: all\n",
            "14:32:30 |     lr_scheduler: reduceonplateau\n",
            "14:32:30 |     lr_scheduler_decay: 0.5\n",
            "14:32:30 |     lr_scheduler_patience: 3\n",
            "14:32:30 |     max_train_steps: -1\n",
            "14:32:30 |     max_train_time: -1\n",
            "14:32:30 |     metrics: default\n",
            "14:32:30 |     model: seq2seq\n",
            "14:32:30 |     model_file: /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model\n",
            "14:32:30 |     momentum: 0\n",
            "14:32:30 |     multitask_weights: [1]\n",
            "14:32:30 |     mutators: None\n",
            "14:32:30 |     nesterov: True\n",
            "14:32:30 |     no_cuda: False\n",
            "14:32:30 |     num_epochs: 1.0\n",
            "14:32:30 |     num_workers: 0\n",
            "14:32:30 |     numlayers: 2\n",
            "14:32:30 |     numsoftmax: 1\n",
            "14:32:30 |     nus: (0.7,)\n",
            "14:32:30 |     optimizer: sgd\n",
            "14:32:30 |     override: \"{'model_file': '/content/drive/MyDrive/colabs/blender-models/from_scratch_model/model', 'task': 'empathetic_dialogues', 'num_epochs': 1.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
            "14:32:30 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "14:32:30 |     person_tokens: False\n",
            "14:32:30 |     rank_candidates: False\n",
            "14:32:30 |     remove_political_convos: False\n",
            "14:32:30 |     rnn_class: lstm\n",
            "14:32:30 |     save_after_valid: False\n",
            "14:32:30 |     save_every_n_secs: -1\n",
            "14:32:30 |     short_final_eval: False\n",
            "14:32:30 |     skip_generation: False\n",
            "14:32:30 |     special_tok_lst: None\n",
            "14:32:30 |     split_lines: False\n",
            "14:32:30 |     starttime: Mar01_14-32\n",
            "14:32:30 |     task: empathetic_dialogues\n",
            "14:32:30 |     temperature: 1.0\n",
            "14:32:30 |     tensorboard_log: False\n",
            "14:32:30 |     tensorboard_logdir: None\n",
            "14:32:30 |     text_truncate: None\n",
            "14:32:30 |     topk: 10\n",
            "14:32:30 |     topp: 0.9\n",
            "14:32:30 |     train_experiencer_only: False\n",
            "14:32:30 |     truncate: 64\n",
            "14:32:30 |     update_freq: 1\n",
            "14:32:30 |     use_reply: label\n",
            "14:32:30 |     validation_cutoff: 1.0\n",
            "14:32:30 |     validation_every_n_epochs: -1\n",
            "14:32:30 |     validation_every_n_secs: -1\n",
            "14:32:30 |     validation_every_n_steps: -1\n",
            "14:32:30 |     validation_max_exs: -1\n",
            "14:32:30 |     validation_metric: accuracy\n",
            "14:32:30 |     validation_metric_mode: None\n",
            "14:32:30 |     validation_patience: 10\n",
            "14:32:30 |     validation_share_agent: False\n",
            "14:32:30 |     verbose: False\n",
            "14:32:30 |     wandb_entity: None\n",
            "14:32:30 |     wandb_log: False\n",
            "14:32:30 |     wandb_name: None\n",
            "14:32:30 |     wandb_project: None\n",
            "14:32:30 |     warmup_rate: 0.0001\n",
            "14:32:30 |     warmup_updates: -1\n",
            "14:32:30 |     weight_decay: None\n",
            "14:32:31 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train\n",
            "14:32:32 | training...\n",
            "14:32:46 | time:14s total_exs:800 total_steps:50 epochs:0.01 time_left:1108s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   33.08     1 470.8  1700   .1100      3.659 57.78  800  .9791    .8200 16.38 9.295   1 260.1 939.3  .00875      .1225 10881   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .0639         0                   50 730.9 2640 3.612\n",
            "\n",
            "14:32:59 | time:26s total_exs:1600 total_steps:100 epochs:0.02 time_left:1039s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.01     1 448.6  1803   .0950      1.971 64.32  800  1.013    .1528 16.75  8.89   1 267.3  1075  .00375     .04375 7262   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "       .09434         0                  100 715.9 2878 4.021\n",
            "\n",
            "14:33:09 | time:37s total_exs:2400 total_steps:150 epochs:0.04 time_left:958s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.33     1 463.9  2199   .1062      2.336 75.83  800  1.054    .1528 16.88 8.674   1 268.9  1274  .00375     .07125 5848   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .1102         0                  150 732.8 3473 4.74\n",
            "\n",
            "14:33:19 | time:47s total_exs:3200 total_steps:200 epochs:0.05 time_left:897s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.46     1   447  2294  .09125      2.521 82.11  800  1.082    .1528 16.64 8.473   1 264.8  1359  .00625     .08875 4785   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1141         0                  200 711.8 3653 5.133\n",
            "\n",
            "14:33:29 | time:56s total_exs:4000 total_steps:250 epochs:0.06 time_left:855s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.91     1 465.8  2424   .1000        2.8 83.26  800  1.118    .1528 16.27 8.373   1 258.8  1347   .0050      .0900 4330   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1269         0                  250 724.7 3771 5.205\n",
            "\n",
            "14:33:40 | time:67s total_exs:4800 total_steps:300 epochs:0.07 time_left:837s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.96     1 445.8  2070  .09125      2.099  74.3  800  1.152    .1528 16.78 8.195   1   266  1235   .0100      .1487 3623   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1344         0                  300 711.8 3306 4.645\n",
            "\n",
            "14:33:50 | time:78s total_exs:5600 total_steps:350 epochs:0.09 time_left:821s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.55     1 443.9  2088  .09625      2.809 75.28  800  1.143    .1529 16.65 8.137   1 266.2  1253   .0025     .01375 3420   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1392         0                  350 710.1 3341 4.706\n",
            "\n",
            "14:34:00 | time:88s total_exs:6400 total_steps:400 epochs:0.10 time_left:796s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   32.07     1 466.4  2413   .1037      2.915 82.77  800   1.18    .1528 16.76 7.936   1 265.6  1374   .0050      .1638 2797   \n",
            "    token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "        .1474         0                  400  732 3787 5.174\n",
            "\n",
            "14:34:09 | time:96s total_exs:7200 total_steps:450 epochs:0.11 time_left:768s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.12     1 438.6  2526   .0800      1.708 92.16  800  1.201    .1515 15.61 7.801   1 249.7  1438       0          0 2444   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1531         0                  450 688.3 3965 5.761\n",
            "\n",
            "14:34:18 | time:106s total_exs:8000 total_steps:500 epochs:0.12 time_left:748s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "    29.3     1 440.9  2334   .0850      1.749 84.69  800  1.198    .1528 16.44 7.771   1 262.9  1391   .0025     .00875 2370   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1527         0                  500 703.7 3725 5.294\n",
            "\n",
            "14:34:27 | time:115s total_exs:8800 total_steps:550 epochs:0.14 time_left:728s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.55     1   450  2513   .1050      2.417 89.34  800  1.204    .1528 16.22 7.692   1 259.1  1447   .0025     .02375 2192   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1528         0                  550 709.1 3960 5.585\n",
            "\n",
            "14:34:37 | time:124s total_exs:9600 total_steps:600 epochs:0.15 time_left:714s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.64     1 460.3  2359   .1062      2.875    82  800  1.207    .1529  16.9   7.6   1 269.1  1379  .00625     .07875 1998   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1600         0                  600 729.4 3738 5.125\n",
            "\n",
            "14:34:47 | time:134s total_exs:10400 total_steps:650 epochs:0.16 time_left:700s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.53     1 461.7  2364   .1150      2.679 81.93  800  1.228    .1528 16.99 7.586   1 269.1  1378   .0100      .1737 1970   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1563         0                  650 730.8 3742 5.122\n",
            "\n",
            "14:34:56 | time:143s total_exs:11200 total_steps:700 epochs:0.17 time_left:684s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.98     1 450.3  2481  .08875      1.841 88.17  800  1.267    .1528 15.94 7.438   1 254.3  1401  .00375     .04625 1699   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1656         0                  700 704.6 3883 5.512\n",
            "\n",
            "14:35:05 | time:152s total_exs:12000 total_steps:750 epochs:0.19 time_left:669s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.04     1 456.2  2509  .08875      2.529 87.99  800  1.265    .1528 16.16 7.371   1 257.3  1415  .00625      .0825 1589   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .1659         0                  750 713.5 3924  5.5\n",
            "\n",
            "14:35:14 | time:162s total_exs:12800 total_steps:800 epochs:0.20 time_left:656s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.33     1   454  2408   .1050      2.955 84.87  800  1.267    .1528 16.11 7.355   1 257.6  1367  .00375      .0100 1563   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1708         0                  800 711.6 3775 5.305\n",
            "\n",
            "14:35:24 | time:172s total_exs:13600 total_steps:850 epochs:0.21 time_left:645s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.25     1 463.5  2361  .08875      2.285 81.52  800  1.279    .1528 16.73 7.291   1 265.5  1353  .00625      .1425 1467   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1694         0                  850 728.9 3714 5.096\n",
            "\n",
            "14:35:34 | time:181s total_exs:14400 total_steps:900 epochs:0.22 time_left:633s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.73     1 443.9  2332  .08625      1.981 84.04  800  1.257    .1527  16.2 7.281   1 258.9  1360  .00375      .0200 1453   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1710         0                  900 702.8 3692 5.254\n",
            "\n",
            "14:35:43 | time:191s total_exs:15200 total_steps:950 epochs:0.24 time_left:621s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "    30.7     1 455.8  2411   .1050      2.208 84.62  800  1.303    .1529 16.53 7.238   1 263.4  1393   .0075      .0650 1391   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .1660         0                  950 719.2 3804 5.29\n",
            "\n",
            "14:35:53 | time:200s total_exs:16000 total_steps:1000 epochs:0.25 time_left:609s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.86     1 452.5  2400   .0900      2.575 84.84  800  1.266    .1529 16.27 7.105   1 259.3  1375  .00625      .0575 1218   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1748         0                 1000 711.9 3775 5.303\n",
            "\n",
            "14:36:02 | time:210s total_exs:16800 total_steps:1050 epochs:0.26 time_left:597s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.39     1 461.1  2493   .1075      2.574 86.49  800  1.307    .1529 16.15 7.118   1 258.1  1395   .0025      .0150 1233   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1747         0                 1050 719.2 3888 5.407\n",
            "\n",
            "14:36:11 | time:219s total_exs:17600 total_steps:1100 epochs:0.27 time_left:585s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.11     1 444.4  2384  .07625      1.334 85.84  800  1.306    .1528  15.7 7.128   1 250.1  1342  .00375      .0675 1246   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1702         0                 1100 694.5 3726 5.366\n",
            "\n",
            "14:36:20 | time:228s total_exs:18400 total_steps:1150 epochs:0.28 time_left:572s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.27     1 452.7  2554   .1013       2.97 90.27  800  1.301    .1529 15.91 6.983   1 253.4  1429  .00375     .07875 1078   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1855         0                 1150 706.1 3984 5.643\n",
            "\n",
            "14:36:30 | time:238s total_exs:19200 total_steps:1200 epochs:0.30 time_left:562s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   32.08     1   470  2377   .1163      2.701 80.91  800  1.314    .1529 16.52 7.015   1 262.9  1329  .00875     .09375 1113   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1754         0                 1200 732.9 3706 5.058\n",
            "\n",
            "14:36:40 | time:247s total_exs:20000 total_steps:1250 epochs:0.31 time_left:552s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.85     1 441.1  2263   .1062      2.276  82.1  800  1.262    .1529 16.82 7.013   1 268.5  1378   .0025     .03375 1111   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1795         0                 1250 709.6 3641 5.132\n",
            "\n",
            "14:36:49 | time:257s total_exs:20800 total_steps:1300 epochs:0.32 time_left:541s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.45     1 445.8  2395  .08875       1.59 85.96  800   1.29    .1516 16.62 6.943   1 265.9  1429       0          0 1035   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1864         0                 1300 711.8 3824 5.374\n",
            "\n",
            "14:36:59 | time:267s total_exs:21600 total_steps:1350 epochs:0.33 time_left:531s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.07     1 466.5  2396   .1113      1.915 82.18  800  1.284    .1529 16.68 6.889   1 266.4  1368  .00375     .03125 981.2   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1812         0                 1350 732.9 3765 5.138\n",
            "\n",
            "14:37:09 | time:276s total_exs:22400 total_steps:1400 epochs:0.35 time_left:521s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.23     1 454.4  2353   .1050      2.825 82.84  800  1.334    .1529 16.34 6.899   1 257.3  1332  .00875      .2575 991.4   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1819         0                 1400 711.7 3685 5.178\n",
            "\n",
            "14:37:18 | time:285s total_exs:23200 total_steps:1450 epochs:0.36 time_left:510s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   29.79     1 449.4  2462  .08875      1.702 87.67  800  1.317    .1529  16.1 6.789   1 257.5  1411  .00125     .01125 888.3   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .1906         0                 1450 706.8 3873 5.48\n",
            "\n",
            "14:37:27 | time:294s total_exs:24000 total_steps:1500 epochs:0.37 time_left:498s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.89     1 461.5  2569   .0975      3.049 89.05  800  1.333    .1529 15.83 6.822   1 253.1  1408   .0025      .0100 917.7   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1874         0                 1500 714.6 3977 5.566\n",
            "\n",
            "14:37:36 | time:304s total_exs:24800 total_steps:1550 epochs:0.38 time_left:488s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.44     1 455.6  2384  .09375      1.969 83.72  800  1.332    .1528 16.07 6.803   1 255.9  1339  .00375     .07125 900.7   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1874         0                 1550 711.5 3723 5.233\n",
            "\n",
            "14:37:46 | time:314s total_exs:25600 total_steps:1600 epochs:0.40 time_left:478s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.88     1 468.2  2399   .1062      2.612 81.98  800  1.309    .1529 16.65 6.848   1 265.5  1361   .0050     .05375 942.2   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1872         0                 1600 733.8 3760 5.125\n",
            "\n",
            "14:37:55 | time:323s total_exs:26400 total_steps:1650 epochs:0.41 time_left:467s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.13     1 449.7  2572   .0950       2.03 91.51  800  1.343    .1529 15.57 6.734   1 248.4  1421   .0025     .04625 840.8   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .1881         0                 1650 698.1 3993 5.72\n",
            "\n",
            "14:38:04 | time:332s total_exs:27200 total_steps:1700 epochs:0.42 time_left:457s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.76     1 461.1  2435   .1075      2.944 84.51  800  1.336    .1529 16.15 6.713   1   257  1357  .00375     .09375  823   \n",
            "    token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "        .1911         0                 1700  718 3792 5.283\n",
            "\n",
            "14:38:14 | time:342s total_exs:28000 total_steps:1750 epochs:0.43 time_left:447s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.89     1 450.3  2359   .1025      2.746 83.81  800  1.315    .1529 16.39  6.78   1 261.3  1369  .00375      .0550 880.4   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .1842         0                 1750 711.6 3728 5.24\n",
            "\n",
            "14:38:23 | time:351s total_exs:28800 total_steps:1800 epochs:0.45 time_left:436s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   29.84     1 438.8  2417   .0825      2.413  88.1  800  1.346    .1528 15.72 6.658   1 251.1  1382  .00375      .0250 778.9   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1964         0                 1800 689.9 3799 5.508\n",
            "\n",
            "14:38:32 | time:360s total_exs:29600 total_steps:1850 epochs:0.46 time_left:426s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   29.78     1 446.5  2422  .08375       1.87 86.77  800  1.357    .1529 16.37 6.695   1   261  1416   .0050      .0525 808.5   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1932         0                 1850 707.5 3837 5.424\n",
            "\n",
            "14:38:41 | time:369s total_exs:30400 total_steps:1900 epochs:0.47 time_left:416s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   29.23     1 438.5  2448   .0850      1.821 89.31  800  1.339    .1528 15.91 6.679   1 252.8  1411   .0025      .1125 795.8   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1974         0                 1900 691.3 3859 5.583\n",
            "\n",
            "14:38:51 | time:378s total_exs:31200 total_steps:1950 epochs:0.48 time_left:406s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.29     1 461.5  2451   .1075      2.451 84.99  800  1.334    .1530 15.88 6.676   1 253.8  1348   .0025     .01625  793   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1928         0                 1950 715.2 3799 5.313\n",
            "\n",
            "14:39:00 | time:388s total_exs:32000 total_steps:2000 epochs:0.50 time_left:395s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.63     1 450.4  2468  .09125       2.48 87.67  800  1.347    .1529 15.67 6.629   1 249.4  1366  .00375      .0875 756.4   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .1921         0                 2000 699.8 3834 5.48\n",
            "\n",
            "14:39:09 | time:397s total_exs:32800 total_steps:2050 epochs:0.51 time_left:385s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.45     1 465.5  2533   .1037      2.353 87.07  800  1.349    .1529 15.99 6.516   1   254  1382  .00625      .1225 675.9   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2040         0                 2050 719.5 3916 5.443\n",
            "\n",
            "14:39:19 | time:406s total_exs:33600 total_steps:2100 epochs:0.52 time_left:375s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   29.54     1 441.3  2308   .0875      1.962 83.67  800  1.339    .1529 16.17 6.624   1 257.7  1347   .0050     .06625 752.6   \n",
            "    token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "        .1908         0                 2100  699 3655 5.23\n",
            "\n",
            "14:39:28 | time:416s total_exs:34400 total_steps:2150 epochs:0.53 time_left:365s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.12     1 465.3  2519   .1050       2.04 86.61  800   1.35    .1522 16.05 6.621   1 256.8  1390       0          0 750.6   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1946         0                 2150 722.2 3910 5.415\n",
            "\n",
            "14:39:37 | time:425s total_exs:35200 total_steps:2200 epochs:0.54 time_left:355s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.45     1 452.7  2485   .0850      2.151 87.82  800  1.356    .1530 15.86 6.576   1 253.5  1391  .00375      .0125 717.8   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .1997         0                 2200 706.2 3876 5.49\n",
            "\n",
            "14:39:46 | time:434s total_exs:36000 total_steps:2250 epochs:0.56 time_left:345s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.71     1   444  2474  .09375      1.962 89.15  800  1.349    .1528 16.05 6.588   1 256.6  1430  .00125     .01125  726   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2014         0                 2250 700.6 3904 5.573\n",
            "\n",
            "14:39:56 | time:443s total_exs:36800 total_steps:2300 epochs:0.57 time_left:335s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "    30.2     1   445  2356   .0925      2.381 84.71  800   1.36    .1529 15.88 6.593   1 253.4  1342   .0050     .03625 730.1   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1991         0                 2300 698.5 3698 5.295\n",
            "\n",
            "14:40:05 | time:453s total_exs:37600 total_steps:2350 epochs:0.58 time_left:325s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.58     1 454.8  2414   .0875      2.158 84.93  800  1.326    .1530 16.38 6.526   1 261.1  1386  .00375     .06125  683   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2001         0                 2350 715.9 3800 5.309\n",
            "\n",
            "14:40:15 | time:462s total_exs:38400 total_steps:2400 epochs:0.59 time_left:316s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.59     1   452  2404   .1062      2.337  85.1  800  1.349    .1530 16.25 6.458   1 258.7  1376   .0050      .0800 637.6   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .2045         0                 2400 710.7 3780 5.32\n",
            "\n",
            "14:40:24 | time:472s total_exs:39200 total_steps:2450 epochs:0.61 time_left:306s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   29.57     1 438.2  2328  .09375      2.181    85  800  1.363    .1530 16.07 6.537   1 255.7  1358   .0050      .0875 690.1   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2043         0                 2450 693.9 3686 5.314\n",
            "\n",
            "14:40:33 | time:481s total_exs:40000 total_steps:2500 epochs:0.62 time_left:296s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.49     1 457.3  2472   .1037      2.906 86.48  800  1.356    .1529 16.35 6.506   1 260.3  1407  .00375     .08375 669.1   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2072         0                 2500 717.6 3879 5.407\n",
            "\n",
            "14:40:43 | time:490s total_exs:40800 total_steps:2550 epochs:0.63 time_left:286s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.55     1 458.2  2435   .0925      2.914 85.03  800  1.361    .1530 16.23 6.534   1 259.1  1377  .00375     .03375 688.4   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1935         0                 2550 717.3 3812 5.315\n",
            "\n",
            "14:40:52 | time:500s total_exs:41600 total_steps:2600 epochs:0.64 time_left:277s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.39     1 443.5  2369   .0775      1.674 85.46  800  1.359    .1530 16.15  6.41   1 256.7  1371  .00875      .1062  608   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1967         0                 2600 700.2 3740 5.344\n",
            "\n",
            "14:41:02 | time:509s total_exs:42400 total_steps:2650 epochs:0.66 time_left:267s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "    30.6     1 456.5  2356   .1062      2.072 82.57  800  1.353    .1530 16.27 6.449   1 259.2  1338   .0050      .0650 631.8   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2029         0                 2650 715.7 3693 5.162\n",
            "\n",
            "14:41:11 | time:519s total_exs:43200 total_steps:2700 epochs:0.67 time_left:257s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.21     1 449.2  2444  .08375      2.139 87.07  800  1.376    .1529 16.23 6.375   1   259  1409   .0025     .04875 586.8   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2078         0                 2700 708.2 3854 5.443\n",
            "\n",
            "14:41:20 | time:527s total_exs:44000 total_steps:2750 epochs:0.68 time_left:247s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   29.69     1 442.4  2499  .08875      2.039 90.37  800  1.393    .1530 15.69 6.364   1 250.5  1415   .0025      .0300 580.7   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .2084         0                 2750 692.8 3914 5.65\n",
            "\n",
            "14:41:29 | time:537s total_exs:44800 total_steps:2800 epochs:0.69 time_left:238s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.43     1 448.4  2395   .1013      2.402 85.45  800  1.346    .1528 16.44 6.323   1 262.6  1403  .00125      .0300 557.1   \n",
            "    token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "        .2091         0                 2800  711 3797 5.342\n",
            "\n",
            "14:41:38 | time:546s total_exs:45600 total_steps:2850 epochs:0.71 time_left:228s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.53     1 461.3  2603   .1087      2.697 90.28  800  1.374    .1530 16.17 6.413   1 258.6  1459  .00125     .00625 609.6   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2034         0                 2850 719.8 4062 5.644\n",
            "\n",
            "14:41:48 | time:555s total_exs:46400 total_steps:2900 epochs:0.72 time_left:218s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.81     1 454.6  2341   .1138      2.397 82.37  800  1.333    .1530 16.88 6.419   1 269.9  1389   .0025     .00875 613.4   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .2088         0                 2900 724.5 3731 5.15\n",
            "\n",
            "14:41:58 | time:566s total_exs:47200 total_steps:2950 epochs:0.73 time_left:209s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "      30     1 445.6  2190   .0975      2.149 78.64  800  1.339    .1530 17.26 6.339   1 271.6  1335   .0100      .2838 566.1   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2121         0                 2950 717.3 3525 4.916\n",
            "\n",
            "14:42:07 | time:574s total_exs:48000 total_steps:3000 epochs:0.74 time_left:199s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.62     1   453  2633  .09125      2.311 92.99  800  1.379    .1529 15.67  6.31   1 250.5  1456   .0025      .0150 550.1   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2111         0                 3000 703.5 4089 5.813\n",
            "\n",
            "14:42:17 | time:584s total_exs:48800 total_steps:3050 epochs:0.75 time_left:190s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.15     1 441.8  2205   .1013      2.531 79.83  800  1.329    .1530 16.74 6.444   1   267  1332   .0075     .05375 628.7   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2024         0                 3050 708.8 3537 4.991\n",
            "\n",
            "14:42:27 | time:594s total_exs:49600 total_steps:3100 epochs:0.77 time_left:180s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   32.25     1 462.7  2290   .1100      3.333 79.17  800  1.351    .1530 16.56 6.351   1   262  1297   .0075      .1850 573.1   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2056         0                 3100 724.7 3586 4.949\n",
            "\n",
            "14:42:36 | time:604s total_exs:50400 total_steps:3150 epochs:0.78 time_left:171s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.12     1 452.5  2361   .1050      1.846 83.48  800   1.36    .1530 16.81 6.297   1 267.7  1397   .0075      .0775 543.1   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2072         0                 3150 720.1 3758 5.219\n",
            "\n",
            "14:42:46 | time:613s total_exs:51200 total_steps:3200 epochs:0.79 time_left:161s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   29.41     1 442.2  2431  .09125      1.772 87.96  800  1.369    .1530 15.83 6.301   1 252.4  1388   .0025      .0500 545.2   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2111         0                 3200 694.6 3819 5.499\n",
            "\n",
            "14:42:55 | time:623s total_exs:52000 total_steps:3250 epochs:0.80 time_left:151s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.97     1 465.4  2472   .1150      2.885 84.98  800   1.37    .1530 15.97 6.304   1   255  1355   .0050     .03375  547   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2110         0                 3250 720.4 3826 5.312\n",
            "\n",
            "14:43:05 | time:632s total_exs:52800 total_steps:3300 epochs:0.82 time_left:142s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "    29.7     1 440.2  2288  .08375       2.18 83.15  800  1.395    .1530 15.79 6.319   1 252.4  1312   .0025     .01625 555.3   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2004         0                 3300 692.7 3600 5.198\n",
            "\n",
            "14:43:14 | time:641s total_exs:53600 total_steps:3350 epochs:0.83 time_left:132s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   32.32     1 468.5  2675   .1125      3.038 91.36  800  1.399    .1529 15.71 6.214   1 250.8  1432   .0025      .0400 499.5   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2086         0                 3350 719.3 4107 5.711\n",
            "\n",
            "14:43:23 | time:651s total_exs:54400 total_steps:3400 epochs:0.84 time_left:122s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.52     1 456.4  2363   .1062      1.992 82.84  800  1.356    .1530 16.53 6.202   1 263.7  1365  .00375     .04625 493.5   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2137         0                 3400 720.1 3729 5.178\n",
            "\n",
            "14:43:33 | time:661s total_exs:55200 total_steps:3450 epochs:0.85 time_left:113s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.64     1 472.8  2299   .1050      2.095 77.82  800  1.365    .1531 16.64 6.207   1 265.8  1293   .0025     .03375 496.1   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2084         0                 3450 738.5 3592 4.865\n",
            "\n",
            "14:43:43 | time:671s total_exs:56000 total_steps:3500 epochs:0.87 time_left:103s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.45     1 461.3  2329   .1125       2.62 80.77  800  1.344    .1531 16.68 6.291   1 265.8  1342   .0050     .06625 539.7   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2107         0                 3500 727.2 3671 5.049\n",
            "\n",
            "14:43:53 | time:681s total_exs:56800 total_steps:3550 epochs:0.88 time_left:94s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.42     1   456  2401  .09125      1.916 84.24  800  1.353    .1531 16.52 6.207   1 263.5  1387  .00375      .0500 496.1   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2142         0                 3550 719.5 3788 5.266\n",
            "\n",
            "14:44:02 | time:690s total_exs:57600 total_steps:3600 epochs:0.89 time_left:84s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.93     1 457.9  2420   .0925      2.304 84.54  800  1.362    .1531 16.19 6.125   1 258.3  1365   .0050      .0475 457.3   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2164         0                 3600 716.3 3785 5.284\n",
            "\n",
            "14:44:12 | time:700s total_exs:58400 total_steps:3650 epochs:0.90 time_left:75s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.43     1 463.8  2395   .1000      2.446 82.62  800  1.354    .1531 16.62 6.168   1   264  1363  .00625      .1187 477.4   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2104         0                 3650 727.9 3759 5.165\n",
            "\n",
            "14:44:21 | time:709s total_exs:59200 total_steps:3700 epochs:0.92 time_left:65s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   29.83     1 444.4  2470  .09125      2.054 88.93  800  1.379    .1531    16 6.225   1 254.9  1417  .00375      .0750 505.2   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2137         0                 3700 699.3 3887 5.559\n",
            "\n",
            "14:44:31 | time:718s total_exs:60000 total_steps:3750 epochs:0.93 time_left:56s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.55     1 455.2  2351  .09125        2.1 82.64  800   1.37    .1531 16.35 6.186   1 260.8  1347  .00375     .05375 485.9   \n",
            "    token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "        .2168         0                 3750  716 3698 5.166\n",
            "\n",
            "14:44:40 | time:728s total_exs:60800 total_steps:3800 epochs:0.94 time_left:46s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.62     1 460.4  2425  .09625      1.844 84.29  800  1.338    .1530 16.84 6.206   1   268  1412   .0025     .09125 495.7   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2111         0                 3800 728.3 3837 5.269\n",
            "\n",
            "14:44:50 | time:738s total_exs:61600 total_steps:3850 epochs:0.95 time_left:36s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.99     1 464.1  2369   .1113      2.979 81.67  800  1.341    .1531 17.14 6.284   1 272.4  1391  .00625      .1087 536.1   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1995         0                 3850 736.6 3760 5.105\n",
            "\n",
            "14:44:59 | time:747s total_exs:62400 total_steps:3900 epochs:0.97 time_left:27s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.96     1 456.4  2479   .0975      2.439 86.92  800  1.369    .1531 16.13 6.186   1 257.1  1397  .00375     .05875 485.9   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2133         0                 3900 713.5 3876 5.434\n",
            "\n",
            "14:45:09 | time:756s total_exs:63200 total_steps:3950 epochs:0.98 time_left:17s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.07     1 449.7  2404   .0850      1.969 85.52  800  1.359    .1505 16.22 6.182   1 259.5  1387       0          0 483.8   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2143         0                 3950 709.2 3791 5.346\n",
            "\n",
            "14:45:18 | time:766s total_exs:64000 total_steps:4000 epochs:0.99 time_left:8s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.73     1 465.4  2453   .1013      2.641 84.34  800  1.372    .1531 16.68 6.091   1 266.1  1403   .0075      .0500 441.7   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2226         0                 4000 731.5 3856 5.272\n",
            "\n",
            "14:45:26 | time:774s total_exs:64640 total_steps:4040 epochs:1.00 time_left:0s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.61     1 448.7  2258   .1000      2.567 80.52  640  1.337    .1531 17.13 6.182   1 273.1  1374 .007812      .0625 483.7   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .2145         0                 4040 721.8 3633 5.034\n",
            "\n",
            "14:45:26 | num_epochs completed:1.0 time elapsed:773.8901369571686s\n",
            "14:45:26 | Using CUDA\n",
            "14:45:26 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model.dict\n",
            "14:45:27 | num words = 22419\n",
            "14:45:27 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "14:45:27 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model\n",
            "14:45:27 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "14:45:27 | running eval: valid\n",
            "14:47:38 | eval completed in 130.83s\n",
            "14:47:38 | \u001b[1mvalid:\n",
            "    accuracy  bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  \\\n",
            "           0 .002249 39.52 572.6  1576   .1682      3.592 43.87 5738 .1443    .1196 15.65 5.808   1 249.1 685.6 .002091   \n",
            "    ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
            "       .02457  333      .2456         0                 4040 821.7 2261\n",
            "\u001b[0m\n",
            "14:47:38 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: test\n",
            "14:47:39 | running eval: test\n",
            "14:49:39 | eval completed in 120.43s\n",
            "14:49:39 | \u001b[1mtest:\n",
            "    accuracy  bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  \\\n",
            "           0 .002115 42.71 604.5  1652   .1960      4.891 43.68 5259 .1431    .1196 15.85 5.827   1 252.6 690.2 .003613   \n",
            "    ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
            "       .04982 339.5      .2416         0                 4040 857.1 2342\n",
            "\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'accuracy': ExactMatchMetric(0),\n",
              "  'bleu-4': BleuMetric(0.002249),\n",
              "  'clen': AverageMetric(39.52),\n",
              "  'ctpb': GlobalAverageMetric(572.6),\n",
              "  'ctps': GlobalTimerMetric(1576),\n",
              "  'ctrunc': AverageMetric(0.1682),\n",
              "  'ctrunclen': AverageMetric(3.592),\n",
              "  'exps': GlobalTimerMetric(43.87),\n",
              "  'exs': SumMetric(5738),\n",
              "  'f1': F1Metric(0.1443),\n",
              "  'gpu_mem': GlobalAverageMetric(0.1196),\n",
              "  'llen': AverageMetric(15.65),\n",
              "  'loss': AverageMetric(5.808),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'ltpb': GlobalAverageMetric(249.1),\n",
              "  'ltps': GlobalTimerMetric(685.6),\n",
              "  'ltrunc': AverageMetric(0.002091),\n",
              "  'ltrunclen': AverageMetric(0.02457),\n",
              "  'ppl': PPLMetric(333),\n",
              "  'token_acc': AverageMetric(0.2456),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(4040),\n",
              "  'tpb': GlobalAverageMetric(821.7),\n",
              "  'tps': GlobalTimerMetric(2261)},\n",
              " {'accuracy': ExactMatchMetric(0),\n",
              "  'bleu-4': BleuMetric(0.002115),\n",
              "  'clen': AverageMetric(42.71),\n",
              "  'ctpb': GlobalAverageMetric(604.5),\n",
              "  'ctps': GlobalTimerMetric(1652),\n",
              "  'ctrunc': AverageMetric(0.196),\n",
              "  'ctrunclen': AverageMetric(4.891),\n",
              "  'exps': GlobalTimerMetric(43.68),\n",
              "  'exs': SumMetric(5259),\n",
              "  'f1': F1Metric(0.1431),\n",
              "  'gpu_mem': GlobalAverageMetric(0.1196),\n",
              "  'llen': AverageMetric(15.85),\n",
              "  'loss': AverageMetric(5.827),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'ltpb': GlobalAverageMetric(252.6),\n",
              "  'ltps': GlobalTimerMetric(690.2),\n",
              "  'ltrunc': AverageMetric(0.003613),\n",
              "  'ltrunclen': AverageMetric(0.04982),\n",
              "  'ppl': PPLMetric(339.5),\n",
              "  'token_acc': AverageMetric(0.2416),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(4040),\n",
              "  'tpb': GlobalAverageMetric(857.1),\n",
              "  'tps': GlobalTimerMetric(2342)})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we'll save it in the \"from_scratch_model\" directory\n",
        "from_scratch_model_path = f'{mydrive_path}from_scratch_model'\n",
        "# !rm -rf $from_scratch_model_path\n",
        "!mkdir -p $from_scratch_model_path\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    # we MUST provide a filename\n",
        "    model_file= f'{from_scratch_model_path}/model',\n",
        "    # train on empathetic dialogues\n",
        "    task='empathetic_dialogues',\n",
        "    # limit training time to 2 minutes, and a batchsize of 16\n",
        "    # max_train_time=2 * 60,\n",
        "    num_epochs =1,\n",
        "    batchsize=16,\n",
        "    \n",
        "    # we specify the model type as seq2seq\n",
        "    model='seq2seq', \n",
        "    # some hyperparamter choices. We'll use attention. We could use pretrained\n",
        "    # embeddings too, with embedding_type='fasttext', but they take a long\n",
        "    # time to download.\n",
        "    attention='dot',\n",
        "    # tie the word embeddings of the encoder/decoder/softmax.\n",
        "    lookuptable='all',\n",
        "    # truncate text and labels at 64 tokens, for memory and time savings\n",
        "    truncate=64,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqv6EM2lTVaY"
      },
      "source": [
        "# 4.Looking at model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh4HxeOxTdKw",
        "outputId": "50a8130b-8a01-4be8-c208-53cbce7b58e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14:50:47 | Using CUDA\n",
            "14:50:47 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model.dict\n",
            "14:50:47 | num words = 22419\n",
            "14:50:47 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "14:50:47 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model\n",
            "14:50:47 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "14:50:47 | Opt:\n",
            "14:50:47 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "14:50:47 |     adam_eps: 1e-08\n",
            "14:50:47 |     add_p1_after_newln: False\n",
            "14:50:47 |     aggregate_micro: False\n",
            "14:50:47 |     allow_missing_init_opts: False\n",
            "14:50:47 |     attention: dot\n",
            "14:50:47 |     attention_length: 48\n",
            "14:50:47 |     attention_time: post\n",
            "14:50:47 |     batchsize: 16\n",
            "14:50:47 |     beam_block_full_context: True\n",
            "14:50:47 |     beam_block_list_filename: None\n",
            "14:50:47 |     beam_block_ngram: -1\n",
            "14:50:47 |     beam_context_block_ngram: -1\n",
            "14:50:47 |     beam_delay: 30\n",
            "14:50:47 |     beam_length_penalty: 0.65\n",
            "14:50:47 |     beam_min_length: 1\n",
            "14:50:47 |     beam_size: 1\n",
            "14:50:47 |     betas: '[0.9, 0.999]'\n",
            "14:50:47 |     bidirectional: False\n",
            "14:50:47 |     bpe_add_prefix_space: None\n",
            "14:50:47 |     bpe_debug: False\n",
            "14:50:47 |     bpe_dropout: None\n",
            "14:50:47 |     bpe_merge: None\n",
            "14:50:47 |     bpe_vocab: None\n",
            "14:50:47 |     compute_tokenized_bleu: False\n",
            "14:50:47 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "14:50:47 |     datatype: train\n",
            "14:50:47 |     decoder: same\n",
            "14:50:47 |     delimiter: '\\n'\n",
            "14:50:47 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "14:50:47 |     dict_endtoken: __end__\n",
            "14:50:47 |     dict_file: /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model.dict\n",
            "14:50:47 |     dict_include_test: False\n",
            "14:50:47 |     dict_include_valid: False\n",
            "14:50:47 |     dict_initpath: None\n",
            "14:50:47 |     dict_language: english\n",
            "14:50:47 |     dict_loaded: True\n",
            "14:50:47 |     dict_lower: False\n",
            "14:50:47 |     dict_max_ngram_size: -1\n",
            "14:50:47 |     dict_maxexs: -1\n",
            "14:50:47 |     dict_maxtokens: -1\n",
            "14:50:47 |     dict_minfreq: 0\n",
            "14:50:47 |     dict_nulltoken: __null__\n",
            "14:50:47 |     dict_starttoken: __start__\n",
            "14:50:47 |     dict_textfields: text,labels\n",
            "14:50:47 |     dict_tokenizer: re\n",
            "14:50:47 |     dict_unktoken: __unk__\n",
            "14:50:47 |     display_add_fields: \n",
            "14:50:47 |     display_examples: False\n",
            "14:50:47 |     download_path: None\n",
            "14:50:47 |     dropout: 0.1\n",
            "14:50:47 |     dynamic_batching: None\n",
            "14:50:47 |     embedding_projection: random\n",
            "14:50:47 |     embedding_type: random\n",
            "14:50:47 |     embeddingsize: 128\n",
            "14:50:47 |     eval_batchsize: None\n",
            "14:50:47 |     eval_dynamic_batching: None\n",
            "14:50:47 |     evaltask: None\n",
            "14:50:47 |     final_extra_opt: \n",
            "14:50:47 |     force_fp16_tokens: False\n",
            "14:50:47 |     fp16: False\n",
            "14:50:47 |     fp16_impl: safe\n",
            "14:50:47 |     gpu: -1\n",
            "14:50:47 |     gradient_clip: 0.1\n",
            "14:50:47 |     hiddensize: 128\n",
            "14:50:47 |     hide_labels: False\n",
            "14:50:47 |     history_add_global_end_token: None\n",
            "14:50:47 |     history_reversed: False\n",
            "14:50:47 |     history_size: -1\n",
            "14:50:47 |     image_cropsize: 224\n",
            "14:50:47 |     image_mode: raw\n",
            "14:50:47 |     image_size: 256\n",
            "14:50:47 |     inference: greedy\n",
            "14:50:47 |     init_model: None\n",
            "14:50:47 |     init_opt: None\n",
            "14:50:47 |     input_dropout: 0.0\n",
            "14:50:47 |     interactive_mode: False\n",
            "14:50:47 |     invsqrt_lr_decay_gamma: -1\n",
            "14:50:47 |     is_debug: False\n",
            "14:50:47 |     label_truncate: None\n",
            "14:50:47 |     learningrate: 1\n",
            "14:50:47 |     log_every_n_secs: -1\n",
            "14:50:47 |     log_every_n_steps: 50\n",
            "14:50:47 |     loglevel: info\n",
            "14:50:47 |     lookuptable: all\n",
            "14:50:47 |     lr_scheduler: reduceonplateau\n",
            "14:50:47 |     lr_scheduler_decay: 0.5\n",
            "14:50:47 |     lr_scheduler_patience: 3\n",
            "14:50:47 |     max_train_steps: -1\n",
            "14:50:47 |     max_train_time: -1\n",
            "14:50:47 |     metrics: default\n",
            "14:50:47 |     model: seq2seq\n",
            "14:50:47 |     model_file: /content/drive/MyDrive/colabs/blender-models/from_scratch_model/model\n",
            "14:50:47 |     momentum: 0\n",
            "14:50:47 |     multitask_weights: [1]\n",
            "14:50:47 |     mutators: None\n",
            "14:50:47 |     nesterov: True\n",
            "14:50:47 |     no_cuda: False\n",
            "14:50:47 |     num_epochs: 1.0\n",
            "14:50:47 |     num_examples: 8\n",
            "14:50:47 |     num_workers: 0\n",
            "14:50:47 |     numlayers: 2\n",
            "14:50:47 |     numsoftmax: 1\n",
            "14:50:47 |     nus: [0.7]\n",
            "14:50:47 |     optimizer: sgd\n",
            "14:50:47 |     override: \"{'task': 'empathetic_dialogues', 'model_file': '/content/drive/MyDrive/colabs/blender-models/from_scratch_model/model', 'num_examples': '8'}\"\n",
            "14:50:47 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "14:50:47 |     person_tokens: False\n",
            "14:50:47 |     rank_candidates: False\n",
            "14:50:47 |     remove_political_convos: False\n",
            "14:50:47 |     rnn_class: lstm\n",
            "14:50:47 |     save_after_valid: False\n",
            "14:50:47 |     save_every_n_secs: -1\n",
            "14:50:47 |     short_final_eval: False\n",
            "14:50:47 |     skip_generation: False\n",
            "14:50:47 |     special_tok_lst: None\n",
            "14:50:47 |     split_lines: False\n",
            "14:50:47 |     starttime: Mar01_14-32\n",
            "14:50:47 |     task: empathetic_dialogues\n",
            "14:50:47 |     temperature: 1.0\n",
            "14:50:47 |     tensorboard_log: False\n",
            "14:50:47 |     tensorboard_logdir: None\n",
            "14:50:47 |     text_truncate: None\n",
            "14:50:47 |     topk: 10\n",
            "14:50:47 |     topp: 0.9\n",
            "14:50:47 |     train_experiencer_only: False\n",
            "14:50:47 |     truncate: 64\n",
            "14:50:47 |     update_freq: 1\n",
            "14:50:47 |     use_reply: label\n",
            "14:50:47 |     validation_cutoff: 1.0\n",
            "14:50:47 |     validation_every_n_epochs: -1\n",
            "14:50:47 |     validation_every_n_secs: -1\n",
            "14:50:47 |     validation_every_n_steps: -1\n",
            "14:50:47 |     validation_max_exs: -1\n",
            "14:50:47 |     validation_metric: accuracy\n",
            "14:50:47 |     validation_metric_mode: None\n",
            "14:50:47 |     validation_patience: 10\n",
            "14:50:47 |     validation_share_agent: False\n",
            "14:50:47 |     verbose: False\n",
            "14:50:47 |     wandb_entity: None\n",
            "14:50:47 |     wandb_log: False\n",
            "14:50:47 |     wandb_name: None\n",
            "14:50:47 |     wandb_project: None\n",
            "14:50:47 |     warmup_rate: 0.0001\n",
            "14:50:47 |     warmup_updates: -1\n",
            "14:50:47 |     weight_decay: None\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: That ' s a great . I ' m sure you ' re a lot of the time .\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: That ' s a great . I ' m sure you ' re a lot of the time .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mA few weeks ago, I was walking through my hallway, minding my own business, when all of a sudden a hand reached out from under a table and grabbed my ankle. I was so suprised. I thought i was got. Turns out, it was my son. \u001b[0;0m\n",
            "\u001b[1;94m    labels: That's funny, hope he didn't give you a heart attack.\u001b[0;0m\n",
            "\u001b[0;95m     model: That ' s a great . I ' m sure you ' re it .\u001b[0;0m\n",
            "\u001b[0mI may have let out a scream that will have him question my manhood for the rest of our lives, lol. \u001b[0;0m\n",
            "\u001b[1;94m    labels: I would probably scream also.\u001b[0;0m\n",
            "\u001b[0;95m     model: That ' s a great . I ' m sure you ' re a lot of the time .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mI'm overly excited because will be flying outside the country for the first time tomorrow.\".  Please enter here...\u001b[0;0m\n",
            "\u001b[1;94m    labels: Wow! That sounds amazing. Where are you going? \u001b[0;0m\n",
            "\u001b[0;95m     model: I ' m sure you ' re a lot of the I ' m sure you ' re a lot of the time .\u001b[0;0m\n",
            "\u001b[0mTraveling to South Africa then to Ghana. Also my first time visiting Africa\u001b[0;0m\n",
            "\u001b[1;94m    labels: You are going to love it I am sure. Safe Travels! \u001b[0;0m\n",
            "\u001b[0;95m     model: I ' m sure you ' re a lot of the time .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mOne time, I was holding my son over my head, when all of a sudden, he threw up all over my face. I almost cried.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Thats horrible, hope he is ok.\u001b[0;0m\n",
            "\u001b[0;95m     model: That ' s a great . I ' m sure you ' re it .\u001b[0;0m\n",
            "\u001b[0myeah, he had just drank a bottle, and I was moving him around like an airplane. In retrospect. I deserved it.\u001b[0;0m\n",
            "\u001b[1;94m    labels: I see, glad he is ok.\u001b[0;0m\n",
            "\u001b[0;95m     model: I ' m sorry to hear that . I ' m sure you ' re a lot of the time .\u001b[0;0m\n"
          ]
        }
      ],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file= f'{from_scratch_model_path}/model',\n",
        "    num_examples=8,\n",
        "    # skip_generation=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsge5PsOYfKl"
      },
      "source": [
        "if the model isn't giving any responses, that can be because that --skip-generation was set to true to speed up training. \n",
        "\n",
        "you need to turn that back off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZsF0FAjYxRh"
      },
      "source": [
        "On the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.display_model --task empathetic_dialogues --model-file zoo:tutorial_transformer_generator/model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niAnX_a1UyTz"
      },
      "source": [
        "# 5.Training a model (use pre-trained models)\n",
        "\n",
        "The easiest way to improve it is to *initialize* using a *pretrained model*, utilizing *transfer learning*. Let's use the one from the interactive session at the beginning of the chat!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pirjhQjTUzh2",
        "outputId": "ebac0faa-cac4-4b22-8169-1aa6489f4590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:43:07 | building dictionary first...\n",
            "09:43:07 | \u001b[33mOverriding opt[\"init_model\"] to zoo:tutorial_transformer_generator/model (previously: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint)\u001b[0m\n",
            "09:43:07 | \u001b[33mOverriding opt[\"dict_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict (previously: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint.dict)\u001b[0m\n",
            "09:43:07 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "09:43:07 | \u001b[33mOverriding opt[\"save_after_valid\"] to True (previously: False)\u001b[0m\n",
            "09:43:07 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "09:43:07 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--save-after-valid False --force-fp16-tokens False --optimizer mem_eff_adam\u001b[0m\n",
            "09:43:07 | Using CUDA\n",
            "09:43:07 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint.dict\n",
            "09:43:07 | num words = 54944\n",
            "09:43:09 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "09:43:09 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint\n",
            "09:43:12 | Opt:\n",
            "09:43:12 |     activation: gelu\n",
            "09:43:12 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "09:43:12 |     adam_eps: 1e-08\n",
            "09:43:12 |     add_p1_after_newln: False\n",
            "09:43:12 |     aggregate_micro: False\n",
            "09:43:12 |     allow_missing_init_opts: False\n",
            "09:43:12 |     attention_dropout: 0.0\n",
            "09:43:12 |     batchsize: 12\n",
            "09:43:12 |     beam_block_full_context: True\n",
            "09:43:12 |     beam_block_list_filename: None\n",
            "09:43:12 |     beam_block_ngram: -1\n",
            "09:43:12 |     beam_context_block_ngram: -1\n",
            "09:43:12 |     beam_delay: 30\n",
            "09:43:12 |     beam_length_penalty: 0.65\n",
            "09:43:12 |     beam_min_length: 1\n",
            "09:43:12 |     beam_size: 1\n",
            "09:43:12 |     betas: '[0.9, 0.999]'\n",
            "09:43:12 |     bpe_add_prefix_space: None\n",
            "09:43:12 |     bpe_debug: False\n",
            "09:43:12 |     bpe_dropout: None\n",
            "09:43:12 |     bpe_merge: None\n",
            "09:43:12 |     bpe_vocab: None\n",
            "09:43:12 |     checkpoint_activations: False\n",
            "09:43:12 |     compute_tokenized_bleu: False\n",
            "09:43:12 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "09:43:12 |     datatype: train\n",
            "09:43:12 |     delimiter: '\\n'\n",
            "09:43:12 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "09:43:12 |     dict_endtoken: __end__\n",
            "09:43:12 |     dict_file: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint.dict\n",
            "09:43:12 |     dict_include_test: False\n",
            "09:43:12 |     dict_include_valid: False\n",
            "09:43:12 |     dict_initpath: None\n",
            "09:43:12 |     dict_language: english\n",
            "09:43:12 |     dict_loaded: True\n",
            "09:43:12 |     dict_lower: True\n",
            "09:43:12 |     dict_max_ngram_size: -1\n",
            "09:43:12 |     dict_maxexs: -1\n",
            "09:43:12 |     dict_maxtokens: -1\n",
            "09:43:12 |     dict_minfreq: 0\n",
            "09:43:12 |     dict_nulltoken: __null__\n",
            "09:43:12 |     dict_starttoken: __start__\n",
            "09:43:12 |     dict_textfields: text,labels\n",
            "09:43:12 |     dict_tokenizer: bpe\n",
            "09:43:12 |     dict_unktoken: __unk__\n",
            "09:43:12 |     display_examples: False\n",
            "09:43:12 |     download_path: None\n",
            "09:43:12 |     dropout: 0.0\n",
            "09:43:12 |     dynamic_batching: full\n",
            "09:43:12 |     embedding_projection: random\n",
            "09:43:12 |     embedding_size: 512\n",
            "09:43:12 |     embedding_type: random\n",
            "09:43:12 |     embeddings_scale: True\n",
            "09:43:12 |     eval_batchsize: None\n",
            "09:43:12 |     eval_dynamic_batching: None\n",
            "09:43:12 |     evaltask: None\n",
            "09:43:12 |     ffn_size: 2048\n",
            "09:43:12 |     final_extra_opt: \n",
            "09:43:12 |     force_fp16_tokens: True\n",
            "09:43:12 |     fp16: True\n",
            "09:43:12 |     fp16_impl: mem_efficient\n",
            "09:43:12 |     gpu: -1\n",
            "09:43:12 |     gradient_clip: 0.1\n",
            "09:43:12 |     hide_labels: False\n",
            "09:43:12 |     history_add_global_end_token: None\n",
            "09:43:12 |     history_reversed: False\n",
            "09:43:12 |     history_size: -1\n",
            "09:43:12 |     image_cropsize: 224\n",
            "09:43:12 |     image_mode: raw\n",
            "09:43:12 |     image_size: 256\n",
            "09:43:12 |     inference: greedy\n",
            "09:43:12 |     init_model: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint\n",
            "09:43:12 |     init_opt: None\n",
            "09:43:12 |     interactive_mode: False\n",
            "09:43:12 |     invsqrt_lr_decay_gamma: -1\n",
            "09:43:12 |     is_debug: False\n",
            "09:43:12 |     label_truncate: 128\n",
            "09:43:12 |     learn_positional_embeddings: True\n",
            "09:43:12 |     learningrate: 1e-05\n",
            "09:43:12 |     load_from_checkpoint: True\n",
            "09:43:12 |     log_every_n_secs: -1\n",
            "09:43:12 |     log_every_n_steps: 50\n",
            "09:43:12 |     loglevel: info\n",
            "09:43:12 |     lr_scheduler: reduceonplateau\n",
            "09:43:12 |     lr_scheduler_decay: 0.5\n",
            "09:43:12 |     lr_scheduler_patience: 3\n",
            "09:43:12 |     max_train_steps: -1\n",
            "09:43:12 |     max_train_time: -1\n",
            "09:43:12 |     metrics: default\n",
            "09:43:12 |     model: transformer/generator\n",
            "09:43:12 |     model_file: /content/drive/MyDrive/colabs/blender-models/pretrained/model\n",
            "09:43:12 |     model_parallel: False\n",
            "09:43:12 |     momentum: 0\n",
            "09:43:12 |     multitask_weights: [1]\n",
            "09:43:12 |     mutators: None\n",
            "09:43:12 |     n_decoder_layers: -1\n",
            "09:43:12 |     n_encoder_layers: -1\n",
            "09:43:12 |     n_heads: 16\n",
            "09:43:12 |     n_layers: 8\n",
            "09:43:12 |     n_positions: 512\n",
            "09:43:12 |     n_segments: 0\n",
            "09:43:12 |     nesterov: True\n",
            "09:43:12 |     no_cuda: False\n",
            "09:43:12 |     num_epochs: 3.0\n",
            "09:43:12 |     num_workers: 0\n",
            "09:43:12 |     nus: [0.7]\n",
            "09:43:12 |     optimizer: mem_eff_adam\n",
            "09:43:12 |     output_scaling: 1.0\n",
            "09:43:12 |     override: \"{'task': 'empathetic_dialogues', 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/colabs/blender-models/pretrained/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 100, 'validation_metric': 'ppl', 'validation_every_n_epochs': 0.25, 'num_epochs': 3.0, 'save_after_valid': True, 'verbose': True, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
            "09:43:12 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "09:43:12 |     person_tokens: False\n",
            "09:43:12 |     rank_candidates: False\n",
            "09:43:12 |     relu_dropout: 0.0\n",
            "09:43:12 |     remove_political_convos: False\n",
            "09:43:12 |     save_after_valid: True\n",
            "09:43:12 |     save_every_n_secs: 600.0\n",
            "09:43:12 |     share_word_embeddings: True\n",
            "09:43:12 |     short_final_eval: False\n",
            "09:43:12 |     skip_generation: True\n",
            "09:43:12 |     special_tok_lst: None\n",
            "09:43:12 |     split_lines: False\n",
            "09:43:12 |     starttime: Mar01_14-53\n",
            "09:43:12 |     task: empathetic_dialogues\n",
            "09:43:12 |     temperature: 1.0\n",
            "09:43:12 |     tensorboard_log: False\n",
            "09:43:12 |     tensorboard_logdir: None\n",
            "09:43:12 |     text_truncate: 512\n",
            "09:43:12 |     topk: 10\n",
            "09:43:12 |     topp: 0.9\n",
            "09:43:12 |     train_experiencer_only: False\n",
            "09:43:13 |     truncate: -1\n",
            "09:43:13 |     update_freq: 1\n",
            "09:43:13 |     use_reply: label\n",
            "09:43:13 |     validation_cutoff: 1.0\n",
            "09:43:13 |     validation_every_n_epochs: 0.25\n",
            "09:43:13 |     validation_every_n_secs: -1\n",
            "09:43:13 |     validation_every_n_steps: -1\n",
            "09:43:13 |     validation_max_exs: -1\n",
            "09:43:13 |     validation_metric: ppl\n",
            "09:43:13 |     validation_metric_mode: None\n",
            "09:43:13 |     validation_patience: 10\n",
            "09:43:13 |     validation_share_agent: False\n",
            "09:43:13 |     variant: xlm\n",
            "09:43:13 |     verbose: True\n",
            "09:43:13 |     wandb_entity: None\n",
            "09:43:13 |     wandb_log: False\n",
            "09:43:13 |     wandb_name: None\n",
            "09:43:13 |     wandb_project: None\n",
            "09:43:13 |     warmup_rate: 0.0001\n",
            "09:43:13 |     warmup_updates: 100\n",
            "09:43:13 |     weight_decay: None\n",
            "09:43:13 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train\n",
            "09:43:19 | training...\n",
            "09:43:45 | time:5536s total_exs:79492 total_steps:823 epochs:1.23 time_left:7968s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   26.71     1  2861  5551       0          0 207.8 5356             32768  4.191    .5764 15.74 2.614 9.9e-06  1686  3271   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.65      .4226  .0001867                  823 4547 8823 1.941\n",
            "\n",
            "09:43:54 | time:5544s total_exs:81032 total_steps:840 epochs:1.25 time_left:7723s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   33.23     1  3010  5978       0          0 179.9 1540             32768  4.102    .5310 17.92 2.645 9.9e-06  1623  3224   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 14.09      .4193   .001299                  840 4634 9202 1.987\n",
            "\n",
            "09:43:54 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "09:43:57 | running eval: valid\n",
            "09:44:10 | eval completed in 13.00s\n",
            "09:44:10 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3571 18332       0          0 453.2 5738    .1659 16.01 2.471 9.9e-06  1413  7256       0          0 11.83   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4408   .001046                  840 4984 25589\n",
            "\u001b[0m\n",
            "09:44:10 | \u001b[1;32mnew best ppl: 11.83 (previous best was 11.91)\u001b[0m\n",
            "09:44:10 | saving best valid model: /content/drive/MyDrive/colabs/blender-models/pretrained/model\n",
            "09:44:14 | saving model checkpoint: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint\n",
            "09:44:44 | time:5594s total_exs:86032 total_steps:890 epochs:1.33 time_left:7015s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   29.12     1  2912  5671       0          0 194.7 5000             32768  4.316    .5440 16.32 2.606 9.9e-06  1632  3178   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.55      .4240     .0002                  890 4545 8850 1.947\n",
            "\n",
            "09:45:11 | time:5621s total_exs:90768 total_steps:940 epochs:1.40 time_left:6387s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.74     1  3101  5765       0          0 176.1 4736             32768  4.048    .5704 16.61 2.621 9.9e-06  1573  2924   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.75      .4216  .0006334                  940 4674 8689 1.859\n",
            "\n",
            "09:45:36 | time:5646s total_exs:95672 total_steps:990 epochs:1.48 time_left:5798s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.96     1  3036  6091       0          0 196.8 4904             32768  4.173    .5688 16.32 2.611 9.9e-06  1601  3211   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.62      .4229  .0004078                  990 4637 9302 2.006\n",
            "\n",
            "09:45:45 | time:5656s total_exs:97280 total_steps:1008 epochs:1.51 time_left:5618s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.78     1  2750  5277       0          0 171.4 1608             32768   4.28    .5571  17.8 2.671 9.9e-06  1590  3051   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 14.46      .4171         0                 1008 4340 8328 1.92\n",
            "\n",
            "09:45:45 | running eval: valid\n",
            "09:45:59 | eval completed in 13.36s\n",
            "09:45:59 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3464 17799       0          0   440 5738    .1659 16.01 2.464 9.9e-06  1371  7045       0          0 11.75   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4421   .001046                 1008 4835 24844\n",
            "\u001b[0m\n",
            "09:45:59 | \u001b[1;32mnew best ppl: 11.75 (previous best was 11.83)\u001b[0m\n",
            "09:45:59 | saving best valid model: /content/drive/MyDrive/colabs/blender-models/pretrained/model\n",
            "09:46:04 | saving model checkpoint: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint\n",
            "09:46:35 | time:5706s total_exs:102140 total_steps:1058 epochs:1.58 time_left:5126s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.86     1  2999  5636       0          0 182.6 4860             32768   4.16    .5561 16.53 2.609 9.9e-06  1607  3019   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.58      .4249   .001029                 1058 4606 8654 1.879\n",
            "\n",
            "09:47:02 | time:5732s total_exs:106660 total_steps:1108 epochs:1.65 time_left:4689s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   33.15     1  2997  5678       0          0 171.3 4520             32768  4.155    .5562 16.94 2.603 9.9e-06  1531  2900   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .0004425     .00354 13.51      .4220   .001106                 1108 4528 8578 1.895\n",
            "\n",
            "09:47:27 | time:5758s total_exs:111428 total_steps:1158 epochs:1.72 time_left:4262s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.43     1  2997  5865       0          0 186.6 4768             32768  4.152    .5765 16.76 2.608 9.9e-06  1598  3128   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .0002097   .0002097 13.58      .4228  .0008389                 1158 4595 8994 1.957\n",
            "\n",
            "09:47:40 | time:5770s total_exs:113564 total_steps:1183 epochs:1.76 time_left:4082s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   34.95     1  2986  6095       0          0 174.4 2136             32768  4.392    .5391 17.15 2.613 9.9e-06  1465  2991   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.65      .4219  .0004682                 1183 4451 9086 2.042\n",
            "\n",
            "09:47:40 | running eval: valid\n",
            "09:47:53 | eval completed in 13.24s\n",
            "09:47:53 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3464 17956       0          0 443.9 5738    .1659 16.01 2.459 9.9e-06  1371  7107       0          0 11.69   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4430   .001046                 1183 4835 25063\n",
            "\u001b[0m\n",
            "09:47:53 | \u001b[1;32mnew best ppl: 11.69 (previous best was 11.75)\u001b[0m\n",
            "09:47:53 | saving best valid model: /content/drive/MyDrive/colabs/blender-models/pretrained/model\n",
            "09:47:57 | saving model checkpoint: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint\n",
            "09:48:27 | time:5818s total_exs:118252 total_steps:1233 epochs:1.83 time_left:3722s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   29.65     1  2780  5470       0          0 184.5 4688             32768  4.234    .5766 16.59 2.598 9.9e-06  1556  3061   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.44      .4268  .0006399                 1233 4336 8531 1.968\n",
            "\n",
            "09:48:54 | time:5845s total_exs:122944 total_steps:1283 epochs:1.90 time_left:3374s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.53     1  2959  5425       0          0 172.1 4692             32768  4.172    .5672 16.83   2.6 9.9e-06  1579  2895   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .0002131    .003836 13.47      .4238  .0002131                 1283 4538 8321 1.834\n",
            "\n",
            "09:49:19 | time:5869s total_exs:127480 total_steps:1333 epochs:1.97 time_left:3058s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.56     1  2863  5979       0          0 189.4 4536             32768  4.475    .5513 16.43 2.584 9.9e-06  1490  3112   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.24      .4253  .0006614                 1333 4353 9091 2.089\n",
            "\n",
            "09:49:31 | time:5881s total_exs:129724 total_steps:1357 epochs:2.01 time_left:2910s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.26     1  2830  5536       0          0 182.9 2244             32768  4.535    .5609 17.35 2.637 9.9e-06  1622  3173   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.98      .4208  .0008913                 1357 4451 8709 1.957\n",
            "\n",
            "09:49:31 | running eval: valid\n",
            "09:49:44 | eval completed in 13.26s\n",
            "09:49:44 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3517 17912       0          0 442.8 5738    .1659 16.01 2.454 9.9e-06  1392  7089       0          0 11.64   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4434   .001046                 1357 4909 25001\n",
            "\u001b[0m\n",
            "09:49:44 | \u001b[1;32mnew best ppl: 11.64 (previous best was 11.69)\u001b[0m\n",
            "09:49:44 | saving best valid model: /content/drive/MyDrive/colabs/blender-models/pretrained/model\n",
            "09:49:48 | saving model checkpoint: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint\n",
            "09:50:19 | time:5929s total_exs:134272 total_steps:1407 epochs:2.08 time_left:2634s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   33.41     1  3039  5774       0          0 172.8 4548             32768  4.287    .5418 16.71 2.595 9.9e-06  1520  2888   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 13.4      .4251  .0006596                 1407 4559 8662  1.9\n",
            "\n",
            "09:50:45 | time:5956s total_exs:139020 total_steps:1457 epochs:2.15 time_left:2351s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.19     1  2962  5683       0          0 182.2 4748             32768  4.343    .5689 16.31 2.573 9.9e-06  1549  2972   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.11      .4260  .0004212                 1457 4511 8655 1.919\n",
            "\n",
            "09:51:12 | time:5982s total_exs:143640 total_steps:1507 epochs:2.22 time_left:2093s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.73     1  2839  5404       0          0 175.9 4620             32768  4.051    .5560 17.17  2.59 9.9e-06  1586  3019   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.33      .4244  .0004329                 1507 4426 8424 1.904\n",
            "\n",
            "09:51:23 | time:5994s total_exs:145960 total_steps:1530 epochs:2.26 time_left:1969s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "    27.7     1  2794  5540       0          0   200 2320             32768   4.39    .5561 16.16 2.559 9.9e-06  1630  3233   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.92      .4285   .001293                 1530 4425 8772 1.983\n",
            "\n",
            "09:51:23 | running eval: valid\n",
            "09:51:37 | eval completed in 13.36s\n",
            "09:51:37 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3571 17786       0          0 439.7 5738    .1658 16.01  2.45 9.9e-06  1413  7040       0          0 11.59   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4438   .001046                 1530 4984 24826\n",
            "\u001b[0m\n",
            "09:51:37 | \u001b[1;32mnew best ppl: 11.59 (previous best was 11.64)\u001b[0m\n",
            "09:51:37 | saving best valid model: /content/drive/MyDrive/colabs/blender-models/pretrained/model\n",
            "09:51:42 | saving model checkpoint: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint\n",
            "09:52:14 | time:6045s total_exs:150148 total_steps:1580 epochs:2.32 time_left:1762s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   36.02     1  3017  5528       0          0 153.5 4188             32768  4.456    .5418 17.26 2.572 9.9e-06  1445  2647   \n",
            "     ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .0004776    .008596 13.1      .4258         0                 1580 4462 8175 1.832\n",
            "\n",
            "09:52:43 | time:6073s total_exs:154868 total_steps:1630 epochs:2.40 time_left:1531s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.77     1  2999  5260       0          0 165.5 4720             32768  4.094    .5418 17.11 2.554 9.9e-06  1615  2833   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.85      .4285  .0006356                 1630 4615 8093 1.754\n",
            "\n",
            "09:53:08 | time:6099s total_exs:159712 total_steps:1680 epochs:2.47 time_left:1306s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.42     1  3044  5968       0          0   190 4844             32768  4.472    .5672  16.5  2.57 9.9e-06  1599  3135   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.07      .4298  .0002064                 1680 4642 9104 1.961\n",
            "\n",
            "09:53:21 | time:6112s total_exs:162120 total_steps:1705 epochs:2.51 time_left:1198s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   29.49     1  2840  5465       0          0 185.3 2408             32768   4.26    .5459 16.48  2.55 9.9e-06  1587  3054   \n",
            "     ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .0008306    .003738 12.8      .4308  .0008306                 1705 4428 8519 1.924\n",
            "\n",
            "09:53:21 | running eval: valid\n",
            "09:53:34 | eval completed in 13.03s\n",
            "09:53:34 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3517 18225       0          0 450.5 5738    .1658 16.01 2.447 9.9e-06  1392  7213       0          0 11.55   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4448   .001046                 1705 4909 25438\n",
            "\u001b[0m\n",
            "09:53:34 | \u001b[1;32mnew best ppl: 11.55 (previous best was 11.59)\u001b[0m\n",
            "09:53:34 | saving best valid model: /content/drive/MyDrive/colabs/blender-models/pretrained/model\n",
            "09:53:39 | saving model checkpoint: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint\n",
            "09:54:11 | time:6161s total_exs:166840 total_steps:1755 epochs:2.58 time_left:1000s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.58     1  3075  5842       0          0 179.3 4720             32768  4.181    .5418 16.81 2.584 9.9e-06  1587  3015   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 13.25      .4255  .0004237                 1755 4662 8857  1.9\n",
            "\n",
            "09:54:21 | Overflow: setting loss scale to 16384.0\n",
            "09:54:34 | time:6185s total_exs:171480 total_steps:1805 epochs:2.65 time_left:809s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "    31.2 .9800  2895  6138       0          0 196.8 4640             22938  4.274    .5765 15.89  2.53 9.9e-06  1475  3126   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.55      .4323  .0002155                 1805 4369 9264 2.121\n",
            "\n",
            "09:54:59 | time:6209s total_exs:176216 total_steps:1855 epochs:2.73 time_left:623s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.54     1  2892  6029       0          0 197.5 4736             16384   4.47    .5688 16.62 2.561 9.9e-06  1574  3282   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.95      .4276  .0006334                 1855 4467 9311 2.085\n",
            "\n",
            "09:55:10 | time:6220s total_exs:178296 total_steps:1878 epochs:2.76 time_left:545s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.86     1  2790  5601       0          0 181.5 2080             16384  4.149    .5448 17.76 2.536 9.9e-06  1606  3224   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .0004808   .0004808 12.63      .4315   .001442                 1878 4397 8826 2.008\n",
            "\n",
            "09:55:10 | running eval: valid\n",
            "09:55:23 | eval completed in 13.30s\n",
            "09:55:23 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3464 17882       0          0 442.1 5738    .1658 16.01 2.444 9.9e-06  1371  7078       0          0 11.52   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4453   .001046                 1878 4835 24960\n",
            "\u001b[0m\n",
            "09:55:23 | \u001b[1;32mnew best ppl: 11.52 (previous best was 11.55)\u001b[0m\n",
            "09:55:23 | saving best valid model: /content/drive/MyDrive/colabs/blender-models/pretrained/model\n",
            "09:55:29 | saving model checkpoint: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint\n",
            "09:56:00 | time:6270s total_exs:182716 total_steps:1928 epochs:2.83 time_left:384s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.35     1  2860  5358       0          0 165.6 4420             16384  4.346    .5688  17.4 2.569 9.9e-06  1538  2882   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.05      .4271   .000905                 1928 4398 8240 1.874\n",
            "\n",
            "09:56:26 | time:6296s total_exs:187564 total_steps:1978 epochs:2.90 time_left:213s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   28.46     1  2760  5343       0          0 187.7 4848             16384  4.069    .5688 16.66 2.564 9.9e-06  1616  3128   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.99      .4268  .0004125                 1978 4376 8472 1.936\n",
            "\n",
            "09:56:52 | time:6322s total_exs:192212 total_steps:2028 epochs:2.97 time_left:56s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.71     1  2855  5413       0          0 176.3 4648             16384  4.093    .5493 16.61 2.539 9.9e-06  1544  2927   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.67      .4299   .001291                 2028 4399 8340 1.896\n",
            "\n",
            "09:57:02 | time:6333s total_exs:193980 total_steps:2049 epochs:3.00 time_left:0s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   33.85     1  2850  5730       0          0 169.3 1768             16384  4.391    .5561  16.6 2.551 9.9e-06  1396  2806   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .001131     .02036 12.83      .4279  .0005656                 2049 4246 8536 2.011\n",
            "\n",
            "09:57:02 | num_epochs completed:3.0 time elapsed:6332.8246467113495s\n",
            "09:57:03 | \u001b[33mOverriding opt[\"init_model\"] to zoo:tutorial_transformer_generator/model (previously: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint)\u001b[0m\n",
            "09:57:03 | \u001b[33mOverriding opt[\"dict_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict (previously: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint.dict)\u001b[0m\n",
            "09:57:03 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "09:57:03 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "09:57:03 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--optimizer mem_eff_adam\u001b[0m\n",
            "09:57:03 | Using CUDA\n",
            "09:57:03 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint.dict\n",
            "09:57:03 | num words = 54944\n",
            "09:57:04 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "09:57:04 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint\n",
            "09:57:07 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "09:57:10 | running eval: valid\n",
            "09:57:23 | eval completed in 13.00s\n",
            "09:57:23 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3413 18929       0          0   468 5738    .1545 16.01 2.444 9.9e-06  1351  7492       0          0 11.52   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4455   .001046                 1878 4764 26422\n",
            "\u001b[0m\n",
            "09:57:23 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: test\n",
            "09:57:25 | running eval: test\n",
            "09:57:38 | eval completed in 12.58s\n",
            "09:57:38 | \u001b[1mtest:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   43.69  3647 18813       0          0 430.6 5259    .1544 16.23 2.466 9.9e-06  1355  6989       0          0 11.78   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4421  .0007606                 1878 5002 25802\n",
            "\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'clen': AverageMetric(40.45),\n",
              "  'ctpb': GlobalAverageMetric(3413),\n",
              "  'ctps': GlobalTimerMetric(1.893e+04),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'exps': GlobalTimerMetric(468),\n",
              "  'exs': SumMetric(5738),\n",
              "  'gpu_mem': GlobalAverageMetric(0.1545),\n",
              "  'llen': AverageMetric(16.01),\n",
              "  'loss': AverageMetric(2.444),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'ltpb': GlobalAverageMetric(1351),\n",
              "  'ltps': GlobalTimerMetric(7492),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'ppl': PPLMetric(11.52),\n",
              "  'token_acc': AverageMetric(0.4455),\n",
              "  'token_em': AverageMetric(0.001046),\n",
              "  'total_train_updates': GlobalFixedMetric(1878),\n",
              "  'tpb': GlobalAverageMetric(4764),\n",
              "  'tps': GlobalTimerMetric(2.642e+04)},\n",
              " {'clen': AverageMetric(43.69),\n",
              "  'ctpb': GlobalAverageMetric(3647),\n",
              "  'ctps': GlobalTimerMetric(1.881e+04),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'exps': GlobalTimerMetric(430.6),\n",
              "  'exs': SumMetric(5259),\n",
              "  'gpu_mem': GlobalAverageMetric(0.1544),\n",
              "  'llen': AverageMetric(16.23),\n",
              "  'loss': AverageMetric(2.466),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'ltpb': GlobalAverageMetric(1355),\n",
              "  'ltps': GlobalTimerMetric(6989),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'ppl': PPLMetric(11.78),\n",
              "  'token_acc': AverageMetric(0.4421),\n",
              "  'token_em': AverageMetric(0.0007606),\n",
              "  'total_train_updates': GlobalFixedMetric(1878),\n",
              "  'tpb': GlobalAverageMetric(5002),\n",
              "  'tps': GlobalTimerMetric(2.58e+04)})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrained_model_path = f'{mydrive_path}pretrained'\n",
        "# !rm -rf $pretrained_model_path\n",
        "!mkdir -p $pretrained_model_path\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    task='empathetic_dialogues', \n",
        "\n",
        "    model='transformer/generator', # ---------------------------------------------- agent\n",
        "    model_file= f'{pretrained_model_path}/model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model='zoo:tutorial_transformer_generator/model', # ---------------------------------------------- model\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\n",
        "    learn_positional_embeddings=True,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    # use a small learning rate with ADAM optimizer\n",
        "    lr=1e-5, \n",
        "    # optimizer='adam',\n",
        "    optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    # early stopping on perplexity\n",
        "    validation_metric='ppl',\n",
        "    # train at most 10 minutes, and validate every 0.25 epochs\n",
        "    # max_train_time=600, \n",
        "    validation_every_n_epochs=0.25,\n",
        "\n",
        "    # save_every_n_secs = 10*60,\n",
        "    num_epochs = 3,\n",
        "    save_after_valid = True,\n",
        "    verbose = True,\n",
        "    \n",
        "    # depend on your gpu. If you have a V100, this is good\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOSk1_qIXmKQ",
        "outputId": "7b7e3549-7b63-47b0-f9c7-93e8eb8415f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10:00:38 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "10:00:38 | Using CUDA\n",
            "10:00:38 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/pretrained/model.dict\n",
            "10:00:38 | num words = 54944\n",
            "10:00:40 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "10:00:40 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/pretrained/model\n",
            "10:00:43 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "10:00:44 | Opt:\n",
            "10:00:44 |     activation: gelu\n",
            "10:00:44 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "10:00:44 |     adam_eps: 1e-08\n",
            "10:00:44 |     add_p1_after_newln: False\n",
            "10:00:44 |     aggregate_micro: False\n",
            "10:00:44 |     allow_missing_init_opts: False\n",
            "10:00:44 |     attention_dropout: 0.0\n",
            "10:00:44 |     batchsize: 12\n",
            "10:00:44 |     beam_block_full_context: True\n",
            "10:00:44 |     beam_block_list_filename: None\n",
            "10:00:44 |     beam_block_ngram: -1\n",
            "10:00:44 |     beam_context_block_ngram: -1\n",
            "10:00:44 |     beam_delay: 30\n",
            "10:00:44 |     beam_length_penalty: 0.65\n",
            "10:00:44 |     beam_min_length: 1\n",
            "10:00:44 |     beam_size: 1\n",
            "10:00:44 |     betas: '[0.9, 0.999]'\n",
            "10:00:44 |     bpe_add_prefix_space: None\n",
            "10:00:44 |     bpe_debug: False\n",
            "10:00:44 |     bpe_dropout: None\n",
            "10:00:44 |     bpe_merge: None\n",
            "10:00:44 |     bpe_vocab: None\n",
            "10:00:44 |     checkpoint_activations: False\n",
            "10:00:44 |     compute_tokenized_bleu: False\n",
            "10:00:44 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "10:00:44 |     datatype: train\n",
            "10:00:44 |     delimiter: '\\n'\n",
            "10:00:44 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "10:00:44 |     dict_endtoken: __end__\n",
            "10:00:44 |     dict_file: /content/drive/MyDrive/colabs/blender-models/pretrained/model.dict\n",
            "10:00:44 |     dict_include_test: False\n",
            "10:00:44 |     dict_include_valid: False\n",
            "10:00:44 |     dict_initpath: None\n",
            "10:00:44 |     dict_language: english\n",
            "10:00:44 |     dict_loaded: True\n",
            "10:00:44 |     dict_lower: True\n",
            "10:00:44 |     dict_max_ngram_size: -1\n",
            "10:00:44 |     dict_maxexs: -1\n",
            "10:00:44 |     dict_maxtokens: -1\n",
            "10:00:44 |     dict_minfreq: 0\n",
            "10:00:44 |     dict_nulltoken: __null__\n",
            "10:00:44 |     dict_starttoken: __start__\n",
            "10:00:44 |     dict_textfields: text,labels\n",
            "10:00:44 |     dict_tokenizer: bpe\n",
            "10:00:44 |     dict_unktoken: __unk__\n",
            "10:00:44 |     display_add_fields: \n",
            "10:00:44 |     display_examples: False\n",
            "10:00:44 |     download_path: None\n",
            "10:00:44 |     dropout: 0.0\n",
            "10:00:44 |     dynamic_batching: full\n",
            "10:00:44 |     embedding_projection: random\n",
            "10:00:44 |     embedding_size: 512\n",
            "10:00:44 |     embedding_type: random\n",
            "10:00:44 |     embeddings_scale: True\n",
            "10:00:44 |     eval_batchsize: None\n",
            "10:00:44 |     eval_dynamic_batching: None\n",
            "10:00:44 |     evaltask: None\n",
            "10:00:44 |     ffn_size: 2048\n",
            "10:00:44 |     final_extra_opt: \n",
            "10:00:44 |     force_fp16_tokens: True\n",
            "10:00:44 |     fp16: True\n",
            "10:00:44 |     fp16_impl: mem_efficient\n",
            "10:00:44 |     gpu: -1\n",
            "10:00:44 |     gradient_clip: 0.1\n",
            "10:00:44 |     hide_labels: False\n",
            "10:00:44 |     history_add_global_end_token: None\n",
            "10:00:44 |     history_reversed: False\n",
            "10:00:44 |     history_size: -1\n",
            "10:00:44 |     image_cropsize: 224\n",
            "10:00:44 |     image_mode: raw\n",
            "10:00:44 |     image_size: 256\n",
            "10:00:44 |     inference: greedy\n",
            "10:00:44 |     init_model: /content/drive/MyDrive/colabs/blender-models/pretrained/model.checkpoint\n",
            "10:00:44 |     init_opt: None\n",
            "10:00:44 |     interactive_mode: False\n",
            "10:00:44 |     invsqrt_lr_decay_gamma: -1\n",
            "10:00:44 |     is_debug: False\n",
            "10:00:44 |     label_truncate: 128\n",
            "10:00:44 |     learn_positional_embeddings: True\n",
            "10:00:44 |     learningrate: 1e-05\n",
            "10:00:44 |     log_every_n_secs: -1\n",
            "10:00:44 |     log_every_n_steps: 50\n",
            "10:00:44 |     loglevel: info\n",
            "10:00:44 |     lr_scheduler: reduceonplateau\n",
            "10:00:44 |     lr_scheduler_decay: 0.5\n",
            "10:00:44 |     lr_scheduler_patience: 3\n",
            "10:00:44 |     max_train_steps: -1\n",
            "10:00:44 |     max_train_time: -1\n",
            "10:00:44 |     metrics: default\n",
            "10:00:44 |     model: transformer/generator\n",
            "10:00:44 |     model_file: /content/drive/MyDrive/colabs/blender-models/pretrained/model\n",
            "10:00:44 |     model_parallel: False\n",
            "10:00:44 |     momentum: 0\n",
            "10:00:44 |     multitask_weights: [1]\n",
            "10:00:44 |     mutators: None\n",
            "10:00:44 |     n_decoder_layers: -1\n",
            "10:00:44 |     n_encoder_layers: -1\n",
            "10:00:44 |     n_heads: 16\n",
            "10:00:44 |     n_layers: 8\n",
            "10:00:44 |     n_positions: 512\n",
            "10:00:44 |     n_segments: 0\n",
            "10:00:44 |     nesterov: True\n",
            "10:00:44 |     no_cuda: False\n",
            "10:00:44 |     num_epochs: 3.0\n",
            "10:00:44 |     num_examples: 8\n",
            "10:00:44 |     num_workers: 0\n",
            "10:00:44 |     nus: [0.7]\n",
            "10:00:44 |     optimizer: mem_eff_adam\n",
            "10:00:44 |     output_scaling: 1.0\n",
            "10:00:44 |     override: \"{'task': 'empathetic_dialogues', 'model_file': '/content/drive/MyDrive/colabs/blender-models/pretrained/model', 'num_examples': '8', 'skip_generation': False}\"\n",
            "10:00:44 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "10:00:44 |     person_tokens: False\n",
            "10:00:44 |     rank_candidates: False\n",
            "10:00:44 |     relu_dropout: 0.0\n",
            "10:00:44 |     remove_political_convos: False\n",
            "10:00:44 |     save_after_valid: True\n",
            "10:00:44 |     save_every_n_secs: 600.0\n",
            "10:00:44 |     share_word_embeddings: True\n",
            "10:00:44 |     short_final_eval: False\n",
            "10:00:44 |     skip_generation: False\n",
            "10:00:44 |     special_tok_lst: None\n",
            "10:00:44 |     split_lines: False\n",
            "10:00:44 |     starttime: Mar01_14-53\n",
            "10:00:44 |     task: empathetic_dialogues\n",
            "10:00:44 |     temperature: 1.0\n",
            "10:00:44 |     tensorboard_log: False\n",
            "10:00:44 |     tensorboard_logdir: None\n",
            "10:00:44 |     text_truncate: 512\n",
            "10:00:44 |     topk: 10\n",
            "10:00:44 |     topp: 0.9\n",
            "10:00:44 |     train_experiencer_only: False\n",
            "10:00:44 |     truncate: -1\n",
            "10:00:44 |     update_freq: 1\n",
            "10:00:44 |     use_reply: label\n",
            "10:00:44 |     validation_cutoff: 1.0\n",
            "10:00:44 |     validation_every_n_epochs: 0.25\n",
            "10:00:44 |     validation_every_n_secs: -1\n",
            "10:00:44 |     validation_every_n_steps: -1\n",
            "10:00:44 |     validation_max_exs: -1\n",
            "10:00:44 |     validation_metric: ppl\n",
            "10:00:44 |     validation_metric_mode: None\n",
            "10:00:44 |     validation_patience: 10\n",
            "10:00:44 |     validation_share_agent: False\n",
            "10:00:44 |     variant: xlm\n",
            "10:00:44 |     verbose: False\n",
            "10:00:44 |     wandb_entity: None\n",
            "10:00:44 |     wandb_log: False\n",
            "10:00:44 |     wandb_name: None\n",
            "10:00:44 |     wandb_project: None\n",
            "10:00:44 |     warmup_rate: 0.0001\n",
            "10:00:44 |     warmup_updates: 100\n",
            "10:00:44 |     weight_decay: None\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh no ! that ' s terrible ! did you get a new tire ?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: that ' s good to hear . i hope you are okay .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mA few weeks ago, I was walking through my hallway, minding my own business, when all of a sudden a hand reached out from under a table and grabbed my ankle. I was so suprised. I thought i was got. Turns out, it was my son. \u001b[0;0m\n",
            "\u001b[1;94m    labels: That's funny, hope he didn't give you a heart attack.\u001b[0;0m\n",
            "\u001b[0;95m     model: oh my goodness ! that ' s so scary ! did you get a new pair of shoes ?\u001b[0;0m\n",
            "\u001b[0mI may have let out a scream that will have him question my manhood for the rest of our lives, lol. \u001b[0;0m\n",
            "\u001b[1;94m    labels: I would probably scream also.\u001b[0;0m\n",
            "\u001b[0;95m     model: i hope he didn ' t .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mI'm overly excited because will be flying outside the country for the first time tomorrow.\".  Please enter here...\u001b[0;0m\n",
            "\u001b[1;94m    labels: Wow! That sounds amazing. Where are you going? \u001b[0;0m\n",
            "\u001b[0;95m     model: oh , that ' s exciting ! i hope you have a great time !\u001b[0;0m\n",
            "\u001b[0mTraveling to South Africa then to Ghana. Also my first time visiting Africa\u001b[0;0m\n",
            "\u001b[1;94m    labels: You are going to love it I am sure. Safe Travels! \u001b[0;0m\n",
            "\u001b[0;95m     model: wow ! that ' s a lot of time to travel . i hope you have a great time !\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mOne time, I was holding my son over my head, when all of a sudden, he threw up all over my face. I almost cried.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Thats horrible, hope he is ok.\u001b[0;0m\n",
            "\u001b[0;95m     model: oh my goodness ! that must have been so embarrassing !\u001b[0;0m\n",
            "\u001b[0myeah, he had just drank a bottle, and I was moving him around like an airplane. In retrospect. I deserved it.\u001b[0;0m\n",
            "\u001b[1;94m    labels: I see, glad he is ok.\u001b[0;0m\n",
            "\u001b[0;95m     model: i hope he is ok .\u001b[0;0m\n"
          ]
        }
      ],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file= f'{pretrained_model_path}/model',\n",
        "    num_examples=8,\n",
        "    skip_generation=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh56lfkKVslE"
      },
      "source": [
        "# 6.Find  model arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qu4VbEFVo1j",
        "outputId": "44290db9-430a-4fc7-f6d5-97d358fe9c5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: TrainModel [-h] [--helpall] [-o INIT_OPT]\n",
            "                  [--allow-missing-init-opts ALLOW_MISSING_INIT_OPTS]\n",
            "                  [-t TASK] [-dt DATATYPE] [-bs BATCHSIZE]\n",
            "                  [-dynb {full,batchsort,None}] [-v] [--debug] [-dp DATAPATH]\n",
            "                  [-m MODEL] [-mf MODEL_FILE] [-im INIT_MODEL] [-et EVALTASK]\n",
            "                  [--final-extra-opt FINAL_EXTRA_OPT]\n",
            "                  [--eval-dynamic-batching {full,batchsort,None,off}]\n",
            "                  [--num-workers NUM_WORKERS] [-eps NUM_EPOCHS]\n",
            "                  [-ttim MAX_TRAIN_TIME] [-tstep MAX_TRAIN_STEPS]\n",
            "                  [-lstep LOG_EVERY_N_STEPS] [-vtim VALIDATION_EVERY_N_SECS]\n",
            "                  [-vstep VALIDATION_EVERY_N_STEPS] [-stim SAVE_EVERY_N_SECS]\n",
            "                  [-sval SAVE_AFTER_VALID] [-veps VALIDATION_EVERY_N_EPOCHS]\n",
            "                  [-vp VALIDATION_PATIENCE] [-vmt VALIDATION_METRIC]\n",
            "                  [-vmm {max,min}] [-mcs METRICS] [-micro AGGREGATE_MICRO]\n",
            "                  [-tblog TENSORBOARD_LOG] [-tblogdir TENSORBOARD_LOGDIR]\n",
            "                  [-wblog WANDB_LOG] [--wandb-project WANDB_PROJECT]\n",
            "                  [--wandb-entity WANDB_ENTITY] [-esz EMBEDDING_SIZE]\n",
            "                  [-nl N_LAYERS] [-hid FFN_SIZE] [--dropout DROPOUT]\n",
            "                  [--attention-dropout ATTENTION_DROPOUT]\n",
            "                  [--relu-dropout RELU_DROPOUT] [--n-heads N_HEADS]\n",
            "                  [--learn-positional-embeddings LEARN_POSITIONAL_EMBEDDINGS]\n",
            "                  [--embeddings-scale EMBEDDINGS_SCALE]\n",
            "                  [--n-segments N_SEGMENTS]\n",
            "                  [--variant {aiayn,bart,prelayernorm,xlm}]\n",
            "                  [--activation {gelu,relu}] [--output-scaling OUTPUT_SCALING]\n",
            "                  [--share-word-embeddings SHARE_WORD_EMBEDDINGS]\n",
            "                  [-nel N_ENCODER_LAYERS] [-ndl N_DECODER_LAYERS]\n",
            "                  [--model-parallel MODEL_PARALLEL]\n",
            "                  [--checkpoint-activations CHECKPOINT_ACTIVATIONS]\n",
            "                  [--beam-size BEAM_SIZE] [--beam-min-length BEAM_MIN_LENGTH]\n",
            "                  [--beam-context-block-ngram BEAM_CONTEXT_BLOCK_NGRAM]\n",
            "                  [--beam-block-ngram BEAM_BLOCK_NGRAM]\n",
            "                  [--beam-block-full-context BEAM_BLOCK_FULL_CONTEXT]\n",
            "                  [--beam-length-penalty BEAM_LENGTH_PENALTY]\n",
            "                  [--inference {nucleus,beam,delayedbeam,greedy,topk}]\n",
            "                  [--topk TOPK] [--topp TOPP] [--beam-delay BEAM_DELAY]\n",
            "                  [--beam-block-list-filename BEAM_BLOCK_LIST_FILENAME]\n",
            "                  [--temperature TEMPERATURE]\n",
            "                  [--compute-tokenized-bleu COMPUTE_TOKENIZED_BLEU]\n",
            "                  [-i INTERACTIVE_MODE]\n",
            "                  [-emb {random,glove,glove-fixed,fasttext,fasttext-fixed,fasttext_cc,fasttext_cc-fixed}]\n",
            "                  [-embp EMBEDDING_PROJECTION] [--fp16 FP16]\n",
            "                  [--fp16-impl {safe,mem_efficient}] [-opt OPTIMIZER]\n",
            "                  [-lr LEARNINGRATE] [-clip GRADIENT_CLIP]\n",
            "                  [--adafactor-eps ADAFACTOR_EPS] [-mom MOMENTUM]\n",
            "                  [--nesterov NESTEROV] [-nu NUS] [-beta BETAS]\n",
            "                  [-wdecay WEIGHT_DECAY] [-rc RANK_CANDIDATES] [-tr TRUNCATE]\n",
            "                  [--text-truncate TEXT_TRUNCATE]\n",
            "                  [--label-truncate LABEL_TRUNCATE]\n",
            "                  [--history-reversed HISTORY_REVERSED] [-histsz HISTORY_SIZE]\n",
            "                  [-pt PERSON_TOKENS] [--split-lines SPLIT_LINES]\n",
            "                  [--delimiter DELIMITER] [--special-tok-lst SPECIAL_TOK_LST]\n",
            "                  [-gpu GPU | --no-cuda] [--bpe-vocab BPE_VOCAB]\n",
            "                  [--bpe-merge BPE_MERGE] [--bpe-dropout BPE_DROPOUT]\n",
            "                  [--lr-scheduler {reduceonplateau,none,fixed,invsqrt,cosine,linear}]\n",
            "                  [--lr-scheduler-patience LR_SCHEDULER_PATIENCE]\n",
            "                  [--lr-scheduler-decay LR_SCHEDULER_DECAY]\n",
            "                  [--invsqrt-lr-decay-gamma INVSQRT_LR_DECAY_GAMMA]\n",
            "\n",
            "Train a model\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help\n",
            "        show this help message and exit\n",
            "  --helpall\n",
            "        Show usage, including advanced arguments.\n",
            "\n",
            "Main ParlAI Arguments:\n",
            "  -o, --init-opt INIT_OPT\n",
            "        Path to json file of options. Note: Further Command-line arguments\n",
            "        override file-based options. (default: None)\n",
            "  --allow-missing-init-opts ALLOW_MISSING_INIT_OPTS\n",
            "        Warn instead of raising if an argument passed in with --init-opt is\n",
            "        not in the target opt. (default: False)\n",
            "  -t, --task TASK\n",
            "        ParlAI task(s), e.g. \"babi:Task1\" or \"babi,cbt\" (default: None)\n",
            "  -dt, --datatype DATATYPE\n",
            "        choose from: train, train:ordered, valid, test. to stream data add\n",
            "        \":stream\" to any option (e.g., train:stream). by default train is\n",
            "        random with replacement, valid is ordered, test is ordered. (default:\n",
            "        train)\n",
            "  -bs, --batchsize BATCHSIZE\n",
            "        batch size for minibatch training schemes (default: 1)\n",
            "  -dynb, --dynamic-batching {full,batchsort,None}\n",
            "        Use dynamic batching (default: None)\n",
            "  -v, --verbose\n",
            "        Print all messages\n",
            "  --debug\n",
            "        Enables some debug behavior\n",
            "  -dp, --datapath DATAPATH\n",
            "        path to datasets, defaults to {parlai_dir}/data (default: None)\n",
            "\n",
            "ParlAI Model Arguments:\n",
            "  -m, --model MODEL\n",
            "        the model class name. can match parlai/agents/<model> for agents in\n",
            "        that directory, or can provide a fully specified module for `from X\n",
            "        import Y` via `-m X:Y` (e.g. `-m\n",
            "        parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`) (default: None)\n",
            "  -mf, --model-file MODEL_FILE\n",
            "        model file name for loading and saving models (default: None)\n",
            "  -im, --init-model INIT_MODEL\n",
            "        Initialize model weights and dict from this file (default: None)\n",
            "\n",
            "Training Loop Arguments:\n",
            "  -et, --evaltask EVALTASK\n",
            "        task to use for valid/test (defaults to the one used for training)\n",
            "        (default: None)\n",
            "  --final-extra-opt FINAL_EXTRA_OPT\n",
            "        A '.opt' file that is used for final eval. Useful for setting skip-\n",
            "        generation to false. 'datatype' must be included as part of the opt.\n",
            "        (default: )\n",
            "  --eval-dynamic-batching {full,batchsort,None,off}\n",
            "        Set dynamic batching at evaluation time. Set to off for train-only\n",
            "        dynamic batching. Set to none (default) to use same setting as\n",
            "        --dynamic-batching. (default: None)\n",
            "  --num-workers NUM_WORKERS\n",
            "        Number of background workers (training only) (default: 0)\n",
            "  -eps, --num-epochs NUM_EPOCHS\n",
            "  -ttim, --max-train-time MAX_TRAIN_TIME\n",
            "  -tstep, --max-train-steps, --max-lr-steps MAX_TRAIN_STEPS\n",
            "        End training after n model updates (default: -1)\n",
            "  -lstep, --log-every-n-steps LOG_EVERY_N_STEPS\n",
            "        Log every n training steps (default: 50)\n",
            "  -vtim, --validation-every-n-secs VALIDATION_EVERY_N_SECS\n",
            "        Validate every n seconds. Saves model to model_file (if set) whenever\n",
            "        best val metric is found (default: -1)\n",
            "  -vstep, --validation-every-n-steps VALIDATION_EVERY_N_STEPS\n",
            "        Validate every n training steps. Saves model to model_file (if set)\n",
            "        whenever best val metric is found (default: -1)\n",
            "  -stim, --save-every-n-secs SAVE_EVERY_N_SECS\n",
            "        Saves the model to model_file.checkpoint after every n seconds\n",
            "        (default -1, never). (default: -1)\n",
            "  -sval, --save-after-valid SAVE_AFTER_VALID\n",
            "        Saves the model to model_file.checkpoint after every validation\n",
            "        (default False).\n",
            "  -veps, --validation-every-n-epochs VALIDATION_EVERY_N_EPOCHS\n",
            "        Validate every n epochs. Saves model to model_file (if set) whenever\n",
            "        best val metric is found (default: -1)\n",
            "  -vp, --validation-patience VALIDATION_PATIENCE\n",
            "        number of iterations of validation where result does not improve\n",
            "        before we stop training (default: 10)\n",
            "  -vmt, --validation-metric VALIDATION_METRIC\n",
            "        key into report table for selecting best validation (default:\n",
            "        accuracy)\n",
            "  -vmm, --validation-metric-mode {max,min}\n",
            "        how to optimize validation metric (max or min) (default: None)\n",
            "  -mcs, --metrics METRICS\n",
            "        list of metrics to show/compute, e.g. all, default,or give a list\n",
            "        split by , like ppl,f1,accuracy,hits@1,rouge,bleuthe rouge metrics\n",
            "        will be computed as rouge-1, rouge-2 and rouge-l (default: default)\n",
            "  -micro, --aggregate-micro AGGREGATE_MICRO\n",
            "        Report micro-averaged metrics instead of macro averaged metrics.\n",
            "        (default: False)\n",
            "\n",
            "Tensorboard Arguments:\n",
            "  -tblog, --tensorboard-log TENSORBOARD_LOG\n",
            "        Tensorboard logging of metrics (default: False)\n",
            "  -tblogdir, --tensorboard-logdir TENSORBOARD_LOGDIR\n",
            "        Tensorboard logging directory, defaults to model_file.tensorboard\n",
            "        (default: None)\n",
            "\n",
            "WandB Arguments:\n",
            "  -wblog, --wandb-log WANDB_LOG\n",
            "        Enable W&B logging of metrics (default: False)\n",
            "  --wandb-project WANDB_PROJECT\n",
            "        W&B project name. Defaults to timestamp. Usually the name of the\n",
            "        sweep. (default: None)\n",
            "  --wandb-entity WANDB_ENTITY\n",
            "        W&B entity name. (default: None)\n",
            "\n",
            "Transformer Arguments:\n",
            "  -esz, --embedding-size EMBEDDING_SIZE\n",
            "        Size of all embedding layers. Must be a multiple of --n-heads.\n",
            "        (default: 300)\n",
            "  -nl, --n-layers N_LAYERS\n",
            "        Number of transformer layers. (default: 2)\n",
            "  -hid, --ffn-size FFN_SIZE\n",
            "        Hidden size of the FFN layers (default: 300)\n",
            "  --dropout DROPOUT\n",
            "        Dropout used around embeddings and before layer layer normalizations.\n",
            "        This is used in Vaswani 2017 and works well on large datasets.\n",
            "        (default: 0.0)\n",
            "  --attention-dropout ATTENTION_DROPOUT\n",
            "        Dropout used after attention softmax. This is not used in Vaswani\n",
            "        2017. (default: 0.0)\n",
            "  --relu-dropout RELU_DROPOUT\n",
            "        Dropout used after the ReLU in the FFN. Not used in Vaswani 2017, but\n",
            "        used in Tensor2Tensor. (default: 0.0)\n",
            "  --n-heads N_HEADS\n",
            "        Number of multihead attention heads (default: 2)\n",
            "  --learn-positional-embeddings LEARN_POSITIONAL_EMBEDDINGS\n",
            "        If off, sinusoidal embeddings are used. If on, position embeddings are\n",
            "        learned from scratch. (default: False)\n",
            "  --embeddings-scale EMBEDDINGS_SCALE\n",
            "  --n-segments N_SEGMENTS\n",
            "        The number of segments that support the model. If zero no segment and\n",
            "        no langs_embedding. (default: 0)\n",
            "  --variant {aiayn,bart,prelayernorm,xlm}\n",
            "        Chooses locations of layer norms, etc. prelayernorm is used to match\n",
            "        some fairseq models (default: aiayn, recommended: xlm)\n",
            "  --activation {gelu,relu}\n",
            "        Nonlinear activation to use. AIAYN uses relu, but more recent papers\n",
            "        prefer gelu. (default: relu, recommended: gelu)\n",
            "  --output-scaling OUTPUT_SCALING\n",
            "        scale the output of every transformer by this quantity. (default: 1.0)\n",
            "  --share-word-embeddings SHARE_WORD_EMBEDDINGS\n",
            "        Share word embeddings table for candidate and contextin the memory\n",
            "        network (default: True)\n",
            "  -nel, --n-encoder-layers N_ENCODER_LAYERS\n",
            "        This will overidde the n-layers for asymmetrical transformers\n",
            "        (default: -1)\n",
            "  -ndl, --n-decoder-layers N_DECODER_LAYERS\n",
            "        This will overidde the n-layers for asymmetrical transformers\n",
            "        (default: -1)\n",
            "  --model-parallel MODEL_PARALLEL\n",
            "        Shard the layers across multiple GPUs. (default: False)\n",
            "  --checkpoint-activations CHECKPOINT_ACTIVATIONS\n",
            "        Recompute activations on backward pass to conserve memory. (default:\n",
            "        False)\n",
            "\n",
            "Torch Generator Agent:\n",
            "  --beam-size BEAM_SIZE\n",
            "        Beam size, if 1 then greedy search (default: 1)\n",
            "  --beam-min-length BEAM_MIN_LENGTH\n",
            "        Minimum length of prediction to be generated by the beam search\n",
            "        (default: 1)\n",
            "  --beam-context-block-ngram BEAM_CONTEXT_BLOCK_NGRAM\n",
            "        Size n-grams to block in beam search from the context. val <= 0\n",
            "        implies no blocking (default: -1)\n",
            "  --beam-block-ngram BEAM_BLOCK_NGRAM\n",
            "        Size n-grams to block in beam search. val <= 0 implies no blocking\n",
            "        (default: -1)\n",
            "  --beam-block-full-context BEAM_BLOCK_FULL_CONTEXT\n",
            "        Block n-grams from the *full* history context. Specify False to block\n",
            "        up to m tokens in the past, where m is truncation parameter for agent\n",
            "        (default: True)\n",
            "  --beam-length-penalty BEAM_LENGTH_PENALTY\n",
            "        Applies a length penalty. Set to 0 for no penalty. (default: 0.65)\n",
            "  --inference {nucleus,beam,delayedbeam,greedy,topk}\n",
            "        Generation algorithm (default: greedy)\n",
            "  --topk TOPK\n",
            "        K used in Top K sampling (default: 10)\n",
            "  --topp TOPP\n",
            "        p used in nucleus sampling (default: 0.9)\n",
            "  --beam-delay BEAM_DELAY\n",
            "        used in delayedbeam search (default: 30)\n",
            "  --beam-block-list-filename BEAM_BLOCK_LIST_FILENAME\n",
            "        Load a text file of hard blocks for beam search to never say.\n",
            "        (default: None)\n",
            "  --temperature TEMPERATURE\n",
            "        temperature to add during decoding (default: 1.0)\n",
            "  --compute-tokenized-bleu COMPUTE_TOKENIZED_BLEU\n",
            "        if true, compute tokenized bleu scores (default: False)\n",
            "\n",
            "TorchAgent Arguments:\n",
            "  -i, --interactive-mode INTERACTIVE_MODE\n",
            "        Whether in full interactive mode or not, which means generating text\n",
            "        or retrieving from a full set of candidates, which is necessary to\n",
            "        actually do full dialogue. However, during training or quick\n",
            "        validation (e.g. PPL for generation or ranking a few candidates for\n",
            "        ranking models) you might want these set to off. Typically, scripts\n",
            "        can set their preferred default behavior at the start, e.g. eval\n",
            "        scripts. (default: False)\n",
            "  -emb, --embedding-type {random,glove,glove-fixed,fasttext,fasttext-fixed,fasttext_cc,fasttext_cc-fixed}\n",
            "        Choose between different strategies for initializing word embeddings.\n",
            "        Default is random, but can also preinitialize from Glove or Fasttext.\n",
            "        Preinitialized embeddings can also be fixed so they are not updated\n",
            "        during training. (default: random)\n",
            "  -embp, --embedding-projection EMBEDDING_PROJECTION\n",
            "        If pretrained embeddings have a different dimensionality than your\n",
            "        embedding size, strategy for projecting to the correct size. If the\n",
            "        dimensions are the same, this is ignored unless you append \"-force\" to\n",
            "        your choice. (default: random)\n",
            "  --fp16 FP16\n",
            "        Use fp16 computations. (default: False)\n",
            "  --fp16-impl {safe,mem_efficient}\n",
            "        Implementation of FP16 to use (default: safe)\n",
            "  -rc, --rank-candidates RANK_CANDIDATES\n",
            "        Whether the model should parse candidates for ranking. (default:\n",
            "        False)\n",
            "  -tr, --truncate TRUNCATE\n",
            "        Truncate input lengths to increase speed / use less memory. (default:\n",
            "        -1)\n",
            "  --text-truncate TEXT_TRUNCATE\n",
            "        Text input truncation length: if not specified, this will default to\n",
            "        `truncate` (default: None)\n",
            "  --label-truncate LABEL_TRUNCATE\n",
            "        Label truncation length: if not specified, this will default to\n",
            "        `truncate` (default: None)\n",
            "  --history-reversed HISTORY_REVERSED\n",
            "        Reverse the history (default: False)\n",
            "  -histsz, --history-size HISTORY_SIZE\n",
            "        Number of past dialog utterances to remember. (default: -1)\n",
            "  -pt, --person-tokens PERSON_TOKENS\n",
            "        add person tokens to history. adds __p1__ in front of input text and\n",
            "        __p2__ in front of past labels when available or past utterances\n",
            "        generated by the model. these are added to the dictionary during\n",
            "        initialization. (default: False)\n",
            "  --split-lines SPLIT_LINES\n",
            "        split the dialogue history on newlines and save in separate vectors\n",
            "        (default: False)\n",
            "  --delimiter DELIMITER\n",
            "        Join history lines with this token, defaults to newline (default: )\n",
            "  --special-tok-lst SPECIAL_TOK_LST\n",
            "        Comma separated list of special tokens. In case of ambiguous parses\n",
            "        from special tokens, the ordering provided in this arg sets\n",
            "        precedence. (default: None)\n",
            "  -gpu, --gpu GPU\n",
            "        which GPU to use (default: -1)\n",
            "  --no-cuda\n",
            "        disable GPUs even if available. otherwise, will use GPUs if available\n",
            "        on the device.\n",
            "\n",
            "Optimizer Arguments:\n",
            "  -opt, --optimizer OPTIMIZER\n",
            "        Optimizer choice. Possible values: adadelta, adagrad, adam, adamw,\n",
            "        sparseadam, adamax, asgd, sgd, radam, rprop, rmsprop, optimizer,\n",
            "        nadam, lbfgs, mem_eff_adam, adafactor. (default: sgd)\n",
            "  -lr, --learningrate LEARNINGRATE\n",
            "        Learning rate (default: 1)\n",
            "  -clip, --gradient-clip GRADIENT_CLIP\n",
            "        gradient clipping using l2 norm (default: 0.1)\n",
            "  --adafactor-eps ADAFACTOR_EPS\n",
            "        Epsilon values for adafactor optimizer: regularization constants for\n",
            "        square gradient and parameter scale respectively (default: 1e-30,1e-3)\n",
            "  -mom, --momentum MOMENTUM\n",
            "        if applicable, momentum value for optimizer. (default: 0)\n",
            "  --nesterov NESTEROV\n",
            "        if applicable, whether to use nesterov momentum. (default: True)\n",
            "  -nu, --nus NUS\n",
            "        if applicable, nu value(s) for optimizer. can use a single value like\n",
            "        0.7 or a comma-separated tuple like 0.7,1.0 (default: 0.7)\n",
            "  -beta, --betas BETAS\n",
            "        if applicable, beta value(s) for optimizer. can use a single value\n",
            "        like 0.9 or a comma-separated tuple like 0.9,0.999 (default:\n",
            "        0.9,0.999)\n",
            "  -wdecay, --weight-decay WEIGHT_DECAY\n",
            "        Weight decay on the weights. (default: None)\n",
            "\n",
            "BPEHelper Arguments:\n",
            "  --bpe-vocab BPE_VOCAB\n",
            "        path to pre-trained tokenizer vocab (default: None)\n",
            "  --bpe-merge BPE_MERGE\n",
            "        path to pre-trained tokenizer merge (default: None)\n",
            "  --bpe-dropout BPE_DROPOUT\n",
            "        Use BPE dropout during training. (default: None)\n",
            "\n",
            "Learning Rate Scheduler:\n",
            "  --lr-scheduler {reduceonplateau,none,fixed,invsqrt,cosine,linear}\n",
            "        Learning rate scheduler. (default: reduceonplateau)\n",
            "  --lr-scheduler-patience LR_SCHEDULER_PATIENCE\n",
            "        LR scheduler patience. In number of validation runs. If using fixed\n",
            "        scheduler, LR is decayed every <patience> validations. (default: 3)\n",
            "  --lr-scheduler-decay LR_SCHEDULER_DECAY\n",
            "        Decay factor for LR scheduler, or how much LR is multiplied by when it\n",
            "        is lowered. (default: 0.5)\n",
            "  --invsqrt-lr-decay-gamma INVSQRT_LR_DECAY_GAMMA\n",
            "        Constant used only to find the lr multiplier for the invsqrt\n",
            "        scheduler. Must be set for --lr-scheduler invsqrt (default: -1)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# note that if you want to see model-specific arguments, you must specify a model name\n",
        "print(TrainModel.help(model='transformer/generator'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjqYycyFZyBj"
      },
      "source": [
        "# Fine-tuning Blenderbot with different datasets and various hyperparameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0zXS5r3jji_"
      },
      "source": [
        "## **Fine-tuning (with XPersona French dataset)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ojpKEcdW-w"
      },
      "source": [
        "### Convert dataset to parlAI  data format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqaZsspFxgMc"
      },
      "source": [
        "[All XPersona datasets](https://github.com/HLTCHKUST/Xpersona/tree/master/dataset) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYysfVrGsv8w",
        "outputId": "5dca8284-fdbd-47c1-e6da-282813adb6a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Xpersona'...\n",
            "remote: Enumerating objects: 282, done.\u001b[K\n",
            "remote: Counting objects: 100% (282/282), done.\u001b[K\n",
            "remote: Compressing objects: 100% (201/201), done.\u001b[K\n",
            "remote: Total 282 (delta 96), reused 241 (delta 78), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (282/282), 45.00 MiB | 16.92 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HLTCHKUST/Xpersona.git\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_train_corrected.json','r') as f:\n",
        "   train_data = json.load(f)\n",
        "\n",
        "dialogs_train = pd.DataFrame(train_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_valid_human_annotated.json','r') as f:\n",
        "   valid_data = json.load(f)\n",
        "\n",
        "dialogs_valid = pd.DataFrame(valid_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_test_human_annotated.json','r') as f:\n",
        "   text_data = json.load(f)\n",
        "\n",
        "dialogs_test = pd.DataFrame(text_data)['dialogue'].tolist()\n",
        "\n",
        "# dialogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXwHLTWbtRdu",
        "outputId": "d0070d89-4439-4caf-c588-9f4e68b1418b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16878 248 249\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[['salut comment vas-tu ? Je me prépare à faire du guépard pour rester en forme.',\n",
              "   \"vous devez être très rapide. la chasse est l'un de mes passe-temps préférés.\"],\n",
              "  [\"je suis ! pour mon hobby j'aime faire la mise en conserve ou un peu tailler.\",\n",
              "   \"je remodèle aussi des maisons quand je ne suis pas à la chasse à l'arc.\"],\n",
              "  [\"c'est bien. quand j'étais au lycée, je me suis placé 6ème au 100m dash!\",\n",
              "   \"c'est génial . avez-vous une saison ou une période préférée de l'année?\"],\n",
              "  [\"Non . mais j'ai une viande préférée car c'est tout ce que je mange exclusivement.\",\n",
              "   'quelle est votre viande préférée à manger?'],\n",
              "  ['je devrais dire sa côte de bœuf. avez-vous des aliments préférés?',\n",
              "   \"j'aime le poulet ou les macaronis et le fromage.\"],\n",
              "  [\"avez-vous prévu quelque chose pour aujourd'hui? je pense que je vais faire de la mise en conserve.\",\n",
              "   'je vais regarder le football. que conservez-vous?'],\n",
              "  ['je pense que je vais pouvoir un peu de confiture. jouez-vous aussi pour le plaisir?',\n",
              "   \"si j'ai le temps en dehors des maisons de chasse et de rénovation. ce qui n'est pas grand chose!\"]]]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "  print(len(dialogs_train), len(dialogs_valid), len(dialogs_test))\n",
        "  \n",
        "  dialogs_train[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfoCk3iRdcon",
        "outputId": "5260ddad-e3e5-45e0-ecde-e6d25a2816dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15776427\n",
            "257114\n",
            "262425\n"
          ]
        }
      ],
      "source": [
        "# pd.read_json(f,orient='records', encoding='utf-8',lines=True)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# dialogs = pd.read_json('XPersona/dataset/Fr_persona_split_train_corrected.json')['dialogue'].tolist()\n",
        "\n",
        "\n",
        "text = \"\"\n",
        "\n",
        "def transfer_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, each_two_turn in enumerate(d):\n",
        "    u1 = each_two_turn[0]\n",
        "    u2 = each_two_turn[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "\n",
        "# d = dialogs[0]\n",
        "# print(d)\n",
        "# print(transfer_dialog(d))\n",
        "\n",
        "data_train = \"\"\n",
        "for d in dialogs_train:\n",
        "  data_train += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_train = open(\"fr_finetuned_train.txt\",\"w\")\n",
        "print(file_train.write(data_train))\n",
        "\n",
        "data_valid = \"\"\n",
        "for d in dialogs_valid:\n",
        "  data_valid += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_valid = open(\"fr_finetuned_valid.txt\",\"w\")\n",
        "print(file_valid.write(data_valid))\n",
        "\n",
        "data_test = \"\"\n",
        "for d in dialogs_test:\n",
        "  data_test += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_test = open(\"fr_finetuned_test.txt\",\"w\")\n",
        "print(file_test.write(data_test))\n",
        "\n",
        "# print(len(data_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17zEkLG-BCKj",
        "outputId": "87a6ccd6-5033-442a-a327-f9a3d3199024"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[\"bonjour que fais tu aujourd'hui?\",\n",
              "  \"je vais bien, je viens de quitter le travail et je suis fatigué, j'ai deux emplois.\"],\n",
              " [\"je viens de finir de regarder un film d'horreur\",\n",
              "  \"j'ai plutôt lu, j'ai lu environ 20 livres cette année.\"],\n",
              " [\"Hou la la ! j'aime un bon film d'horreur. aimer ce temps plus frais\",\n",
              "  'mais un bon film est toujours bon.'],\n",
              " ['Oui ! mon fils est au premier cycle du secondaire et je viens de le laisser le regarder aussi',\n",
              "  'je travaille aussi dans les films.'],\n",
              " [\"bon ! ! j'ai travaillé dans le domaine des services à la personne\",\n",
              "  \"oui c'est soigné, je stunt double, c'est tellement amusant et dur de travailler.\"],\n",
              " ['oui, je parie que vous pouvez vous blesser. ma femme travaille et je reste à la maison',\n",
              "  \"chouette, je n'ai qu'un seul parent alors maintenant j'aide ma maman.\"],\n",
              " [\"je parie qu'elle l'apprécie beaucoup.\",\n",
              "  \"elle m'a bien élevé, je suis comme elle.\"],\n",
              " ['mon père était toujours occupé à travailler chez lui',\n",
              "  'maintenant que je suis plus vieux home depot est mon jouet r us.']]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# data_test[:500]\n",
        "dialogs_test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGQ1O7UC08w_",
        "outputId": "868f70f8-10a1-47f7-cd5f-7ab2beb8f967"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['salut comment vas-tu ? Je me prépare à faire du guépard pour rester en forme.',\n",
              " \"vous devez être très rapide. la chasse est l'un de mes passe-temps préférés.\"]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dialogs_train[:1][0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWMwH86PNCbp"
      },
      "source": [
        "### Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjvTMvo85uma"
      },
      "outputs": [],
      "source": [
        "!cat fr_finetuned_test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE68W1ka5KxJ"
      },
      "outputs": [],
      "source": [
        "!parlai build_dict --task fromfile:parlaiformat --fromfile_datapath \"fr_finetuned_train.txt\" --dict-file \"fr_finetuned_train.dict\" \\\n",
        "    --dict_language french"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do_-nxIgwAn7"
      },
      "outputs": [],
      "source": [
        "finetuned_model_path = f'{mydrive_path}finetuned-XPersona-400m-beam_prmts_infrnc'\n",
        "init_model = 'zoo:blender/blender_3B/model'\n",
        "dict_file  = 'zoo:blender/blender_3B/model.dict'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "aabl4pkAhDXA",
        "outputId": "c35fffb1-7a0a-439b-8b99-dd1be58a6650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: missing operand\n",
            "Try 'mkdir --help' for more information.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-45f2bb4fbcbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mparlai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m TrainModel.main(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mparlai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mparlai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_agent_from_shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mparlai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStopTrainException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mparlai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: KeyboardInterrupt: "
          ]
        }
      ],
      "source": [
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    # use a small learning rate with ADAM optimizer\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    # warmup_updates=100,\n",
        "    validation_metric='ppl',\n",
        "    validation_every_n_epochs=0.25,\n",
        "    num_epochs = 1,\n",
        "    verbose = True,\n",
        "    \n",
        "    # depend on your gpu. If you have a V100, this is good\n",
        "    batchsize=8, fp16=True, fp16_impl='mem_efficient',\n",
        "    force_fp16_tokens = True,\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        "\n",
        "    # --beam-min-length 20 --beam-block-ngram 3 --beam-context-block-ngram 3 --beam-size 10 --inference beam\n",
        "    beam_min_length= 20,\n",
        "    beam_block_ngram= 3,\n",
        "    beam_context_block_ngram= 3,\n",
        "    beam_size= 10,\n",
        "\n",
        "    inference= \"beam\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwy69K0ehlEM"
      },
      "outputs": [],
      "source": [
        "print(f'{finetuned_model_path}/model')\n",
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat',\n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    dict_file= dict_file,\n",
        "\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    beam_min_length= 20,\n",
        "    beam_block_ngram= 3,\n",
        "    beam_context_block_ngram= 3,\n",
        "    beam_size= 10,\n",
        "\n",
        "    inference= \"beam\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV_6-0Atbv3x"
      },
      "outputs": [],
      "source": [
        "test_set_length = sum(len(d) for d in dialogs_test)\n",
        "model_path = f'{finetuned_model_path}/model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4NXUh64iGhP",
        "outputId": "625c5eb1-fb10-49c6-ea6b-db7cb6896d96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n",
            "/usr/local/lib/python3.7/dist-packages/parlai/core/torch_generator_agent.py:1610: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  hyp_ids = best_idxs // voc_size\n"
          ]
        }
      ],
      "source": [
        "# another_model_path = f'{mydrive_path}finetuned-beam_prmts_infrnc'\n",
        "\n",
        "!(parlai display_model \\\n",
        "    --task 'fromfile:parlaiformat' \\\n",
        "    --fromfile-datapath 'fr_finetuned' \\\n",
        "    --datatype test \\\n",
        "    --fromfile-datatype-extension True \\\n",
        "    --model-file $model_path \\\n",
        "    --dict-file $dict_file \\\n",
        "    --num-examples $test_set_length \\\n",
        "    --skip-generation False \\\n",
        "    --beam-min-length 20 \\\n",
        "    --beam-block-ngram 3 \\\n",
        "    --beam-context-block-ngram 3 \\\n",
        "    --beam-size 10 \\\n",
        "    --inference \"beam\") \\\n",
        "    > \"07-finetuned-beam_prmts_infrnc-dict-3epochs-testset-output.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM23FJnphHFd"
      },
      "outputs": [],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat',\n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    datatype= \"test\",\n",
        "    num_examples=test_set_length,\n",
        "    skip_generation=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLyR1jVwo64O"
      },
      "outputs": [],
      "source": [
        "!parlai train_model -t blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues \\\n",
        "    --multitask-weights 1,3,3,3 -veps 0.25 --attention-dropout 0.0 --batchsize 128 \\\n",
        "    --model transformer/generator --embedding-size 2560 --ffn-size 10240 --variant prelayernorm --n-heads 32 \\\n",
        "    --n-positions 128 --n-encoder-layers 2 --n-decoder-layers 24 --history-add-global-end-token end \\\n",
        "    --delimiter '  ' --dict-tokenizer bytelevelbpe  --dropout 0.1 --fp16 True --init-model zoo:blender/reddit_3B/model \\\n",
        "    --dict-file zoo:blender/reddit_3B/model.dict --label-truncate 128 --log_every_n_secs 10 -lr 7e-06 --lr-scheduler reduceonplateau --lr-scheduler-patience 3 --optimizer adam --relu-dropout 0.0 --activation gelu --model-parallel true --save-after-valid True --text-truncate 128 --truncate 128 --warmup_updates 100 --fp16-impl mem_efficient --update-freq 2 --gradient-clip 0.1 --skip-generation True -vp 10 -vmt ppl -vmm min --model-file /tmp/test_train_27B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPk5TVnoK9DN"
      },
      "source": [
        "## **Fine-tuning (with French Reddit Discussion dataset)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5yjQ-TTMEi0"
      },
      "source": [
        "### Convert dataset to ParlAI  format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VOKvmEpMEjA",
        "outputId": "d8769074-3c54-4e27-a0cd-3f330900d207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Xpersona'...\n",
            "remote: Enumerating objects: 282, done.\u001b[K\n",
            "remote: Counting objects: 100% (282/282), done.\u001b[K\n",
            "remote: Compressing objects: 100% (201/201), done.\u001b[K\n",
            "remote: Total 282 (delta 96), reused 241 (delta 78), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (282/282), 45.00 MiB | 19.95 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n",
            "Checking out files: 100% (218/218), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HLTCHKUST/Xpersona.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJdI6ImvMEjB"
      },
      "source": [
        "[All XPersona datasets](https://github.com/HLTCHKUST/Xpersona/tree/master/dataset) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBsoJiE3MEjB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_train_corrected.json','r') as f:\n",
        "   train_data = json.load(f)\n",
        "\n",
        "dialogs_train = pd.DataFrame(train_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_valid_human_annotated.json','r') as f:\n",
        "   valid_data = json.load(f)\n",
        "\n",
        "dialogs_valid = pd.DataFrame(valid_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_test_human_annotated.json','r') as f:\n",
        "   text_data = json.load(f)\n",
        "\n",
        "dialogs_test = pd.DataFrame(text_data)['dialogue'].tolist()\n",
        "\n",
        "# dialogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IMBUb1OMEjB",
        "outputId": "c09f30a4-b9a3-4869-92f8-a06dac576778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16878 248 249\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[['salut comment vas-tu ? Je me prépare à faire du guépard pour rester en forme.',\n",
              "   \"vous devez être très rapide. la chasse est l'un de mes passe-temps préférés.\"],\n",
              "  [\"je suis ! pour mon hobby j'aime faire la mise en conserve ou un peu tailler.\",\n",
              "   \"je remodèle aussi des maisons quand je ne suis pas à la chasse à l'arc.\"],\n",
              "  [\"c'est bien. quand j'étais au lycée, je me suis placé 6ème au 100m dash!\",\n",
              "   \"c'est génial . avez-vous une saison ou une période préférée de l'année?\"],\n",
              "  [\"Non . mais j'ai une viande préférée car c'est tout ce que je mange exclusivement.\",\n",
              "   'quelle est votre viande préférée à manger?'],\n",
              "  ['je devrais dire sa côte de bœuf. avez-vous des aliments préférés?',\n",
              "   \"j'aime le poulet ou les macaronis et le fromage.\"],\n",
              "  [\"avez-vous prévu quelque chose pour aujourd'hui? je pense que je vais faire de la mise en conserve.\",\n",
              "   'je vais regarder le football. que conservez-vous?'],\n",
              "  ['je pense que je vais pouvoir un peu de confiture. jouez-vous aussi pour le plaisir?',\n",
              "   \"si j'ai le temps en dehors des maisons de chasse et de rénovation. ce qui n'est pas grand chose!\"]]]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "  print(len(dialogs_train), len(dialogs_valid), len(dialogs_test))\n",
        "  \n",
        "  dialogs_train[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CRdnAbyMEjC",
        "outputId": "77c22dfe-857d-478a-8420-0c9cd7154ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15776427\n",
            "257114\n",
            "262425\n"
          ]
        }
      ],
      "source": [
        "# pd.read_json(f,orient='records', encoding='utf-8',lines=True)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# dialogs = pd.read_json('XPersona/dataset/Fr_persona_split_train_corrected.json')['dialogue'].tolist()\n",
        "\n",
        "\n",
        "text = \"\"\n",
        "\n",
        "def transfer_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, each_two_turn in enumerate(d):\n",
        "    u1 = each_two_turn[0]\n",
        "    u2 = each_two_turn[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "\n",
        "# d = dialogs[0]\n",
        "# print(d)\n",
        "# print(transfer_dialog(d))\n",
        "\n",
        "data_train = \"\"\n",
        "for d in dialogs_train:\n",
        "  data_train += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_train = open(\"fr_finetuned_train.txt\",\"w\")\n",
        "print(file_train.write(data_train))\n",
        "\n",
        "data_valid = \"\"\n",
        "for d in dialogs_valid:\n",
        "  data_valid += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_valid = open(\"fr_finetuned_valid.txt\",\"w\")\n",
        "print(file_valid.write(data_valid))\n",
        "\n",
        "data_test = \"\"\n",
        "for d in dialogs_test:\n",
        "  data_test += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_test = open(\"fr_finetuned_test.txt\",\"w\")\n",
        "print(file_test.write(data_test))\n",
        "\n",
        "# print(len(data_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBmrkx6sMEjC",
        "outputId": "87a6ccd6-5033-442a-a327-f9a3d3199024"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[\"bonjour que fais tu aujourd'hui?\",\n",
              "  \"je vais bien, je viens de quitter le travail et je suis fatigué, j'ai deux emplois.\"],\n",
              " [\"je viens de finir de regarder un film d'horreur\",\n",
              "  \"j'ai plutôt lu, j'ai lu environ 20 livres cette année.\"],\n",
              " [\"Hou la la ! j'aime un bon film d'horreur. aimer ce temps plus frais\",\n",
              "  'mais un bon film est toujours bon.'],\n",
              " ['Oui ! mon fils est au premier cycle du secondaire et je viens de le laisser le regarder aussi',\n",
              "  'je travaille aussi dans les films.'],\n",
              " [\"bon ! ! j'ai travaillé dans le domaine des services à la personne\",\n",
              "  \"oui c'est soigné, je stunt double, c'est tellement amusant et dur de travailler.\"],\n",
              " ['oui, je parie que vous pouvez vous blesser. ma femme travaille et je reste à la maison',\n",
              "  \"chouette, je n'ai qu'un seul parent alors maintenant j'aide ma maman.\"],\n",
              " [\"je parie qu'elle l'apprécie beaucoup.\",\n",
              "  \"elle m'a bien élevé, je suis comme elle.\"],\n",
              " ['mon père était toujours occupé à travailler chez lui',\n",
              "  'maintenant que je suis plus vieux home depot est mon jouet r us.']]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# data_test[:500]\n",
        "dialogs_test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk4NFkKbMEjD",
        "outputId": "868f70f8-10a1-47f7-cd5f-7ab2beb8f967"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['salut comment vas-tu ? Je me prépare à faire du guépard pour rester en forme.',\n",
              " \"vous devez être très rapide. la chasse est l'un de mes passe-temps préférés.\"]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dialogs_train[:1][0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWKlYOA4OJLc"
      },
      "source": [
        "### Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbpHSTbXK9Dq"
      },
      "outputs": [],
      "source": [
        "# !cat fr_finetuned_test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7Zy6kVBK9Dr"
      },
      "outputs": [],
      "source": [
        "# !parlai build_dict --task fromfile:parlaiformat --fromfile_datapath \"fr_finetuned_train.txt\" --dict-file \"fr_finetuned_train.dict\" \\\n",
        "#     --dict_language french"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_ln6O2vK9Dr"
      },
      "outputs": [],
      "source": [
        "finetuned_model_path = f'{mydrive_path}finetuned-fr_reddit'\n",
        "init_model = 'zoo:blender/blender_90M/model'\n",
        "dict_file  = 'zoo:blender/blender_90M/model.dict'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "03YcSUX6K9Ds",
        "outputId": "cfac971f-6ed6-4821-b204-6adaef2bdbb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                                 /@&%###%&&@@#\n",
            "                      .,*/((((##@@@&%%#%%&&@@@&%%#/*.\n",
            "             #@@&&&%%%%##(((///*****//(((###%%%&&&@@@@@&&%#%%#.\n",
            "         .%&@@@@@&&&%%%####((((////((((####%%%&&&@@@@@&&%%#%%####,\n",
            "           ./,,#(//**,,.....,,,,***////((((########%%%%%%%%###(((\n",
            "              /*(//**,,,....,,,,***////((((########%%%%%%%%###(#%*\n",
            "               (*,...      ...,,,***//////((((((///////(/*...,/#@@@(\n",
            "               **,,..         ...,,,,,,,,,,........,,*///*...*(#@@@@&&*\n",
            "               ./,,..          ...,,,,,,,,,........,,*//*,...*#/,,,,,/%#\n",
            "                (*,..          ...,,,,,,,,,........,,*//*,..,/(      .,#(\n",
            "                **,..          ...,,,,,,,,,.........,*//*,..,((       .,(#\n",
            "                 /*,..          ....,,,,,,,.....  ..,***,,,,(#         ..#&\n",
            "                 **,..          ....,,,,,,,....   ..,***,,,*#.         .,%@\n",
            "                 ./,...       B l e n d e r B o t ...***,,,*#          .*%@\n",
            "                  /*,..          ...,,,,,,,....    .,**,,,,/#         ..(%/\n",
            "                  /*,,..         ...,,,,,,,...    ..,*,,,,,(.         ..#&\n",
            "                  ,/*,..         ...,,,,,,,...    ..,*,,,,*#         ..*%(\n",
            "                   /*,..         ...,,,,,.....    ..,*,*,,/(         ..#&\n",
            "                   /**,..        ...,,,,.....    ...,***,*(.       ,,(%.\n",
            "                    (/*,,..      ....,,.....     ...,****(&@@@&&&#,\n",
            "                     (/*,,...   .....,,......     ..,****#@,\n",
            "                     *(/*,,/....*(###%(,(%%##(*.  ./,,**(\n",
            "                      ,//**(,........,/((#.........*,**(\n",
            "                      .(#//*,,,,,,.*.,/((%/,,.....,,*/@\n",
            "                    ((######//****,/.,/(#%#***,***(&@@@@@(\n",
            "                   *&%%#####%%%%%%%#//(#%&%%&&@@@@@@@@@@@@*\n",
            "                   &&%%%###((((((####%%%%&&&&&@@@@@@@@@@&&@.\n",
            "                  *##%%%##(((((((####%%%%%&&&&@@@@@@@@@&#/*,\n",
            "                 .(##%#/,  .,*((##%%%&&&&%%%#####%&&@&&%#(/*.\n",
            "                 /(###(,   .,*/(##%%%&&&&%%%######%&&&&%#(/*,\n",
            "                */((((*.  ..,//((##%%%%%%%%#######%&&&&%%#(/*,\n",
            "               .//(((/,   .,*//((###%%%%%%########%%&&&%%#((/,.\n",
            "              .&####(((((((((######%%%%%%%%&&&&&&&@@@@@@@@@@@@@#\n",
            "               *&#.   .*/((((#######%%%%%%&&&&&&&@@@@@#/.   (&/\n",
            "07:32:50 | building data: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/BST0B.tgz\n",
            "07:32:50 | Downloading http://parl.ai/downloads/_models/blender/BST0B.tgz to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/BST0B.tgz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading BST0B.tgz: 100%|██████████| 161M/161M [00:02<00:00, 54.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "07:32:56 | building dictionary first...\n",
            "07:32:56 | No model with opt yet at: /content/drive/MyDrive/colabs/blender-models/finetuned-fr_reddit/model(.opt)\n",
            "07:32:56 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,verbose: True,is_debug: False,datapath: /usr/local/lib/python3.7/dist-packages/data,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: fr_finetuned,fromfile_datatype_extension: True,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,beam_block_full_context: True,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,interactive_mode: False,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None\u001b[0m\n",
            "07:32:56 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues --numthreads 1 --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 16 --num-epochs -1 --save-every-n-secs 60.0 --save-after-valid True --validation-max-exs 20000 --validation-patience 15 --validation-metric-mode min --log-every-n-secs 2 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --num-topics 5 --train-experiencer-only False --dropout 0.1 --learn-positional-embeddings True --skip-generation False --fp16-impl apex --optimizer adamax --learningrate 7.5e-06 --max-lr-steps -1 --warmup-updates -1 --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
            "07:32:56 | Using CUDA\n",
            "07:32:56 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict\n",
            "07:32:56 | num words = 54944\n",
            "07:32:57 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3effb23e670a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"beam\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;31m# Create model and assign it to the specified task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/agents.py\u001b[0m in \u001b[0;36mcreate_agent\u001b[0;34m(opt, requireModelExists)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;31m# loaded ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0mcompare_init_model_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequireModelExists\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'load'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;31m# double check that we didn't forget to set model_file on loadable model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, opt, shared)\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsdp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfsdp_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfsdp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay_halving\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# load the block_list for beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mhalf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \"\"\"\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbfloat16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcompute_should_use_set_data\u001b[0;34m(tensor, tensor_applied)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_compatible_shallow_copy_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 \u001b[0;31m# If the new tensor has compatible tensor type as the existing tensor,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0;31m# the current behavior is to change the tensor in-place using `.data =`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    # use a small learning rate with ADAM optimizer\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    validation_metric='ppl',\n",
        "    validation_every_n_epochs=0.25,\n",
        "    num_epochs = 3,\n",
        "    verbose = True,\n",
        "    \n",
        "    # depend on your gpu. If you have a V100, this is good\n",
        "    batchsize=8, fp16=True, fp16_impl='mem_efficient',\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        "\n",
        "    # --beam-min-length 20 --beam-block-ngram 3 --beam-context-block-ngram 3 --beam-size 10 --inference beam\n",
        "    beam_min_length= 20,\n",
        "    beam_block_ngram= 3,\n",
        "    beam_context_block_ngram= 3,\n",
        "    beam_size= 10,\n",
        "\n",
        "    inference= \"beam\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSP58B3dK9Dt",
        "outputId": "141bb191-90aa-4156-ada5-764b4eaab82f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colabs/blender-models/finetuned-dict/model\n",
            "13:29:27 | \u001b[33mOverriding opt[\"dict_file\"] to fr_finetuned_train.dict (previously: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict)\u001b[0m\n",
            "13:29:27 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "13:29:27 | Using CUDA\n",
            "13:29:27 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-dict/model.dict\n",
            "13:29:28 | num words = 54944\n",
            "13:29:30 | Total parameters: 87,508,992 (86,984,704 trainable)\n",
            "13:29:30 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-dict/model\n",
            "13:29:34 | creating task(s): fromfile:parlaiformat\n",
            "13:29:34 | Loading ParlAI text data: fr_finetuned_valid.txt\n",
            "13:29:34 | Opt:\n",
            "13:29:34 |     activation: gelu\n",
            "13:29:34 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "13:29:34 |     adam_eps: 1e-08\n",
            "13:29:34 |     add_p1_after_newln: False\n",
            "13:29:34 |     aggregate_micro: False\n",
            "13:29:34 |     allow_missing_init_opts: False\n",
            "13:29:34 |     attention_dropout: 0.0\n",
            "13:29:34 |     batchsize: 8\n",
            "13:29:34 |     beam_block_full_context: True\n",
            "13:29:34 |     beam_block_list_filename: None\n",
            "13:29:34 |     beam_block_ngram: 3\n",
            "13:29:34 |     beam_context_block_ngram: 3\n",
            "13:29:34 |     beam_delay: 30\n",
            "13:29:34 |     beam_length_penalty: 0.65\n",
            "13:29:34 |     beam_min_length: 20\n",
            "13:29:34 |     beam_size: 10\n",
            "13:29:34 |     betas: '[0.9, 0.999]'\n",
            "13:29:34 |     bpe_add_prefix_space: None\n",
            "13:29:34 |     bpe_debug: False\n",
            "13:29:34 |     bpe_dropout: None\n",
            "13:29:34 |     bpe_merge: None\n",
            "13:29:34 |     bpe_vocab: None\n",
            "13:29:34 |     checkpoint_activations: False\n",
            "13:29:34 |     compute_tokenized_bleu: False\n",
            "13:29:34 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "13:29:34 |     datatype: train\n",
            "13:29:34 |     delimiter: '\\n'\n",
            "13:29:34 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "13:29:34 |     dict_endtoken: __end__\n",
            "13:29:34 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-dict/model.dict\n",
            "13:29:34 |     dict_include_test: False\n",
            "13:29:34 |     dict_include_valid: False\n",
            "13:29:34 |     dict_initpath: None\n",
            "13:29:34 |     dict_language: english\n",
            "13:29:34 |     dict_loaded: True\n",
            "13:29:34 |     dict_lower: True\n",
            "13:29:34 |     dict_max_ngram_size: -1\n",
            "13:29:34 |     dict_maxexs: -1\n",
            "13:29:34 |     dict_maxtokens: -1\n",
            "13:29:34 |     dict_minfreq: 0\n",
            "13:29:34 |     dict_nulltoken: __null__\n",
            "13:29:34 |     dict_starttoken: __start__\n",
            "13:29:34 |     dict_textfields: text,labels\n",
            "13:29:34 |     dict_tokenizer: bpe\n",
            "13:29:34 |     dict_unktoken: __unk__\n",
            "13:29:34 |     display_add_fields: \n",
            "13:29:34 |     display_examples: False\n",
            "13:29:34 |     download_path: None\n",
            "13:29:34 |     dropout: 0.0\n",
            "13:29:34 |     dynamic_batching: full\n",
            "13:29:34 |     embedding_projection: random\n",
            "13:29:34 |     embedding_size: 512\n",
            "13:29:34 |     embedding_type: random\n",
            "13:29:34 |     embeddings_scale: True\n",
            "13:29:34 |     eval_batchsize: None\n",
            "13:29:34 |     eval_dynamic_batching: None\n",
            "13:29:34 |     evaltask: None\n",
            "13:29:34 |     ffn_size: 2048\n",
            "13:29:34 |     final_extra_opt: \n",
            "13:29:34 |     force_fp16_tokens: True\n",
            "13:29:34 |     fp16: True\n",
            "13:29:34 |     fp16_impl: mem_efficient\n",
            "13:29:34 |     fromfile_datapath: fr_finetuned\n",
            "13:29:34 |     fromfile_datatype_extension: True\n",
            "13:29:34 |     gpu: -1\n",
            "13:29:34 |     gradient_clip: 0.1\n",
            "13:29:34 |     hide_labels: False\n",
            "13:29:34 |     history_add_global_end_token: None\n",
            "13:29:34 |     history_reversed: False\n",
            "13:29:34 |     history_size: -1\n",
            "13:29:34 |     image_cropsize: 224\n",
            "13:29:34 |     image_mode: raw\n",
            "13:29:34 |     image_size: 256\n",
            "13:29:34 |     inference: beam\n",
            "13:29:34 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model\n",
            "13:29:34 |     init_opt: None\n",
            "13:29:34 |     interactive_mode: False\n",
            "13:29:34 |     invsqrt_lr_decay_gamma: -1\n",
            "13:29:34 |     is_debug: False\n",
            "13:29:34 |     label_truncate: 128\n",
            "13:29:34 |     learn_positional_embeddings: False\n",
            "13:29:34 |     learningrate: 1e-05\n",
            "13:29:34 |     log_every_n_secs: -1\n",
            "13:29:34 |     log_every_n_steps: 50\n",
            "13:29:34 |     loglevel: info\n",
            "13:29:34 |     lr_scheduler: reduceonplateau\n",
            "13:29:34 |     lr_scheduler_decay: 0.5\n",
            "13:29:34 |     lr_scheduler_patience: 3\n",
            "13:29:34 |     max_train_steps: -1\n",
            "13:29:34 |     max_train_time: -1\n",
            "13:29:34 |     metrics: default\n",
            "13:29:34 |     model: transformer/generator\n",
            "13:29:34 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-dict/model\n",
            "13:29:34 |     model_parallel: False\n",
            "13:29:34 |     momentum: 0\n",
            "13:29:34 |     multitask_weights: [1]\n",
            "13:29:34 |     mutators: None\n",
            "13:29:34 |     n_decoder_layers: -1\n",
            "13:29:34 |     n_encoder_layers: -1\n",
            "13:29:34 |     n_heads: 16\n",
            "13:29:34 |     n_layers: 8\n",
            "13:29:34 |     n_positions: 512\n",
            "13:29:34 |     n_segments: 0\n",
            "13:29:34 |     nesterov: True\n",
            "13:29:34 |     no_cuda: False\n",
            "13:29:34 |     num_epochs: 3.0\n",
            "13:29:34 |     num_examples: 20\n",
            "13:29:34 |     num_workers: 0\n",
            "13:29:34 |     nus: [0.7]\n",
            "13:29:34 |     optimizer: mem_eff_adam\n",
            "13:29:34 |     output_scaling: 1.0\n",
            "13:29:34 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'fr_finetuned', 'fromfile_datatype_extension': True, 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-dict/model', 'dict_file': 'fr_finetuned_train.dict', 'num_examples': '20', 'skip_generation': False, 'beam_min_length': 20, 'beam_block_ngram': 3, 'beam_context_block_ngram': 3, 'beam_size': 10, 'inference': 'beam'}\"\n",
            "13:29:34 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "13:29:34 |     person_tokens: False\n",
            "13:29:34 |     rank_candidates: False\n",
            "13:29:34 |     relu_dropout: 0.0\n",
            "13:29:34 |     save_after_valid: False\n",
            "13:29:34 |     save_every_n_secs: -1\n",
            "13:29:34 |     share_word_embeddings: True\n",
            "13:29:34 |     short_final_eval: False\n",
            "13:29:34 |     skip_generation: False\n",
            "13:29:34 |     special_tok_lst: None\n",
            "13:29:34 |     split_lines: False\n",
            "13:29:34 |     starttime: Mar17_11-53\n",
            "13:29:34 |     task: fromfile:parlaiformat\n",
            "13:29:34 |     temperature: 1.0\n",
            "13:29:34 |     tensorboard_log: False\n",
            "13:29:34 |     tensorboard_logdir: None\n",
            "13:29:34 |     text_truncate: 512\n",
            "13:29:34 |     topk: 10\n",
            "13:29:34 |     topp: 0.9\n",
            "13:29:34 |     truncate: -1\n",
            "13:29:34 |     update_freq: 1\n",
            "13:29:34 |     use_reply: label\n",
            "13:29:34 |     validation_cutoff: 1.0\n",
            "13:29:34 |     validation_every_n_epochs: 0.25\n",
            "13:29:34 |     validation_every_n_secs: -1\n",
            "13:29:34 |     validation_every_n_steps: -1\n",
            "13:29:34 |     validation_max_exs: -1\n",
            "13:29:34 |     validation_metric: ppl\n",
            "13:29:34 |     validation_metric_mode: None\n",
            "13:29:34 |     validation_patience: 10\n",
            "13:29:34 |     validation_share_agent: False\n",
            "13:29:34 |     variant: xlm\n",
            "13:29:34 |     verbose: False\n",
            "13:29:34 |     wandb_entity: None\n",
            "13:29:34 |     wandb_log: False\n",
            "13:29:34 |     wandb_name: None\n",
            "13:29:34 |     wandb_project: None\n",
            "13:29:34 |     warmup_rate: 0.0001\n",
            "13:29:34 |     warmup_updates: 100\n",
            "13:29:34 |     weight_decay: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/parlai/core/torch_generator_agent.py:1610: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  hyp_ids = best_idxs // voc_size\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m- - - NEW EPISODE: fr_finetuned_valid.txt- - -\u001b[0;0m\n",
            "\u001b[0msalut ! comment vas-tu ce soir ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: je le fais bien . juste se détendre avec mes deux chiens.\u001b[0;0m\n",
            "\u001b[0;95m     model: salut , je vais bien . comment allez - vous aujourd ' hui ?\u001b[0;0m\n",
            "\u001b[0mgénial . dans mon temps libre je fais du bénévolat.\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est bien. quel genre de travail bénévole faites-vous?\u001b[0;0m\n",
            "\u001b[0;95m     model: avez - vous des animaux domestiques ? j ' aime lire .\u001b[0;0m\n",
            "\u001b[0mje travaille dans un refuge pour sans-abri dans ma ville.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Bien pour vous . vous aimez les voitures anciennes? j'ai deux anciennes mustangs.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d ' entendre ça . avez - vous des animaux domestiques ?\u001b[0;0m\n",
            "\u001b[0mcool . pas vraiment dans les voitures. mon travail de jour est dur.\u001b[0;0m\n",
            "\u001b[1;94m    labels: les voitures sont mon truc. voitures anciennes . j'adore travailler sur cela. est-ce que tu aimes ça ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d ' entendre ça . avez - vous des animaux domestiques ?\u001b[0;0m\n",
            "\u001b[0moui, j'aime la foule, faire connaissance avec les gens.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je n'ai pas pensé à l'aspect foule de la lutte. je n'aime pas les foules.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d ' entendre ça . avez - vous des animaux domestiques ?\u001b[0;0m\n",
            "\u001b[0mcompris . quand j'ai besoin d'être seul, je m'entraîne plusieurs fois par semaine.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je suis d'accord . travailler est un excellent moyen de faire ses doléances. Aimez-vous la musique country ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d ' entendre ça . avez - vous des animaux domestiques ?\u001b[0;0m\n",
            "\u001b[0mun petit peu . je peux admirer taylor swift.\u001b[0;0m\n",
            "\u001b[1;94m    labels: lol. elle est plus pop maintenant. ses vieux trucs étaient country. j'aime le country classique.\u001b[0;0m\n",
            "\u001b[0;95m     model: cela semble intéressant . avez - vous des animaux domestiques ?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finetuned_valid.txt- - -\u001b[0;0m\n",
            "\u001b[0msalut comment allez-vous ? je vais bien et vous?\u001b[0;0m\n",
            "\u001b[1;94m    labels: je vais bien. sirotant juste du thé. que faites-vous pour le travail?\u001b[0;0m\n",
            "\u001b[0;95m     model: j ' aime vra__unk__ bien . je suis assez jeune . toi ?\u001b[0;0m\n",
            "\u001b[0mje faisais de l'aide à domicile mais maintenant je suis handicapé.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je suis désolé de l'entendre . Qu'est-il arrivé\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est génial . j ' aime faire de la randonnée .\u001b[0;0m\n",
            "\u001b[0mj'ai obtenu mon diplôme et obtenu mon permis de travail un certain temps et est devenu je vais.\u001b[0;0m\n",
            "\u001b[1;94m    labels: alors que faites-vous maintenant pour vous amuser? J'aime lire .\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est génial . avez - vous déjà été à l ' école ?\u001b[0;0m\n",
            "\u001b[0mj'ai des chiens et je les promène. et un chat.\u001b[0;0m\n",
            "\u001b[1;94m    labels: quel genre de chiens avez-vous?\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est génial . avez - vous des animaux domestiques ?\u001b[0;0m\n",
            "\u001b[0mj'aime lire sur michael jackson et la psychologie aujourd'hui.\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'aime lire . j'ai une grande bibliothèque chez moi.\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est génial . je n ' ai pas d ' animaux de compagnie .\u001b[0;0m\n",
            "\u001b[0mc'est très agréable j'aime lire et lire des revues.\u001b[0;0m\n",
            "\u001b[1;94m    labels: j'ai du mal à entendre donc la lecture est mon endroit heureux.\u001b[0;0m\n",
            "\u001b[0;95m     model: quelle est votre couleur préférée avec moi ?\u001b[0;0m\n",
            "\u001b[0mcool ça va. ma maman était très dure avec moi.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je travaille comme guide d'excursion de musée de malentendants.\u001b[0;0m\n",
            "\u001b[0;95m     model: cela semble intéressant . quel est votre plat préféré ?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finetuned_valid.txt- - -\u001b[0;0m\n",
            "\u001b[0msalut ! vous aimez les tortues?\u001b[0;0m\n",
            "\u001b[1;94m    labels: en fait, personnellement, je préfère le chat.\u001b[0;0m\n",
            "\u001b[0;95m     model: pas vra__unk__ , mais j ' aime la musique . et toi ?\u001b[0;0m\n",
            "\u001b[0mj'ai une tortue son nom est rapide. les chatons sont sympas aussi, tho!\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est un nom adorable pour une tortue. j'ai 2 chats\u001b[0;0m\n",
            "\u001b[0;95m     model: j ' aime aussi le chat , mais je n ' ai jamais entendu parler .\u001b[0;0m\n",
            "\u001b[0mquels sont vos noms de chatons?\u001b[0;0m\n",
            "\u001b[1;94m    labels: neige et hiver, du nom de ma saison préférée\u001b[0;0m\n",
            "\u001b[0;95m     model: je n ' ai pas de noms , mais j ' aime les chiens .\u001b[0;0m\n",
            "\u001b[0mJ'aime ça ! je vais à l'école maternelle.\u001b[0;0m\n",
            "\u001b[1;94m    labels: oh tu es si jeune!\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d ' entendre ça . quel âge avez - vous ?\u001b[0;0m\n",
            "\u001b[0mquel âge avez-vous ? j'ai fêté mes quatre ans au jour de mon anniversaire!\u001b[0;0m\n",
            "\u001b[1;94m    labels: je suis une vieille femme. j'ai gagné une médaille d'or en 1993\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d ' entendre ça . je n ' ai pas d ' enfants\u001b[0;0m\n",
            "\u001b[0mHou la la ! ! tu as gagné l'or! êtes-vous riche ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: non . je dépense tout mon argent sur des chapeaux! j'en ai plus d'un millier\u001b[0;0m\n",
            "\u001b[0;95m     model: non , je ne suis pas riche , mais j ' aimerais être riche\u001b[0;0m\n"
          ]
        }
      ],
      "source": [
        "print(f'{finetuned_model_path}/model')\n",
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat',\n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    dict_file= dict_file,\n",
        "\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    beam_min_length= 20,\n",
        "    beam_block_ngram= 3,\n",
        "    beam_context_block_ngram= 3,\n",
        "    beam_size= 10,\n",
        "\n",
        "    inference= \"beam\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCHLQ9wvK9Dv"
      },
      "outputs": [],
      "source": [
        "test_set_length = sum(len(d) for d in dialogs_test)\n",
        "model_path = f'{finetuned_model_path}/model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBJ9WeG3K9Dv",
        "outputId": "625c5eb1-fb10-49c6-ea6b-db7cb6896d96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n",
            "/usr/local/lib/python3.7/dist-packages/parlai/core/torch_generator_agent.py:1610: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  hyp_ids = best_idxs // voc_size\n"
          ]
        }
      ],
      "source": [
        "# another_model_path = f'{mydrive_path}finetuned-beam_prmts_infrnc'\n",
        "\n",
        "!(parlai display_model \\\n",
        "    --task 'fromfile:parlaiformat' \\\n",
        "    --fromfile-datapath 'fr_finetuned' \\\n",
        "    --datatype test \\\n",
        "    --fromfile-datatype-extension True \\\n",
        "    --model-file $model_path \\\n",
        "    --dict-file $dict_file \\\n",
        "    --num-examples $test_set_length \\\n",
        "    --skip-generation False \\\n",
        "    --beam-min-length 20 \\\n",
        "    --beam-block-ngram 3 \\\n",
        "    --beam-context-block-ngram 3 \\\n",
        "    --beam-size 10 \\\n",
        "    --inference \"beam\") \\\n",
        "    > \"07-finetuned-beam_prmts_infrnc-dict-3epochs-testset-output.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZUN-StxK9Dw"
      },
      "outputs": [],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat',\n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    datatype= \"test\",\n",
        "    num_examples=test_set_length,\n",
        "    skip_generation=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkkZEusOn-z-"
      },
      "source": [
        "## **Fine-tuning (400M model)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMhngZUDn-0I"
      },
      "source": [
        "### Convert dataset to ParlAI  format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuitIjumn-0J",
        "outputId": "30c64f65-f16f-4666-a143-a6b11a338797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Xpersona' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HLTCHKUST/Xpersona.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtnAcMgin-0K"
      },
      "source": [
        "[All XPersona datasets](https://github.com/HLTCHKUST/Xpersona/tree/master/dataset) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gizngqoNn-0K"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_train_corrected.json','r') as f:\n",
        "   train_data = json.load(f)\n",
        "\n",
        "dialogs_train = pd.DataFrame(train_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_valid_human_annotated.json','r') as f:\n",
        "   valid_data = json.load(f)\n",
        "\n",
        "dialogs_valid = pd.DataFrame(valid_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_test_human_annotated.json','r') as f:\n",
        "   text_data = json.load(f)\n",
        "\n",
        "dialogs_test = pd.DataFrame(text_data)['dialogue'].tolist()\n",
        "\n",
        "# dialogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N85dMEUcn-0L",
        "outputId": "fe2fb81d-65a8-4fa1-fada-5cd552d87509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15776427\n",
            "257114\n",
            "262425\n"
          ]
        }
      ],
      "source": [
        "# pd.read_json(f,orient='records', encoding='utf-8',lines=True)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# dialogs = pd.read_json('XPersona/dataset/Fr_persona_split_train_corrected.json')['dialogue'].tolist()\n",
        "\n",
        "\n",
        "text = \"\"\n",
        "\n",
        "def transfer_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, each_two_turn in enumerate(d):\n",
        "    u1 = each_two_turn[0]\n",
        "    u2 = each_two_turn[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "\n",
        "# d = dialogs[0]\n",
        "# print(d)\n",
        "# print(transfer_dialog(d))\n",
        "\n",
        "data_train = \"\"\n",
        "for d in dialogs_train:\n",
        "  data_train += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_train = open(\"fr_finetuned_train.txt\",\"w\")\n",
        "print(file_train.write(data_train))\n",
        "\n",
        "data_valid = \"\"\n",
        "for d in dialogs_valid:\n",
        "  data_valid += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_valid = open(\"fr_finetuned_valid.txt\",\"w\")\n",
        "print(file_valid.write(data_valid))\n",
        "\n",
        "data_test = \"\"\n",
        "for d in dialogs_test:\n",
        "  data_test += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_test = open(\"fr_finetuned_test.txt\",\"w\")\n",
        "print(file_test.write(data_test))\n",
        "\n",
        "# print(len(data_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFHdCs8cn-0N"
      },
      "source": [
        "### Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGmZitgEn-0O"
      },
      "outputs": [],
      "source": [
        "finetuned_model_path = f'{mydrive_path}finetuned-400m'\n",
        "init_model = 'zoo:blender/blender_400Mdistill/model'\n",
        "dict_file  = 'zoo:blender/blender_400Mdistill/model.dict'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPRM1wNAn-0O",
        "outputId": "6991b25f-ab2a-4044-9c6d-6ba2ef528cac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                                 /@&%###%&&@@#\n",
            "                      .,*/((((##@@@&%%#%%&&@@@&%%#/*.\n",
            "             #@@&&&%%%%##(((///*****//(((###%%%&&&@@@@@&&%#%%#.\n",
            "         .%&@@@@@&&&%%%####((((////((((####%%%&&&@@@@@&&%%#%%####,\n",
            "           ./,,#(//**,,.....,,,,***////((((########%%%%%%%%###(((\n",
            "              /*(//**,,,....,,,,***////((((########%%%%%%%%###(#%*\n",
            "               (*,...      ...,,,***//////((((((///////(/*...,/#@@@(\n",
            "               **,,..         ...,,,,,,,,,,........,,*///*...*(#@@@@&&*\n",
            "               ./,,..          ...,,,,,,,,,........,,*//*,...*#/,,,,,/%#\n",
            "                (*,..          ...,,,,,,,,,........,,*//*,..,/(      .,#(\n",
            "                **,..          ...,,,,,,,,,.........,*//*,..,((       .,(#\n",
            "                 /*,..          ....,,,,,,,.....  ..,***,,,,(#         ..#&\n",
            "                 **,..          ....,,,,,,,....   ..,***,,,*#.         .,%@\n",
            "                 ./,...       B l e n d e r B o t ...***,,,*#          .*%@\n",
            "                  /*,..          ...,,,,,,,....    .,**,,,,/#         ..(%/\n",
            "                  /*,,..         ...,,,,,,,...    ..,*,,,,,(.         ..#&\n",
            "                  ,/*,..         ...,,,,,,,...    ..,*,,,,*#         ..*%(\n",
            "                   /*,..         ...,,,,,.....    ..,*,*,,/(         ..#&\n",
            "                   /**,..        ...,,,,.....    ...,***,*(.       ,,(%.\n",
            "                    (/*,,..      ....,,.....     ...,****(&@@@&&&#,\n",
            "                     (/*,,...   .....,,......     ..,****#@,\n",
            "                     *(/*,,/....*(###%(,(%%##(*.  ./,,**(\n",
            "                      ,//**(,........,/((#.........*,**(\n",
            "                      .(#//*,,,,,,.*.,/((%/,,.....,,*/@\n",
            "                    ((######//****,/.,/(#%#***,***(&@@@@@(\n",
            "                   *&%%#####%%%%%%%#//(#%&%%&&@@@@@@@@@@@@*\n",
            "                   &&%%%###((((((####%%%%&&&&&@@@@@@@@@@&&@.\n",
            "                  *##%%%##(((((((####%%%%%&&&&@@@@@@@@@&#/*,\n",
            "                 .(##%#/,  .,*((##%%%&&&&%%%#####%&&@&&%#(/*.\n",
            "                 /(###(,   .,*/(##%%%&&&&%%%######%&&&&%#(/*,\n",
            "                */((((*.  ..,//((##%%%%%%%%#######%&&&&%%#(/*,\n",
            "               .//(((/,   .,*//((###%%%%%%########%%&&&%%#((/,.\n",
            "              .&####(((((((((######%%%%%%%%&&&&&&&@@@@@@@@@@@@@#\n",
            "               *&#.   .*/((((#######%%%%%%&&&&&&&@@@@@#/.   (&/\n",
            "07:36:28 | building data: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/BST400Mdistill_v1.1.tgz\n",
            "07:36:28 | Downloading http://parl.ai/downloads/_models/blender/BST400Mdistill_v1.1.tgz to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/BST400Mdistill_v1.1.tgz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading BST400Mdistill_v1.1.tgz: 100%|██████████| 3.71G/3.71G [00:59<00:00, 62.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "07:38:50 | building dictionary first...\n",
            "07:38:50 | \u001b[33mOverriding opt[\"init_model\"] to zoo:blender/blender_400Mdistill/model (previously: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model)\u001b[0m\n",
            "07:38:50 | \u001b[33mOverriding opt[\"multitask_weights\"] to (1.0, 3.0, 3.0, 3.0) (previously: [1.0, 3.0, 3.0, 3.0])\u001b[0m\n",
            "07:38:50 | \u001b[33mOverriding opt[\"num_epochs\"] to 5.0 (previously: 3.0)\u001b[0m\n",
            "07:38:50 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: fr_finetuned,fromfile_datatype_extension: True,checkpoint_activations: False,verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,world_logs: ,save_format: conversations,log_keep_fields: all,interactive_mode: False\u001b[0m\n",
            "07:38:50 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--init-opt /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt --allow-missing-init-opts True --task blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --dynamic-batching full --eval-batchsize 8 --num-epochs -1 --validation-every-n-secs 900.0 --save-after-valid True --validation-every-n-epochs -1.0 --validation-patience 20 --distributed-world-size 8 --port 61337 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --chosen-topic-delimiter \n",
            " --num-topics 5 --train-experiencer-only False --remove-political-convos False --teacher-model /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model --task-loss-coeff 1.0 --encoder-loss-coeff 24.0 --hidden-loss-coeff 5.0 --pred-loss-coeff 8.0 --embedding-loss-coeff 0.35 --self-attn-loss-coeff 0.6 --enc-dec-attn-loss-coeff 3.0 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --inference beam --force-fp16-tokens False --learningrate 0.0004 --gpu 0 --bpe-vocab /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json --bpe-merge /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt --bpe-add-prefix-space True --max-lr-steps -1 --parlai-home /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI --show-advanced-args False --numthreads 1 --rank 0\u001b[0m\n",
            "07:38:50 | Using CUDA\n",
            "07:38:50 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model.dict\n",
            "07:38:50 | num words = 8008\n",
            "07:39:08 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "07:39:08 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model\n",
            "07:39:56 | Opt:\n",
            "07:39:56 |     activation: gelu\n",
            "07:39:56 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "07:39:56 |     adam_eps: 1e-08\n",
            "07:39:56 |     add_p1_after_newln: False\n",
            "07:39:56 |     aggregate_micro: False\n",
            "07:39:56 |     allow_missing_init_opts: False\n",
            "07:39:56 |     attention_dropout: 0.0\n",
            "07:39:56 |     batchsize: 8\n",
            "07:39:56 |     beam_block_full_context: True\n",
            "07:39:56 |     beam_block_list_filename: None\n",
            "07:39:56 |     beam_block_ngram: -1\n",
            "07:39:56 |     beam_context_block_ngram: -1\n",
            "07:39:56 |     beam_delay: 30\n",
            "07:39:56 |     beam_length_penalty: 0.65\n",
            "07:39:56 |     beam_min_length: 1\n",
            "07:39:56 |     beam_size: 1\n",
            "07:39:56 |     betas: '[0.9, 0.999]'\n",
            "07:39:56 |     bpe_add_prefix_space: None\n",
            "07:39:56 |     bpe_debug: False\n",
            "07:39:56 |     bpe_dropout: None\n",
            "07:39:56 |     bpe_merge: /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model.dict-merges.txt\n",
            "07:39:56 |     bpe_vocab: /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model.dict-vocab.json\n",
            "07:39:56 |     checkpoint_activations: False\n",
            "07:39:56 |     compute_tokenized_bleu: False\n",
            "07:39:56 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "07:39:56 |     datatype: train\n",
            "07:39:56 |     delimiter: '  '\n",
            "07:39:56 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "07:39:56 |     dict_endtoken: __end__\n",
            "07:39:56 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model.dict\n",
            "07:39:56 |     dict_include_test: False\n",
            "07:39:56 |     dict_include_valid: False\n",
            "07:39:56 |     dict_initpath: None\n",
            "07:39:56 |     dict_language: english\n",
            "07:39:56 |     dict_loaded: True\n",
            "07:39:56 |     dict_lower: False\n",
            "07:39:56 |     dict_max_ngram_size: -1\n",
            "07:39:56 |     dict_maxexs: -1\n",
            "07:39:56 |     dict_maxtokens: -1\n",
            "07:39:56 |     dict_minfreq: 0\n",
            "07:39:56 |     dict_nulltoken: __null__\n",
            "07:39:56 |     dict_starttoken: __start__\n",
            "07:39:56 |     dict_textfields: text,labels\n",
            "07:39:56 |     dict_tokenizer: bytelevelbpe\n",
            "07:39:56 |     dict_unktoken: __unk__\n",
            "07:39:56 |     display_examples: False\n",
            "07:39:56 |     download_path: None\n",
            "07:39:56 |     dropout: 0.1\n",
            "07:39:56 |     dynamic_batching: None\n",
            "07:39:56 |     embedding_projection: random\n",
            "07:39:56 |     embedding_size: 1280\n",
            "07:39:56 |     embedding_type: random\n",
            "07:39:56 |     embeddings_scale: True\n",
            "07:39:56 |     eval_batchsize: None\n",
            "07:39:56 |     eval_dynamic_batching: None\n",
            "07:39:56 |     evaltask: None\n",
            "07:39:56 |     ffn_size: 5120\n",
            "07:39:56 |     final_extra_opt: \n",
            "07:39:56 |     force_fp16_tokens: True\n",
            "07:39:56 |     fp16: True\n",
            "07:39:56 |     fp16_impl: mem_efficient\n",
            "07:39:56 |     fromfile_datapath: fr_finetuned\n",
            "07:39:56 |     fromfile_datatype_extension: True\n",
            "07:39:56 |     gpu: -1\n",
            "07:39:56 |     gradient_clip: 0.1\n",
            "07:39:56 |     hide_labels: False\n",
            "07:39:56 |     history_add_global_end_token: end\n",
            "07:39:56 |     history_reversed: False\n",
            "07:39:56 |     history_size: -1\n",
            "07:39:56 |     image_cropsize: 224\n",
            "07:39:56 |     image_mode: raw\n",
            "07:39:56 |     image_size: 256\n",
            "07:39:56 |     inference: greedy\n",
            "07:39:56 |     init_model: zoo:blender/blender_400Mdistill/model\n",
            "07:39:56 |     init_opt: None\n",
            "07:39:56 |     interactive_mode: False\n",
            "07:39:56 |     invsqrt_lr_decay_gamma: -1\n",
            "07:39:56 |     is_debug: False\n",
            "07:39:56 |     label_truncate: 128\n",
            "07:39:56 |     learn_positional_embeddings: False\n",
            "07:39:56 |     learningrate: 7e-06\n",
            "07:39:56 |     load_from_checkpoint: True\n",
            "07:39:56 |     log_every_n_secs: 10.0\n",
            "07:39:56 |     log_every_n_steps: 50\n",
            "07:39:56 |     log_keep_fields: all\n",
            "07:39:56 |     loglevel: info\n",
            "07:39:56 |     lr_scheduler: reduceonplateau\n",
            "07:39:56 |     lr_scheduler_decay: 0.5\n",
            "07:39:56 |     lr_scheduler_patience: 3\n",
            "07:39:56 |     max_train_steps: -1\n",
            "07:39:56 |     max_train_time: -1\n",
            "07:39:56 |     metrics: default\n",
            "07:39:56 |     model: transformer/generator\n",
            "07:39:56 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model\n",
            "07:39:56 |     model_parallel: False\n",
            "07:39:56 |     momentum: 0\n",
            "07:39:56 |     multitask_weights: '(1.0, 3.0, 3.0, 3.0)'\n",
            "07:39:56 |     mutators: None\n",
            "07:39:56 |     n_decoder_layers: 12\n",
            "07:39:56 |     n_encoder_layers: 2\n",
            "07:39:56 |     n_heads: 32\n",
            "07:39:56 |     n_layers: 2\n",
            "07:39:56 |     n_positions: 128\n",
            "07:39:56 |     n_segments: 0\n",
            "07:39:56 |     nesterov: True\n",
            "07:39:56 |     no_cuda: False\n",
            "07:39:56 |     num_epochs: 5.0\n",
            "07:39:56 |     num_workers: 0\n",
            "07:39:56 |     nus: [0.7]\n",
            "07:39:56 |     optimizer: mem_eff_adam\n",
            "07:39:56 |     output_scaling: 1.0\n",
            "07:39:56 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'fr_finetuned', 'fromfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-400m/model', 'init_model': 'zoo:blender/blender_400Mdistill/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict', 'n_heads': 32, 'n_layers': 2, 'n_positions': 128, 'n_encoder_layers': 2, 'n_decoder_layers': 12, 'embedding_size': 1280, 'ffn_size': 5120, 'label_truncate': 128, 'text_truncate': 128, 'truncate': 128, 'dropout': 0.1, 'log_every_n_secs': 10.0, 'multitask_weights': (1.0, 3.0, 3.0, 3.0), 'attention_dropout': 0.0, 'activation': 'gelu', 'history_add_global_end_token': 'end', 'delimiter': '  ', 'dict_tokenizer': 'bytelevelbpe', 'variant': 'prelayernorm', 'optimizer': 'mem_eff_adam', 'learningrate': 7e-06, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'relu_dropout': 0.0, 'model_parallel': False, 'warmup_updates': 100, 'update_freq': 2, 'gradient_clip': 0.1, 'validation_every_n_epochs': 0.25, 'num_epochs': 5.0, 'verbose': True, 'batchsize': 8, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'validation_patience': 10, 'validation_metric': 'ppl', 'validation_metric_mode': 'min'}\"\n",
            "07:39:56 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "07:39:56 |     person_tokens: False\n",
            "07:39:56 |     rank_candidates: False\n",
            "07:39:56 |     relu_dropout: 0.0\n",
            "07:39:56 |     save_after_valid: False\n",
            "07:39:56 |     save_every_n_secs: -1\n",
            "07:39:56 |     save_format: conversations\n",
            "07:39:56 |     share_word_embeddings: True\n",
            "07:39:56 |     short_final_eval: False\n",
            "07:39:56 |     skip_generation: True\n",
            "07:39:56 |     special_tok_lst: None\n",
            "07:39:56 |     split_lines: False\n",
            "07:39:56 |     starttime: Mar30_11-32\n",
            "07:39:56 |     task: fromfile:parlaiformat\n",
            "07:39:56 |     temperature: 1.0\n",
            "07:39:56 |     tensorboard_log: False\n",
            "07:39:56 |     tensorboard_logdir: None\n",
            "07:39:56 |     text_truncate: 128\n",
            "07:39:56 |     topk: 10\n",
            "07:39:56 |     topp: 0.9\n",
            "07:39:56 |     truncate: 128\n",
            "07:39:56 |     update_freq: 2\n",
            "07:39:56 |     use_reply: label\n",
            "07:39:56 |     validation_cutoff: 1.0\n",
            "07:39:56 |     validation_every_n_epochs: 0.25\n",
            "07:39:56 |     validation_every_n_secs: -1\n",
            "07:39:56 |     validation_every_n_steps: -1\n",
            "07:39:56 |     validation_max_exs: -1\n",
            "07:39:56 |     validation_metric: ppl\n",
            "07:39:56 |     validation_metric_mode: min\n",
            "07:39:56 |     validation_patience: 10\n",
            "07:39:56 |     validation_share_agent: False\n",
            "07:39:56 |     variant: prelayernorm\n",
            "07:39:56 |     verbose: True\n",
            "07:39:56 |     wandb_entity: None\n",
            "07:39:56 |     wandb_log: False\n",
            "07:39:56 |     wandb_name: None\n",
            "07:39:56 |     wandb_project: None\n",
            "07:39:56 |     warmup_rate: 0.0001\n",
            "07:39:56 |     warmup_updates: 100\n",
            "07:39:56 |     weight_decay: None\n",
            "07:39:56 |     world_logs: \n",
            "07:39:57 | creating task(s): fromfile:parlaiformat\n",
            "07:39:57 | Loading ParlAI text data: fr_finetuned_train.txt\n",
            "07:40:00 | training...\n",
            "07:40:01 | time:9080s total_exs:372384 total_steps:23274 epochs:3.00 time_left:6052s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   41.31     1 330.5 760.6       0          0 17.45   16             16384  1.785    .3103 26.81 1.816 .0004 214.5 493.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.144      .5688         0                 1940  545 1254 1.152\n",
            "\n",
            "07:40:01 | creating task(s): fromfile:parlaiformat\n",
            "07:40:01 | Loading ParlAI text data: fr_finetuned_valid.txt\n",
            "07:40:01 | running eval: valid\n",
            "07:40:01 | \u001b[33m--skip-generation true produces limited metrics\u001b[0m\n",
            "07:40:19 | eval completed in 18.42s\n",
            "07:40:19 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
            "   217.6 846.8 11230   .6908      110.8 105.1 1934    .2600 27.35  2.12 .0004 216.8  2875       0          0 8.333      .5416   \n",
            "    token_em  total_train_updates  tpb   tps  \n",
            "           0                 1940 1064 14105\n",
            "\u001b[0m\n",
            "07:40:19 | \u001b[1mdid not beat best ppl: 6.6321 impatience: 1\u001b[0m\n",
            "07:40:29 | time:9108s total_exs:372712 total_steps:23294 epochs:3.00 time_left:6058s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.4     1 830.7  3404   .6616      91.58 32.78  328             16384  1.749    .3123 26.64 2.116 .0004 213.1 873.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 8.299      .5380         0                 1960 1044 4277 2.019\n",
            "\n",
            "07:40:39 | time:9118s total_exs:373040 total_steps:23315 epochs:3.01 time_left:6051s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.8     1 814.4  3278   .6311      86.96  32.2  328             16384  1.761    .3124 26.06 2.007 .0004 208.5 839.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.443      .5470         0                 1981 1023 4117 2.072\n",
            "\n",
            "07:40:50 | time:9128s total_exs:373368 total_steps:23335 epochs:3.01 time_left:6045s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.4     1 802.9  3274   .6128      84.06 32.62  328             16384  1.681    .3123 26.61 1.979 .0004 212.9   868   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.238      .5517         0                 2001 1016 4142 2.011\n",
            "\n",
            "07:41:00 | time:9139s total_exs:373696 total_steps:23356 epochs:3.01 time_left:6038s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     183     1 804.1  3263   .6189      82.49 32.47  328             16384  1.832    .3165 25.23 2.098 .0004 201.8 819.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 8.154      .5369   .003049                 2022 1006 4082 2.09\n",
            "\n",
            "07:41:10 | time:9149s total_exs:374016 total_steps:23376 epochs:3.01 time_left:6032s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   180.3     1   801  3198   .6188      80.16 31.94  320             16384  1.691    .3165 26.88  2.03 .0004   215 858.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.612      .5474         0                 2042 1016 4056 1.996\n",
            "\n",
            "07:41:20 | time:9159s total_exs:374336 total_steps:23396 epochs:3.02 time_left:6025s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.7     1 828.3  3302   .6594      95.14 31.89  320             16384  1.752    .3124 26.87 2.026 .0004   215   857   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.581      .5486         0                 2062 1043 4159 1.993\n",
            "\n",
            "07:41:30 | time:9169s total_exs:374672 total_steps:23417 epochs:3.02 time_left:6019s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.6     1 804.9  3320   .6161      87.01    33  336             16384  1.815    .3124 25.13 2.073 .0004 201.1 829.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.95      .5433   .002976                 2083 1006 4150 2.063\n",
            "\n",
            "07:41:40 | time:9179s total_exs:374992 total_steps:23437 epochs:3.02 time_left:6012s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     187     1 818.1  3268   .6375      84.76 31.96  320             16384  1.644    .3124  25.9 1.848 .0004 207.2 827.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.346      .5765   .003125                 2103 1025 4096 1.997\n",
            "\n",
            "07:41:50 | time:9189s total_exs:375312 total_steps:23457 epochs:3.02 time_left:6006s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   176.8     1 795.1  3177   .5969      77.41 31.97  320             16384  1.679    .3165 26.43 1.931 .0004 211.4 844.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0  6.9      .5669   .003125                 2123 1006 4022 1.999\n",
            "\n",
            "07:42:00 | time:9199s total_exs:375640 total_steps:23477 epochs:3.03 time_left:5999s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   205.1     1   827  3385   .6768      101.7 32.74  328             16384  1.692    .3124 27.35 2.044 .0004 218.8 895.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.72      .5458         0                 2143 1046 4280 2.022\n",
            "\n",
            "07:42:10 | time:9209s total_exs:375968 total_steps:23498 epochs:3.03 time_left:5992s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.4     1   817  3280   .6311      93.31 32.11  328             16384  1.772    .3124  25.7 2.087 .0004 205.6 825.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 8.062      .5401         0                 2164 1023 4105 2.069\n",
            "\n",
            "07:42:20 | time:9219s total_exs:376288 total_steps:23518 epochs:3.03 time_left:5986s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.6     1 813.3  3243   .6219      88.92  31.9  320             16384  1.692    .3124 27.46 2.021 .0004 219.7 875.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.545      .5369         0                 2184 1033 4119 1.994\n",
            "\n",
            "07:42:31 | time:9230s total_exs:376624 total_steps:23539 epochs:3.03 time_left:5979s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.4     1 822.8  3356   .6399      89.56 32.63  336             16384  1.662    .3165 26.35 1.969 .0004 210.8 859.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 7.161      .5608         0                 2205 1034 4216 2.04\n",
            "\n",
            "07:42:41 | time:9240s total_exs:376952 total_steps:23559 epochs:3.04 time_left:5972s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.2     1 812.5  3310   .6311      91.69 32.59  328             16384  1.619    .3124 26.24 1.952 .0004   210 855.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.046      .5598         0                 2225 1022 4165 2.012\n",
            "\n",
            "07:42:51 | time:9250s total_exs:377280 total_steps:23580 epochs:3.04 time_left:5966s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.1     1 805.6  3226   .6189      87.41 32.04  328             16384  1.655    .3165 26.09 1.929 .0004 208.7 835.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.88      .5575         0                 2246 1014 4062 2.064\n",
            "\n",
            "07:43:01 | time:9260s total_exs:377608 total_steps:23600 epochs:3.04 time_left:5959s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.8     1 815.1  3326   .6463      87.94 32.64  328             16384   1.73    .3124  26.2 2.063 .0004 209.6 855.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.869      .5403         0                 2266 1025 4181 2.015\n",
            "\n",
            "07:43:11 | time:9270s total_exs:377928 total_steps:23620 epochs:3.04 time_left:5953s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.5     1 809.4  3204   .6375      95.29 31.67  320             16384  1.675    .3165  26.8 2.063 .0004 214.4 848.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.869      .5377         0                 2286 1024 4053 2.016\n",
            "\n",
            "07:43:22 | time:9280s total_exs:378256 total_steps:23641 epochs:3.05 time_left:5946s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.6     1 815.8  3269   .6402      88.64 32.05  328             16384  1.683    .3124 26.93 2.084 .0004 215.4 863.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 8.04      .5376         0                 2307 1031 4132 2.066\n",
            "\n",
            "07:43:32 | time:9291s total_exs:378592 total_steps:23662 epochs:3.05 time_left:5939s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   178.3     1 794.3  3259   .6042      78.99 32.82  336             16384  1.723    .3124 24.74 1.938 .0004 197.9 811.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 6.942      .5677   .002976                 2328 992.2 4071 2.052\n",
            "\n",
            "07:43:42 | time:9301s total_exs:378928 total_steps:23683 epochs:3.05 time_left:5932s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.3     1 800.7  3270   .6161      84.23 32.67  336             16384  1.699    .3124 25.83 2.031 .0004 206.6 843.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.619      .5434         0                 2349 1007 4113 2.042\n",
            "\n",
            "07:43:52 | time:9311s total_exs:379264 total_steps:23704 epochs:3.06 time_left:5925s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.2     1 815.9  3329   .6280       94.2 32.64  336             16384  1.681    .3124 26.91 1.981 .0004 215.3 878.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 7.247      .5517         0                 2370 1031 4207 2.04\n",
            "\n",
            "07:44:02 | time:9321s total_exs:379592 total_steps:23724 epochs:3.06 time_left:5919s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.1     1 826.8  3372   .6280      89.79 32.63  328             16384  1.676    .3124 26.74 1.972 .0004 213.9 872.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.186      .5593         0                 2390 1041 4244 2.016\n",
            "\n",
            "07:44:13 | time:9332s total_exs:379920 total_steps:23745 epochs:3.06 time_left:5912s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.4     1 808.6  3237   .6280      90.28 32.02  328             16384  1.748    .3124 25.34 1.898 .0004 202.7 811.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.673      .5747         0                 2411 1011 4049 2.065\n",
            "\n",
            "07:44:23 | time:9342s total_exs:380248 total_steps:23765 epochs:3.06 time_left:5905s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.7     1 818.8  3324   .6280      91.32 32.48  328             16384  1.677    .3124 27.31 2.061 .0004 218.5 887.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.858      .5425         0                 2431 1037 4212 2.005\n",
            "\n",
            "07:44:33 | time:9352s total_exs:380568 total_steps:23785 epochs:3.07 time_left:5899s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.3     1 815.5  3261   .6250       89.4 31.99  320             16384   1.78    .3165 26.08     2 .0004 208.7 834.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 7.386      .5500         0                 2451 1024 4096 2.04\n",
            "\n",
            "07:44:43 | time:9362s total_exs:380896 total_steps:23806 epochs:3.07 time_left:5892s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.1     1   821  3314   .6433      89.52  32.3  328             16384  1.758    .3165 26.16 1.976 .0004 209.2 844.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.216      .5588         0                 2472 1030 4159 2.082\n",
            "\n",
            "07:44:53 | time:9372s total_exs:381232 total_steps:23827 epochs:3.07 time_left:5885s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   179.8     1 804.4  3274   .6131      79.24 32.57  336             16384   1.71    .3124 25.86 1.947 .0004 206.9 842.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.006      .5625         0                 2493 1011 4117 2.036\n",
            "\n",
            "07:45:03 | time:9382s total_exs:381552 total_steps:23847 epochs:3.07 time_left:5878s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.3     1 816.4  3255   .6375      91.23 31.89  320             16384   1.63    .3124 27.05  1.94 .0004 216.4 862.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.955      .5657         0                 2513 1033 4117 1.994\n",
            "\n",
            "07:45:14 | time:9392s total_exs:381880 total_steps:23867 epochs:3.08 time_left:5872s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.1     1 805.8  3294   .6159       88.4  32.7  328             16384  1.705    .3165 26.05 1.977 .0004 208.4 852.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.219      .5610   .003049                 2533 1014 4146 2.017\n",
            "\n",
            "07:45:24 | time:9403s total_exs:382208 total_steps:23888 epochs:3.08 time_left:5865s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.8     1 819.8  3355   .6311      85.32 32.74  328             16384  1.711    .3124 24.58 1.947 .0004 196.7 804.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.008      .5623         0                 2554 1016 4160 2.108\n",
            "\n",
            "07:45:34 | time:9413s total_exs:382544 total_steps:23909 epochs:3.08 time_left:5858s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.1     1 813.2  3308   .6339      85.45 32.54  336             16384    1.8    .3124 25.33 1.915 .0004 202.6 824.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.787      .5626         0                 2575 1016 4132 2.034\n",
            "\n",
            "07:45:44 | time:9423s total_exs:382880 total_steps:23930 epochs:3.08 time_left:5851s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.2     1 823.2  3343   .6458       90.3 32.49  336             16384  1.704    .3124 26.73 1.978 .0004 213.9 868.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.226      .5577         0                 2596 1037 4212 2.031\n",
            "\n",
            "07:45:54 | time:9433s total_exs:383208 total_steps:23950 epochs:3.09 time_left:5844s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   225.6     1 824.7  3367   .6555      122.5 32.66  328             16384   1.67    .3165  26.5 1.957 .0004   212 865.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.075      .5645         0                 2616 1037 4232 2.014\n",
            "\n",
            "07:46:05 | time:9444s total_exs:383536 total_steps:23971 epochs:3.09 time_left:5837s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.3     1 826.6  3278   .6585      94.97 31.73  328             16384  1.679    .3124 26.91  1.97 .0004 215.3   854   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.169      .5515         0                 2637 1042 4133 2.043\n",
            "\n",
            "07:46:15 | time:9454s total_exs:383872 total_steps:23992 epochs:3.09 time_left:5830s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.5     1   811  3325   .6310      86.13  32.8  336             16384  1.745    .3165 25.93 2.019 .0004 207.5 850.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 7.534      .5518   .002976                 2658 1018 4176 2.05\n",
            "\n",
            "07:46:25 | time:9464s total_exs:384192 total_steps:24012 epochs:3.10 time_left:5824s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.2     1   813  3233   .6312      87.53 31.81  320             16384   1.65    .3124 26.74 1.936 .0004 213.9 850.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.93      .5713         0                 2678 1027 4083 1.988\n",
            "\n",
            "07:46:35 | time:9474s total_exs:384528 total_steps:24033 epochs:3.10 time_left:5817s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.5     1   816  3336   .6310      86.49 32.71  336             16384  1.766    .3124 25.88 2.093 .0004   207 846.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 8.11      .5406         0                 2699 1023 4183 2.045\n",
            "\n",
            "07:46:45 | time:9484s total_exs:384856 total_steps:24053 epochs:3.10 time_left:5810s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.2     1 820.3  3331   .6311      88.67 32.49  328             16384  1.725    .3165 26.45 1.965 .0004 211.6 859.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.135      .5566         0                 2719 1032 4190 2.005\n",
            "\n",
            "07:46:56 | time:9495s total_exs:385184 total_steps:24074 epochs:3.10 time_left:5803s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     194     1 824.4  3305   .6524      90.91 32.07  328             16384  1.699    .3124 26.26 1.996 .0004   210   842   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.363      .5622   .003049                 2740 1034 4147 2.067\n",
            "\n",
            "07:47:06 | time:9505s total_exs:385504 total_steps:24094 epochs:3.11 time_left:5797s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.5     1 815.6  3253   .6312      86.52 31.91  320             16384  1.742    .3124 26.75  1.99 .0004   214 853.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.313      .5551         0                 2760 1030 4107 1.995\n",
            "\n",
            "07:47:16 | time:9515s total_exs:385832 total_steps:24114 epochs:3.11 time_left:5790s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.3     1 823.6  3342   .6524      87.32 32.47  328             16384  1.671    .3165 26.87 1.909 .0004 214.9 872.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.745      .5682   .003049                 2780 1038 4215 2.002\n",
            "\n",
            "07:47:26 | time:9525s total_exs:386160 total_steps:24135 epochs:3.11 time_left:5783s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     196     1 819.5  3274   .6402       93.6 31.96  328             16384  1.659    .3124 26.41 1.902 .0004 211.3 843.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.699      .5688         0                 2801 1031 4118 2.057\n",
            "\n",
            "07:47:37 | time:9536s total_exs:386496 total_steps:24156 epochs:3.11 time_left:5776s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   181.5     1 806.5  3277   .6131      80.71  32.5  336             16384  1.856    .3124 25.84  1.91 .0004 206.7 839.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.753      .5659   .002976                 2822 1013 4117 2.032\n",
            "\n",
            "07:47:47 | time:9546s total_exs:386816 total_steps:24176 epochs:3.12 time_left:5769s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.8     1 812.9  3253   .6344      93.19 32.01  320             16384  1.655    .3124 27.04 1.968 .0004 216.3 865.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.154      .5600         0                 2842 1029 4119 2.001\n",
            "\n",
            "07:47:57 | time:9556s total_exs:387144 total_steps:24196 epochs:3.12 time_left:5763s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.9     1 812.4  3291   .6494      91.37  32.4  328             16384  1.881    .3124 26.33 1.998 .0004 210.7 853.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 7.377      .5482         0                 2862 1023 4144    2\n",
            "\n",
            "07:48:07 | time:9566s total_exs:387456 total_steps:24216 epochs:3.12 time_left:5756s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.2     1 817.4  3186   .6410      95.03 31.18  312             16384  1.712    .3165 28.85 1.962 .0004 230.8 899.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.117      .5580         0                 2882 1048 4086 2.012\n",
            "\n",
            "07:48:17 | time:9576s total_exs:387792 total_steps:24237 epochs:3.12 time_left:5749s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     184     1 805.2  3276   .6042      83.33 32.55  336             16384  1.809    .3124 24.81 1.937 .0004 198.5 807.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.94      .5678         0                 2903 1004 4083 2.035\n",
            "\n",
            "07:48:27 | time:9586s total_exs:388112 total_steps:24257 epochs:3.13 time_left:5743s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.3     1 816.7  3254   .6312      90.24 31.88  320             16384  1.616    .3124 28.17 1.938 .0004 225.3 897.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.943      .5614         0                 2923 1042 4152 1.993\n",
            "\n",
            "07:48:37 | time:9596s total_exs:388432 total_steps:24277 epochs:3.13 time_left:5736s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.7     1 820.2  3281   .6438      91.17    32  320             16384  1.618    .3124 26.96 1.857 .0004 215.7 862.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.403      .5805    .00625                 2943 1036 4143    2\n",
            "\n",
            "07:48:47 | time:9606s total_exs:388760 total_steps:24297 epochs:3.13 time_left:5729s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.1     1 808.2  3273   .6280      88.03 32.39  328             16384  1.675    .3165 26.28 1.924 .0004 210.2 851.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.848      .5654   .003049                 2963 1018 4124 1.999\n",
            "\n",
            "07:48:58 | time:9617s total_exs:389088 total_steps:24318 epochs:3.13 time_left:5722s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.8     1   811  3221   .6220      90.44 31.77  328             16384  1.702    .3124 26.59 1.935 .0004 212.8   845   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.925      .5602         0                 2984 1024 4066 2.047\n",
            "\n",
            "07:49:08 | time:9627s total_exs:389416 total_steps:24338 epochs:3.14 time_left:5715s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.1     1 811.5  3315   .6341      93.71 32.68  328             16384   1.78    .3124 26.93 1.886 .0004 215.4   880   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.591      .5695         0                 3004 1027 4195 2.015\n",
            "\n",
            "07:49:18 | time:9637s total_exs:389744 total_steps:24359 epochs:3.14 time_left:5709s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   200.8     1 829.2  3299   .6707      97.17 31.83  328             16384  1.668    .3124 26.75 2.029 .0004   214 851.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 7.61      .5513         0                 3025 1043 4151 2.05\n",
            "\n",
            "07:49:28 | time:9647s total_exs:390064 total_steps:24379 epochs:3.14 time_left:5702s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     200     1 815.5  3263   .6344      98.11 32.01  320             16384  1.632    .3124 28.78 1.907 .0004 230.2 921.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.734      .5672         0                 3045 1046 4184 2.001\n",
            "\n",
            "07:49:38 | time:9657s total_exs:390384 total_steps:24399 epochs:3.15 time_left:5695s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     202     1 821.1  3281   .6406      99.37 31.97  320             16384  1.741    .3124 26.31  1.87 .0004 210.5 841.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.485      .5697         0                 3065 1032 4122 1.998\n",
            "\n",
            "07:49:48 | time:9667s total_exs:390704 total_steps:24419 epochs:3.15 time_left:5689s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   204.5     1 832.2  3298   .6531      100.5 31.71  320             16384  1.745    .3124 26.57     2 .0004 212.6 842.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.389      .5488   .003125                 3085 1045 4141 1.982\n",
            "\n",
            "07:49:59 | time:9677s total_exs:391040 total_steps:24440 epochs:3.15 time_left:5681s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     188     1 809.2  3318   .6250      86.85  32.8  336             16384  1.708    .3124 25.57  1.93 .0004 204.5 838.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.892      .5610   .002976                 3106 1014 4157 2.051\n",
            "\n",
            "07:50:09 | time:9688s total_exs:391368 total_steps:24460 epochs:3.15 time_left:5674s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.2     1   817  3318   .6250      90.04 32.49  328             16384  1.669    .3165  26.3 1.984 .0004 210.4 854.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.273      .5527         0                 3126 1027 4172 2.005\n",
            "\n",
            "07:50:19 | time:9698s total_exs:391696 total_steps:24481 epochs:3.16 time_left:5667s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.7     1 810.2  3323   .6311      84.41 32.81  328             16384  1.729    .3038 25.47 1.968 .0004 203.7 835.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.158      .5618         0                 3147 1014 4159 2.115\n",
            "\n",
            "07:50:29 | time:9708s total_exs:392024 total_steps:24501 epochs:3.16 time_left:5660s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   173.6     1 796.3  3257   .5854      74.08 32.72  328             16384  1.704    .3165 24.79 1.865 .0004 198.3 811.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 6.459      .5737   .006098                 3167 994.7 4068 2.02\n",
            "\n",
            "07:50:39 | time:9718s total_exs:392352 total_steps:24522 epochs:3.16 time_left:5654s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.4     1 809.5  3223   .6311      93.23 31.85  328             16384  1.653    .3165 26.43 1.966 .0004 211.4 841.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.142      .5628         0                 3188 1021 4064 2.053\n",
            "\n",
            "07:50:49 | time:9728s total_exs:392680 total_steps:24542 epochs:3.16 time_left:5647s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.2     1 815.9  3303   .6433      94.23 32.39  328             16384  1.637    .3124 26.92 1.989 .0004 215.4 871.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.308      .5539         0                 3208 1031 4175 1.999\n",
            "\n",
            "07:51:00 | time:9738s total_exs:393008 total_steps:24563 epochs:3.17 time_left:5640s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.3     1 813.1  3244   .6220      88.65 31.91  328             16384  1.718    .3165 25.84 1.922 .0004 206.7 824.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.831      .5716         0                 3229 1020 4068 2.056\n",
            "\n",
            "07:51:10 | time:9749s total_exs:393344 total_steps:24584 epochs:3.17 time_left:5633s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.5     1 818.7  3328   .6488      94.19 32.52  336             16384  1.623    .3124  26.1 1.983 .0004 208.8 848.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.263      .5588         0                 3250 1028 4177 2.033\n",
            "\n",
            "07:51:20 | time:9759s total_exs:393680 total_steps:24605 epochs:3.17 time_left:5625s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.2     1 806.1  3292   .6190      87.39 32.67  336             16384  1.638    .3165  26.6 1.955 .0004 212.8   869   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.065      .5618         0                 3271 1019 4161 2.042\n",
            "\n",
            "07:51:30 | time:9769s total_exs:394000 total_steps:24625 epochs:3.17 time_left:5619s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.4     1 823.8  3289   .6594      93.42 31.94  320             16384  1.603    .3124 26.87 1.887 .0004 214.9 858.3   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0  6.6      .5671         0                 3291 1039 4148 1.997\n",
            "\n",
            "07:51:40 | time:9779s total_exs:394328 total_steps:24645 epochs:3.18 time_left:5612s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.2     1 818.4  3325   .6372      92.89  32.5  328             16384  1.641    .3124 26.07 1.908 .0004 208.6 847.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.739      .5690         0                 3311 1027 4173 2.004\n",
            "\n",
            "07:51:51 | time:9789s total_exs:394656 total_steps:24666 epochs:3.18 time_left:5605s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.8     1 813.2  3279   .6372      85.16 32.26  328             16384  1.697    .3165 25.33 1.879 .0004 202.6 817.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.546      .5725         0                 3332 1016 4097 2.078\n",
            "\n",
            "07:52:01 | time:9800s total_exs:394992 total_steps:24687 epochs:3.18 time_left:5598s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   199.5     1 809.5  3310   .6220      98.29 32.71  336             16384  1.691    .3124 25.88 1.945 .0004   207 846.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.996      .5574         0                 3353 1017 4157 2.045\n",
            "\n",
            "07:52:11 | time:9810s total_exs:395328 total_steps:24708 epochs:3.18 time_left:5590s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.1     1 808.8  3321   .6280      91.98 32.85  336             16384  1.652    .3124 25.19 1.906 .0004 201.5 827.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.727      .5779         0                 3374 1010 4149 2.054\n",
            "\n",
            "07:52:21 | time:9820s total_exs:395664 total_steps:24729 epochs:3.19 time_left:5583s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     193     1 821.2  3358   .6339      90.34 32.71  336             16384  1.644    .3124 26.59 1.977 .0004 212.7 869.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.218      .5617         0                 3395 1034 4228 2.045\n",
            "\n",
            "07:52:32 | time:9831s total_exs:396000 total_steps:24750 epochs:3.19 time_left:5576s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     193     1 819.7  3345   .6369      90.57 32.64  336             16384  1.618    .3124 25.61 1.872 .0004 204.9 835.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.502      .5755   .002976                 3416 1025 4181 2.041\n",
            "\n",
            "07:52:42 | time:9841s total_exs:396336 total_steps:24771 epochs:3.19 time_left:5569s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.6     1   798  3249   .6042       88.9 32.57  336             16384  1.665    .3124 25.75 2.007 .0004   206 838.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.444      .5520         0                 3437 1004 4087 2.036\n",
            "\n",
            "07:52:52 | time:9851s total_exs:396656 total_steps:24791 epochs:3.20 time_left:5562s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   199.2     1 821.6  3254   .6344      96.49 31.68  320             16384  1.602    .3165 26.83 1.953 .0004 214.6   850   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 7.047      .5589   .003125                 3457 1036 4104 1.98\n",
            "\n",
            "07:53:02 | time:9861s total_exs:396992 total_steps:24812 epochs:3.20 time_left:5555s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.3     1 812.9  3309   .6339      84.72 32.57  336             16384  1.634    .3124 25.12  1.95 .0004   201 818.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.028      .5621   .005952                 3478 1014 4128 2.036\n",
            "\n",
            "07:53:13 | time:9872s total_exs:397320 total_steps:24832 epochs:3.20 time_left:5548s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.1     1 807.8  3278   .6280      90.17 32.46  328             16384  1.625    .3165 26.25 1.911 .0004   210 852.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.762      .5597   .003049                 3498 1018 4130 2.004\n",
            "\n",
            "07:53:23 | time:9882s total_exs:397640 total_steps:24852 epochs:3.20 time_left:5541s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.1     1 821.5  3287   .6469      87.38 32.01  320             16384  1.643    .3124 26.01 1.911 .0004 208.1 832.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.758      .5702   .003125                 3518 1030 4120 2.037\n",
            "\n",
            "07:53:33 | time:9892s total_exs:397968 total_steps:24873 epochs:3.21 time_left:5534s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.1     1 817.9  3285   .6372      95.86 32.13  328             16384  1.604    .3124    26 1.831 .0004   208 835.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.238      .5804         0                 3539 1026 4120 2.07\n",
            "\n",
            "07:53:43 | time:9902s total_exs:398296 total_steps:24893 epochs:3.21 time_left:5527s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.6     1 808.6  3297   .6220      82.49 32.62  328             16384  1.614    .3124 26.32 1.924 .0004 210.6 858.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.848      .5664         0                 3559 1019 4156 2.013\n",
            "\n",
            "07:53:53 | time:9912s total_exs:398624 total_steps:24914 epochs:3.21 time_left:5520s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     183     1 809.2  3231   .6250      81.88 31.94  328             16384  1.569    .3124 26.38 1.824 .0004   211 842.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.198      .5772         0                 3580 1020 4073 2.058\n",
            "\n",
            "07:54:03 | time:9922s total_exs:398952 total_steps:24934 epochs:3.21 time_left:5513s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.6     1   818  3326   .6402       91.4 32.53  328             16384   1.63    .3165 25.89 1.888 .0004 207.1 842.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.607      .5699         0                 3600 1025 4168 2.006\n",
            "\n",
            "07:54:13 | time:9932s total_exs:399280 total_steps:24955 epochs:3.22 time_left:5506s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.1     1   826  3365   .6555      89.81 32.59  328             16384   1.66    .3124 26.25 1.912 .0004   210 855.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.77      .5630   .003049                 3621 1036 4221 2.099\n",
            "\n",
            "07:54:24 | time:9943s total_exs:399616 total_steps:24976 epochs:3.22 time_left:5498s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   181.5     1   804  3273   .6101      80.99 32.57  336             16384  1.572    .3124 25.18 1.812 .0004 201.5 820.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.121      .5780         0                 3642 1005 4093 2.036\n",
            "\n",
            "07:54:34 | time:9953s total_exs:399944 total_steps:24996 epochs:3.22 time_left:5491s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.2     1 807.3  3310   .6311      91.27  32.8  328             16384  1.613    .3124 25.73 1.833 .0004 205.8   844   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.255      .5772         0                 3662 1013 4154 2.025\n",
            "\n",
            "07:54:44 | time:9963s total_exs:400272 total_steps:25017 epochs:3.22 time_left:5484s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     199     1 834.4  3376   .6677      94.69 32.37  328             16384  1.675    .3165 26.12 1.887 .0004   209 845.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.601      .5660         0                 3683 1043 4222 2.086\n",
            "\n",
            "07:54:54 | time:9973s total_exs:400592 total_steps:25037 epochs:3.23 time_left:5477s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   179.4     1   805  3207   .6125      78.75 31.87  320             16384  1.592    .3124 27.48  1.89 .0004 219.8 875.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.618      .5669         0                 3703 1025 4083 1.993\n",
            "\n",
            "07:55:04 | time:9983s total_exs:400928 total_steps:25058 epochs:3.23 time_left:5470s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.7     1 812.7  3348   .6250      86.12 32.95  336             16384  1.596    .3124  24.1 1.821 .0004 192.8   794   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.176      .5857         0                 3724 1005 4142 2.06\n",
            "\n",
            "07:55:15 | time:9994s total_exs:401264 total_steps:25079 epochs:3.23 time_left:5463s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   180.9     1 807.6  3273   .6161      79.95 32.43  336             16384  1.666    .3124 24.08 1.868 .0004 192.7 780.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.478      .5767   .002976                 3745 1000 4054 2.027\n",
            "\n",
            "07:55:25 | time:10004s total_exs:401592 total_steps:25099 epochs:3.24 time_left:5456s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     198     1   813  3334   .6372      96.38 32.81  328             16384  1.647    .3124 26.32  1.89 .0004 210.6 863.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.621      .5679         0                 3765 1024 4198 2.021\n",
            "\n",
            "07:55:35 | time:10014s total_exs:401920 total_steps:25120 epochs:3.24 time_left:5449s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.9     1 823.4  3345   .6372      91.01  32.5  328             16384  1.714    .3124 24.15 1.928 .0004 193.2   785   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.877      .5620         0                 3786 1017 4130 2.092\n",
            "\n",
            "07:55:45 | time:10024s total_exs:402256 total_steps:25141 epochs:3.24 time_left:5441s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.5     1   820  3351   .6399      89.04  32.7  336             16384  1.623    .3124 25.39 1.936 .0004 203.1 830.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.934      .5648   .002976                 3807 1023 4181 2.044\n",
            "\n",
            "07:55:55 | time:10034s total_exs:402592 total_steps:25162 epochs:3.24 time_left:5434s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.6     1 811.9  3307   .6220      82.07 32.58  336             16384  1.626    .3124 25.35 1.897 .0004 202.8 825.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.663      .5643         0                 3828 1015 4133 2.037\n",
            "\n",
            "07:56:05 | time:10044s total_exs:402912 total_steps:25182 epochs:3.25 time_left:5427s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.7     1   810  3220   .6438      90.42 31.81  320             16384  1.599    .3124 26.86 1.962 .0004 214.8 854.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.116      .5648   .003125                 3848 1025 4075 1.988\n",
            "\n",
            "07:56:16 | time:10054s total_exs:403240 total_steps:25202 epochs:3.25 time_left:5420s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.2     1   818  3350   .6463      89.94 32.76  328             16384   1.69    .3124 26.35 1.928 .0004 210.8 863.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.872      .5629         0                 3868 1029 4213 2.022\n",
            "\n",
            "07:56:21 | time:10060s total_exs:403416 total_steps:25213 epochs:3.25 time_left:5416s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.9     1 814.7  3288   .6420      94.02 32.29  176             16384  1.577    .3124  27.9 1.955 .0004 223.2 900.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.063      .5464         0                 3879 1038 4189 2.086\n",
            "\n",
            "07:56:21 | running eval: valid\n",
            "07:56:39 | eval completed in 18.49s\n",
            "07:56:39 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
            "   217.6 846.8 11186   .6908      110.8 104.7 1934    .2601 27.35 2.011 .0004 216.8  2864       0          0 7.468      .5583   \n",
            "    token_em  total_train_updates  tpb   tps  \n",
            "           0                 3879 1064 14050\n",
            "\u001b[0m\n",
            "07:56:39 | \u001b[1mdid not beat best ppl: 6.6321 impatience: 2\u001b[0m\n",
            "07:56:50 | time:10089s total_exs:403744 total_steps:25234 epochs:3.25 time_left:5419s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.6     1 795.6  3239   .6037      83.19 32.56  328             16384  1.591    .3125 26.12 1.849 .0004   209 850.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.353      .5683         0                 3900 1005 4089 2.085\n",
            "\n",
            "07:57:00 | time:10099s total_exs:404072 total_steps:25254 epochs:3.26 time_left:5412s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   201.1     1 828.3  3382   .6433      97.58 32.66  328             16384  1.585    .3124 26.37 1.876 .0004   211 861.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.527      .5637         0                 3920 1039 4243 2.014\n",
            "\n",
            "07:57:10 | time:10109s total_exs:404400 total_steps:25275 epochs:3.26 time_left:5405s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.1     1 817.8  3266   .6555      99.84 31.95  328             18725  1.571    .3125 27.44 1.923 .0004 219.5 876.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.844      .5626         0                 3941 1037 4143 2.057\n",
            "\n",
            "07:57:20 | time:10119s total_exs:404736 total_steps:25296 epochs:3.26 time_left:5397s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   181.7     1 813.7  3303   .6161      79.99 32.48  336             32768  1.635    .3124 24.71 1.878 .0004 197.7 802.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.538      .5718   .002976                 3962 1011 4106 2.03\n",
            "\n",
            "07:57:30 | time:10129s total_exs:405064 total_steps:25316 epochs:3.26 time_left:5390s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.1     1 813.8  3336   .6402      89.38 32.79  328             32768  1.614    .3124 26.62 1.829 .0004   213 873.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.229      .5767         0                 3982 1027 4209 2.022\n",
            "\n",
            "07:57:41 | time:10140s total_exs:405392 total_steps:25337 epochs:3.27 time_left:5383s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.2     1 814.5  3249   .6220       90.4 31.91  328             32768  1.642    .3125 26.86 1.911 .0004 214.9 857.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.762      .5683         0                 4003 1029 4106 2.055\n",
            "\n",
            "07:57:51 | time:10150s total_exs:405712 total_steps:25357 epochs:3.27 time_left:5376s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   211.5     1   839  3328   .6750      106.6 31.73  320             32768  1.648    .3165 27.48 1.959 .0004 219.8 871.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.093      .5511         0                 4023 1059 4200 1.984\n",
            "\n",
            "07:58:01 | time:10160s total_exs:406032 total_steps:25377 epochs:3.27 time_left:5369s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   200.8     1 813.5  3254   .6281      99.12    32  320             32768  1.638    .3125  26.8 1.863 .0004 214.4 857.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.442      .5693         0                 4043 1028 4112    2\n",
            "\n",
            "07:58:11 | time:10170s total_exs:406368 total_steps:25398 epochs:3.27 time_left:5362s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.9     1 818.6  3333   .6488      88.57 32.57  336             32768  1.619    .3124 26.43 1.826 .0004 211.5 860.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.206      .5791         0                 4064 1030 4194 2.036\n",
            "\n",
            "07:58:21 | time:10180s total_exs:406696 total_steps:25418 epochs:3.28 time_left:5355s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.3     1   830  3374   .6494      93.59 32.51  328             32768  1.591    .3124 27.42 1.854 .0004 219.3 891.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.385      .5760         0                 4084 1049 4265 2.005\n",
            "\n",
            "07:58:32 | time:10191s total_exs:407024 total_steps:25439 epochs:3.28 time_left:5347s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.8     1 819.2  3259   .6372      94.44 31.82  328             32768  1.605    .3124 27.27 1.914 .0004 218.1 867.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.78      .5686         0                 4105 1037 4126 2.05\n",
            "\n",
            "07:58:42 | time:10201s total_exs:407360 total_steps:25460 epochs:3.28 time_left:5340s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.8     1 809.4  3303   .6310      88.64 32.65  336             32768  1.624    .3124 26.26 1.925 .0004 210.1 857.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.854      .5626         0                 4126 1019 4161 2.041\n",
            "\n",
            "07:58:52 | time:10211s total_exs:407688 total_steps:25480 epochs:3.28 time_left:5333s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.4     1 817.6  3353   .6341      91.21 32.81  328             32768  1.621    .3125 26.14 1.829 .0004 209.1 857.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.229      .5894         0                 4146 1027 4211 2.023\n",
            "\n",
            "07:59:02 | time:10221s total_exs:408016 total_steps:25501 epochs:3.29 time_left:5326s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   209.2     1 824.6  3343   .6494      106.1 32.43  328             32768  1.607    .3125 25.91 1.863 .0004 207.3 840.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.443      .5639         0                 4167 1032 4183 2.088\n",
            "\n",
            "07:59:12 | time:10231s total_exs:408336 total_steps:25521 epochs:3.29 time_left:5319s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.8     1   819  3277   .6344      93.45    32  320             32768  1.522    .3125 27.57 1.864 .0004 220.6 882.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.45      .5700         0                 4187 1040 4159 2.001\n",
            "\n",
            "07:59:22 | time:10241s total_exs:408664 total_steps:25541 epochs:3.29 time_left:5311s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.5     1 808.7  3311   .6311      86.43 32.75  328             32768  1.628    .3125  26.2 1.937 .0004 209.6 858.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.935      .5617         0                 4207 1018 4169 2.02\n",
            "\n",
            "07:59:32 | time:10251s total_exs:408992 total_steps:25562 epochs:3.30 time_left:5304s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.3     1   806  3232   .6220      82.57 32.08  328             32768  1.628    .3125 26.16 1.869 .0004 209.3 839.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.484      .5796         0                 4228 1015 4071 2.065\n",
            "\n",
            "07:59:43 | time:10261s total_exs:409320 total_steps:25582 epochs:3.30 time_left:5297s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.2     1   814  3300   .6433      91.41 32.43  328             32768  1.586    .3125 26.48 1.835 .0004 211.9 858.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.263      .5818         0                 4248 1026 4159 1.999\n",
            "\n",
            "07:59:53 | time:10272s total_exs:409648 total_steps:25603 epochs:3.30 time_left:5290s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   200.6     1 827.9  3297   .6524      97.16 31.86  328             32768  1.586    .3125 27.25 1.875 .0004   218   868   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.524      .5694         0                 4269 1046 4165 2.051\n",
            "\n",
            "08:00:03 | time:10282s total_exs:409984 total_steps:25624 epochs:3.30 time_left:5282s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.1     1 804.5  3264   .6101      85.48 32.46  336             32768  1.603    .3124 25.63 1.807 .0004   205 831.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.095      .5874         0                 4290 1010 4096 2.029\n",
            "\n",
            "08:00:13 | time:10292s total_exs:410304 total_steps:25644 epochs:3.31 time_left:5275s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   203.6     1   832  3319   .6625      99.61 31.91  320             32768   1.65    .3125 25.95 1.913 .0004 207.6   828   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.77      .5743         0                 4310 1040 4147 1.995\n",
            "\n",
            "08:00:23 | time:10302s total_exs:410624 total_steps:25664 epochs:3.31 time_left:5268s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.3     1 802.6  3204   .5969      85.02 31.93  320             32768  1.665    .3125 26.35 1.819 .0004 210.8 841.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.168      .5755         0                 4330 1013 4045 1.996\n",
            "\n",
            "08:00:34 | time:10313s total_exs:410960 total_steps:25685 epochs:3.31 time_left:5261s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.8     1 815.2  3308   .6339      85.89 32.46  336             32768  1.589    .3125 26.24 1.865 .0004 209.9 851.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.456      .5698         0                 4351 1025 4159 2.029\n",
            "\n",
            "08:00:44 | time:10323s total_exs:411288 total_steps:25705 epochs:3.31 time_left:5254s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.3     1 825.8  3354   .6555      99.09  32.5  328             32768  1.606    .3165  27.5 1.912 .0004   220 893.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.767      .5680         0                 4371 1046 4248 2.004\n",
            "\n",
            "08:00:54 | time:10333s total_exs:411616 total_steps:25726 epochs:3.32 time_left:5246s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.2     1 817.9  3267   .6311      91.01 31.96  328             32768  1.585    .3125 26.83 1.799 .0004 214.7 857.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.041      .5839         0                 4392 1033 4125 2.058\n",
            "\n",
            "08:01:04 | time:10343s total_exs:411944 total_steps:25746 epochs:3.32 time_left:5239s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     193     1 817.9  3346   .6402      90.73 32.73  328             32768  1.633    .3125 25.92 1.877 .0004 207.3 848.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.533      .5742         0                 4412 1025 4195 2.019\n",
            "\n",
            "08:01:15 | time:10353s total_exs:412272 total_steps:25767 epochs:3.32 time_left:5232s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.6     1 812.9  3220   .6341      90.99 31.69  328             32768  1.664    .3166 26.66 1.869 .0004 213.2 844.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.481      .5720         0                 4433 1026 4065 2.04\n",
            "\n",
            "08:01:25 | time:10364s total_exs:412608 total_steps:25788 epochs:3.32 time_left:5225s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.2     1 803.1  3279   .6220      81.77 32.66  336             32768  1.638    .3166 25.61 1.888 .0004 204.9 836.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.604      .5711   .002976                 4454 1008 4115 2.042\n",
            "\n",
            "08:01:35 | time:10374s total_exs:412928 total_steps:25808 epochs:3.33 time_left:5218s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.9     1 812.9  3237   .6344      90.26 31.86  320             32768  1.808    .3125 27.18 1.947 .0004 217.4 865.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.011      .5624   .003125                 4474 1030 4103 1.991\n",
            "\n",
            "08:01:45 | time:10384s total_exs:413256 total_steps:25828 epochs:3.33 time_left:5210s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   225.7     1   833  3376   .6646      121.6 32.42  328             32768  1.672    .3166 26.93 1.905 .0004 215.4 873.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.72      .5662         0                 4494 1048 4249 1.999\n",
            "\n",
            "08:01:55 | time:10394s total_exs:413584 total_steps:25849 epochs:3.33 time_left:5203s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.3     1 822.3  3317   .6372      89.49 32.27  328             32768  1.663    .3166 25.06 1.863 .0004 200.5 808.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.444      .5721   .003049                 4515 1023 4126 2.077\n",
            "\n",
            "08:02:06 | time:10405s total_exs:413920 total_steps:25870 epochs:3.33 time_left:5195s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.4     1 804.8  3270   .6220      81.84  32.5  336             32768  1.625    .3125 25.45 1.791 .0004 203.6 827.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.995      .5901         0                 4536 1008 4097 2.032\n",
            "\n",
            "08:02:16 | time:10415s total_exs:414240 total_steps:25890 epochs:3.34 time_left:5188s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.2     1 823.6  3267   .6562      94.29 31.73  320             32768  1.615    .3166 27.25 1.894 .0004   218 864.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.645      .5724         0                 4556 1042 4132 1.984\n",
            "\n",
            "08:02:26 | time:10425s total_exs:414576 total_steps:25911 epochs:3.34 time_left:5181s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.8     1 813.3  3306   .6310      89.18 32.51  336             32768  1.582    .3125 25.18 1.808 .0004 201.5 818.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.097      .5847         0                 4577 1015 4124 2.032\n",
            "\n",
            "08:02:36 | time:10435s total_exs:414896 total_steps:25931 epochs:3.34 time_left:5174s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.1     1 801.6  3206   .6281      90.94 31.99  320             32768   1.61    .3125 26.12 1.836 .0004 208.9 835.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.271      .5806         0                 4597 1010 4041    2\n",
            "\n",
            "08:02:46 | time:10445s total_exs:415224 total_steps:25951 epochs:3.35 time_left:5167s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.4     1 814.5  3341   .6311      86.54 32.81  328             32768   1.61    .3166 25.36 1.807 .0004 202.9 832.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.089      .5837   .003049                 4617 1017 4173 2.028\n",
            "\n",
            "08:02:56 | time:10455s total_exs:415552 total_steps:25972 epochs:3.35 time_left:5159s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   179.5     1 809.1  3299   .6311      78.37 32.61  328             32768  1.684    .3125 24.37 1.784 .0004   195 794.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.956      .5883         0                 4638 1004 4093 2.103\n",
            "\n",
            "08:03:06 | time:10465s total_exs:415888 total_steps:25993 epochs:3.35 time_left:5152s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.7     1 811.5  3323   .6131       88.3 32.76  336             32768  1.682    .3125 25.34 1.774 .0004 202.7 830.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.892      .5902         0                 4659 1014 4153 2.048\n",
            "\n",
            "08:03:11 | Overflow: setting loss scale to 16384.0\n",
            "08:03:17 | time:10475s total_exs:416216 total_steps:26013 epochs:3.35 time_left:5144s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.7 .9500 817.8  3354   .6433      89.52 32.81  328             22938  1.631    .3125 26.12 1.881 .0004   209   857   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.561      .5704         0                 4679 1027 4211 2.025\n",
            "\n",
            "08:03:27 | time:10486s total_exs:416536 total_steps:26033 epochs:3.36 time_left:5137s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.3     1 816.7  3252   .6219      87.17 31.85  320             16384  1.648    .3166 26.63 1.956 .0004 213.1 848.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 7.072      .5658         0                 4699 1030 4100 2.027\n",
            "\n",
            "08:03:37 | time:10496s total_exs:416864 total_steps:26054 epochs:3.36 time_left:5130s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.6     1 809.4  3257   .6220      83.43 32.19  328             16384  1.645    .3125 25.28 1.813 .0004 202.3 813.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.128      .5798         0                 4720 1012 4071 2.073\n",
            "\n",
            "08:03:47 | time:10506s total_exs:417200 total_steps:26075 epochs:3.36 time_left:5122s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.7     1 825.7  3352   .6607      92.49 32.48  336             16384  1.606    .3125 25.59 1.885 .0004 204.7 831.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.585      .5756   .005952                 4741 1030 4184 2.03\n",
            "\n",
            "08:03:57 | time:10516s total_exs:417520 total_steps:26095 epochs:3.36 time_left:5115s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.3     1 813.3  3252   .6312      93.67 31.98  320             16384  1.608    .3125  26.8 1.865 .0004 214.4 857.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.456      .5784         0                 4761 1028 4109 1.999\n",
            "\n",
            "08:04:07 | time:10526s total_exs:417856 total_steps:26116 epochs:3.37 time_left:5108s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.7     1 805.4  3348   .6280      89.01 33.26  336             16384   1.82    .3039 24.28 1.837 .0004 194.2 807.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 6.278      .5818         0                 4782 999.7 4156 2.079\n",
            "\n",
            "08:04:17 | time:10536s total_exs:418184 total_steps:26136 epochs:3.37 time_left:5100s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.3     1 813.1  3299   .6220       88.7 32.46  328             16384  1.579    .3125 27.11 1.837 .0004 216.9 880.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.281      .5819         0                 4802 1030 4179 1.999\n",
            "\n",
            "08:04:28 | time:10547s total_exs:418512 total_steps:26157 epochs:3.37 time_left:5093s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.8     1 826.9  3342   .6524       91.4 32.34  328             16384  1.634    .3125 25.77 1.833 .0004 206.1 833.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.255      .5809         0                 4823 1033 4176 2.081\n",
            "\n",
            "08:04:38 | time:10557s total_exs:418848 total_steps:26178 epochs:3.37 time_left:5085s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.4     1 811.5  3295   .6250         94 32.48  336             16384   1.62    .3125 26.94 1.763 .0004 215.5 875.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.827      .5895         0                 4844 1027 4170 2.03\n",
            "\n",
            "08:04:48 | time:10567s total_exs:419176 total_steps:26198 epochs:3.38 time_left:5078s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   206.2     1 828.7  3391   .6799      102.6 32.73  328             16384  1.603    .3125 26.29 1.921 .0004 210.3 860.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.828      .5635         0                 4864 1039 4251 2.019\n",
            "\n",
            "08:04:58 | time:10577s total_exs:419504 total_steps:26219 epochs:3.38 time_left:5071s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.8     1 806.3  3204   .6189      84.98 31.79  328             16384  1.612    .3125  26.5 1.848 .0004   212 842.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.344      .5760         0                 4885 1018 4047 2.046\n",
            "\n",
            "08:05:09 | time:10588s total_exs:419840 total_steps:26240 epochs:3.38 time_left:5063s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.9     1 807.7  3320   .6250      85.97 32.88  336             16384  1.574    .3125  26.2 1.845 .0004 209.6 861.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.327      .5801         0                 4906 1017 4182 2.055\n",
            "\n",
            "08:05:19 | time:10598s total_exs:420160 total_steps:26260 epochs:3.39 time_left:5056s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.5     1 809.8  3238   .6281      89.32 31.99  320             16384  1.679    .3125 26.76 1.825 .0004 214.1 855.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.201      .5791         0                 4926 1024 4093 1.999\n",
            "\n",
            "08:05:29 | time:10608s total_exs:420488 total_steps:26280 epochs:3.39 time_left:5049s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.1     1   826  3351   .6494      91.89 32.45  328             16384  1.601    .3125 26.22 1.828 .0004 209.8 850.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.219      .5787         0                 4946 1036 4202 2.003\n",
            "\n",
            "08:05:39 | time:10618s total_exs:420816 total_steps:26301 epochs:3.39 time_left:5041s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     198     1 822.6  3271   .6494      95.19 31.81  328             16384  1.657    .3125 26.75 1.852 .0004   214 850.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.37      .5798         0                 4967 1037 4122 2.05\n",
            "\n",
            "08:05:49 | time:10628s total_exs:421136 total_steps:26321 epochs:3.39 time_left:5034s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.6     1 815.4  3260   .6312      91.67 31.98  320             16384  1.612    .3125 27.21 1.822 .0004 217.7 870.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.184      .5816   .003125                 4987 1033 4130 1.999\n",
            "\n",
            "08:05:59 | time:10638s total_exs:421456 total_steps:26341 epochs:3.40 time_left:5027s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     213     1 829.7  3307   .6625      109.3 31.89  320             16384  1.557    .3125 28.41 1.816 .0004 227.3   906   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.149      .5757         0                 5007 1057 4213 1.993\n",
            "\n",
            "08:06:09 | time:10648s total_exs:421784 total_steps:26361 epochs:3.40 time_left:5019s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.9     1   817  3332   .6311      89.72 32.62  328             16384  1.609    .3166 26.51 1.774 .0004 212.1 864.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.892      .5803         0                 5027 1029 4196 2.016\n",
            "\n",
            "08:06:20 | time:10658s total_exs:422112 total_steps:26382 epochs:3.40 time_left:5012s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.3     1   810  3255   .6220      91.09 32.15  328             16384   1.62    .3166 25.67 1.848 .0004 205.4 825.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.348      .5794   .003049                 5048 1015 4081 2.072\n",
            "\n",
            "08:06:30 | time:10669s total_exs:422432 total_steps:26402 epochs:3.40 time_left:5005s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.8     1   814  3232   .6250      90.08 31.76  320             16384  1.578    .3125 27.54  1.77 .0004 220.3 874.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.87      .5874         0                 5068 1034 4107 1.985\n",
            "\n",
            "08:06:40 | time:10679s total_exs:422768 total_steps:26423 epochs:3.41 time_left:4997s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.4     1 814.4  3328   .6280      85.57 32.69  336             16384  1.659    .3125 25.68 1.793 .0004 205.5 839.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.007      .5841         0                 5089 1020 4167 2.043\n",
            "\n",
            "08:06:50 | time:10689s total_exs:423088 total_steps:26443 epochs:3.41 time_left:4990s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   180.2     1 791.2  3138   .6125      81.34 31.73  320             16384  1.559    .3125 26.07  1.88 .0004 208.6 827.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 6.552      .5715         0                 5109 999.8 3965 1.983\n",
            "\n",
            "08:07:00 | time:10699s total_exs:423416 total_steps:26463 epochs:3.41 time_left:4983s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.6     1 828.5  3394   .6585         99 32.77  328             16384   1.58    .3166 27.14 1.787 .0004 217.1 889.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.971      .5816         0                 5129 1046 4283 2.023\n",
            "\n",
            "08:07:10 | time:10709s total_exs:423744 total_steps:26484 epochs:3.41 time_left:4975s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.6     1 819.4  3310   .6494      96.17 32.31  328             16384  1.583    .3166 26.16 1.782 .0004 209.3 845.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.94      .5862         0                 5150 1029 4155 2.082\n",
            "\n",
            "08:07:20 | time:10719s total_exs:424072 total_steps:26504 epochs:3.42 time_left:4968s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   179.6     1 800.3  3280   .5976      79.52 32.78  328             16384  1.623    .3125 24.67 1.766 .0004 197.4 808.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 5.848      .5956   .003049                 5170 997.7 4089 2.024\n",
            "\n",
            "08:07:31 | time:10729s total_exs:424400 total_steps:26525 epochs:3.42 time_left:4960s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.3     1 803.3  3218   .6250      86.87 32.05  328             16384  1.608    .3125 26.61 1.778 .0004 212.9 852.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.919      .5902         0                 5191 1016 4071 2.066\n",
            "\n",
            "08:07:41 | time:10740s total_exs:424728 total_steps:26545 epochs:3.42 time_left:4953s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     183     1 803.2  3286   .6128      82.57 32.73  328             16384  1.631    .3166  25.5  1.76 .0004   204 834.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.811      .5925         0                 5211 1007 4121 2.018\n",
            "\n",
            "08:07:51 | time:10750s total_exs:425056 total_steps:26566 epochs:3.42 time_left:4946s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     192     1 819.3  3281   .6372      89.59 32.03  328             16384  1.561    .3166 26.85 1.745 .0004 214.8 860.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.728      .5940         0                 5232 1034 4141 2.062\n",
            "\n",
            "08:08:01 | time:10760s total_exs:425384 total_steps:26586 epochs:3.43 time_left:4938s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.3     1 813.9  3299   .6250      90.59 32.43  328             16384  1.588    .3166 26.86 1.851 .0004 214.9 871.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.368      .5775         0                 5252 1029 4171 2.002\n",
            "\n",
            "08:08:11 | time:10770s total_exs:425712 total_steps:26607 epochs:3.43 time_left:4931s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.5     1 808.5  3238   .6220      85.43 32.04  328             16384  1.786    .3125 25.45 1.847 .0004 203.6 815.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.339      .5733         0                 5273 1012 4054 2.064\n",
            "\n",
            "08:08:22 | time:10780s total_exs:426048 total_steps:26628 epochs:3.43 time_left:4923s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   207.5     1 830.2  3392   .6518      103.8 32.68  336             16384  1.693    .3125 25.43 1.841 .0004 203.5 831.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.301      .5840   .005952                 5294 1034 4223 2.043\n",
            "\n",
            "08:08:32 | time:10791s total_exs:426376 total_steps:26648 epochs:3.44 time_left:4916s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.7     1 816.7  3312   .6463      89.64 32.44  328             16384  1.688    .3166 27.28 1.904 .0004 218.2   885   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.714      .5702         0                 5314 1035 4197 2.001\n",
            "\n",
            "08:08:42 | time:10801s total_exs:426704 total_steps:26669 epochs:3.44 time_left:4908s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.1     1 797.9  3219   .6067      82.36 32.28  328             16384  1.636    .3125 24.84 1.774 .0004 198.7 801.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 5.894      .5859         0                 5335 996.6 4021 2.079\n",
            "\n",
            "08:08:52 | time:10811s total_exs:427032 total_steps:26689 epochs:3.44 time_left:4901s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.8     1 812.6  3325   .6280      85.21 32.73  328             16384  1.766    .3125 25.71 1.744 .0004 205.7 841.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.718      .5967   .003049                 5355 1018 4166 2.019\n",
            "\n",
            "08:09:02 | time:10821s total_exs:427360 total_steps:26710 epochs:3.44 time_left:4893s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.8     1 816.9  3247   .6524      90.68  31.8  328             16384   1.61    .3166 26.59 1.772 .0004 212.7 845.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.885      .5903         0                 5376 1030 4092 2.047\n",
            "\n",
            "08:09:12 | time:10831s total_exs:427688 total_steps:26730 epochs:3.45 time_left:4886s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.1     1 809.8  3295   .6280      89.92 32.55  328             16384  1.607    .3125 27.09 1.825 .0004 216.7 881.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.206      .5805   .003049                 5396 1027 4177 2.01\n",
            "\n",
            "08:09:23 | time:10841s total_exs:428016 total_steps:26751 epochs:3.45 time_left:4878s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.4     1   810  3267   .6250      83.15 32.27  328             16384  1.582    .3125  25.8 1.849 .0004 206.4 832.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.356      .5823         0                 5417 1016 4100 2.08\n",
            "\n",
            "08:09:33 | time:10851s total_exs:428336 total_steps:26771 epochs:3.45 time_left:4871s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.4     1 817.5  3270   .6375      91.24    32  320             16384  1.612    .3125 25.69 1.835 .0004 205.5 822.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.268      .5794   .003125                 5437 1023 4092    2\n",
            "\n",
            "08:09:43 | time:10862s total_exs:428672 total_steps:26792 epochs:3.45 time_left:4863s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.6     1 808.4  3292   .6250      93.51 32.58  336             16384  1.673    .3125 26.71 1.862 .0004 213.7 870.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.434      .5773         0                 5458 1022 4162 2.036\n",
            "\n",
            "08:09:53 | time:10872s total_exs:429008 total_steps:26813 epochs:3.46 time_left:4856s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.8     1 815.2  3370   .6399      86.86 33.07  336             16384  1.675    .3039 24.82 1.838 .0004 198.6 820.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.282      .5779         0                 5479 1014 4191 2.067\n",
            "\n",
            "08:10:03 | time:10882s total_exs:429328 total_steps:26833 epochs:3.46 time_left:4848s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.1     1 807.1  3206   .6219      89.25 31.78  320             16384  1.548    .3125  26.7 1.806 .0004 213.6 848.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.083      .5900   .003125                 5499 1021 4055 1.987\n",
            "\n",
            "08:10:13 | time:10892s total_exs:429656 total_steps:26853 epochs:3.46 time_left:4841s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.3     1 815.1  3312   .6372      86.45  32.5  328             16384  1.554    .3125 26.77 1.868 .0004 214.2 870.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.476      .5729         0                 5519 1029 4182 2.006\n",
            "\n",
            "08:10:23 | time:10902s total_exs:429984 total_steps:26874 epochs:3.46 time_left:4833s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   200.8     1 823.7  3347   .6433      97.87  32.5  328             16384   1.63    .3125 25.78  1.79 .0004 206.2   838   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.987      .5868   .003049                 5540 1030 4185 2.095\n",
            "\n",
            "08:10:34 | time:10913s total_exs:430320 total_steps:26895 epochs:3.47 time_left:4826s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.3     1 805.2  3271   .6220      82.64  32.5  336             16384  1.602    .3125 26.21 1.852 .0004 209.7 851.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.373      .5749         0                 5561 1015 4123 2.032\n",
            "\n",
            "08:10:44 | time:10923s total_exs:430640 total_steps:26915 epochs:3.47 time_left:4818s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.7     1 820.3  3255   .6469      100.2 31.75  320             16384   1.56    .3125    27 1.837 .0004   216 857.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.277      .5761         0                 5581 1036 4113 1.985\n",
            "\n",
            "08:10:54 | time:10933s total_exs:430968 total_steps:26935 epochs:3.47 time_left:4811s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     189     1 808.8  3310   .6341      87.86 32.74  328             16384  1.554    .3125 26.84  1.82 .0004 214.7 878.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.174      .5856   .003049                 5601 1023 4189 2.021\n",
            "\n",
            "08:11:04 | time:10943s total_exs:431296 total_steps:26956 epochs:3.47 time_left:4803s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.9     1 813.3  3275   .6433      89.24 32.21  328             16384  1.577    .3125 26.21 1.853 .0004 209.7 844.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.381      .5757         0                 5622 1023 4119 2.076\n",
            "\n",
            "08:11:14 | time:10953s total_exs:431624 total_steps:26976 epochs:3.48 time_left:4796s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   180.9     1 799.7  3280   .6006      80.92 32.81  328             16384  1.609    .3125 25.52 1.793 .0004 204.1 837.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.005      .5783         0                 5642 1004 4117 2.021\n",
            "\n",
            "08:11:24 | time:10963s total_exs:431960 total_steps:26997 epochs:3.48 time_left:4788s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.9     1 814.5  3415   .6310      85.08 33.54  336             16384  1.642    .3125 24.86  1.76 .0004 198.9 833.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.812      .5919         0                 5663 1013 4248 2.13\n",
            "\n",
            "08:11:35 | time:10973s total_exs:432288 total_steps:27018 epochs:3.48 time_left:4780s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.4     1 820.6  3268   .6402      92.78 31.86  328             16384  1.616    .3125 25.91 1.816 .0004 207.3 825.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.145      .5908         0                 5684 1028 4093 2.051\n",
            "\n",
            "08:11:45 | time:10984s total_exs:432624 total_steps:27039 epochs:3.49 time_left:4773s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.5     1 819.6  3343   .6310      88.03 32.62  336             16384  1.681    .3125  25.6 1.834 .0004 204.8 835.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.258      .5768         0                 5705 1024 4178 2.039\n",
            "\n",
            "08:11:55 | time:10994s total_exs:432952 total_steps:27059 epochs:3.49 time_left:4765s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.2     1 803.2  3255   .6067      90.79 32.42  328             16384  1.971    .3125 25.73 1.717 .0004 205.8   834   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.565      .6030         0                 5725 1009 4089 1.999\n",
            "\n",
            "08:12:05 | time:11004s total_exs:433280 total_steps:27080 epochs:3.49 time_left:4758s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.6     1 812.9  3262   .6433      95.03  32.1  328             16384  1.621    .3125 26.16 1.856 .0004 209.3 839.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.398      .5805   .003049                 5746 1022 4102 2.068\n",
            "\n",
            "08:12:15 | time:11014s total_exs:433608 total_steps:27100 epochs:3.49 time_left:4750s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.1     1 815.7  3317   .6372      91.18 32.53  328             16384  1.644    .3166 26.17 1.878 .0004 209.4 851.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.539      .5716         0                 5766 1025 4169 2.008\n",
            "\n",
            "08:12:26 | time:11025s total_exs:433936 total_steps:27121 epochs:3.50 time_left:4743s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.2     1 811.4  3237   .6280      89.76 31.91  328             16384  1.584    .3125 25.28 1.777 .0004 202.2 806.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.911      .5933         0                 5787 1014 4043 2.057\n",
            "\n",
            "08:12:36 | time:11035s total_exs:434264 total_steps:27141 epochs:3.50 time_left:4735s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.3     1 804.4  3268   .6280       86.7  32.5  328             16384   1.63    .3125 27.15 1.807 .0004 217.2 882.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.092      .5777         0                 5807 1022 4150 2.006\n",
            "\n",
            "08:12:42 | time:11041s total_exs:434448 total_steps:27153 epochs:3.50 time_left:4731s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.6     1 784.5  3119   .6141      84.53 31.81  184             16384  1.643    .3039 24.41 1.755 .0004 195.3 776.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 5.786      .5833         0                 5819 979.8 3896 2.099\n",
            "\n",
            "08:12:42 | running eval: valid\n",
            "08:13:00 | eval completed in 18.44s\n",
            "08:13:00 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
            "   217.6 846.8 11219   .6908      110.8   105 1934    .2602 27.35  1.97 .0004 216.8  2872       0          0 7.169      .5685   \n",
            "    token_em  total_train_updates  tpb   tps  \n",
            "    .0005171                 5819 1064 14091\n",
            "\u001b[0m\n",
            "08:13:00 | \u001b[1mdid not beat best ppl: 6.6321 impatience: 3\u001b[0m\n",
            "08:13:10 | time:11069s total_exs:434784 total_steps:27174 epochs:3.50 time_left:4731s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.5     1 820.3  3349   .6280      95.94 32.66  336             16384  1.651    .3166 25.29 1.787 .0004 202.3 825.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.974      .5846         0                 5840 1023 4175 2.042\n",
            "\n",
            "08:13:20 | time:11079s total_exs:435112 total_steps:27194 epochs:3.51 time_left:4723s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   178.4     1 797.9  3271   .6128       78.7  32.8  328             16384  1.601    .3166 25.75 1.857 .0004   206 844.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.405      .5748         0                 5860 1004 4116 2.023\n",
            "\n",
            "08:13:31 | time:11090s total_exs:435440 total_steps:27215 epochs:3.51 time_left:4716s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.4     1 818.1  3245   .6280       92.1 31.73  328             16384  1.512    .3166 27.11 1.757 .0004 216.9 860.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.795      .5908         0                 5881 1035 4105 2.043\n",
            "\n",
            "08:13:41 | time:11100s total_exs:435768 total_steps:27235 epochs:3.51 time_left:4708s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   200.6     1 824.4  3347   .6494       97.5 32.48  328             16384   1.64    .3125  26.8 1.795 .0004 214.4 870.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.021      .5856         0                 5901 1039 4218 2.005\n",
            "\n",
            "08:13:51 | time:11110s total_exs:436096 total_steps:27256 epochs:3.51 time_left:4701s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.8     1   814  3256   .6372      90.09    32  328             16384  1.547    .3166 25.88 1.843 .0004 207.1 828.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.315      .5813         0                 5922 1021 4084 2.062\n",
            "\n",
            "08:14:01 | time:11120s total_exs:436424 total_steps:27276 epochs:3.52 time_left:4693s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.1     1 814.4  3327   .6372      87.29 32.68  328             16384   1.54    .3125 26.08 1.764 .0004 208.6 852.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.837      .5886         0                 5942 1023 4179 2.017\n",
            "\n",
            "08:14:12 | time:11130s total_exs:436752 total_steps:27297 epochs:3.52 time_left:4686s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.8     1   807  3210   .6220      84.94 31.82  328             16384  1.554    .3039 25.91 1.793 .0004 207.3 824.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.008      .5865         0                 5963 1014 4035 2.049\n",
            "\n",
            "08:14:22 | time:11141s total_exs:437072 total_steps:27317 epochs:3.52 time_left:4678s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.9     1 813.4  3214   .6406      92.25 31.61  320             16384   1.55    .3166 26.97 1.802 .0004 215.8 852.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.061      .5855         0                 5983 1029 4066 1.976\n",
            "\n",
            "08:14:32 | time:11151s total_exs:437400 total_steps:27337 epochs:3.52 time_left:4671s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.1     1 818.8  3319   .6433      92.71 32.43  328             16384  1.619    .3125 26.68  1.75 .0004 213.4 865.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.754      .5955   .003049                 6003 1032 4185 2.002\n",
            "\n",
            "08:14:42 | time:11161s total_exs:437728 total_steps:27358 epochs:3.53 time_left:4663s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.9     1 814.2  3300   .6433      88.08 32.42  328             16384  1.568    .3166 24.64 1.746 .0004 197.1 798.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.729      .5904         0                 6024 1011 4099 2.089\n",
            "\n",
            "08:14:52 | time:11171s total_exs:438056 total_steps:27378 epochs:3.53 time_left:4655s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     190     1 820.4  3363   .6220      87.41 32.79  328             16384  1.647    .3125  26.1 1.706 .0004 208.8 855.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.506      .6018         0                 6044 1029 4219 2.024\n",
            "\n",
            "08:15:02 | time:11181s total_exs:438384 total_steps:27399 epochs:3.53 time_left:4648s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.6     1 814.7  3253   .6341      87.76 31.95  328             16384  1.587    .3125 26.09 1.794 .0004 208.7 833.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.011      .5840         0                 6065 1023 4087 2.058\n",
            "\n",
            "08:15:12 | time:11191s total_exs:438712 total_steps:27419 epochs:3.53 time_left:4640s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.2     1 825.2  3369   .6555      95.09 32.66  328             16384  1.539    .3125 26.05 1.771 .0004 208.4 850.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.875      .5937         0                 6085 1034 4220 2.014\n",
            "\n",
            "08:15:23 | time:11202s total_exs:439040 total_steps:27440 epochs:3.54 time_left:4633s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.7     1 806.4  3227   .6250      88.92 32.01  328             16384  1.521    .3125 25.79 1.751 .0004 206.3 825.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.761      .5906         0                 6106 1013 4053 2.06\n",
            "\n",
            "08:15:33 | time:11212s total_exs:439376 total_steps:27461 epochs:3.54 time_left:4625s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.1     1 820.7  3344   .6429      94.53 32.59  336             16384  1.521    .3125 26.49 1.822 .0004 211.9 863.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.181      .5789         0                 6127 1033 4207 2.037\n",
            "\n",
            "08:15:43 | time:11222s total_exs:439704 total_steps:27481 epochs:3.54 time_left:4617s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.7     1   808  3301   .6280      89.72 32.68  328             16384  1.576    .3125 26.62 1.749 .0004   213   870   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.75      .5948   .003049                 6147 1021 4171 2.016\n",
            "\n",
            "08:15:53 | time:11232s total_exs:440032 total_steps:27502 epochs:3.55 time_left:4609s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.5     1   822  3286   .6494      92.77 31.98  328             16384  1.543    .3125 26.77 1.851 .0004 214.2 856.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.365      .5754         0                 6168 1036 4142 2.059\n",
            "\n",
            "08:16:04 | time:11243s total_exs:440368 total_steps:27523 epochs:3.55 time_left:4602s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.9     1 811.9  3301   .6280      83.44 32.52  336             16384  1.609    .3125 24.97 1.811 .0004 199.8 812.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.116      .5891   .002976                 6189 1012 4113 2.033\n",
            "\n",
            "08:16:14 | time:11253s total_exs:440696 total_steps:27543 epochs:3.55 time_left:4594s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     188     1 813.4  3330   .6280      86.28 32.75  328             16384  1.575    .3125 25.77 1.719 .0004 206.1 843.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.582      .5959         0                 6209 1019 4174 2.018\n",
            "\n",
            "08:16:24 | time:11263s total_exs:441024 total_steps:27564 epochs:3.55 time_left:4586s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.1     1 828.7  3302   .6585      98.46 31.88  328             16384  1.562    .3125 26.57 1.895 .0004 212.5 846.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.649      .5662         0                 6230 1041 4149 2.051\n",
            "\n",
            "08:16:34 | time:11273s total_exs:441360 total_steps:27585 epochs:3.56 time_left:4578s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.6     1 821.9  3395   .6577      92.83 33.04  336             16384  1.526    .3125 25.24 1.849 .0004 201.9 833.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.352      .5748         0                 6251 1024 4229 2.065\n",
            "\n",
            "08:16:44 | time:11283s total_exs:441696 total_steps:27606 epochs:3.56 time_left:4570s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   181.6     1 803.4  3284   .6190      81.17  32.7  336             16384  1.558    .3125  25.3 1.789 .0004 202.4 827.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.984      .5939         0                 6272 1006 4112 2.044\n",
            "\n",
            "08:16:55 | time:11293s total_exs:442024 total_steps:27626 epochs:3.56 time_left:4563s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.2     1 825.2  3375   .6402      90.02 32.72  328             16384  1.563    .3167 25.88 1.769 .0004   207 846.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.865      .5818         0                 6292 1032 4222 2.022\n",
            "\n",
            "08:17:05 | time:11304s total_exs:442352 total_steps:27647 epochs:3.56 time_left:4555s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   200.1     1 821.1  3311   .6433      97.43 32.26  328             16384  1.529    .3125  25.7 1.804 .0004 205.6 829.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.077      .5799         0                 6313 1027 4140 2.08\n",
            "\n",
            "08:17:15 | time:11314s total_exs:442688 total_steps:27668 epochs:3.57 time_left:4547s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.3     1 818.2  3354   .6310      89.04  32.8  336             16384  1.654    .3166 25.24 1.749 .0004 201.9 827.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.746      .5993         0                 6334 1020 4182 2.05\n",
            "\n",
            "08:17:25 | time:11324s total_exs:443016 total_steps:27688 epochs:3.57 time_left:4539s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.1     1 807.1  3305   .6250      86.25 32.76  328             16384  1.564    .3125 26.01 1.795 .0004 208.1   852   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.02      .5911         0                 6354 1015 4157 2.025\n",
            "\n",
            "08:17:35 | time:11334s total_exs:443344 total_steps:27709 epochs:3.57 time_left:4532s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.9     1 810.2  3238   .6280      85.64 31.98  328             16384  1.599    .3125 26.15  1.77 .0004 209.2 836.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.871      .5890         0                 6375 1019 4075 2.061\n",
            "\n",
            "08:17:45 | time:11344s total_exs:443672 total_steps:27729 epochs:3.57 time_left:4524s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.2     1 832.2  3382   .6524      94.21 32.51  328             16384  1.673    .3126 26.04 1.848 .0004 208.3 846.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.346      .5819   .003049                 6395 1041 4229 2.002\n",
            "\n",
            "08:17:56 | time:11355s total_exs:444000 total_steps:27750 epochs:3.58 time_left:4517s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.5     1   819  3256   .6402      93.12 31.81  328             16384  1.775    .3125 25.95 1.719 .0004 207.6 825.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.581      .6039         0                 6416 1027 4082 2.047\n",
            "\n",
            "08:18:06 | time:11365s total_exs:444328 total_steps:27770 epochs:3.58 time_left:4509s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.7     1   823  3367   .6433      92.87 32.73  328             16384  1.687    .3125 24.86 1.825 .0004 198.9 813.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.203      .5764         0                 6436 1022 4181 2.018\n",
            "\n",
            "08:18:16 | time:11375s total_exs:444656 total_steps:27791 epochs:3.58 time_left:4501s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   201.5     1 824.4  3302   .6494      98.49 32.04  328             16384  1.747    .3126 26.01 1.797 .0004   208 833.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.03      .5835         0                 6457 1032 4135 2.062\n",
            "\n",
            "08:18:26 | time:11385s total_exs:444976 total_steps:27811 epochs:3.58 time_left:4494s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.6     1 811.2  3216   .6188      85.15 31.72  320             16384  1.843    .3167 27.43 1.748 .0004 219.4 869.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.744      .6006         0                 6477 1031 4086 1.982\n",
            "\n",
            "08:18:36 | time:11395s total_exs:445312 total_steps:27832 epochs:3.59 time_left:4486s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.4     1 810.3  3317   .6280      86.15 32.75  336             16384  1.573    .3126 25.26  1.78 .0004   202   827   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.928      .5951         0                 6498 1012 4144 2.047\n",
            "\n",
            "08:18:47 | time:11406s total_exs:445640 total_steps:27852 epochs:3.59 time_left:4478s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.7     1 824.8  3365   .6616      93.56 32.63  328             16384  1.513    .3167 27.32 1.727 .0004 218.6 891.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.622      .5971         0                 6518 1043 4256 2.014\n",
            "\n",
            "08:18:57 | time:11416s total_exs:445968 total_steps:27873 epochs:3.59 time_left:4470s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     190     1 812.6  3248   .6341      88.47 31.98  328             16384  1.582    .3125 27.19 1.873 .0004 217.5 869.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.505      .5746   .006098                 6539 1030 4118 2.061\n",
            "\n",
            "08:19:07 | time:11426s total_exs:446296 total_steps:27893 epochs:3.60 time_left:4463s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.3     1 818.7  3332   .6372         96 32.56  328             16384  1.566    .3125 27.27 1.793 .0004 218.2   888   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 6.007      .5902         0                 6559 1037 4220 2.01\n",
            "\n",
            "08:19:17 | time:11436s total_exs:446624 total_steps:27914 epochs:3.60 time_left:4455s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.9     1 829.4  3331   .6616      95.19 32.13  328             16384  1.615    .3125 25.41 1.726 .0004 203.2 816.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.618      .5967         0                 6580 1033 4147 2.07\n",
            "\n",
            "08:19:27 | time:11446s total_exs:446960 total_steps:27935 epochs:3.60 time_left:4447s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.9     1 815.5  3337   .6369      89.93 32.74  336             16384  1.592    .3040 25.23 1.701 .0004 201.9 826.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.48      .6036         0                 6601 1017 4163 2.046\n",
            "\n",
            "08:19:38 | time:11457s total_exs:447296 total_steps:27956 epochs:3.60 time_left:4439s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.4     1 812.7  3335   .6399      91.84 32.83  336             16384  1.692    .3126 25.71 1.687 .0004 205.7 844.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.404      .6053   .002976                 6622 1018 4179 2.052\n",
            "\n",
            "08:19:48 | time:11467s total_exs:447616 total_steps:27976 epochs:3.61 time_left:4432s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.4     1 816.5  3257   .6375      89.37 31.91  320             16384  1.535    .3167 26.63 1.694 .0004   213 849.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.442      .6022         0                 6642 1030 4107 1.995\n",
            "\n",
            "08:19:58 | time:11477s total_exs:447952 total_steps:27997 epochs:3.61 time_left:4424s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     190     1 810.7  3328   .6250      88.71 32.84  336             16384  1.621    .3126 24.97 1.751 .0004 199.8 820.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.762      .5862         0                 6663 1010 4148 2.053\n",
            "\n",
            "08:20:08 | time:11487s total_exs:448288 total_steps:28018 epochs:3.61 time_left:4416s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.3     1 802.2  3257   .6131      81.99 32.48  336             29647  1.613    .3166 25.64  1.77 .0004 205.1 832.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.869      .5908         0                 6684 1007 4089 2.03\n",
            "\n",
            "08:20:19 | time:11498s total_exs:448624 total_steps:28039 epochs:3.61 time_left:4408s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.4     1 826.4  3356   .6369      86.05 32.49  336             32768  1.634    .3126 25.37 1.772 .0004   203 824.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.884      .5884   .005952                 6705 1029 4180 2.031\n",
            "\n",
            "08:20:29 | time:11508s total_exs:448960 total_steps:28060 epochs:3.62 time_left:4400s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.9     1 806.8  3294   .6071      90.02 32.67  336             32768  1.624    .3126 25.33 1.843 .0004 202.6 827.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.318      .5737   .002976                 6726 1009 4122 2.042\n",
            "\n",
            "08:20:39 | time:11518s total_exs:449296 total_steps:28081 epochs:3.62 time_left:4392s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.8     1 824.7  3432   .6637      91.71  33.3  336             32768   1.58    .3167 24.99 1.823 .0004 199.9 831.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.191      .5833         0                 6747 1025 4264 2.081\n",
            "\n",
            "08:20:49 | time:11528s total_exs:449632 total_steps:28102 epochs:3.62 time_left:4384s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.4     1 814.5  3337   .6369      89.63 32.78  336             32768  1.684    .3126 26.39  1.74 .0004 211.1 864.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.698      .5990         0                 6768 1026 4202 2.049\n",
            "\n",
            "08:21:00 | time:11538s total_exs:449952 total_steps:28122 epochs:3.63 time_left:4376s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.9     1 823.2  3277   .6375      92.99 31.84  320             32768   1.72    .3167 27.77 1.751 .0004 222.2 884.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.761      .5967         0                 6788 1045 4161 1.99\n",
            "\n",
            "08:21:10 | time:11549s total_exs:450288 total_steps:28143 epochs:3.63 time_left:4368s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   178.9     1 795.3  3243   .6101      79.51 32.62  336             32768  1.748    .3126 24.08 1.663 .0004 192.6 785.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 5.274      .6078         0                 6809 987.9 4028 2.039\n",
            "\n",
            "08:21:20 | time:11559s total_exs:450616 total_steps:28163 epochs:3.63 time_left:4361s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.1     1 825.2  3349   .6524      98.97 32.47  328             32768  1.619    .3126 27.47 1.802 .0004 219.8   892   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.062      .5856         0                 6829 1045 4241 2.004\n",
            "\n",
            "08:21:30 | time:11569s total_exs:450944 total_steps:28184 epochs:3.63 time_left:4353s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.9     1   833  3374   .6585      98.81 32.41  328             32768  1.747    .3167 26.54 1.864 .0004 212.3 860.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.45      .5774         0                 6850 1045 4234 2.089\n",
            "\n",
            "08:21:40 | time:11579s total_exs:451280 total_steps:28205 epochs:3.64 time_left:4345s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   180.3     1   802  3337   .6071       80.1 33.29  336             32768  1.893    .3167 24.86 1.762 .0004 198.9 827.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.824      .5925         0                 6871 1001 4165 2.081\n",
            "\n",
            "08:21:51 | time:11590s total_exs:451616 total_steps:28226 epochs:3.64 time_left:4337s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.1     1 805.2  3286   .6101      84.49 32.65  336             32768  2.265    .3126 25.22 1.811 .0004 201.8 823.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.118      .5783         0                 6892 1007 4109 2.041\n",
            "\n",
            "08:22:01 | time:11600s total_exs:451944 total_steps:28246 epochs:3.64 time_left:4329s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.5     1 815.3  3309   .6280      84.61 32.47  328             32768  2.434    .3126 26.37 1.823 .0004   211 856.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.192      .5800         0                 6912 1026 4165 2.004\n",
            "\n",
            "08:22:11 | time:11610s total_exs:452272 total_steps:28267 epochs:3.64 time_left:4321s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     191     1   808  3207   .6159         90 31.75  328             32768  1.981    .3126  26.2 1.758 .0004 209.6 831.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.803      .5877         0                 6933 1018 4039 2.047\n",
            "\n",
            "08:22:21 | time:11620s total_exs:452592 total_steps:28287 epochs:3.65 time_left:4314s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.2     1 803.4  3203   .6219      88.79  31.9  320             32768  1.543    .3126 27.34 1.757 .0004 218.8 872.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.796      .5859         0                 6953 1022 4075 1.994\n",
            "\n",
            "08:22:31 | time:11630s total_exs:452920 total_steps:28307 epochs:3.65 time_left:4306s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.7     1 820.8  3347   .6402      92.07 32.62  328             32768  1.705    .3167 26.32  1.82 .0004 210.6 858.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.174      .5823         0                 6973 1031 4206 2.012\n",
            "\n",
            "08:22:41 | time:11640s total_exs:453248 total_steps:28328 epochs:3.65 time_left:4298s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.1     1 802.9  3262   .6311      87.72  32.5  328             32768  1.604    .3126 26.11 1.678 .0004 208.9 848.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.353      .6032         0                 6994 1012 4111 2.094\n",
            "\n",
            "08:22:51 | time:11650s total_exs:453576 total_steps:28348 epochs:3.65 time_left:4290s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.7     1 819.3  3330   .6585      92.28 32.51  328             32768  1.582    .3126 27.01 1.839 .0004 216.1 878.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.289      .5799         0                 7014 1035 4208 2.007\n",
            "\n",
            "08:23:02 | time:11661s total_exs:453904 total_steps:28369 epochs:3.66 time_left:4283s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.5     1 808.3  3239   .6189      86.48 32.05  328             32768  1.557    .3126 25.38 1.785 .0004 203.1 813.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.957      .5820         0                 7035 1011 4052 2.065\n",
            "\n",
            "08:23:12 | time:11671s total_exs:454232 total_steps:28389 epochs:3.66 time_left:4275s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.7     1 825.1  3384   .6341      82.57 32.81  328             32768   1.58    .3126  25.2 1.702 .0004 201.6 826.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.484      .6001         0                 7055 1027 4211 2.025\n",
            "\n",
            "08:23:22 | time:11681s total_exs:454560 total_steps:28410 epochs:3.66 time_left:4267s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     193     1 803.1  3218   .6220      92.57 32.06  328             32768  1.561    .3126 26.82  1.76 .0004 214.6   860   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.812      .5930         0                 7076 1018 4078 2.065\n",
            "\n",
            "08:23:32 | time:11691s total_exs:454888 total_steps:28430 epochs:3.66 time_left:4259s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.8     1 825.6  3373   .6524      95.57 32.69  328             32768  1.514    .3167 26.49 1.782 .0004   212   866   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.943      .5992   .003049                 7096 1038 4239 2.018\n",
            "\n",
            "08:23:42 | time:11701s total_exs:455216 total_steps:28451 epochs:3.67 time_left:4251s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.4     1 809.6  3260   .6220      85.18 32.21  328             32768  1.639    .3167 26.48 1.761 .0004 211.8 852.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.818      .5987   .003049                 7117 1021 4113 2.075\n",
            "\n",
            "08:23:52 | time:11711s total_exs:455544 total_steps:28471 epochs:3.67 time_left:4244s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.2     1 812.6  3321   .6341      88.65 32.69  328             32768  1.542    .3167 25.48 1.689 .0004 203.8 832.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.416      .5991         0                 7137 1016 4154 2.018\n",
            "\n",
            "08:24:03 | time:11722s total_exs:455872 total_steps:28492 epochs:3.67 time_left:4236s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.6     1 808.3  3218   .6189      86.59 31.85  328             32768  1.539    .3126 27.11 1.806 .0004 216.9 863.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.085      .5843         0                 7158 1025 4082 2.052\n",
            "\n",
            "08:24:13 | time:11732s total_exs:456200 total_steps:28512 epochs:3.68 time_left:4228s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.6     1 815.1  3321   .6280       87.7  32.6  328             32768  1.555    .3126 26.81 1.758 .0004 214.5 874.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0  5.8      .5937         0                 7178 1030 4196 2.01\n",
            "\n",
            "08:24:23 | time:11742s total_exs:456528 total_steps:28533 epochs:3.68 time_left:4220s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.9     1 818.1  3276   .6402      82.63 32.03  328             32768  1.577    .3167 24.65 1.714 .0004 197.2 789.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.55      .5954         0                 7199 1015 4065 2.062\n",
            "\n",
            "08:24:33 | time:11752s total_exs:456848 total_steps:28553 epochs:3.68 time_left:4213s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.6     1 814.7  3259   .6219      86.79 32.01  320             32768  1.535    .3126 26.43 1.734 .0004 211.5 846.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.665      .5983         0                 7219 1026 4106 2.001\n",
            "\n",
            "08:24:43 | time:11762s total_exs:457184 total_steps:28574 epochs:3.68 time_left:4205s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.1     1 812.8  3335   .6369      94.48 32.83  336             32768  1.598    .3126 26.37 1.735 .0004   211 865.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.67      .5956         0                 7240 1024 4201 2.052\n",
            "\n",
            "08:24:53 | time:11772s total_exs:457512 total_steps:28594 epochs:3.69 time_left:4197s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.2     1 811.5  3317   .6280      84.81  32.7  328             32768   1.57    .3126 26.41 1.787 .0004 211.3 863.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.971      .5777         0                 7260 1023 4181 2.019\n",
            "\n",
            "08:25:03 | time:11782s total_exs:457840 total_steps:28615 epochs:3.69 time_left:4189s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.4     1 809.4  3273   .6341      91.27 32.35  328             32768  1.549    .3126 26.26 1.778 .0004 210.1 849.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.92      .5839         0                 7281 1019 4123 2.085\n",
            "\n",
            "08:25:14 | time:11792s total_exs:458168 total_steps:28635 epochs:3.69 time_left:4181s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.1     1 815.6  3327   .6433      91.18 32.64  328             32768  1.498    .3126  26.7 1.692 .0004 213.6 871.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.432      .5978         0                 7301 1029 4199 2.013\n",
            "\n",
            "08:25:24 | time:11803s total_exs:458496 total_steps:28656 epochs:3.69 time_left:4173s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.9     1   824  3307   .6494       95.9 32.11  328             32768  1.749    .3126 26.24  1.77 .0004 209.9 842.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.87      .5850         0                 7322 1034 4150 2.068\n",
            "\n",
            "08:25:34 | time:11813s total_exs:458824 total_steps:28676 epochs:3.70 time_left:4165s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.5     1 804.7  3271   .6067      83.96 32.52  328             32768  1.552    .3167 26.95  1.73 .0004 215.6 876.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.638      .5953         0                 7342 1020 4148 2.006\n",
            "\n",
            "08:25:44 | time:11823s total_exs:459152 total_steps:28697 epochs:3.70 time_left:4158s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.6     1 824.7  3277   .6402      93.56 31.79  328             32768  1.665    .3126 25.73 1.782 .0004 205.9 818.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.94      .5905         0                 7363 1031 4095 2.047\n",
            "\n",
            "08:25:54 | time:11833s total_exs:459472 total_steps:28717 epochs:3.70 time_left:4150s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.7     1   806  3223   .6219      88.97 31.99  320             32768  1.512    .3126 27.11  1.74 .0004 216.9 867.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.698      .6026         0                 7383 1023 4090 1.999\n",
            "\n",
            "08:26:04 | time:11843s total_exs:459800 total_steps:28737 epochs:3.70 time_left:4142s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.6     1 815.4  3327   .6341      85.68 32.64  328             32768  1.604    .3126 25.11  1.75 .0004 200.9 819.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.755      .6006   .003049                 7403 1016 4146 2.015\n",
            "\n",
            "08:26:14 | time:11853s total_exs:460128 total_steps:28758 epochs:3.71 time_left:4134s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     196     1   824  3339   .6433      92.99 32.42  328             32768  1.596    .3126 26.14 1.761 .0004 209.1 847.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.816      .5928         0                 7424 1033 4186 2.088\n",
            "\n",
            "08:26:25 | time:11864s total_exs:460464 total_steps:28779 epochs:3.71 time_left:4126s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.8     1 823.4  3393   .6429      91.85 32.97  336             32768  1.513    .3167 26.28 1.732 .0004 210.3 866.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.65      .5985         0                 7445 1034 4260 2.061\n",
            "\n",
            "08:26:35 | time:11874s total_exs:460784 total_steps:28799 epochs:3.71 time_left:4118s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.2     1 817.1  3268   .6531        100    32  320             32768  1.487    .3126 27.64 1.781 .0004 221.2 884.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.938      .5879         0                 7465 1038 4153    2\n",
            "\n",
            "08:26:45 | time:11884s total_exs:461120 total_steps:28820 epochs:3.72 time_left:4110s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.2     1 817.7  3320   .6429      86.03 32.48  336             32768  1.658    .3167 24.29 1.751 .0004 194.3 788.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.761      .5988         0                 7486 1012 4109 2.03\n",
            "\n",
            "08:26:55 | time:11894s total_exs:461456 total_steps:28841 epochs:3.72 time_left:4102s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   181.5     1 805.7  3347   .6101      80.83 33.23  336             32768  1.712    .3040 24.96 1.749 .0004 199.6 829.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.75      .5912   .002976                 7507 1005 4176 2.077\n",
            "\n",
            "08:27:05 | time:11904s total_exs:461784 total_steps:28861 epochs:3.72 time_left:4094s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.8     1 819.3  3344   .6433      90.39 32.65  328             32768    1.6    .3126  25.9 1.722 .0004 207.2 845.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.594      .6038   .003049                 7527 1027 4190 2.014\n",
            "\n",
            "08:27:16 | time:11915s total_exs:462112 total_steps:28882 epochs:3.72 time_left:4087s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.8     1 821.2  3266   .6494      90.14 31.81  328             32768  1.596    .3167 26.58 1.809 .0004 212.6 845.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.106      .5859         0                 7548 1034 4111 2.048\n",
            "\n",
            "08:27:26 | time:11925s total_exs:462448 total_steps:28903 epochs:3.73 time_left:4078s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.2     1 813.3  3367   .6250      87.49 33.12  336             32768  1.614    .3126 25.21 1.746 .0004 201.7   835   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.731      .5960   .002976                 7569 1015 4202 2.07\n",
            "\n",
            "08:27:36 | time:11935s total_exs:462784 total_steps:28924 epochs:3.73 time_left:4070s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.8     1 817.8  3329   .6339      92.55 32.56  336             32768  1.617    .3126  25.3 1.797 .0004 202.4 823.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.032      .5800         0                 7590 1020 4153 2.036\n",
            "\n",
            "08:27:46 | time:11945s total_exs:463104 total_steps:28944 epochs:3.73 time_left:4063s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.3     1 801.1  3164   .6188      90.15 31.59  320             32768  1.559    .3126 26.88 1.738 .0004 215.1 849.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.686      .5920         0                 7610 1016 4013 1.975\n",
            "\n",
            "08:27:57 | time:11956s total_exs:463440 total_steps:28965 epochs:3.73 time_left:4055s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.3     1 824.3  3364   .6458      84.27 32.64  336             32768  1.534    .3126 24.99 1.786 .0004 199.9 815.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.963      .5943         0                 7631 1024 4179 2.04\n",
            "\n",
            "08:28:07 | time:11966s total_exs:463768 total_steps:28985 epochs:3.74 time_left:4047s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.9     1 802.1  3276   .6220      87.68 32.67  328             32768  1.628    .3126 26.09 1.758 .0004 208.7 852.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.799      .5917         0                 7651 1011 4129 2.015\n",
            "\n",
            "08:28:17 | time:11976s total_exs:464096 total_steps:29006 epochs:3.74 time_left:4039s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.2     1 830.2  3298   .6585      92.48 31.78  328             32768   1.57    .3126 25.28 1.729 .0004 202.2 803.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.634      .5969         0                 7672 1032 4101 2.045\n",
            "\n",
            "08:28:27 | time:11986s total_exs:464416 total_steps:29026 epochs:3.74 time_left:4031s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     187     1   806  3190   .6188      86.22 31.66  320             32768   1.53    .3167 27.16 1.798 .0004 217.3 860.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.04      .5913         0                 7692 1023 4050 1.979\n",
            "\n",
            "08:28:37 | time:11996s total_exs:464736 total_steps:29046 epochs:3.74 time_left:4024s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.4     1 816.5  3262   .6531      96.31 31.96  320             32768  1.516    .3167 27.11 1.769 .0004 216.9 866.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.864      .5879         0                 7712 1033 4129 1.998\n",
            "\n",
            "08:28:48 | time:12006s total_exs:465072 total_steps:29067 epochs:3.75 time_left:4015s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.2     1   807  3303   .6310       81.3 32.74  336             32768  1.557    .3126 25.24 1.764 .0004 201.9 826.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.838      .5867   .002976                 7733 1009 4129 2.047\n",
            "\n",
            "08:28:58 | time:12017s total_exs:465392 total_steps:29087 epochs:3.75 time_left:4008s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.1     1   810  3223   .6188      89.84 31.83  320             32768  1.472    .3126 27.22 1.745 .0004 217.8 866.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.728      .5958         0                 7753 1028 4089 1.99\n",
            "\n",
            "08:29:00 | time:12019s total_exs:465480 total_steps:29092 epochs:3.75 time_left:4006s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   212.4     1 841.9  3571   .6932      107.1 33.93   88             32768  1.346    .3040 28.41 1.806 .0004 227.3 963.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.089      .5748         0                 7758 1069 4535 2.006\n",
            "\n",
            "08:29:00 | running eval: valid\n",
            "08:29:19 | eval completed in 18.46s\n",
            "08:29:19 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
            "   217.6 846.8 11201   .6908      110.8 104.8 1934    .2602 27.35 1.928 .0004 216.8  2868       0          0 6.875      .5717   \n",
            "    token_em  total_train_updates  tpb   tps  \n",
            "           0                 7758 1064 14069\n",
            "\u001b[0m\n",
            "08:29:19 | \u001b[1mdid not beat best ppl: 6.6321 impatience: 4\u001b[0m\n",
            "08:29:29 | time:12048s total_exs:465808 total_steps:29113 epochs:3.75 time_left:4004s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.2     1 809.9  3220   .6311      91.92  31.8  328             32768  1.529    .3167 27.21 1.741 .0004 217.7 865.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.701      .5873         0                 7779 1028 4085 2.037\n",
            "\n",
            "08:29:39 | time:12058s total_exs:466136 total_steps:29133 epochs:3.76 time_left:3996s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.8     1 821.7  3368   .6372      88.06 32.79  328             32768  1.547    .3126 26.51  1.71 .0004 212.1 869.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.531      .6009         0                 7799 1034 4238 2.023\n",
            "\n",
            "08:29:49 | time:12068s total_exs:466464 total_steps:29154 epochs:3.76 time_left:3988s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.7     1 808.2  3252   .6280      83.64 32.19  328             32768  1.614    .3126  25.6 1.767 .0004 204.8   824   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.853      .5904         0                 7820 1013 4076 2.073\n",
            "\n",
            "08:30:00 | time:12079s total_exs:466800 total_steps:29175 epochs:3.76 time_left:3980s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.8     1 823.6  3361   .6458      95.85 32.65  336             32768  1.504    .3126 26.38 1.723 .0004   211 861.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.601      .5991         0                 7841 1035 4223 2.041\n",
            "\n",
            "08:30:10 | time:12089s total_exs:467120 total_steps:29195 epochs:3.76 time_left:3972s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   216.1     1 827.5  3278   .6594      112.6 31.69  320             32768  1.515    .3167 28.32 1.783 .0004 226.6 897.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.946      .5972         0                 7861 1054 4176 1.981\n",
            "\n",
            "08:30:20 | time:12099s total_exs:467440 total_steps:29215 epochs:3.77 time_left:3964s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.4     1 800.3  3182   .6250      92.41 31.81  320             32768  1.577    .3167 26.53 1.792 .0004 212.2 843.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.001      .5873   .003125                 7881 1013 4026 1.988\n",
            "\n",
            "08:30:30 | time:12109s total_exs:467776 total_steps:29236 epochs:3.77 time_left:3956s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.8     1 809.8  3323   .6190      84.59 32.82  336             32768  1.548    .3126 24.84 1.694 .0004 198.7 815.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.439      .6098         0                 7902 1008 4138 2.052\n",
            "\n",
            "08:30:40 | time:12119s total_exs:468112 total_steps:29257 epochs:3.77 time_left:3948s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.7     1 813.9  3313   .6190      82.99 32.56  336             32768   1.55    .3126 26.52 1.745 .0004 212.1 863.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.724      .5853         0                 7923 1026 4176 2.035\n",
            "\n",
            "08:30:50 | time:12129s total_exs:468432 total_steps:29277 epochs:3.77 time_left:3940s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.1     1   810  3241   .6312      91.81 32.01  320             32768  1.537    .3126 27.89 1.721 .0004 223.1 892.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.592      .5961         0                 7943 1033 4133 2.001\n",
            "\n",
            "08:31:01 | time:12139s total_exs:468760 total_steps:29297 epochs:3.78 time_left:3932s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.9     1 815.1  3316   .6341      89.02 32.54  328             32768   1.59    .3123 25.27 1.752 .0004 202.2 822.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.768      .5898   .003049                 7963 1017 4138 2.007\n",
            "\n",
            "08:31:11 | time:12150s total_exs:469088 total_steps:29318 epochs:3.78 time_left:3925s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.7     1 809.6  3254   .6220      81.47 32.16  328             32768  1.594    .3123 25.67 1.788 .0004 205.3 825.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.975      .5880   .003049                 7984 1015 4080 2.071\n",
            "\n",
            "08:31:21 | time:12160s total_exs:469408 total_steps:29338 epochs:3.78 time_left:3917s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.7     1 813.7  3221   .6406      97.02 31.67  320             32768  1.484    .3124  28.6 1.708 .0004 228.8 905.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.517      .6044         0                 8004 1042 4127 1.979\n",
            "\n",
            "08:31:31 | time:12170s total_exs:469736 total_steps:29358 epochs:3.78 time_left:3909s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.3     1 815.9  3311   .6189      82.28 32.47  328             32768  1.549    .3124  25.7 1.718 .0004 205.6 834.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.575      .6002   .003049                 8024 1022 4146 2.002\n",
            "\n",
            "08:31:41 | time:12180s total_exs:470064 total_steps:29379 epochs:3.79 time_left:3901s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.4     1 819.2  3273   .6524      95.95 31.96  328             32768  1.523    .3165 25.96 1.744 .0004 207.7 829.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.721      .5921         0                 8045 1027 4102 2.057\n",
            "\n",
            "08:31:51 | time:12190s total_exs:470392 total_steps:29399 epochs:3.79 time_left:3893s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   203.5     1 835.5  3408   .6677      99.06 32.63  328             32768  1.461    .3123 27.24 1.701 .0004 217.9 888.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.482      .6002         0                 8065 1053 4297 2.014\n",
            "\n",
            "08:32:01 | time:12200s total_exs:470712 total_steps:29419 epochs:3.79 time_left:3885s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   204.6     1 826.2  3269   .6500      101.4 31.65  320             32768  1.587    .3226 27.41 1.753 .0004 219.3 867.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.774      .5925         0                 8085 1045 4136 2.014\n",
            "\n",
            "08:32:12 | time:12211s total_exs:471040 total_steps:29440 epochs:3.79 time_left:3877s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     189     1 812.2  3283   .6250      87.44 32.34  328             32768  1.584    .3124 25.31 1.771 .0004 202.5 818.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.879      .5902         0                 8106 1015 4102 2.083\n",
            "\n",
            "08:32:22 | time:12221s total_exs:471376 total_steps:29461 epochs:3.80 time_left:3869s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.9     1 830.5  3368   .6607      99.12 32.45  336             32768  1.573    .3124 26.28 1.715 .0004 210.2 852.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.555      .5903         0                 8127 1041 4221 2.028\n",
            "\n",
            "08:32:32 | time:12231s total_exs:471712 total_steps:29482 epochs:3.80 time_left:3861s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.2     1 811.9  3323   .6220      86.72 32.74  336             32768  1.532    .3124 26.17 1.799 .0004 209.4 856.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.041      .5871   .002976                 8148 1021 4180 2.046\n",
            "\n",
            "08:32:43 | time:12242s total_exs:472048 total_steps:29503 epochs:3.80 time_left:3853s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.1     1 821.7  3344   .6339      86.41 32.56  336             32768  1.495    .3124 25.08 1.686 .0004 200.7 816.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.399      .6031   .005952                 8169 1022 4161 2.035\n",
            "\n",
            "08:32:53 | time:12252s total_exs:472376 total_steps:29523 epochs:3.81 time_left:3845s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.1     1 814.2  3333   .6311      86.36 32.75  328             32768  1.502    .3124 26.34 1.805 .0004 210.7 862.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.082      .5880         0                 8189 1025 4196 2.017\n",
            "\n",
            "08:33:03 | time:12262s total_exs:472704 total_steps:29544 epochs:3.81 time_left:3837s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   200.4     1 826.5  3274   .6524      97.08 31.69  328             32768  1.516    .3124 27.27 1.758 .0004 218.2 864.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.798      .5913         0                 8210 1045 4138 2.038\n",
            "\n",
            "08:33:13 | time:12272s total_exs:473032 total_steps:29564 epochs:3.81 time_left:3829s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.1     1 800.8  3244   .6067      81.95  32.4  328             32768  1.534    .3164 26.12 1.773 .0004   209 846.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.887      .5965         0                 8230 1010 4090    2\n",
            "\n",
            "08:33:24 | time:12282s total_exs:473360 total_steps:29585 epochs:3.81 time_left:3821s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.6     1 811.1  3238   .6311      86.26 31.93  328             32768  1.578    .3124 25.38 1.672 .0004   203 810.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.322      .6059         0                 8251 1014 4048 2.056\n",
            "\n",
            "08:33:34 | time:12293s total_exs:473696 total_steps:29606 epochs:3.82 time_left:3813s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     194     1 819.2  3334   .6458      91.56 32.56  336             32768  1.568    .3124  27.1 1.704 .0004 216.8 882.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.494      .6000         0                 8272 1036 4217 2.035\n",
            "\n",
            "08:33:44 | time:12303s total_exs:474024 total_steps:29626 epochs:3.82 time_left:3805s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   178.3     1 798.1  3256   .6037       78.5 32.64  328             32768  1.589    .3124 25.17 1.686 .0004 201.3 821.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 5.395      .5993         0                 8292 999.4 4077 2.015\n",
            "\n",
            "08:33:54 | time:12313s total_exs:474352 total_steps:29647 epochs:3.82 time_left:3797s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.7     1 805.3  3235   .6250         92 32.14  328             32768  1.518    .3164 26.92 1.704 .0004 215.4 865.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.495      .5985   .003049                 8313 1021 4100 2.069\n",
            "\n",
            "08:34:04 | time:12323s total_exs:474672 total_steps:29667 epochs:3.82 time_left:3789s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.4     1 818.1  3233   .6312      86.11 31.62  320             32768  1.439    .3124 28.05 1.761 .0004 224.4 886.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.815      .5969         0                 8333 1042 4120 1.976\n",
            "\n",
            "08:34:14 | time:12333s total_exs:475000 total_steps:29687 epochs:3.83 time_left:3781s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.4     1 805.8  3264   .6220      84.65  32.4  328             32768   1.55    .3124 26.07 1.731 .0004 208.5 844.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.644      .5923         0                 8353 1014 4108 1.998\n",
            "\n",
            "08:34:25 | time:12344s total_exs:475328 total_steps:29708 epochs:3.83 time_left:3773s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.2     1 808.7  3238   .6311      92.11 32.03  328             32768  1.487    .3124 27.14  1.78 .0004 217.1 869.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.931      .5914         0                 8374 1026 4108 2.063\n",
            "\n",
            "08:34:35 | time:12354s total_exs:475664 total_steps:29729 epochs:3.83 time_left:3765s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.6     1 810.1  3338   .6280      83.29 32.97  336             32768  1.566    .3164 24.09 1.701 .0004 192.7 794.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.479      .6025         0                 8395 1003 4133 2.061\n",
            "\n",
            "08:34:45 | time:12364s total_exs:475992 total_steps:29749 epochs:3.83 time_left:3757s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.1     1 820.8  3325   .6494       94.5 32.41  328             32768  1.561    .3164 26.46 1.764 .0004 211.7 857.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.837      .5915   .003049                 8415 1032 4183 1.998\n",
            "\n",
            "08:34:55 | time:12374s total_exs:476320 total_steps:29770 epochs:3.84 time_left:3749s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.7     1 811.7  3263   .6402      93.25 32.16  328             32768  1.507    .3124 26.22 1.654 .0004 209.8 843.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.227      .6097         0                 8436 1021 4106 2.069\n",
            "\n",
            "08:35:05 | time:12384s total_exs:476648 total_steps:29790 epochs:3.84 time_left:3740s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   179.3     1   813  3330   .6128       77.7 32.76  328             32768  1.514    .3124 24.65 1.723 .0004 197.2 807.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.602      .5966         0                 8456 1010 4137 2.023\n",
            "\n",
            "08:35:16 | time:12395s total_exs:476976 total_steps:29811 epochs:3.84 time_left:3732s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     197     1 820.7  3277   .6616      94.38 31.94  328             32768  1.491    .3124 26.62  1.62 .0004   213 850.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.052      .6112         0                 8477 1034 4127 2.057\n",
            "\n",
            "08:35:26 | time:12405s total_exs:477304 total_steps:29831 epochs:3.85 time_left:3724s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.7     1 810.8  3283   .6280      90.36 32.39  328             32768  1.445    .3164 27.24 1.613 .0004 217.9 882.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.017      .6203         0                 8497 1029 4165 1.997\n",
            "\n",
            "08:35:36 | time:12415s total_exs:477632 total_steps:29852 epochs:3.85 time_left:3716s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.2     1 807.6  3266   .6250      87.25 32.36  328             32768  1.548    .3164 25.51 1.711 .0004 204.1 825.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.536      .5937         0                 8518 1012 4092 2.083\n",
            "\n",
            "08:35:46 | time:12425s total_exs:477960 total_steps:29872 epochs:3.85 time_left:3708s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.3     1 823.1  3354   .6494      93.39  32.6  328             32768    1.6    .3124 26.84 1.669 .0004 214.7 874.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.307      .6078         0                 8538 1038 4229 2.01\n",
            "\n",
            "08:35:56 | time:12435s total_exs:478288 total_steps:29893 epochs:3.85 time_left:3700s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.6     1 820.5  3263   .6341      88.05 31.82  328             32768  1.549    .3124 26.33 1.737 .0004 210.6 837.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.679      .5940   .003049                 8559 1031 4101 2.049\n",
            "\n",
            "08:36:06 | time:12445s total_exs:478608 total_steps:29913 epochs:3.86 time_left:3693s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.3     1 809.2  3234   .6344      89.13 31.97  320             32768    1.6    .3124 26.37 1.817 .0004   211 843.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.152      .5860         0                 8579 1020 4077 1.999\n",
            "\n",
            "08:36:17 | time:12455s total_exs:478936 total_steps:29933 epochs:3.86 time_left:3684s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.9     1 814.3  3330   .6250      85.16 32.72  328             32768  1.493    .3124 26.11 1.653 .0004 208.9 854.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.222      .6112         0                 8599 1023 4184 2.018\n",
            "\n",
            "08:36:27 | time:12466s total_exs:479264 total_steps:29954 epochs:3.86 time_left:3676s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.3     1 817.3  3243   .6585      94.18 31.75  328             32768  1.471    .3124 27.48 1.708 .0004 219.9 872.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.52      .6026         0                 8620 1037 4116 2.045\n",
            "\n",
            "08:36:37 | time:12476s total_exs:479592 total_steps:29974 epochs:3.86 time_left:3668s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   201.3     1 831.3  3383   .6555      97.36 32.56  328             32768  1.526    .3124 27.31 1.778 .0004 218.5 889.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.916      .5852         0                 8640 1050 4272 2.01\n",
            "\n",
            "08:36:47 | time:12486s total_exs:479912 total_steps:29994 epochs:3.87 time_left:3661s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.2     1 815.6  3264   .6438      95.28 32.02  320             32768  1.487    .3165 26.82 1.699 .0004 214.6 858.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.466      .6011         0                 8660 1030 4123 2.038\n",
            "\n",
            "08:36:57 | time:12496s total_exs:480240 total_steps:30015 epochs:3.87 time_left:3652s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   180.1     1 807.9  3282   .6128      79.09  32.5  328             54613  1.608    .3165 23.79 1.635 .0004 190.3 773.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 5.132      .6141   .003049                 8681 998.2 4055 2.093\n",
            "\n",
            "08:37:07 | time:12506s total_exs:480568 total_steps:30035 epochs:3.87 time_left:3644s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.3     1 818.6  3331   .6433      95.94 32.55  328             65536  1.543    .3164 26.15 1.759 .0004 209.2 851.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.807      .5951         0                 8701 1028 4182 2.009\n",
            "\n",
            "08:37:17 | time:12516s total_exs:480896 total_steps:30056 epochs:3.87 time_left:3636s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.3     1 823.1  3329   .6463      91.46 32.36  328             65536  1.535    .3124 25.48 1.714 .0004 203.8 824.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.553      .5997         0                 8722 1027 4153 2.085\n",
            "\n",
            "08:37:27 | time:12526s total_exs:481224 total_steps:30076 epochs:3.88 time_left:3628s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.9     1 807.3  3290   .6189         87  32.6  328             65536  1.598    .3289 25.92 1.791 .0004 207.4   845   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.994      .5819         0                 8742 1015 4135 2.01\n",
            "\n",
            "08:37:38 | time:12537s total_exs:481552 total_steps:30097 epochs:3.88 time_left:3620s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.9     1 812.5  3262   .6280       92.3 32.11  328             65536    1.6    .3124 25.24 1.769 .0004 201.9 810.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.864      .5870         0                 8763 1014 4072 2.068\n",
            "\n",
            "08:37:48 | time:12547s total_exs:481872 total_steps:30117 epochs:3.88 time_left:3612s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.8     1 812.5  3238   .6250      85.25 31.88  320             65536  1.531    .3124 26.52 1.702 .0004 212.2 845.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.484      .5934         0                 8783 1025 4084 1.993\n",
            "\n",
            "08:37:58 | time:12557s total_exs:482208 total_steps:30138 epochs:3.88 time_left:3604s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.8     1 817.1  3339   .6339      89.71 32.69  336             65536  1.594    .3039 24.98 1.722 .0004 199.8 816.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.594      .5976         0                 8804 1017 4156 2.043\n",
            "\n",
            "08:38:08 | time:12567s total_exs:482528 total_steps:30158 epochs:3.89 time_left:3596s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     203     1 822.6  3288   .6500      100.2 31.98  320             65536   1.53    .3124 27.67 1.748 .0004 221.4 884.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.745      .5977         0                 8824 1044 4173 1.999\n",
            "\n",
            "08:38:18 | time:12577s total_exs:482856 total_steps:30178 epochs:3.89 time_left:3588s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.8     1 818.9  3317   .6463      96.43  32.4  328             65536  1.464    .3165 26.94 1.716 .0004 215.5 872.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.561      .5968   .003049                 8844 1034 4190    2\n",
            "\n",
            "08:38:25 | Overflow: setting loss scale to 32768.0\n",
            "08:38:28 | time:12587s total_exs:483176 total_steps:30198 epochs:3.89 time_left:3580s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.7 .9500 822.4  3285   .6531      93.85 31.96  320             52429  1.381    .3165 26.18 1.674 .0004 209.5 836.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.332      .6083         0                 8864 1032 4122 2.036\n",
            "\n",
            "08:38:39 | time:12598s total_exs:483504 total_steps:30219 epochs:3.90 time_left:3572s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     201     1 820.5  3257   .6433      98.43 31.76  328             32768  1.534    .3124 27.68 1.678 .0004 221.4   879   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.352      .6060         0                 8885 1042 4136 2.047\n",
            "\n",
            "08:38:49 | time:12608s total_exs:483832 total_steps:30239 epochs:3.90 time_left:3564s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.7     1 809.7  3289   .6372      87.44  32.5  328             32768  1.499    .3124 26.67 1.691 .0004 213.3 866.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.426      .6091         0                 8905 1023 4156 2.006\n",
            "\n",
            "08:38:59 | time:12618s total_exs:484152 total_steps:30259 epochs:3.90 time_left:3556s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     204     1 818.1  3248   .6562      101.7 31.76  320             32768  1.461    .3124 27.29 1.735 .0004 218.3 866.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.669      .5954   .003125                 8925 1036 4115 2.024\n",
            "\n",
            "08:39:09 | time:12628s total_exs:484480 total_steps:30280 epochs:3.90 time_left:3548s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.9     1   815  3310   .6402         91 32.48  328             32768  1.499    .3124 25.69 1.733 .0004 205.5 834.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.656      .5966         0                 8946 1021 4144 2.094\n",
            "\n",
            "08:39:19 | time:12638s total_exs:484800 total_steps:30300 epochs:3.91 time_left:3540s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   210.9     1   826  3300   .6531      107.6 31.96  320             32768  1.468    .3124  27.5 1.649 .0004   220 879.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.203      .6066         0                 8966 1046 4180 1.998\n",
            "\n",
            "08:39:29 | time:12648s total_exs:485136 total_steps:30321 epochs:3.91 time_left:3532s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.1     1 804.5  3286   .6399      82.57 32.68  336             32768    1.6    .3039 26.58 1.708 .0004 212.7 868.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.519      .5994         0                 8987 1017 4154 2.042\n",
            "\n",
            "08:39:39 | time:12658s total_exs:485464 total_steps:30341 epochs:3.91 time_left:3524s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     203     1 831.8  3388   .6555      99.08 32.59  328             32768  1.539    .3165 26.36 1.767 .0004 210.9 858.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.853      .5919         0                 9007 1043 4247 2.009\n",
            "\n",
            "08:39:50 | time:12669s total_exs:485792 total_steps:30362 epochs:3.91 time_left:3516s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.6     1   810  3248   .6280      93.33 32.08  328             32768  1.519    .3165 26.72  1.67 .0004 213.8 857.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.312      .6047         0                 9028 1024 4106 2.067\n",
            "\n",
            "08:40:00 | time:12679s total_exs:486120 total_steps:30382 epochs:3.92 time_left:3508s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.2     1 820.1  3319   .6402      90.65 32.38  328             32768  1.471    .3124 26.04 1.694 .0004 208.3 843.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.443      .6015         0                 9048 1028 4163 2.001\n",
            "\n",
            "08:40:10 | time:12689s total_exs:486448 total_steps:30403 epochs:3.92 time_left:3500s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.4     1 819.7  3252   .6524      89.93 31.73  328             32768  1.539    .3124 25.99 1.689 .0004 207.9 824.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.415      .6043         0                 9069 1028 4076 2.046\n",
            "\n",
            "08:40:20 | time:12699s total_exs:486776 total_steps:30423 epochs:3.92 time_left:3491s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   179.8     1 791.9  3239   .6006       80.8 32.72  328             32768    1.6    .3165 25.48 1.718 .0004 203.9 833.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 5.572      .5988         0                 9089 995.8 4072 2.017\n",
            "\n",
            "08:40:30 | time:12709s total_exs:487104 total_steps:30444 epochs:3.92 time_left:3483s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.6     1 824.6  3344   .6402      85.51 32.44  328             32768  1.682    .3124 25.77 1.745 .0004 206.1 835.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.727      .5880         0                 9110 1031 4179 2.089\n",
            "\n",
            "08:40:41 | time:12720s total_exs:487440 total_steps:30465 epochs:3.93 time_left:3475s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.8     1 806.9  3280   .6131      85.94 32.52  336             32768  1.569    .3124 25.82 1.772 .0004 206.5 839.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.881      .5882         0                 9131 1013 4119 2.033\n",
            "\n",
            "08:40:51 | time:12730s total_exs:487776 total_steps:30486 epochs:3.93 time_left:3467s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   181.6     1 810.3  3333   .6161      80.34 32.91  336             32768  1.724    .3165 25.97  1.66 .0004 207.8 854.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.257      .6120         0                 9152 1018 4188 2.057\n",
            "\n",
            "08:41:01 | time:12740s total_exs:488112 total_steps:30507 epochs:3.93 time_left:3458s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   180.1     1 803.3  3286   .6101      79.73 32.73  336             32768  1.896    .3039 25.38 1.639 .0004   203 830.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.152      .6114         0                 9173 1006 4117 2.046\n",
            "\n",
            "08:41:11 | time:12750s total_exs:488448 total_steps:30528 epochs:3.94 time_left:3450s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.9     1 815.9  3351   .6399      90.94 32.86  336             32768  1.718    .3039 25.85 1.777 .0004 206.8 849.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.913      .5876         0                 9194 1023 4200 2.054\n",
            "\n",
            "08:41:22 | time:12761s total_exs:488768 total_steps:30548 epochs:3.94 time_left:3442s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.5     1 801.5  3160   .6125      84.29 31.54  320             32768    1.5    .3125 26.91 1.682 .0004 215.3 848.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.377      .6073         0                 9214 1017 4009 1.971\n",
            "\n",
            "08:41:32 | time:12771s total_exs:489096 total_steps:30568 epochs:3.94 time_left:3434s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.1     1 811.1  3288   .6341      90.69 32.43  328             32768  1.834    .3165 25.69 1.703 .0004 205.5   833   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.492      .6043         0                 9234 1017 4121 1.997\n",
            "\n",
            "08:41:42 | time:12781s total_exs:489424 total_steps:30589 epochs:3.94 time_left:3426s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.3     1 812.1  3284   .6189      87.82 32.34  328             32768  1.648    .3125 25.91 1.773 .0004 207.3   838   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.888      .5885         0                 9255 1019 4122 2.08\n",
            "\n",
            "08:41:52 | time:12791s total_exs:489752 total_steps:30609 epochs:3.95 time_left:3418s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.8     1 820.8  3355   .6433      91.22  32.7  328             32768   1.75    .3165 26.47 1.693 .0004 211.7 865.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.435      .6052   .003049                 9275 1033 4221 2.017\n",
            "\n",
            "08:42:02 | time:12801s total_exs:490080 total_steps:30630 epochs:3.95 time_left:3410s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.5     1 811.2  3262   .6402      92.09 32.17  328             32768  1.556    .3125 25.44 1.775 .0004 203.5 818.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.902      .5846         0                 9296 1015 4081 2.072\n",
            "\n",
            "08:42:13 | time:12812s total_exs:490416 total_steps:30651 epochs:3.95 time_left:3401s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   181.2     1   806  3282   .6161      80.47 32.57  336             32768   1.72    .3125 25.24 1.677 .0004   202 822.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.349      .6034         0                 9317 1008 4104 2.036\n",
            "\n",
            "08:42:23 | time:12822s total_exs:490744 total_steps:30671 epochs:3.95 time_left:3393s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.2     1 806.8  3288   .6098      88.39  32.6  328             32768  1.542    .3165 25.08  1.64 .0004 200.7 817.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.155      .6130   .006098                 9337 1007 4106 2.013\n",
            "\n",
            "08:42:33 | time:12832s total_exs:491072 total_steps:30692 epochs:3.96 time_left:3385s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.9     1 816.9  3249   .6159       94.8 31.82  328             32768  1.519    .3165 26.41 1.747 .0004 211.3 840.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.738      .5952         0                 9358 1028 4089 2.05\n",
            "\n",
            "08:42:43 | time:12842s total_exs:491392 total_steps:30712 epochs:3.96 time_left:3377s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   204.2     1 834.6  3295   .6594      99.83 31.58  320             32768  1.548    .3125 27.38 1.686 .0004   219 864.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.399      .6023         0                 9378 1054 4159 1.974\n",
            "\n",
            "08:42:53 | time:12852s total_exs:491728 total_steps:30733 epochs:3.96 time_left:3369s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.9     1 819.7  3354   .6429      95.44 32.73  336             32768  2.145    .3125 26.24 1.707 .0004   210 859.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.514      .6037         0                 9399 1030 4213 2.046\n",
            "\n",
            "08:43:04 | time:12862s total_exs:492048 total_steps:30753 epochs:3.96 time_left:3361s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.8     1 811.8  3233   .6312      87.36 31.86  320             32768  2.046    .3165 26.61 1.699 .0004 212.9 847.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.468      .5962         0                 9419 1025 4080 1.991\n",
            "\n",
            "08:43:14 | time:12872s total_exs:492376 total_steps:30773 epochs:3.97 time_left:3353s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     187     1 807.4  3303   .6067      86.11 32.72  328             32768  1.533    .3125 25.69 1.754 .0004 205.5 840.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.776      .6008         0                 9439 1013 4143 2.018\n",
            "\n",
            "08:43:24 | time:12883s total_exs:492704 total_steps:30794 epochs:3.97 time_left:3344s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   203.6     1 823.5  3280   .6433      100.7 31.86  328             32768  1.583    .3125 26.71 1.719 .0004 213.7 850.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.58      .5957   .003049                 9460 1037 4131 2.051\n",
            "\n",
            "08:43:34 | time:12893s total_exs:493040 total_steps:30815 epochs:3.97 time_left:3336s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.2     1 815.5  3318   .6310      88.28 32.55  336             32768  1.606    .3039 26.37 1.672 .0004 210.9 858.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.323      .6059   .002976                 9481 1026 4176 2.034\n",
            "\n",
            "08:43:44 | time:12903s total_exs:493368 total_steps:30835 epochs:3.97 time_left:3328s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.2     1 806.2  3295   .6220      83.48  32.7  328             32768   1.63    .3124 26.14 1.772 .0004 209.1 854.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.885      .5897         0                 9501 1015 4150 2.016\n",
            "\n",
            "08:43:54 | time:12913s total_exs:493696 total_steps:30856 epochs:3.98 time_left:3320s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.2     1 816.1  3346   .6402      83.17  32.8  328             32768  1.794    .3125 25.83 1.623 .0004 206.6 847.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.066      .6189         0                 9522 1023 4193 2.112\n",
            "\n",
            "08:44:05 | time:12924s total_exs:494032 total_steps:30877 epochs:3.98 time_left:3311s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   207.7     1 823.8  3351   .6429      104.8 32.54  336             32768  1.728    .3125  25.9 1.734 .0004 207.2 842.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.665      .5986         0                 9543 1031 4193 2.034\n",
            "\n",
            "08:44:15 | time:12934s total_exs:494368 total_steps:30898 epochs:3.98 time_left:3303s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   180.7     1 800.1  3291   .6131      80.68 32.91  336             32768  1.756    .3125 24.71 1.728 .0004 197.7 813.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 5.63      .5942   .002976                 9564 997.7 4104 2.057\n",
            "\n",
            "08:44:25 | time:12944s total_exs:494696 total_steps:30918 epochs:3.99 time_left:3295s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   200.2     1 819.6  3323   .6677      97.71 32.44  328             32768  1.547    .3165 26.53 1.637 .0004 212.2 860.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.142      .6141   .003049                 9584 1032 4184 2.002\n",
            "\n",
            "08:44:35 | time:12954s total_exs:495016 total_steps:30938 epochs:3.99 time_left:3287s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.2     1 826.8  3291   .6469       92.9 31.84  320             32768   1.55    .3165 26.91 1.712 .0004 215.2 856.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.537      .6036         0                 9604 1042 4148 2.026\n",
            "\n",
            "08:44:45 | time:12964s total_exs:495344 total_steps:30959 epochs:3.99 time_left:3278s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   177.9     1 792.6  3188   .5945      78.85 32.18  328             32768  1.699    .3125 24.78 1.729 .0004 198.2 797.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 5.634      .6013   .003049                 9625 990.9 3986 2.074\n",
            "\n",
            "08:44:55 | time:12974s total_exs:495672 total_steps:30979 epochs:3.99 time_left:3270s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.2     1 809.5  3315   .6341      80.99 32.76  328             32768  1.842    .3125 25.96 1.677 .0004 207.7 850.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.351      .6043         0                 9645 1017 4165 2.025\n",
            "\n",
            "08:45:05 | time:12984s total_exs:496000 total_steps:31000 epochs:4.00 time_left:3262s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.5     1 813.7  3329   .6463      87.77 32.73  328             32768  2.024    .3039  25.1 1.718 .0004 200.8 821.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.572      .5946         0                 9666 1014 4150 2.109\n",
            "\n",
            "08:45:16 | time:12995s total_exs:496320 total_steps:31020 epochs:4.00 time_left:3254s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.9     1 810.1  3179   .6344      92.64 31.39  320             32768  1.773    .3125 28.01 1.633 .0004 224.1 879.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.119      .6135         0                 9686 1034 4058 1.962\n",
            "\n",
            "08:45:22 | time:13001s total_exs:496512 total_steps:31032 epochs:4.00 time_left:3249s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.3     1 825.4  3316   .6458      92.14 32.14  192             32768  1.808    .3125 26.35 1.688 .0004 210.8 846.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.411      .6068         0                 9698 1036 4163 2.009\n",
            "\n",
            "08:45:22 | running eval: valid\n",
            "08:45:40 | eval completed in 18.44s\n",
            "08:45:40 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
            "   217.6 846.8 11216   .6908      110.8   105 1934    .2601 27.35 1.921 .0004 216.8  2872       0          0 6.828      .5777   \n",
            "    token_em  total_train_updates  tpb   tps  \n",
            "     .001034                 9698 1064 14087\n",
            "\u001b[0m\n",
            "08:45:40 | \u001b[1mdid not beat best ppl: 6.6321 impatience: 5\u001b[0m\n",
            "08:45:50 | time:13029s total_exs:496840 total_steps:31052 epochs:4.00 time_left:3246s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.8     1   810  3321   .6280      89.51  32.8  328             32768  2.191    .3165 26.09 1.779 .0004 208.7 855.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.923      .5861         0                 9718 1019 4177 2.021\n",
            "\n",
            "08:46:00 | time:13039s total_exs:497168 total_steps:31073 epochs:4.01 time_left:3237s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.4     1   824  3368   .6585      91.45  32.7  328             32768  2.273    .3125 25.27  1.73 .0004 202.1 826.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.641      .5931         0                 9739 1026 4194 2.104\n",
            "\n",
            "08:46:11 | time:13049s total_exs:497504 total_steps:31094 epochs:4.01 time_left:3229s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.5     1 816.7  3318   .6458      90.39  32.5  336             32768  1.578    .3125 25.56 1.666 .0004 204.5 830.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.291      .6064         0                 9760 1021 4148 2.031\n",
            "\n",
            "08:46:21 | time:13060s total_exs:497840 total_steps:31115 epochs:4.01 time_left:3221s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.5     1 812.4  3303   .6280       88.9 32.52  336             32768  1.523    .3125  25.5 1.718 .0004   204 829.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.575      .6016         0                 9781 1016 4132 2.033\n",
            "\n",
            "08:46:31 | time:13070s total_exs:498168 total_steps:31135 epochs:4.01 time_left:3212s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.7     1 813.4  3296   .6433      93.07 32.42  328             32768  1.535    .3125 26.47 1.726 .0004 211.8 858.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.617      .6034         0                 9801 1025 4155 2.004\n",
            "\n",
            "08:46:41 | time:13080s total_exs:498488 total_steps:31155 epochs:4.02 time_left:3204s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.7     1 814.1  3228   .6375      90.95 31.72  320             32768   1.61    .3125 27.08 1.685 .0004 216.6 858.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.392      .6023         0                 9821 1031 4087 2.021\n",
            "\n",
            "08:46:51 | time:13090s total_exs:498816 total_steps:31176 epochs:4.02 time_left:3196s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.7     1 814.3  3302   .6280      89.88 32.44  328             32768  1.734    .3125  24.8 1.675 .0004 198.4 804.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.338      .6066         0                 9842 1013 4106 2.091\n",
            "\n",
            "08:47:01 | time:13100s total_exs:499144 total_steps:31196 epochs:4.02 time_left:3188s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   181.1     1 803.9  3296   .6067      80.58  32.8  328             32768  1.657    .3125 25.67 1.738 .0004 205.4 841.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.684      .5971   .003049                 9862 1009 4138 2.025\n",
            "\n",
            "08:47:12 | time:13111s total_exs:499472 total_steps:31217 epochs:4.02 time_left:3180s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.9     1 817.7  3259   .6494      92.68 31.88  328             32768  1.496    .3125 28.02 1.683 .0004 224.1 893.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.38      .6074         0                 9883 1042 4152 2.053\n",
            "\n",
            "08:47:22 | time:13121s total_exs:499808 total_steps:31238 epochs:4.03 time_left:3171s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     180     1 801.8  3254   .6101      79.81 32.47  336             32768  1.495    .3125 25.26 1.674 .0004 202.1 820.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.336      .6057         0                 9904 1004 4074 2.029\n",
            "\n",
            "08:47:32 | time:13131s total_exs:500144 total_steps:31259 epochs:4.03 time_left:3163s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.6     1 806.4  3310   .6310      84.79 32.84  336             32768  1.522    .3125 25.99 1.596 .0004 207.9 853.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.932      .6200         0                 9925 1014 4164 2.053\n",
            "\n",
            "08:47:42 | time:13141s total_exs:500472 total_steps:31279 epochs:4.03 time_left:3155s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.4     1 817.7  3320   .6494      96.14 32.48  328             32768  1.654    .3125 27.39 1.641 .0004 219.1 889.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.163      .6066         0                 9945 1037 4209 2.003\n",
            "\n",
            "08:47:52 | time:13151s total_exs:500792 total_steps:31299 epochs:4.03 time_left:3147s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.5     1 809.3  3237   .6250      89.38 31.99  320             32768  1.529    .3125 26.32 1.636 .0004 210.6 842.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.134      .6071         0                 9965 1020 4079 2.039\n",
            "\n",
            "08:48:03 | time:13162s total_exs:501120 total_steps:31320 epochs:4.04 time_left:3138s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.5     1 819.6  3292   .6494      90.03 32.14  328             32768  1.546    .3125 25.88 1.681 .0004   207 831.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.369      .6053         0                 9986 1027 4124 2.071\n",
            "\n",
            "08:48:13 | time:13172s total_exs:501448 total_steps:31340 epochs:4.04 time_left:3130s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.6     1 818.5  3325   .6433      93.27  32.5  328             32768  1.587    .3165 26.09 1.722 .0004 208.7 847.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.595      .6045         0                10006 1027 4173 2.006\n",
            "\n",
            "08:48:23 | time:13182s total_exs:501776 total_steps:31361 epochs:4.04 time_left:3122s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.9     1 814.6  3265   .6341      82.04 32.06  328             32768  1.619    .3125 25.28 1.722 .0004 202.3 810.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.597      .6038         0                10027 1017 4076 2.064\n",
            "\n",
            "08:48:33 | time:13192s total_exs:502104 total_steps:31381 epochs:4.05 time_left:3114s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.8     1 810.3  3324   .6433      86.53 32.82  328             32768  1.653    .3125 26.56 1.666 .0004 212.5 871.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.293      .6096         0                10047 1023 4196 2.024\n",
            "\n",
            "08:48:43 | time:13202s total_exs:502432 total_steps:31402 epochs:4.05 time_left:3105s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.6     1 823.2  3297   .6463      90.74 32.04  328             32768  1.445    .3165 27.09 1.687 .0004 216.7 867.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.404      .6079         0                10068 1040 4165 2.063\n",
            "\n",
            "08:48:54 | time:13213s total_exs:502768 total_steps:31423 epochs:4.05 time_left:3097s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     194     1 820.7  3338   .6399       91.4 32.54  336             32768  1.589    .3125 25.66 1.712 .0004 205.3   835   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.538      .6033   .002976                10089 1026 4173 2.034\n",
            "\n",
            "08:49:04 | time:13223s total_exs:503096 total_steps:31443 epochs:4.05 time_left:3089s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.1     1 811.1  3327   .6250      88.73 32.82  328             32768  1.538    .3125 24.88 1.658 .0004   199 816.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.249      .6100         0                10109 1010 4144 2.026\n",
            "\n",
            "08:49:14 | time:13233s total_exs:503424 total_steps:31464 epochs:4.06 time_left:3080s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.7     1   815  3247   .6402      94.86 31.87  328             32768  1.543    .3125 26.32 1.764 .0004 210.6   839   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.837      .5975         0                10130 1026 4086 2.053\n",
            "\n",
            "08:49:24 | time:13243s total_exs:503752 total_steps:31484 epochs:4.06 time_left:3072s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.6     1 807.8  3299   .6250      86.62 32.67  328             32768  1.453    .3125 26.61 1.723 .0004 212.9 869.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.602      .5985         0                10150 1021 4169 2.017\n",
            "\n",
            "08:49:34 | time:13253s total_exs:504080 total_steps:31505 epochs:4.06 time_left:3064s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.2     1 825.6  3296   .6524      93.99 31.93  328             32768  1.512    .3125 25.41 1.704 .0004 203.2 811.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.497      .6064         0                10171 1029 4107 2.058\n",
            "\n",
            "08:49:45 | time:13264s total_exs:504416 total_steps:31526 epochs:4.06 time_left:3055s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.8     1 812.2  3312   .6488      90.31 32.62  336             32768  1.561    .3125 25.93 1.675 .0004 207.4 845.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.339      .6073   .005952                10192 1020 4157 2.039\n",
            "\n",
            "08:49:55 | time:13274s total_exs:504744 total_steps:31546 epochs:4.07 time_left:3047s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.7     1 809.7  3282   .6311      88.47 32.43  328             32768  1.512    .3125 26.79 1.622 .0004 214.3 868.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.061      .6119   .003049                10212 1024 4151 1.999\n",
            "\n",
            "08:50:05 | time:13284s total_exs:505072 total_steps:31567 epochs:4.07 time_left:3039s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.1     1 815.3  3288   .6341      85.13 32.26  328             32768  1.513    .3125 26.37 1.638 .0004 210.9 850.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.143      .6114         0                10233 1026 4139 2.077\n",
            "\n",
            "08:50:15 | time:13294s total_exs:505400 total_steps:31587 epochs:4.07 time_left:3030s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.8     1 812.7  3297   .6311      91.21 32.46  328             32768  1.461    .3165 26.51 1.649 .0004 212.1 860.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0  5.2      .6143         0                10253 1025 4158 2.001\n",
            "\n",
            "08:50:25 | time:13304s total_exs:505736 total_steps:31608 epochs:4.07 time_left:3022s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   180.2     1 806.8  3368   .6190      79.32  33.4  336             32768  1.635    .3039 24.47 1.705 .0004 195.8 817.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.504      .5983   .002976                10274 1003 4185 2.123\n",
            "\n",
            "08:50:35 | time:13314s total_exs:506064 total_steps:31629 epochs:4.08 time_left:3014s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.3     1 810.6  3266   .6311      89.02 32.24  328             32768  1.495    .3125 26.02 1.654 .0004 208.1 838.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.226      .6087         0                10295 1019 4105 2.075\n",
            "\n",
            "08:50:45 | time:13324s total_exs:506384 total_steps:31649 epochs:4.08 time_left:3006s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   200.9     1 822.2  3279   .6594      98.14  31.9  320             32768  1.532    .3125 26.66  1.65 .0004 213.3 850.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.209      .6137    .00625                10315 1036 4130 1.994\n",
            "\n",
            "08:50:56 | time:13335s total_exs:506720 total_steps:31670 epochs:4.08 time_left:2997s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.5     1 824.7  3361   .6488      92.45  32.6  336             32768  1.554    .3125 26.38 1.647 .0004   211 859.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.191      .6081         0                10336 1036 4221 2.038\n",
            "\n",
            "08:51:06 | time:13345s total_exs:507056 total_steps:31691 epochs:4.09 time_left:2989s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.5     1 813.3  3330   .6369      90.87 32.76  336             32768  1.519    .3125 26.01 1.634 .0004 208.1 852.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.127      .6166   .008929                10357 1021 4182 2.048\n",
            "\n",
            "08:51:16 | time:13355s total_exs:507392 total_steps:31712 epochs:4.09 time_left:2980s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.6     1 824.1  3386   .6548      93.55 32.87  336             32768  1.597    .3125 25.21 1.717 .0004 201.7 828.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.569      .6013         0                10378 1026 4215 2.054\n",
            "\n",
            "08:51:26 | time:13365s total_exs:507720 total_steps:31732 epochs:4.09 time_left:2972s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   199.9     1 825.6  3352   .6463      96.69 32.49  328             32768  1.587    .3165 26.05 1.738 .0004 208.4 846.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.689      .5896         0                10398 1034 4199 2.003\n",
            "\n",
            "08:51:36 | time:13375s total_exs:508040 total_steps:31752 epochs:4.09 time_left:2964s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.6     1 810.2  3239   .6219      85.32 31.98  320             32768  1.557    .3125 27.07 1.684 .0004 216.6 865.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.388      .6033         0                10418 1027 4105 2.033\n",
            "\n",
            "08:51:47 | time:13386s total_exs:508368 total_steps:31773 epochs:4.10 time_left:2955s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.2     1 829.1  3366   .6463      93.57 32.48  328             32768  1.504    .3125 26.39 1.733 .0004 211.1 857.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.656      .6030         0                10439 1040 4223 2.092\n",
            "\n",
            "08:51:57 | time:13396s total_exs:508704 total_steps:31794 epochs:4.10 time_left:2947s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.1     1 801.8  3274   .6101       85.9 32.66  336             32768  1.529    .3125 26.14 1.751 .0004 209.1 853.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.762      .5937   .002976                10460 1011 4127 2.042\n",
            "\n",
            "08:52:07 | time:13406s total_exs:509040 total_steps:31815 epochs:4.10 time_left:2938s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.1     1 803.9  3261   .6250      85.56 32.45  336             32768  1.465    .3125 26.54 1.584 .0004 212.3 861.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.875      .6189   .002976                10481 1016 4123 2.029\n",
            "\n",
            "08:52:17 | time:13416s total_exs:509368 total_steps:31835 epochs:4.10 time_left:2930s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.1     1 827.9  3378   .6524      94.62 32.64  328             32768   1.42    .3125 26.67 1.593 .0004 213.4 870.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.921      .6205         0                10501 1041 4249 2.009\n",
            "\n",
            "08:52:27 | time:13426s total_exs:509696 total_steps:31856 epochs:4.11 time_left:2922s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     184     1 805.8  3272   .6159      83.32 32.49  328             32768  1.681    .3125 25.81 1.772 .0004 206.5 838.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.883      .5899   .003049                10522 1012 4111 2.088\n",
            "\n",
            "08:52:38 | time:13437s total_exs:510024 total_steps:31876 epochs:4.11 time_left:2913s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.6     1 822.6  3334   .6372      95.74 32.42  328             32768  1.527    .3125 26.42 1.801 .0004 211.4 856.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.053      .5834   .003049                10542 1034 4191 2.004\n",
            "\n",
            "08:52:48 | time:13447s total_exs:510344 total_steps:31896 epochs:4.11 time_left:2905s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.7     1   810  3242   .6312      91.47 32.02  320             32768   1.58    .3125 27.22 1.731 .0004 217.7 871.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.646      .5978         0                10562 1028 4113 2.037\n",
            "\n",
            "08:52:58 | time:13457s total_exs:510672 total_steps:31917 epochs:4.11 time_left:2897s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.6     1 799.6  3211   .6067      82.65 32.13  328             32768   1.38    .3125 27.24 1.563 .0004   218 875.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.773      .6304   .003049                10583 1018 4087 2.07\n",
            "\n",
            "08:53:08 | time:13467s total_exs:510992 total_steps:31937 epochs:4.12 time_left:2889s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.9     1 817.6  3257   .6375      91.69 31.87  320             32768  1.654    .3125 27.31 1.743 .0004 218.5 870.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.713      .5935         0                10603 1036 4128 1.992\n",
            "\n",
            "08:53:17 | Overflow: setting loss scale to 16384.0\n",
            "08:53:18 | time:13477s total_exs:511312 total_steps:31957 epochs:4.12 time_left:2881s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.5 .9500 820.6  3279   .6531      95.97 31.97  320             31130  1.452    .3165 26.07 1.701 .0004 208.6 833.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.477      .5998   .003125                10623 1029 4112 1.998\n",
            "\n",
            "08:53:28 | time:13487s total_exs:511648 total_steps:31978 epochs:4.12 time_left:2872s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   176.4     1 797.8  3262   .6012      76.72 32.71  336             16384  1.516    .3125 25.32 1.653 .0004 202.5 828.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.222      .6078         0                10644 1000 4090 2.045\n",
            "\n",
            "08:53:38 | time:13497s total_exs:511976 total_steps:31998 epochs:4.12 time_left:2864s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.9     1 816.2  3313   .6341      91.83 32.47  328             16384   1.48    .3165 26.35  1.65 .0004 210.8 855.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.206      .6122   .003049                10664 1027 4168 2.004\n",
            "\n",
            "08:53:49 | time:13508s total_exs:512304 total_steps:32019 epochs:4.13 time_left:2856s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.3     1 816.9  3289   .6402       89.2 32.21  328             16384  1.559    .3125 26.58 1.662 .0004 212.6 856.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.269      .6074   .003049                10685 1029 4145 2.077\n",
            "\n",
            "08:53:59 | time:13518s total_exs:512640 total_steps:32040 epochs:4.13 time_left:2847s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.4     1 811.2  3318   .6220      91.01 32.72  336             16384  2.192    .3166 26.25 1.663 .0004   210   859   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.274      .6072   .002976                10706 1021 4177 2.045\n",
            "\n",
            "08:54:09 | time:13528s total_exs:512968 total_steps:32060 epochs:4.13 time_left:2839s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.8     1 818.4  3322   .6372      87.46 32.47  328             16384  1.602    .3125 26.51 1.679 .0004 212.1 860.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.358      .6043         0                10726 1031 4183 2.007\n",
            "\n",
            "08:54:19 | time:13538s total_exs:513296 total_steps:32081 epochs:4.14 time_left:2830s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.9     1   804  3232   .6250      85.37 32.15  328             16384  1.538    .3125 25.96 1.695 .0004 207.7 834.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.447      .5959         0                10747 1012 4066 2.072\n",
            "\n",
            "08:54:29 | time:13548s total_exs:513624 total_steps:32101 epochs:4.14 time_left:2822s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.9     1 820.3  3327   .6463      94.36 32.44  328             16384  1.511    .3125 25.52 1.685 .0004 204.1 827.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.39      .6078         0                10767 1024 4155 2.003\n",
            "\n",
            "08:54:40 | time:13559s total_exs:513952 total_steps:32122 epochs:4.14 time_left:2814s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.2     1   818  3306   .6433      88.91 32.33  328             16384    1.5    .3125 26.98 1.695 .0004 215.8 872.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.447      .6035         0                10788 1034 4178 2.083\n",
            "\n",
            "08:54:50 | time:13569s total_exs:514280 total_steps:32142 epochs:4.14 time_left:2805s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.6     1 802.5  3269   .6159      87.27 32.59  328             16384   1.98    .3165 26.57  1.66 .0004 212.6   866   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.26      .6140         0                10808 1015 4135 2.014\n",
            "\n",
            "08:55:00 | time:13579s total_exs:514608 total_steps:32163 epochs:4.15 time_left:2797s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.5     1 809.7  3237   .6372       87.3 31.98  328             16384   1.69    .3125 26.09 1.652 .0004 208.7 834.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.219      .6099         0                10829 1018 4071 2.061\n",
            "\n",
            "08:55:10 | time:13589s total_exs:514944 total_steps:32184 epochs:4.15 time_left:2789s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     197     1 821.7  3343   .6339      94.25 32.55  336             16384  1.583    .3166 26.72  1.69 .0004 213.8 869.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.42      .6094         0                10850 1035 4213 2.034\n",
            "\n",
            "08:55:21 | time:13600s total_exs:515280 total_steps:32205 epochs:4.15 time_left:2780s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.4     1 810.6  3301   .6220       81.1 32.58  336             16384  1.594    .3166 25.89 1.624 .0004 207.1 843.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.072      .6181         0                10871 1018 4144 2.036\n",
            "\n",
            "08:55:31 | time:13610s total_exs:515600 total_steps:32225 epochs:4.15 time_left:2772s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   221.1     1 829.8  3259   .6687      117.4 31.42  320             16384  1.669    .3229  26.8 1.746 .0004 214.4   842   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.731      .6012         0                10891 1044 4101 1.964\n",
            "\n",
            "08:55:41 | time:13620s total_exs:515936 total_steps:32246 epochs:4.16 time_left:2763s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.2     1 813.1  3315   .6280      84.56 32.61  336             16384  1.551    .3125 24.95 1.683 .0004 199.6 813.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.384      .6046         0                10912 1013 4129 2.039\n",
            "\n",
            "08:55:52 | time:13630s total_exs:516272 total_steps:32267 epochs:4.16 time_left:2755s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   201.7     1 832.6  3393   .6607      97.66  32.6  336             16384  1.527    .3125 26.82 1.659 .0004 214.5 874.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.256      .6053         0                10933 1047 4267 2.038\n",
            "\n",
            "08:56:02 | time:13641s total_exs:516608 total_steps:32288 epochs:4.16 time_left:2746s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.5     1 806.1  3267   .6161      83.78 32.42  336             16384  1.559    .3125 24.99 1.634 .0004 199.9 810.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.122      .6170         0                10954 1006 4077 2.027\n",
            "\n",
            "08:56:12 | time:13651s total_exs:516944 total_steps:32309 epochs:4.16 time_left:2738s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.5     1 811.3  3297   .6310      86.13 32.51  336             16384  1.488    .3166 26.18 1.668 .0004 209.5 851.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.304      .6125   .002976                10975 1021 4148 2.032\n",
            "\n",
            "08:56:23 | time:13662s total_exs:517280 total_steps:32330 epochs:4.17 time_left:2729s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.4     1 816.8  3318   .6399      87.26  32.5  336             16384  1.546    .3166  25.3 1.657 .0004 202.4 822.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.245      .6095         0                10996 1019 4140 2.031\n",
            "\n",
            "08:56:33 | time:13672s total_exs:517608 total_steps:32350 epochs:4.17 time_left:2721s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.9     1 803.9  3277   .6220      84.46 32.61  328             16384  1.612    .3125 25.95 1.657 .0004 207.6 846.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.245      .6104         0                11016 1012 4124 2.011\n",
            "\n",
            "08:56:43 | time:13682s total_exs:517936 total_steps:32371 epochs:4.17 time_left:2712s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   210.6     1 841.1  3341   .6799      105.5 31.78  328             16384    1.6    .3125 26.44 1.738 .0004 211.5 840.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.686      .5916         0                11037 1053 4181 2.046\n",
            "\n",
            "08:56:53 | time:13692s total_exs:518264 total_steps:32391 epochs:4.18 time_left:2704s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.8     1 799.8  3243   .6128      84.78 32.44  328             16384  1.674    .3166 26.17 1.737 .0004 209.4 848.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.679      .5925         0                11057 1009 4092 2.002\n",
            "\n",
            "08:57:03 | time:13702s total_exs:518592 total_steps:32412 epochs:4.18 time_left:2696s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.5     1 825.5  3355   .6341      91.29 32.52  328             16384  1.561    .3125 25.19 1.795 .0004 201.5 818.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 6.017      .5835         0                11078 1027 4174 2.096\n",
            "\n",
            "08:57:14 | time:13713s total_exs:518928 total_steps:32433 epochs:4.18 time_left:2687s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.6     1 811.5  3317   .6310      87.12  32.7  336             16384  1.503    .3125 26.34 1.674 .0004 210.7 861.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.335      .6043   .002976                11099 1022 4178 2.044\n",
            "\n",
            "08:57:24 | time:13723s total_exs:519248 total_steps:32453 epochs:4.18 time_left:2679s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   203.8     1 828.7  3275   .6531      100.2 31.61  320             16384  1.566    .3125  28.1 1.716 .0004 224.8 888.3   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.56      .5934         0                11119 1054 4163 1.976\n",
            "\n",
            "08:57:34 | time:13733s total_exs:519576 total_steps:32473 epochs:4.19 time_left:2670s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.4     1 808.3  3281   .6280       85.4 32.48  328             16384  1.648    .3125 26.22 1.636 .0004 209.8 851.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.137      .6092         0                11139 1018 4133 2.003\n",
            "\n",
            "08:57:44 | time:13743s total_exs:519896 total_steps:32493 epochs:4.19 time_left:2662s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.7     1 813.5  3252   .6344      89.01 31.98  320             16384  1.597    .3165 26.88 1.664 .0004 215.1 859.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.281      .6046         0                11159 1029 4112 2.038\n",
            "\n",
            "08:57:54 | time:13753s total_exs:520224 total_steps:32514 epochs:4.19 time_left:2654s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.8     1 816.2  3247   .6524      93.75 31.83  328             16384   1.55    .3166  26.2  1.68 .0004 209.6 833.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.365      .6116         0                11180 1026 4081 2.053\n",
            "\n",
            "08:58:05 | time:13763s total_exs:520560 total_steps:32535 epochs:4.19 time_left:2645s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     184     1   807  3311   .6190      83.15 32.82  336             16384  1.563    .3040 25.82 1.728 .0004 206.5 847.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.629      .5953         0                11201 1014 4158 2.051\n",
            "\n",
            "08:58:15 | time:13773s total_exs:520888 total_steps:32555 epochs:4.20 time_left:2637s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.4     1 819.7  3362   .6555      92.99 32.82  328             16384  1.486    .3125 25.68 1.668 .0004 205.4 842.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0  5.3      .6169         0                11221 1025 4205 2.026\n",
            "\n",
            "08:58:25 | time:13784s total_exs:521216 total_steps:32576 epochs:4.20 time_left:2628s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     188     1 805.8  3247   .6220      87.29 32.23  328             16384  1.491    .3166  25.7 1.663 .0004 205.6 828.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.274      .6111         0                11242 1011 4075 2.077\n",
            "\n",
            "08:58:35 | time:13794s total_exs:521552 total_steps:32597 epochs:4.20 time_left:2620s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.2     1 813.1  3376   .6310      93.57 33.21  336             16384   1.53    .3125  24.6 1.719 .0004 196.8   817   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.581      .5961         0                11263 1010 4193 2.076\n",
            "\n",
            "08:58:45 | time:13804s total_exs:521880 total_steps:32617 epochs:4.20 time_left:2611s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.6     1 810.8  3317   .6250      88.22 32.73  328             16384  1.553    .3125 26.62 1.662 .0004   213 871.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.27      .6036         0                11283 1024 4188 2.019\n",
            "\n",
            "08:58:55 | time:13814s total_exs:522208 total_steps:32638 epochs:4.21 time_left:2603s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   200.5     1   825  3309   .6524      97.43 32.09  328             16384  1.543    .3166 26.36 1.717 .0004 210.9   846   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.569      .5892         0                11304 1036 4155 2.067\n",
            "\n",
            "08:59:05 | time:13824s total_exs:522536 total_steps:32658 epochs:4.21 time_left:2595s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.3     1 809.1  3300   .6220      87.13 32.62  328             16384  1.475    .3125 26.94  1.69 .0004 215.5 878.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.421      .6040         0                11324 1025 4179 2.014\n",
            "\n",
            "08:59:15 | time:13834s total_exs:522864 total_steps:32679 epochs:4.21 time_left:2586s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.2     1 807.9  3270   .6159      81.24 32.38  328             16384  1.864    .3125 24.91 1.641 .0004 199.3 806.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.162      .6063         0                11345 1007 4077 2.087\n",
            "\n",
            "08:59:26 | time:13845s total_exs:523200 total_steps:32700 epochs:4.22 time_left:2578s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.3     1 807.2  3283   .6131      84.41 32.53  336             16384  1.528    .3125 26.55 1.659 .0004 212.4 863.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.252      .6056         0                11366 1020 4146 2.034\n",
            "\n",
            "08:59:36 | time:13855s total_exs:523528 total_steps:32720 epochs:4.22 time_left:2569s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.2     1 818.2  3322   .6494      92.88 32.48  328             16384  1.973    .3126 27.67  1.58 .0004 221.4 898.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.856      .6217         0                11386 1040 4221 2.007\n",
            "\n",
            "08:59:46 | time:13865s total_exs:523848 total_steps:32740 epochs:4.22 time_left:2561s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.5     1 816.6  3254   .6406      96.47 31.88  320             16384  1.571    .3166 27.93 1.704 .0004 223.5 890.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.495      .5991         0                11406 1040 4145 2.03\n",
            "\n",
            "08:59:56 | time:13875s total_exs:524176 total_steps:32761 epochs:4.22 time_left:2553s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.9     1 813.4  3294   .6311      86.21  32.4  328             16384  1.744    .3125 25.56 1.675 .0004 204.5 828.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.339      .6073         0                11427 1018 4122 2.086\n",
            "\n",
            "09:00:06 | time:13885s total_exs:524504 total_steps:32781 epochs:4.23 time_left:2544s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.5     1   803  3285   .6220      88.12 32.73  328             16384  2.392    .3126 26.59 1.647 .0004 212.8 870.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.19      .6038         0                11447 1016 4156 2.018\n",
            "\n",
            "09:00:16 | time:13895s total_exs:524832 total_steps:32802 epochs:4.23 time_left:2536s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.4     1   809  3272   .6280      81.32 32.36  328             16384   1.74    .3125 24.77 1.733 .0004 198.1 801.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.659      .5942   .003049                11468 1007 4073 2.084\n",
            "\n",
            "09:00:26 | time:13905s total_exs:525160 total_steps:32822 epochs:4.23 time_left:2527s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.7     1 814.6  3328   .6250      85.92 32.68  328             16384  1.862    .3125 26.47 1.605 .0004 211.7   865   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.979      .6193         0                11488 1026 4193 2.018\n",
            "\n",
            "09:00:37 | time:13916s total_exs:525488 total_steps:32843 epochs:4.23 time_left:2519s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.8     1 809.7  3228   .6250      85.62  31.9  328             16384  2.611    .3125 26.08  1.67 .0004 208.6 831.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.314      .6011   .003049                11509 1018 4060 2.055\n",
            "\n",
            "09:00:47 | time:13926s total_exs:525824 total_steps:32864 epochs:4.24 time_left:2510s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.6     1 805.8  3284   .6250      88.84 32.61  336             16384  1.604    .3126 26.09 1.569 .0004 208.7 850.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.804      .6231         0                11530 1015 4135 2.038\n",
            "\n",
            "09:00:57 | time:13936s total_exs:526160 total_steps:32885 epochs:4.24 time_left:2502s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.9     1 815.3  3325   .6339      89.96 32.62  336             16384  2.079    .3126 25.86 1.595 .0004 206.9 843.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.929      .6183         0                11551 1022 4168 2.039\n",
            "\n",
            "09:01:07 | time:13946s total_exs:526488 total_steps:32905 epochs:4.24 time_left:2493s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.2     1 830.4  3389   .6494      94.44 32.65  328             16384  1.913    .3166 26.02 1.641 .0004 208.1 849.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.163      .6067         0                11571 1039 4238 2.013\n",
            "\n",
            "09:01:18 | time:13957s total_exs:526816 total_steps:32926 epochs:4.24 time_left:2485s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.1     1 807.2  3248   .6311      82.19 32.19  328             16384  2.079    .3126 25.82 1.632 .0004 206.6 831.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.112      .6120         0                11592 1014 4080 2.073\n",
            "\n",
            "09:01:28 | time:13967s total_exs:527136 total_steps:32946 epochs:4.25 time_left:2477s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.1     1 799.9  3178   .6094      83.08 31.77  320             16384  1.725    .3126  27.1 1.528 .0004 216.8 861.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.607      .6327   .003125                11612 1017 4039 1.987\n",
            "\n",
            "09:01:38 | time:13977s total_exs:527472 total_steps:32967 epochs:4.25 time_left:2468s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.7     1 813.1  3338   .6369      86.07 32.84  336             16384  1.587    .3125 25.14 1.647 .0004 201.1 825.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.194      .6019   .002976                11633 1014 4163 2.053\n",
            "\n",
            "09:01:40 | time:13979s total_exs:527544 total_steps:32971 epochs:4.25 time_left:2466s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   161.2     1 784.7  3644   .5972      63.11 37.15   72             16384  1.698    .3040 21.03 1.523 .0004 168.2 781.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 4.587      .6347         0                11637 952.9 4425 2.203\n",
            "\n",
            "09:01:40 | running eval: valid\n",
            "09:01:58 | eval completed in 18.49s\n",
            "09:01:58 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
            "   217.6 846.8 11185   .6908      110.8 104.7 1934    .2601 27.35 1.914 .0004 216.8  2864       0          0 6.778      .5801   \n",
            "    token_em  total_train_updates  tpb   tps  \n",
            "           0                11637 1064 14049\n",
            "\u001b[0m\n",
            "09:01:58 | \u001b[1mdid not beat best ppl: 6.6321 impatience: 6\u001b[0m\n",
            "09:02:09 | time:14008s total_exs:527872 total_steps:32992 epochs:4.25 time_left:2461s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   179.8     1 800.8  3225   .6067      79.71 32.22  328             16384   1.65    .3126 26.08 1.711 .0004 208.6 840.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.532      .5984         0                11658 1009 4066 2.063\n",
            "\n",
            "09:02:19 | time:14018s total_exs:528200 total_steps:33012 epochs:4.26 time_left:2452s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.4     1   811  3283   .6220      88.05 32.38  328             16384  1.644    .3126 27.47 1.602 .0004 219.8 889.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.961      .6205         0                11678 1031 4172 1.999\n",
            "\n",
            "09:02:29 | time:14028s total_exs:528520 total_steps:33032 epochs:4.26 time_left:2444s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.9     1 822.3  3271   .6531      100.1 31.82  320             16384  1.599    .3166 27.37 1.695 .0004   219 870.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.448      .6011         0                11698 1041 4142 2.028\n",
            "\n",
            "09:02:39 | time:14038s total_exs:528848 total_steps:33053 epochs:4.26 time_left:2436s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     191     1 816.3  3257   .6311      88.94 31.92  328             16384  1.616    .3126 26.17 1.651 .0004 209.4 835.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.21      .6113         0                11719 1026 4092 2.058\n",
            "\n",
            "09:02:50 | time:14048s total_exs:529184 total_steps:33074 epochs:4.26 time_left:2427s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     186     1 809.7  3308   .6220      84.78 32.69  336             16384  1.684    .3126 25.47 1.629 .0004 203.8 832.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0  5.1      .6244   .002976                11740 1013 4141 2.043\n",
            "\n",
            "09:03:00 | time:14059s total_exs:529520 total_steps:33095 epochs:4.27 time_left:2418s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   177.9     1   797  3294   .6071      78.32 33.06  336             16384  1.603    .3125 24.64 1.601 .0004 197.1 814.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 4.959      .6178   .002976                11761 994.1 4108 2.066\n",
            "\n",
            "09:03:10 | time:14069s total_exs:529856 total_steps:33116 epochs:4.27 time_left:2410s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.9     1 810.7  3321   .6280      83.53 32.77  336             16384  1.768    .3166 25.68  1.69 .0004 205.4 841.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.42      .6010         0                11782 1016 4163 2.049\n",
            "\n",
            "09:03:20 | time:14079s total_exs:530192 total_steps:33137 epochs:4.27 time_left:2401s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.5     1 812.5  3305   .6369       88.9 32.54  336             16384  1.588    .3126 26.74  1.69 .0004   214 870.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.421      .6055         0                11803 1026 4175 2.034\n",
            "\n",
            "09:03:30 | time:14089s total_exs:530512 total_steps:33157 epochs:4.27 time_left:2393s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.9     1 808.5  3196   .6375      95.83 31.62  320             16384  1.642    .3126 27.53 1.692 .0004 220.3 870.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.428      .6025         0                11823 1029 4066 1.976\n",
            "\n",
            "09:03:41 | time:14100s total_exs:530840 total_steps:33177 epochs:4.28 time_left:2384s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.1     1 817.8  3321   .6524      92.89 32.49  328             16384  1.616    .3126 27.05 1.642 .0004 216.4 878.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.167      .6085         0                11843 1034 4200 2.001\n",
            "\n",
            "09:03:51 | time:14110s total_exs:531168 total_steps:33198 epochs:4.28 time_left:2376s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   203.6     1 826.6  3287   .6402      100.3 31.82  328             16384  1.553    .3126  26.9 1.563 .0004 215.2 855.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.772      .6281         0                11864 1042 4143 2.047\n",
            "\n",
            "09:04:01 | time:14120s total_exs:531496 total_steps:33218 epochs:4.28 time_left:2367s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.1     1 804.4  3292   .6220      84.58 32.74  328             16384  1.789    .3126 25.98 1.661 .0004 207.8 850.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.262      .6110   .006098                11884 1012 4142 2.023\n",
            "\n",
            "09:04:11 | time:14130s total_exs:531824 total_steps:33239 epochs:4.28 time_left:2359s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.3     1 810.4  3234   .6250      91.02 31.93  328             16384  1.552    .3126 26.59 1.727 .0004 212.7   849   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.623      .5957         0                11905 1023 4083 2.058\n",
            "\n",
            "09:04:21 | time:14140s total_exs:532152 total_steps:33259 epochs:4.29 time_left:2350s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     187     1 812.6  3318   .6402      85.42 32.67  328             16384  1.599    .3126 25.54 1.675 .0004 204.3 834.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.337      .6107         0                11925 1017 4153 2.015\n",
            "\n",
            "09:04:31 | time:14150s total_exs:532472 total_steps:33279 epochs:4.29 time_left:2342s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.9     1 808.1  3229   .6312      92.88 31.96  320             16384  1.615    .3126 27.88 1.624 .0004 223.1 891.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.074      .6149         0                11945 1031 4120 2.033\n",
            "\n",
            "09:04:42 | time:14161s total_exs:532800 total_steps:33300 epochs:4.29 time_left:2334s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.7     1 814.7  3263   .6341      88.89 32.04  328             16384  2.265    .3126  27.1 1.557 .0004 216.8 868.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.743      .6271   .003049                11966 1032 4132 2.064\n",
            "\n",
            "09:04:52 | time:14171s total_exs:533128 total_steps:33320 epochs:4.30 time_left:2325s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.5     1   815  3329   .6372      96.58 32.67  328             16384  1.623    .3166 25.95 1.595 .0004 207.6 847.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.93      .6234         0                11986 1023 4176 2.015\n",
            "\n",
            "09:05:02 | time:14181s total_exs:533464 total_steps:33341 epochs:4.30 time_left:2317s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.8     1 822.1  3403   .6488      91.03 33.12  336             16384  1.628    .3126 25.06 1.582 .0004 200.5   830   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.867      .6220         0                12007 1023 4233 2.11\n",
            "\n",
            "09:05:12 | time:14191s total_exs:533792 total_steps:33362 epochs:4.30 time_left:2308s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   175.6     1 791.6  3164   .5945      76.63 31.97  328             16384  1.561    .3126 25.02 1.585 .0004 200.2 800.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 4.88      .6154   .003049                12028 991.8 3964 2.062\n",
            "\n",
            "09:05:22 | time:14201s total_exs:534128 total_steps:33383 epochs:4.30 time_left:2299s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.9     1 798.4  3259   .6161       86.1 32.65  336             16384  1.565    .3166 25.57 1.603 .0004 204.6   835   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.969      .6136         0                12049 1003 4094 2.041\n",
            "\n",
            "09:05:33 | time:14212s total_exs:534464 total_steps:33404 epochs:4.31 time_left:2291s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.6     1 811.7  3340   .6310      81.16 32.92  336             16384  1.816    .3126 25.25 1.618 .0004   202 831.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.042      .6148         0                12070 1014 4172 2.058\n",
            "\n",
            "09:05:43 | time:14222s total_exs:534792 total_steps:33424 epochs:4.31 time_left:2282s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.2     1   803  3258   .6159      88.78 32.46  328             16384  1.511    .3126 26.86 1.635 .0004 214.9 871.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.132      .6109   .006098                12090 1018 4130 2.003\n",
            "\n",
            "09:05:53 | time:14232s total_exs:535120 total_steps:33445 epochs:4.31 time_left:2274s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   177.8     1 803.2  3213   .6098      77.41    32  328             16384  1.503    .3126 25.43 1.623 .0004 203.5 813.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.07      .6102   .003049                12111 1007 4027 2.062\n",
            "\n",
            "09:06:03 | time:14242s total_exs:535448 total_steps:33465 epochs:4.31 time_left:2265s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.3     1 821.7  3331   .6524      94.62 32.43  328             16384  1.492    .3126 27.53 1.629 .0004 220.3 892.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.097      .6121         0                12131 1042 4223    2\n",
            "\n",
            "09:06:14 | time:14252s total_exs:535776 total_steps:33486 epochs:4.32 time_left:2257s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.7     1 811.2  3236   .6433      93.29 31.91  328             16384  1.673    .3126 26.25 1.705 .0004   210 837.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.499      .6072         0                12152 1021 4073 2.055\n",
            "\n",
            "09:06:24 | time:14263s total_exs:536112 total_steps:33507 epochs:4.32 time_left:2248s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.5     1 822.5  3379   .6429      81.64 32.86  336             16384  1.722    .3126 25.07 1.571 .0004 200.5 823.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.813      .6297         0                12173 1023 4202 2.054\n",
            "\n",
            "09:06:34 | time:14273s total_exs:536432 total_steps:33527 epochs:4.32 time_left:2240s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.4     1 797.7  3180   .6219      85.71 31.89  320             16384  1.623    .3166 26.35 1.705 .0004 210.8 840.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.499      .6080         0                12193 1008 4020 1.993\n",
            "\n",
            "09:06:44 | time:14283s total_exs:536768 total_steps:33548 epochs:4.32 time_left:2231s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.4     1 823.5  3387   .6339      85.46  32.9  336             16384  1.561    .3040 26.51 1.632 .0004 212.1 872.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.114      .6083         0                12214 1036 4259 2.057\n",
            "\n",
            "09:06:54 | time:14293s total_exs:537104 total_steps:33569 epochs:4.33 time_left:2222s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     185     1 809.2  3332   .6220      83.87 32.94  336             16384  2.158    .3040 25.31 1.733 .0004 202.5 833.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.66      .5957         0                12235 1012 4166 2.059\n",
            "\n",
            "09:07:05 | time:14304s total_exs:537440 total_steps:33590 epochs:4.33 time_left:2214s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.3     1 813.6  3326   .6310      89.62  32.7  336             16384  1.969    .3126 25.32 1.589 .0004 202.5   828   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.901      .6202   .002976                12256 1016 4154 2.044\n",
            "\n",
            "09:07:15 | time:14314s total_exs:537768 total_steps:33610 epochs:4.33 time_left:2205s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.7     1 826.5  3353   .6463      93.35 32.45  328             16384   1.77    .3166 26.71 1.594 .0004 213.7 866.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.925      .6225         0                12276 1040 4220 2.001\n",
            "\n",
            "09:07:25 | time:14324s total_exs:538096 total_steps:33631 epochs:4.34 time_left:2196s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     184     1 813.8  3325   .6189      82.23 32.68  328             16384  1.581    .3126 24.77 1.676 .0004 198.2 809.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.344      .6039         0                12297 1012 4134 2.103\n",
            "\n",
            "09:07:35 | time:14334s total_exs:538424 total_steps:33651 epochs:4.34 time_left:2188s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.4     1 818.7  3341   .6463      86.11 32.64  328             16384  1.539    .3126 26.62 1.681 .0004 212.9 868.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.372      .5982         0                12317 1032 4210 2.015\n",
            "\n",
            "09:07:45 | time:14344s total_exs:538752 total_steps:33672 epochs:4.34 time_left:2179s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.1     1 811.7  3258   .6341      87.64 32.11  328             16384  1.555    .3126 25.86 1.624 .0004 206.9 830.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.072      .6158         0                12338 1019 4088 2.069\n",
            "\n",
            "09:07:55 | time:14354s total_exs:539088 total_steps:33693 epochs:4.34 time_left:2171s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.4     1   830  3381   .6399      94.66 32.59  336             16384  1.608    .3126 25.93 1.642 .0004 207.5   845   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.166      .6113         0                12359 1037 4226 2.037\n",
            "\n",
            "09:08:06 | time:14364s total_exs:539416 total_steps:33713 epochs:4.35 time_left:2162s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   201.4     1 822.6  3356   .6463      98.55 32.64  328             16384  1.761    .3126 25.97 1.664 .0004 207.7 847.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.279      .6057         0                12379 1030 4204 2.013\n",
            "\n",
            "09:08:16 | time:14375s total_exs:539744 total_steps:33734 epochs:4.35 time_left:2154s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.4     1 819.3  3250   .6402      88.98 31.73  328             16384  1.658    .3166 26.62 1.679 .0004 212.9 844.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.36      .6077         0                12400 1032 4094 2.044\n",
            "\n",
            "09:08:26 | time:14385s total_exs:540064 total_steps:33754 epochs:4.35 time_left:2145s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   203.3     1 820.3  3278   .6469      100.7 31.97  320             16384  1.483    .3126 27.35 1.643 .0004 218.8 874.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.169      .6129         0                12420 1039 4153 1.998\n",
            "\n",
            "09:08:36 | time:14395s total_exs:540392 total_steps:33774 epochs:4.35 time_left:2137s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.6     1 804.4  3296   .6189      82.09 32.78  328             16384  1.665    .3126 26.63 1.598 .0004 213.1 873.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.945      .6221         0                12440 1017 4170 2.022\n",
            "\n",
            "09:08:46 | time:14405s total_exs:540712 total_steps:33794 epochs:4.36 time_left:2129s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     193     1 809.2  3228   .6250      91.86 31.91  320             16384  1.525    .3288 27.47 1.625 .0004 219.8 876.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.08      .6106         0                12460 1029 4105 2.031\n",
            "\n",
            "09:08:56 | time:14415s total_exs:541040 total_steps:33815 epochs:4.36 time_left:2120s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.3     1 810.9  3252   .6280      87.95 32.08  328             16384  1.552    .3126 24.15 1.679 .0004 193.2 774.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.362      .6059   .006098                12481 1004 4027 2.068\n",
            "\n",
            "09:09:06 | time:14425s total_exs:541368 total_steps:33835 epochs:4.36 time_left:2111s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.4     1 808.8  3290   .6159      86.31 32.54  328             16384  1.646    .3126 26.11 1.673 .0004 208.9 849.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.328      .6067         0                12501 1018 4139 2.009\n",
            "\n",
            "09:09:17 | time:14436s total_exs:541696 total_steps:33856 epochs:4.36 time_left:2103s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   207.1     1 831.4  3306   .6677      103.2 31.81  328             16384  1.728    .3126 26.79 1.694 .0004 214.3 852.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.441      .6049         0                12522 1046 4158 2.051\n",
            "\n",
            "09:09:27 | time:14446s total_exs:542016 total_steps:33876 epochs:4.37 time_left:2095s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   199.5     1 821.9  3266   .6531       96.8 31.79  320             16384  1.789    .3166 26.35 1.645 .0004 210.8 837.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.181      .6177         0                12542 1033 4104 1.987\n",
            "\n",
            "09:09:37 | time:14456s total_exs:542352 total_steps:33897 epochs:4.37 time_left:2086s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   175.2     1 797.9  3309   .5952      75.46 33.18  336             16384  2.264    .3126 24.19 1.613 .0004 193.5 802.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 5.018      .6097         0                12563 991.5 4112 2.074\n",
            "\n",
            "09:09:47 | time:14466s total_exs:542688 total_steps:33918 epochs:4.37 time_left:2077s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.6     1 806.3  3283   .6190      83.85 32.57  336             16384  2.209    .3126  24.8 1.639 .0004 198.4 807.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.149      .6094   .002976                12584 1005 4090 2.036\n",
            "\n",
            "09:09:58 | time:14477s total_exs:543024 total_steps:33939 epochs:4.37 time_left:2068s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.3     1 809.6  3309   .6280      87.07  32.7  336             16384   1.61    .3126 26.11 1.622 .0004 208.9 853.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.062      .6143         0                12605 1018 4163 2.044\n",
            "\n",
            "09:10:08 | time:14487s total_exs:543352 total_steps:33959 epochs:4.38 time_left:2060s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.7     1 801.5  3272   .6128       82.5 32.66  328             19661  1.568    .3126 24.78 1.607 .0004 198.2 809.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 4.985      .6134   .003049                12625 999.7 4081 2.016\n",
            "\n",
            "09:10:18 | time:14497s total_exs:543680 total_steps:33980 epochs:4.38 time_left:2051s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.7     1 808.1  3251   .6220      81.67 32.19  328             32768  1.538    .3126 25.06 1.611 .0004 200.5 806.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.007      .6133         0                12646 1009 4058 2.072\n",
            "\n",
            "09:10:28 | time:14507s total_exs:544008 total_steps:34000 epochs:4.38 time_left:2043s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.5     1 818.5  3338   .6372      88.16 32.62  328             32768  1.582    .3126 26.34 1.579 .0004 210.7 859.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.849      .6245         0                12666 1029 4197 2.012\n",
            "\n",
            "09:10:38 | time:14517s total_exs:544336 total_steps:34021 epochs:4.39 time_left:2034s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.3     1 809.2  3271   .6220      85.12 32.34  328             32768  1.804    .3126 25.64 1.692 .0004 205.1 829.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.433      .6032   .003049                12687 1014 4100 2.083\n",
            "\n",
            "09:10:48 | time:14527s total_exs:544664 total_steps:34041 epochs:4.39 time_left:2026s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.3     1 818.7  3320   .6372      92.95 32.44  328             32768  1.635    .3126 25.47 1.665 .0004 203.8 826.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.284      .6054   .003049                12707 1023 4146 2.005\n",
            "\n",
            "09:10:59 | time:14538s total_exs:544992 total_steps:34062 epochs:4.39 time_left:2017s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.2     1 803.5  3201   .6128      82.78 31.87  328             32768  1.579    .3126 24.85 1.616 .0004 198.8 792.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.035      .6136   .003049                12728 1002 3993 2.056\n",
            "\n",
            "09:11:07 | Overflow: setting loss scale to 16384.0\n",
            "09:11:09 | time:14548s total_exs:545328 total_steps:34083 epochs:4.39 time_left:2008s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   179.3 .9524 805.9  3306   .6071      78.58 32.82  336             29647  1.845    .3126  24.3 1.601 .0004 194.4 797.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.96      .6170         0                12749 1000 4104 2.052\n",
            "\n",
            "09:11:19 | time:14558s total_exs:545664 total_steps:34104 epochs:4.40 time_left:2000s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.3     1 811.4  3304   .6190       82.9 32.58  336             16384  2.669    .3126 25.55 1.622 .0004 204.4 832.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.063      .6112   .002976                12770 1016 4136 2.036\n",
            "\n",
            "09:11:29 | time:14568s total_exs:545984 total_steps:34124 epochs:4.40 time_left:1991s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.4     1 811.8  3236   .6156      90.97 31.89  320             16384   3.02    .3126 26.97 1.528 .0004 215.8   860   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.609      .6331         0                12790 1028 4096 1.993\n",
            "\n",
            "09:11:39 | time:14578s total_exs:546312 total_steps:34144 epochs:4.40 time_left:1983s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.1     1 809.3  3294   .6128      87.91 32.55  328             16384  1.816    .3166 26.19 1.665 .0004 209.5 852.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.285      .6078         0                12810 1019 4146 2.01\n",
            "\n",
            "09:11:50 | time:14589s total_exs:546640 total_steps:34165 epochs:4.40 time_left:1974s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   179.4     1 796.5  3182   .6006      79.87 31.96  328             16384  2.148    .3126 25.87 1.595 .0004   207   827   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.927      .6162         0                12831 1003 4009 2.059\n",
            "\n",
            "09:12:00 | time:14599s total_exs:546968 total_steps:34185 epochs:4.41 time_left:1966s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.9     1 817.8  3320   .6341      90.64 32.48  328             16384  1.663    .3166 26.15 1.656 .0004 209.2 849.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.241      .6052         0                12851 1027 4170 2.005\n",
            "\n",
            "09:12:10 | time:14609s total_exs:547288 total_steps:34205 epochs:4.41 time_left:1957s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.7     1 823.1  3282   .6562      95.81  31.9  320             16384  1.722    .3288 26.16 1.603 .0004 209.3 834.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.966      .6152         0                12871 1032 4117 2.03\n",
            "\n",
            "09:12:20 | time:14619s total_exs:547616 total_steps:34226 epochs:4.41 time_left:1949s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.9     1 822.7  3263   .6585      95.09 31.73  328             16384  1.735    .3166 26.96 1.657 .0004 215.7 855.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.244      .6109         0                12892 1038 4118 2.042\n",
            "\n",
            "09:12:30 | time:14629s total_exs:547944 total_steps:34246 epochs:4.41 time_left:1940s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.4     1 813.8  3309   .6402      94.73 32.53  328             16384  1.516    .3126 26.02 1.705 .0004 208.1 846.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.503      .6042         0                12912 1022 4155 2.006\n",
            "\n",
            "09:12:40 | time:14639s total_exs:548272 total_steps:34267 epochs:4.42 time_left:1931s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.7     1 820.6  3306   .6524      91.08 32.23  328             16384  1.538    .3126  25.8 1.607 .0004 206.4 831.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.987      .6173         0                12933 1027 4137 2.076\n",
            "\n",
            "09:12:51 | time:14649s total_exs:548600 total_steps:34287 epochs:4.42 time_left:1923s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.6     1   819  3348   .6341      90.25  32.7  328             16384   1.62    .3126 24.97 1.709 .0004 199.8 816.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.525      .6002         0                12953 1019 4164 2.017\n",
            "\n",
            "09:13:01 | time:14660s total_exs:548928 total_steps:34308 epochs:4.42 time_left:1914s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.5     1 822.4  3266   .6463      93.75 31.77  328             16384  1.694    .3126 27.63 1.593 .0004 221.1   878   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.916      .6231         0                12974 1043 4144 2.045\n",
            "\n",
            "09:13:11 | time:14670s total_exs:549248 total_steps:34328 epochs:4.43 time_left:1906s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.3     1 807.2  3228   .6281      83.42 31.99  320             16384  1.645    .3126 26.26 1.607 .0004 210.1 840.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.989      .6155         0                12994 1017 4068    2\n",
            "\n",
            "09:13:21 | time:14680s total_exs:549576 total_steps:34348 epochs:4.43 time_left:1897s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.8     1 820.4  3355   .6433      88.29 32.72  328             16384  1.606    .3126 26.86 1.727 .0004 214.9 878.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.624      .5990         0                13014 1035 4234 2.021\n",
            "\n",
            "09:13:31 | time:14690s total_exs:549904 total_steps:34369 epochs:4.43 time_left:1889s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     195     1 825.2  3354   .6494      91.88 32.52  328             16384  1.553    .3126 26.49 1.677 .0004 211.9 861.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.35      .6054         0                13035 1037 4216 2.094\n",
            "\n",
            "09:13:41 | time:14700s total_exs:550232 total_steps:34389 epochs:4.43 time_left:1880s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.1     1 830.9  3397   .6616      98.23  32.7  328             16384   1.56    .3126 26.56 1.671 .0004 212.5 868.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.318      .6046   .003049                13055 1043 4265 2.017\n",
            "\n",
            "09:13:51 | time:14710s total_exs:550560 total_steps:34410 epochs:4.44 time_left:1872s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.2     1 803.1  3204   .6159      81.79 31.92  328             16384  1.546    .3126 25.15 1.642 .0004 201.2 802.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.167      .6085         0                13076 1004 4007 2.055\n",
            "\n",
            "09:14:01 | time:14720s total_exs:550888 total_steps:34430 epochs:4.44 time_left:1863s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.9     1 820.2  3362   .6372      86.38  32.8  328             16384  1.543    .3166 25.58 1.605 .0004 204.7   839   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.98      .6204         0                13096 1025 4202 2.025\n",
            "\n",
            "09:14:12 | time:14731s total_exs:551216 total_steps:34451 epochs:4.44 time_left:1854s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   211.5     1   821  3264   .6341      108.9  31.8  328             16384   1.55    .3167 25.73 1.657 .0004 205.8 818.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.242      .6078         0                13117 1027 4082 2.049\n",
            "\n",
            "09:14:22 | time:14741s total_exs:551552 total_steps:34472 epochs:4.44 time_left:1846s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.3     1 813.1  3309   .6280      89.65 32.55  336             16384   1.64    .3126 25.62 1.623 .0004   205 834.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.068      .6212         0                13138 1018 4143 2.035\n",
            "\n",
            "09:14:32 | time:14751s total_exs:551880 total_steps:34492 epochs:4.45 time_left:1837s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.4     1 814.2  3328   .6402      83.65  32.7  328             16384  1.607    .3166 25.13 1.626 .0004   201 821.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.085      .6173         0                13158 1015 4150 2.021\n",
            "\n",
            "09:14:42 | time:14761s total_exs:552208 total_steps:34513 epochs:4.45 time_left:1828s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.8     1 810.2  3273   .6189      87.52 32.32  328             16384  1.675    .3126 26.07  1.68 .0004 208.5 842.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.366      .6085         0                13179 1019 4116 2.083\n",
            "\n",
            "09:14:52 | time:14771s total_exs:552528 total_steps:34533 epochs:4.45 time_left:1820s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.8     1 813.6  3230   .6438      96.12 31.77  320             16384  2.092    .3126 27.32 1.585 .0004 218.6 867.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.877      .6294         0                13199 1032 4098 1.986\n",
            "\n",
            "09:15:03 | time:14782s total_exs:552864 total_steps:34554 epochs:4.45 time_left:1811s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.8     1 807.4  3273   .6101      83.89 32.43  336             16384  1.539    .3126 26.38 1.645 .0004 211.1 855.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.182      .6094         0                13220 1018 4128 2.027\n",
            "\n",
            "09:15:13 | time:14792s total_exs:553192 total_steps:34574 epochs:4.46 time_left:1803s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     205     1 833.7  3386   .6616      100.8 32.49  328             16384  1.492    .3126 27.14 1.635 .0004 217.1 881.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.128      .6100         0                13240 1051 4268 2.003\n",
            "\n",
            "09:15:23 | time:14802s total_exs:553520 total_steps:34595 epochs:4.46 time_left:1794s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.8     1   810  3260   .6128       82.5 32.19  328             16384  1.663    .3126 25.86 1.614 .0004 206.9 832.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.025      .6177         0                13261 1017 4092 2.073\n",
            "\n",
            "09:15:33 | time:14812s total_exs:553840 total_steps:34615 epochs:4.46 time_left:1786s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.9     1 811.1  3233   .6312      92.47 31.89  320             16384  1.704    .3126 26.37 1.606 .0004 210.9 840.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.981      .6242   .003125                13281 1022 4074 1.993\n",
            "\n",
            "09:15:43 | time:14822s total_exs:554168 total_steps:34635 epochs:4.46 time_left:1777s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.8     1   807  3290   .6128      85.97 32.62  328             16384  1.815    .3166 25.48 1.706 .0004 203.8 830.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.508      .6012         0                13301 1011 4121 2.011\n",
            "\n",
            "09:15:54 | time:14833s total_exs:554496 total_steps:34656 epochs:4.47 time_left:1769s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   203.8     1 821.3  3278   .6646      101.1 31.93  328             16384  1.782    .3126 27.63 1.729 .0004   221 882.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.636      .6004         0                13322 1042 4161 2.054\n",
            "\n",
            "09:16:04 | time:14843s total_exs:554824 total_steps:34676 epochs:4.47 time_left:1760s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.6     1 809.8  3305   .6250      87.39 32.65  328             16384  1.937    .3126 26.51 1.708 .0004 212.1 865.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.516      .6038   .003049                13342 1022 4171 2.018\n",
            "\n",
            "09:16:14 | time:14853s total_exs:555152 total_steps:34697 epochs:4.47 time_left:1751s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.1     1 806.9  3192   .6250      86.27 31.65  328             16384  1.863    .3126 26.23  1.54 .0004 209.9 830.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.667      .6286         0                13363 1017 4022 2.039\n",
            "\n",
            "09:16:24 | time:14863s total_exs:555480 total_steps:34717 epochs:4.48 time_left:1743s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.1     1 820.5  3352   .6372      87.52 32.68  328             16384  1.744    .3126 25.87  1.67 .0004 206.9 845.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.314      .6005         0                13383 1027 4197 2.013\n",
            "\n",
            "09:16:34 | time:14873s total_exs:555808 total_steps:34738 epochs:4.48 time_left:1734s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.9     1 810.2  3246   .6280      88.59 32.05  328             16384  1.876    .3126 26.62 1.659 .0004   213 853.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.256      .6090         0                13404 1023 4099 2.063\n",
            "\n",
            "09:16:45 | time:14884s total_exs:556144 total_steps:34759 epochs:4.48 time_left:1725s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   181.2     1 803.6  3298   .6161      80.74 32.83  336             16384  2.397    .3126 26.43 1.585 .0004 211.5 867.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.88      .6211         0                13425 1015 4165 2.052\n",
            "\n",
            "09:16:55 | time:14894s total_exs:556472 total_steps:34779 epochs:4.48 time_left:1717s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.5     1 813.5  3331   .6341      90.84 32.76  328             16384  1.837    .3126 25.92 1.585 .0004 207.3   849   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.878      .6210         0                13445 1021 4180 2.022\n",
            "\n",
            "09:17:05 | time:14904s total_exs:556800 total_steps:34800 epochs:4.49 time_left:1708s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.5     1 810.9  3316   .6189       84.1 32.72  328             16384  1.572    .3126    25 1.676 .0004   200 817.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.345      .6000         0                13466 1011 4134 2.109\n",
            "\n",
            "09:17:15 | time:14914s total_exs:557136 total_steps:34821 epochs:4.49 time_left:1699s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.6     1 817.6  3346   .6310      89.38 32.74  336             16384  1.509    .3167  26.8 1.634 .0004 214.4 877.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.122      .6137   .002976                13487 1032 4224 2.047\n",
            "\n",
            "09:17:25 | time:14924s total_exs:557464 total_steps:34841 epochs:4.49 time_left:1690s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.7     1 808.2  3313   .6311      87.69 32.79  328             16384  1.883    .3167 25.61 1.587 .0004 204.9 839.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.89      .6204         0                13507 1013 4153 2.022\n",
            "\n",
            "09:17:35 | time:14934s total_exs:557792 total_steps:34862 epochs:4.49 time_left:1682s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.5     1 818.7  3253   .6433      95.17 31.79  328             16384  1.767    .3126 26.91 1.667 .0004 215.3 855.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.296      .6068         0                13528 1034 4109 2.047\n",
            "\n",
            "09:17:45 | time:14944s total_exs:558112 total_steps:34882 epochs:4.50 time_left:1673s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.2     1 818.2  3268   .6312      94.94 31.95  320             16384  1.483    .3126 27.44 1.525 .0004 219.6 876.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.597      .6306         0                13548 1038 4145 1.997\n",
            "\n",
            "09:17:56 | time:14955s total_exs:558440 total_steps:34902 epochs:4.50 time_left:1665s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.1     1 797.8  3235   .6128      84.33 32.44  328             16384  1.581    .3126 26.08 1.473 .0004 208.6 845.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.36      .6384   .006098                13568 1006 4081 2.003\n",
            "\n",
            "09:18:00 | time:14959s total_exs:558576 total_steps:34911 epochs:4.50 time_left:1661s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.3     1 836.1  3238   .6471      97.79 30.98  136             16384  1.672    .3167 26.77 1.639 .0004 214.2 829.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.15      .6116         0                13577 1050 4067 2.081\n",
            "\n",
            "09:18:00 | running eval: valid\n",
            "09:18:18 | eval completed in 18.43s\n",
            "09:18:18 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
            "   217.6 846.8 11222   .6908      110.8   105 1934    .2602 27.35 1.887 .0004 216.8  2873       0          0 6.599      .5853   \n",
            "    token_em  total_train_updates  tpb   tps  \n",
            "    .0005171                13577 1064 14095\n",
            "\u001b[0m\n",
            "09:18:18 | \u001b[1;32mnew best ppl: 6.599 (previous best was 6.632)\u001b[0m\n",
            "09:18:18 | saving best valid model: /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model\n",
            "09:18:44 | time:15003s total_exs:558832 total_steps:34927 epochs:4.50 time_left:1659s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   205.7     1 831.8  2568   .6680      101.8  24.7  256             16384  1.542    .3126 27.24 1.649 .0004 217.9 672.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.202      .6154         0                13593 1050 3241 1.544\n",
            "\n",
            "09:18:55 | time:15014s total_exs:559048 total_steps:34940 epochs:4.50 time_left:1653s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.9     1 817.4  2176   .6481      93.75 21.29  216             16384  1.525    .3126 26.33 1.616 .0004 210.7 560.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.033      .6197         0                13606 1028 2737 1.31\n",
            "\n",
            "09:19:05 | time:15024s total_exs:559248 total_steps:34953 epochs:4.51 time_left:1648s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.5     1 818.3  1962   .6450      94.22 19.18  200             16384  1.508    .3126 26.61 1.615 .0004 212.9 510.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.026      .6148         0                13619 1031 2473 1.247\n",
            "\n",
            "09:19:15 | time:15034s total_exs:559440 total_steps:34965 epochs:4.51 time_left:1644s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   204.7     1 827.8  1970   .6562      101.2 19.04  192             16384  1.571    .3127 24.17 1.588 .0004 193.4 460.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.894      .6270         0                13631 1021 2430 1.19\n",
            "\n",
            "09:19:26 | time:15044s total_exs:559704 total_steps:34981 epochs:4.51 time_left:1637s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   204.8     1 799.5  2600   .6212      104.8 26.01  264             16384  1.595    .3126 25.73 1.666 .0004 205.8 669.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.289      .6117         0                13647 1005 3269 1.607\n",
            "\n",
            "09:19:36 | time:15055s total_exs:560016 total_steps:35001 epochs:4.51 time_left:1629s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.8     1 815.2  3082   .6506      89.92 30.25  312             16384  3.408    .3127 27.49 1.649 .0004 219.9 831.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0  5.2      .6135         0                13667 1035 3914 1.942\n",
            "\n",
            "09:19:46 | time:15065s total_exs:560320 total_steps:35020 epochs:4.51 time_left:1621s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.3     1 789.7  2964   .6020      87.57 30.03  304             16384   1.78    .3126 26.14 1.696 .0004 209.1   785   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 5.453      .6065         0                13686 998.8 3749 1.877\n",
            "\n",
            "09:19:56 | time:15075s total_exs:560624 total_steps:35039 epochs:4.52 time_left:1613s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.5     1 824.7  3076   .6414      93.43 29.83  304             16384  1.886    .3041 26.07 1.601 .0004 208.6 777.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.956      .6111         0                13705 1033 3854 1.865\n",
            "\n",
            "09:20:06 | time:15085s total_exs:560920 total_steps:35057 epochs:4.52 time_left:1605s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   205.3     1 816.7  3023   .6554      103.2 29.61  296             16384  2.375    .3126 26.33 1.557 .0004 210.6 779.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.746      .6279         0                13723 1027 3802 1.828\n",
            "\n",
            "09:20:16 | time:15095s total_exs:561224 total_steps:35076 epochs:4.52 time_left:1597s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.2     1 807.2  3029   .6118      81.25 30.02  304             16384  2.045    .3127 26.25 1.635 .0004   210   788   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.127      .6062   .003289                13742 1017 3817 1.911\n",
            "\n",
            "09:20:27 | time:15106s total_exs:561536 total_steps:35096 epochs:4.52 time_left:1589s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     193     1 826.8  3125   .6571      89.63 30.24  312             16384  2.324    .3127 25.92 1.598 .0004 207.3 783.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.945      .6221         0                13762 1034 3909 1.942\n",
            "\n",
            "09:20:37 | time:15116s total_exs:561872 total_steps:35117 epochs:4.53 time_left:1580s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.7     1 811.1  3316   .6280      89.27 32.71  336             16384  2.697    .3167 25.98 1.603 .0004 207.8 849.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.97      .6213   .002976                13783 1019 4166 2.045\n",
            "\n",
            "09:20:47 | time:15126s total_exs:562208 total_steps:35138 epochs:4.53 time_left:1571s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   171.8     1 794.4  3286   .5833      72.49 33.09  336             16384  2.182    .3126 24.12 1.604 .0004   193 798.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 4.974      .6173   .002976                13804 987.4 4084 2.068\n",
            "\n",
            "09:20:58 | time:15137s total_exs:562544 total_steps:35159 epochs:4.53 time_left:1562s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.9     1 819.5  3347   .6429      90.43 32.67  336             16384  3.049    .3127 25.29 1.614 .0004 202.3 826.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.025      .6163   .002976                13825 1022 4173 2.042\n",
            "\n",
            "09:21:08 | time:15147s total_exs:562872 total_steps:35179 epochs:4.53 time_left:1554s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.7     1 798.6  3237   .6067      84.83 32.43  328             16384    1.7    .3126 26.25 1.636 .0004   210 851.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.133      .6154   .003049                13845 1009 4088 1.999\n",
            "\n",
            "09:21:18 | time:15157s total_exs:563200 total_steps:35200 epochs:4.54 time_left:1545s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     189     1 819.9  3291   .6463      86.56 32.11  328             16384  2.132    .3126 26.09  1.71 .0004 208.8 837.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.526      .5974   .003049                13866 1029 4129 2.068\n",
            "\n",
            "09:21:28 | time:15167s total_exs:563536 total_steps:35221 epochs:4.54 time_left:1536s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     188     1 807.4  3286   .6339       87.1 32.55  336             16384  1.627    .3127 25.55 1.647 .0004 204.4 831.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.19      .6057         0                13887 1012 4117 2.035\n",
            "\n",
            "09:21:38 | time:15177s total_exs:563864 total_steps:35241 epochs:4.54 time_left:1527s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     199     1 816.3  3316   .6372      96.94  32.5  328             16384  1.855    .3127    26 1.624 .0004   208 844.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.073      .6091   .003049                13907 1024 4161 2.006\n",
            "\n",
            "09:21:49 | time:15188s total_exs:564192 total_steps:35262 epochs:4.55 time_left:1519s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   201.5     1 826.1  3292   .6555       98.2 31.88  328             16384  1.667    .3126 26.26 1.619 .0004   210   837   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.049      .6159         0                13928 1036 4129 2.055\n",
            "\n",
            "09:21:59 | time:15198s total_exs:564520 total_steps:35282 epochs:4.55 time_left:1510s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.3     1 814.5  3313   .6372      91.48 32.54  328             16384  1.517    .3127 26.67 1.585 .0004 213.4   868   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.879      .6239         0                13948 1028 4182 2.009\n",
            "\n",
            "09:22:09 | time:15208s total_exs:564840 total_steps:35302 epochs:4.55 time_left:1502s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.6     1 804.6  3214   .6156      88.06 31.96  320             16384  1.831    .3127  26.3 1.608 .0004 210.4 840.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.992      .6100         0                13968 1015 4055 2.035\n",
            "\n",
            "09:22:19 | time:15218s total_exs:565168 total_steps:35323 epochs:4.55 time_left:1493s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.5     1 829.8  3390   .6585      86.79 32.69  328             16384  1.781    .3041 24.45 1.694 .0004 195.6 799.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.442      .5959         0                13989 1025 4189 2.107\n",
            "\n",
            "09:22:29 | time:15228s total_exs:565504 total_steps:35344 epochs:4.56 time_left:1484s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   178.5     1 797.8  3271   .6220      78.74  32.8  336             16384  1.491    .3126 24.71 1.565 .0004 197.7 810.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 4.782      .6230         0                14010 995.5 4082 2.05\n",
            "\n",
            "09:22:39 | time:15238s total_exs:565832 total_steps:35364 epochs:4.56 time_left:1475s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.4     1 814.3  3322   .6341      84.65 32.64  328             16384  1.471    .3127 26.61 1.616 .0004 212.9 868.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.031      .6216         0                14030 1027 4190 2.015\n",
            "\n",
            "09:22:50 | time:15249s total_exs:566160 total_steps:35385 epochs:4.56 time_left:1467s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.7     1 807.8  3217   .6341      91.69 31.86  328             16384  2.004    .3167  26.3  1.57 .0004 210.4 837.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.807      .6211         0                14051 1018 4055 2.052\n",
            "\n",
            "09:23:00 | time:15259s total_exs:566488 total_steps:35405 epochs:4.56 time_left:1458s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     197     1 818.5  3335   .6402      94.67 32.59  328             16384  1.627    .3127  25.8 1.699 .0004 206.4 840.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.469      .6018   .009146                14071 1025 4176 2.012\n",
            "\n",
            "09:23:10 | time:15269s total_exs:566816 total_steps:35426 epochs:4.57 time_left:1449s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.8     1 820.1  3242   .6372      92.33 31.63  328             16384  1.564    .3167 27.94 1.624 .0004 223.5 883.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.076      .6153         0                14092 1044 4126 2.038\n",
            "\n",
            "09:23:20 | time:15279s total_exs:567144 total_steps:35446 epochs:4.57 time_left:1440s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.3     1 821.9  3332   .6463      89.52 32.43  328             16384  1.594    .3127 26.16 1.539 .0004 209.2 848.3   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.66      .6327         0                14112 1031 4181 2.002\n",
            "\n",
            "09:23:30 | time:15289s total_exs:567472 total_steps:35467 epochs:4.57 time_left:1432s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   177.3     1 795.7  3220   .6006      77.86 32.37  328             16384  1.747    .3167 25.34 1.506 .0004 202.7 820.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 4.507      .6342         0                14133 998.4 4040 2.085\n",
            "\n",
            "09:23:40 | time:15299s total_exs:567800 total_steps:35487 epochs:4.57 time_left:1423s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.3     1 812.8  3328   .6494      89.68 32.75  328             16384  1.854    .3127  25.8 1.651 .0004 206.4 845.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.213      .6081         0                14153 1019 4173 2.024\n",
            "\n",
            "09:23:51 | time:15309s total_exs:568128 total_steps:35508 epochs:4.58 time_left:1414s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.9     1 811.5  3303   .6067      83.46 32.56  328             16384    1.8    .3127 24.46 1.573 .0004 195.7 796.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.821      .6197         0                14174 1007 4099 2.099\n",
            "\n",
            "09:24:01 | time:15320s total_exs:568456 total_steps:35528 epochs:4.58 time_left:1406s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.5     1 806.7  3298   .6159      87.63 32.71  328             16384  1.824    .3127 25.95 1.669 .0004 207.6   849   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.306      .6039         0                14194 1014 4147 2.022\n",
            "\n",
            "09:24:11 | time:15330s total_exs:568784 total_steps:35549 epochs:4.58 time_left:1397s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.1     1 819.6  3256   .6402      84.68 31.78  328             16384  1.473    .3167 26.36 1.581 .0004 210.9 837.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.858      .6255         0                14215 1030 4094 2.048\n",
            "\n",
            "09:24:21 | time:15340s total_exs:569104 total_steps:35569 epochs:4.59 time_left:1388s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   203.8     1 818.9  3273   .6562      101.4 31.97  320             16384  1.705    .3167 27.46 1.642 .0004 219.7   878   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.167      .6091   .003125                14235 1039 4151 1.999\n",
            "\n",
            "09:24:31 | time:15350s total_exs:569432 total_steps:35589 epochs:4.59 time_left:1380s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     191     1 819.4  3344   .6463      88.62 32.64  328             16384  1.641    .3127 27.23 1.585 .0004 217.9   889   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.879      .6286         0                14255 1037 4233 2.013\n",
            "\n",
            "09:24:41 | time:15360s total_exs:569760 total_steps:35610 epochs:4.59 time_left:1371s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.1     1 808.2  3279   .6189      86.04 32.45  328             16384  1.529    .3041  24.8  1.57 .0004 198.4 804.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.807      .6246         0                14276 1007 4084 2.09\n",
            "\n",
            "09:24:51 | time:15370s total_exs:570088 total_steps:35630 epochs:4.59 time_left:1362s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.8     1 825.6  3345   .6524      91.58 32.41  328             16384  1.662    .3127 26.46 1.578 .0004 211.7 857.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.845      .6235         0                14296 1037 4202 2.001\n",
            "\n",
            "09:25:01 | time:15380s total_exs:570408 total_steps:35650 epochs:4.60 time_left:1354s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     207     1   827  3298   .6562      103.6  31.9  320             16384  1.659    .3127 27.86 1.577 .0004 222.8 888.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.841      .6223         0                14316 1050 4186 2.03\n",
            "\n",
            "09:25:11 | time:15390s total_exs:570728 total_steps:35670 epochs:4.60 time_left:1345s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.8     1 823.5  3284   .6500      93.83  31.9  320             16384  1.652    .3127 27.58   1.6 .0004 220.6 879.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.955      .6188         0                14336 1044 4164 2.03\n",
            "\n",
            "09:25:22 | time:15401s total_exs:571056 total_steps:35691 epochs:4.60 time_left:1336s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     191     1 822.5  3317   .6463      88.19 32.26  328             16384   1.76    .3127 25.69 1.677 .0004 205.5 828.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.349      .6069         0                14357 1028 4146 2.079\n",
            "\n",
            "09:25:32 | time:15411s total_exs:571376 total_steps:35711 epochs:4.60 time_left:1328s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.1     1 805.8  3193   .6250      96.42  31.7  320             16384  1.531    .3167 26.76 1.636 .0004 214.1 848.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.134      .6153         0                14377 1020 4041 1.982\n",
            "\n",
            "09:25:42 | time:15421s total_exs:571696 total_steps:35731 epochs:4.61 time_left:1319s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.6     1 818.1  3256   .6438      100.3 31.84  320             16384  2.122    .3127 27.75  1.53 .0004   222 883.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.619      .6333         0                14397 1040 4139 1.99\n",
            "\n",
            "09:25:52 | time:15431s total_exs:572032 total_steps:35752 epochs:4.61 time_left:1310s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     189     1 803.2  3281   .6161      88.62 32.68  336             16384  1.878    .3127 25.31 1.607 .0004 202.5 827.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.988      .6209         0                14418 1006 4109 2.043\n",
            "\n",
            "09:26:03 | time:15441s total_exs:572368 total_steps:35773 epochs:4.61 time_left:1301s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.8     1 817.2  3314   .6488      100.7 32.45  336             16384  1.714    .3127 26.32 1.643 .0004 210.5 853.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.17      .6148         0                14439 1028 4168 2.028\n",
            "\n",
            "09:26:13 | time:15451s total_exs:572696 total_steps:35793 epochs:4.61 time_left:1293s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.1     1 821.2  3365   .6463      94.46 32.78  328             16384  1.626    .3127 26.57 1.561 .0004 212.6   871   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.763      .6224         0                14459 1034 4236 2.024\n",
            "\n",
            "09:26:23 | time:15462s total_exs:573016 total_steps:35813 epochs:4.62 time_left:1284s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.4     1 814.1  3242   .6438      89.63 31.86  320             16384  1.515    .3167 26.71 1.577 .0004 213.7   851   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.84      .6178         0                14479 1028 4093 2.028\n",
            "\n",
            "09:26:33 | time:15472s total_exs:573344 total_steps:35834 epochs:4.62 time_left:1275s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.8     1 808.4  3202   .6341      86.74 31.69  328             16384  1.631    .3167  26.3  1.56 .0004 210.4 833.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.761      .6171         0                14500 1019 4035 2.039\n",
            "\n",
            "09:26:43 | time:15482s total_exs:573672 total_steps:35854 epochs:4.62 time_left:1267s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.4     1 820.9  3364   .6463      87.81 32.78  328             16384  1.651    .3127 25.76 1.565 .0004   206 844.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.783      .6236         0                14520 1027 4208 2.024\n",
            "\n",
            "09:26:53 | time:15492s total_exs:573992 total_steps:35874 epochs:4.62 time_left:1258s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     186     1 803.4  3214   .6156      85.56    32  320             16384   1.55    .3127  26.8 1.656 .0004 214.4 857.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.236      .6078         0                14540 1018 4071 2.036\n",
            "\n",
            "09:27:03 | time:15502s total_exs:574320 total_steps:35895 epochs:4.63 time_left:1249s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   179.6     1 803.4  3257   .6311      79.18 32.43  328             16384  1.559    .3167 24.89 1.623 .0004 199.1 807.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.066      .6099         0                14561 1003 4065 2.089\n",
            "\n",
            "09:27:14 | time:15512s total_exs:574656 total_steps:35916 epochs:4.63 time_left:1240s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.5     1 814.2  3312   .6369      86.77 32.54  336             16384   1.56    .3127 25.68 1.565 .0004 205.4 835.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.781      .6280         0                14582 1020 4148 2.034\n",
            "\n",
            "09:27:24 | time:15523s total_exs:574992 total_steps:35937 epochs:4.63 time_left:1232s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     185     1 804.9  3322   .6101      84.36 33.02  336             16384  1.742    .3127 25.03 1.593 .0004 200.2 826.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.921      .6101         0                14603 1005 4149 2.064\n",
            "\n",
            "09:27:34 | time:15533s total_exs:575328 total_steps:35958 epochs:4.64 time_left:1223s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.2     1 832.5  3445   .6518      87.13  33.1  336             16384  1.984    .3127 24.94 1.575 .0004 199.5 825.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.829      .6195         0                14624 1032 4270 2.069\n",
            "\n",
            "09:27:44 | time:15543s total_exs:575648 total_steps:35978 epochs:4.64 time_left:1214s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   214.6     1 823.1  3251   .6562      111.7  31.6  320             16384  1.517    .3127 27.22 1.546 .0004 217.8 860.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.692      .6307         0                14644 1041 4111 1.975\n",
            "\n",
            "09:27:54 | time:15553s total_exs:575984 total_steps:35999 epochs:4.64 time_left:1205s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.5     1 815.4  3326   .6429      89.58 32.64  336             16384  1.811    .3127 25.44 1.609 .0004 203.5 830.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.997      .6242   .002976                14665 1019 4157 2.04\n",
            "\n",
            "09:28:04 | time:15563s total_exs:576312 total_steps:36019 epochs:4.64 time_left:1196s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     193     1 817.4  3332   .6311      90.84 32.61  328             16384  1.559    .3167 26.39 1.628 .0004 211.1 860.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.094      .6153         0                14685 1029 4192 2.015\n",
            "\n",
            "09:28:14 | time:15573s total_exs:576632 total_steps:36039 epochs:4.65 time_left:1188s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.2     1 810.8  3246   .6281      83.82 32.03  320             16384  1.458    .3127 27.47 1.576 .0004 219.8 879.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.836      .6222         0                14705 1031 4126 2.037\n",
            "\n",
            "09:28:25 | time:15584s total_exs:576960 total_steps:36060 epochs:4.65 time_left:1179s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.6     1 807.6  3272   .6280      82.67 32.42  328             16384    1.5    .3041 25.52 1.597 .0004 204.2 827.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.94      .6235         0                14726 1012 4100 2.088\n",
            "\n",
            "09:28:35 | time:15594s total_exs:577288 total_steps:36080 epochs:4.65 time_left:1170s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   211.9     1   826  3371   .6524      108.7 32.65  328             17203  1.621    .3127 26.05 1.594 .0004 208.4 850.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.924      .6254         0                14746 1034 4221 2.013\n",
            "\n",
            "09:28:45 | time:15604s total_exs:577616 total_steps:36101 epochs:4.65 time_left:1161s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.9     1 815.9  3288   .6189      91.95 32.24  328             32768  1.555    .3127 26.93 1.632 .0004 215.4 868.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.116      .6060         0                14767 1031 4157 2.076\n",
            "\n",
            "09:28:55 | time:15614s total_exs:577944 total_steps:36121 epochs:4.66 time_left:1153s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.8     1 817.5  3313   .6311      92.64 32.42  328             32768  1.743    .3167 26.68  1.52 .0004 213.5   865   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.572      .6338         0                14787 1031 4178 2.001\n",
            "\n",
            "09:29:05 | time:15624s total_exs:578272 total_steps:36142 epochs:4.66 time_left:1144s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.6     1 821.7  3335   .6433      93.86 32.46  328             32768  1.786    .3127  26.4 1.537 .0004 211.2   857   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.653      .6309         0                14808 1033 4192 2.092\n",
            "\n",
            "09:29:15 | time:15634s total_exs:578600 total_steps:36162 epochs:4.66 time_left:1135s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.9     1 810.1  3287   .6402      95.64 32.46  328             32768  1.564    .3167 26.91 1.597 .0004 215.2 873.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.939      .6133   .003049                14828 1025 4160 2.001\n",
            "\n",
            "09:29:26 | time:15644s total_exs:578928 total_steps:36183 epochs:4.66 time_left:1126s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.5     1 812.2  3270   .6372      89.95 32.21  328             32768  1.608    .3167 26.58 1.588 .0004 212.7 856.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.893      .6240         0                14849 1025 4126 2.074\n",
            "\n",
            "09:29:36 | time:15654s total_exs:579256 total_steps:36203 epochs:4.67 time_left:1118s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     186     1 801.8  3281   .6250      85.74 32.74  328             32768  1.513    .3127    26 1.672 .0004   208 851.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.323      .6061         0                14869 1010 4133 2.021\n",
            "\n",
            "09:29:46 | time:15665s total_exs:579584 total_steps:36224 epochs:4.67 time_left:1109s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.9     1 817.4  3255   .6372       90.7 31.85  328             32768   1.42    .3167 26.28 1.631 .0004 210.2   837   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.109      .6089         0                14890 1028 4092 2.052\n",
            "\n",
            "09:29:56 | time:15675s total_exs:579920 total_steps:36245 epochs:4.67 time_left:1100s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.5     1 816.4  3316   .6369      87.46  32.5  336             32768  1.572    .3127 25.56 1.597 .0004 204.5 830.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.936      .6176         0                14911 1021 4147 2.031\n",
            "\n",
            "09:30:06 | time:15685s total_exs:580248 total_steps:36265 epochs:4.67 time_left:1091s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.2     1 805.2  3268   .6189      87.58 32.46  328             32768  1.545    .3127 27.99 1.653 .0004   224 908.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.223      .6067         0                14931 1029 4176 2.004\n",
            "\n",
            "09:30:16 | time:15695s total_exs:580568 total_steps:36285 epochs:4.68 time_left:1083s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.6     1 811.2  3233   .6281      97.22 31.88  320             32768  1.539    .3167 27.46 1.621 .0004 219.7 875.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.058      .6176         0                14951 1031 4109 2.028\n",
            "\n",
            "09:30:26 | time:15705s total_exs:580888 total_steps:36305 epochs:4.68 time_left:1074s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   200.8     1   822  3281   .6562      98.03 31.93  320             32768   1.61    .3127 27.01 1.576 .0004 216.1 862.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.833      .6271         0                14971 1038 4143 2.029\n",
            "\n",
            "09:30:37 | time:15716s total_exs:581216 total_steps:36326 epochs:4.68 time_left:1065s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.4     1   815  3261   .6341      92.52 32.01  328             32768  1.633    .3167 26.82 1.666 .0004 214.6 858.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.293      .6069   .006098                14992 1030 4120 2.059\n",
            "\n",
            "09:30:47 | time:15726s total_exs:581544 total_steps:36346 epochs:4.69 time_left:1056s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.1     1 821.7  3353   .6463      89.42 32.64  328             32768  2.119    .3127 26.72  1.68 .0004 213.7 872.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.368      .6061         0                15012 1035 4225 2.015\n",
            "\n",
            "09:30:57 | time:15736s total_exs:581872 total_steps:36367 epochs:4.69 time_left:1048s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.5     1 815.4  3294   .6250      87.62 32.31  328             32768  1.902    .3167 25.33 1.585 .0004 202.6 818.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.878      .6219         0                15033 1018 4112 2.081\n",
            "\n",
            "09:31:07 | time:15746s total_exs:582200 total_steps:36387 epochs:4.69 time_left:1039s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.8     1 823.9  3356   .6585      88.79 32.59  328             32768   1.51    .3167  26.9 1.563 .0004 215.2 876.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.773      .6246         0                15053 1039 4233 2.01\n",
            "\n",
            "09:31:17 | time:15756s total_exs:582520 total_steps:36407 epochs:4.69 time_left:1030s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.5     1 793.5  3163   .6156      84.33 31.89  320             32768  1.572    .3127 25.84 1.594 .0004 206.7   824   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.924      .6214         0                15073 1000 3987 2.033\n",
            "\n",
            "09:31:27 | time:15766s total_exs:582840 total_steps:36427 epochs:4.70 time_left:1022s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.5     1 811.5  3239   .6188      90.08 31.93  320             32768   1.91    .3167 26.81 1.614 .0004 214.5   856   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.025      .6129         0                15093 1026 4095 2.035\n",
            "\n",
            "09:31:38 | time:15776s total_exs:583168 total_steps:36448 epochs:4.70 time_left:1013s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.2     1 829.3  3315   .6585      94.54 31.98  328             32768  1.492    .3167 26.45 1.615 .0004 211.6 845.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.028      .6130         0                15114 1041 4161 2.061\n",
            "\n",
            "09:31:48 | time:15787s total_exs:583504 total_steps:36469 epochs:4.70 time_left:1004s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     197     1 823.7  3349   .6458      94.06 32.53  336             32768  1.626    .3127 26.51 1.652 .0004 212.1 862.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.219      .6105         0                15135 1036 4212 2.033\n",
            "\n",
            "09:31:58 | time:15797s total_exs:583832 total_steps:36489 epochs:4.70 time_left:995s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   205.2     1 835.7  3418   .6707      100.8 32.72  328             32768  1.567    .3127 27.93  1.58 .0004 223.5   914   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.857      .6190         0                15155 1059 4332 2.018\n",
            "\n",
            "09:32:08 | time:15807s total_exs:584160 total_steps:36510 epochs:4.71 time_left:986s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.7     1 812.6  3284   .6280      84.16 32.33  328             32768  1.735    .3127 25.07 1.623 .0004 200.5 810.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.069      .6130         0                15176 1013 4094 2.081\n",
            "\n",
            "09:32:18 | time:15817s total_exs:584496 total_steps:36531 epochs:4.71 time_left:977s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.8     1 812.6  3319   .6369      91.26 32.67  336             32768  2.941    .3127 24.57 1.643 .0004 196.6   803   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.17      .6092         0                15197 1009 4122 2.042\n",
            "\n",
            "09:32:29 | time:15828s total_exs:584832 total_steps:36552 epochs:4.71 time_left:968s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.3     1 797.2  3231   .6042      84.64 32.42  336             32768  1.857    .3127 25.39 1.615 .0004 203.1 823.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.028      .6150   .002976                15218 1000 4054 2.026\n",
            "\n",
            "09:32:39 | time:15838s total_exs:585160 total_steps:36572 epochs:4.71 time_left:959s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.3     1 824.4  3351   .6555      95.27 32.52  328             32768  2.367    .3127  27.4 1.626 .0004 219.2 891.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.084      .6168   .006098                15238 1044 4243 2.006\n",
            "\n",
            "09:32:49 | time:15848s total_exs:585488 total_steps:36593 epochs:4.72 time_left:951s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.6     1   806  3262   .6220      88.86 32.38  328             32768  1.633    .3127 26.17 1.564 .0004 209.4 847.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.78      .6218   .003049                15259 1015 4110 2.084\n",
            "\n",
            "09:32:59 | time:15858s total_exs:585808 total_steps:36613 epochs:4.72 time_left:942s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.7     1 814.5  3252   .6312      90.88 31.94  320             32768  2.199    .3289 26.56 1.602 .0004 212.5 848.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.963      .6165         0                15279 1027 4101 1.997\n",
            "\n",
            "09:33:09 | time:15868s total_exs:586136 total_steps:36633 epochs:4.72 time_left:933s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.9     1 824.4  3349   .6524       92.8 32.49  328             32768  1.702    .3127 26.54 1.614 .0004 212.3 862.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.021      .6122         0                15299 1037 4211 2.004\n",
            "\n",
            "09:33:19 | time:15878s total_exs:586464 total_steps:36654 epochs:4.72 time_left:924s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   202.1     1 832.2  3329   .6433      98.06    32  328             32768  1.619    .3127 26.31 1.618 .0004 210.5   842   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.041      .6095         0                15320 1043 4171 2.061\n",
            "\n",
            "09:33:30 | time:15889s total_exs:586800 total_steps:36675 epochs:4.73 time_left:915s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     188     1 803.6  3326   .6280      87.56 33.11  336             32768  1.792    .3041  24.8 1.581 .0004 198.4 821.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.862      .6269   .002976                15341 1002 4147 2.07\n",
            "\n",
            "09:33:40 | time:15899s total_exs:587136 total_steps:36696 epochs:4.73 time_left:906s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     197     1 822.8  3360   .6518       94.2 32.67  336             32768  1.574    .3041 26.28 1.545 .0004 210.2 858.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.686      .6189         0                15362 1033 4218 2.042\n",
            "\n",
            "09:33:50 | time:15909s total_exs:587472 total_steps:36717 epochs:4.73 time_left:897s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.1     1 814.8  3347   .6399      86.22 32.87  336             32768  1.451    .3167 25.85 1.521 .0004 206.8 849.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.578      .6312         0                15383 1022 4197 2.054\n",
            "\n",
            "09:34:00 | time:15919s total_exs:587800 total_steps:36737 epochs:4.74 time_left:889s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.1     1 812.6  3294   .6220      85.56 32.43  328             32768  1.793    .3127 25.37 1.674 .0004   203 822.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.331      .6074         0                15403 1016 4117 1.999\n",
            "\n",
            "09:34:11 | time:15930s total_exs:588128 total_steps:36758 epochs:4.74 time_left:880s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.9     1 806.4  3220   .6220      89.09 31.94  328             32768  1.525    .3167 26.58 1.618 .0004 212.6 848.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.046      .6150         0                15424 1019 4068 2.056\n",
            "\n",
            "09:34:21 | time:15940s total_exs:588464 total_steps:36779 epochs:4.74 time_left:871s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     188     1   814  3318   .6190      86.29 32.61  336             32768  1.767    .3167 26.18 1.626 .0004 209.5 853.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.083      .6173   .002976                15445 1023 4172 2.039\n",
            "\n",
            "09:34:31 | time:15950s total_exs:588784 total_steps:36799 epochs:4.74 time_left:862s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.6     1 804.4  3205   .6156      90.03 31.87  320             32768  1.553    .3167 25.72  1.57 .0004 205.7 819.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.806      .6284         0                15465 1010 4025 1.992\n",
            "\n",
            "09:34:41 | time:15960s total_exs:589112 total_steps:36819 epochs:4.75 time_left:853s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.7     1 812.8  3300   .6189      88.09 32.48  328             32768  1.645    .3167 27.16 1.554 .0004 217.3 882.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.73      .6238         0                15485 1030 4182 2.003\n",
            "\n",
            "09:34:51 | time:15970s total_exs:589440 total_steps:36840 epochs:4.75 time_left:845s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   179.6     1 801.2  3200   .6159      79.49 31.95  328             32768  1.658    .3167 25.09 1.554 .0004 200.8 801.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.732      .6279         0                15506 1002 4002 2.057\n",
            "\n",
            "09:34:57 | time:15976s total_exs:589608 total_steps:36850 epochs:4.75 time_left:840s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   171.1     1 798.2  3259   .5774      71.29 32.66  168             32768  1.587    .3127 26.43 1.644 .0004 211.5 863.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.178      .6044   .005952                15516 1010 4123 1.988\n",
            "\n",
            "09:34:57 | running eval: valid\n",
            "09:35:15 | eval completed in 18.49s\n",
            "09:35:15 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
            "   217.6 846.8 11189   .6908      110.8 104.7 1934    .2603 27.35 1.892 .0004 216.8  2865       0          0 6.636      .5830   \n",
            "    token_em  total_train_updates  tpb   tps  \n",
            "           0                15516 1064 14053\n",
            "\u001b[0m\n",
            "09:35:15 | \u001b[1mdid not beat best ppl: 6.5991 impatience: 1\u001b[0m\n",
            "09:35:25 | time:16004s total_exs:589928 total_steps:36870 epochs:4.75 time_left:832s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   199.3     1   827  3301   .6500       95.9 31.93  320             32768  1.547    .3127 27.84 1.575 .0004 222.8 889.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.831      .6204         0                15536 1050 4190 2.021\n",
            "\n",
            "09:35:35 | time:16014s total_exs:590248 total_steps:36890 epochs:4.76 time_left:824s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.8     1 809.1  3209   .6156      93.66 31.73  320             32768  1.602    .3127 27.06 1.611 .0004 216.5 858.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.009      .6170   .003125                15556 1026 4067 2.021\n",
            "\n",
            "09:35:46 | time:16025s total_exs:590576 total_steps:36911 epochs:4.76 time_left:815s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.6     1 815.6  3235   .6433      84.69 31.73  328             32768  1.742    .3168 26.83 1.532 .0004 214.6 851.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.626      .6323         0                15577 1030 4086 2.045\n",
            "\n",
            "09:35:56 | time:16035s total_exs:590896 total_steps:36931 epochs:4.76 time_left:806s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   180.5     1 803.4  3202   .6156      80.12 31.89  320             32768  1.682    .3127 27.39 1.601 .0004 219.2 873.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.96      .6216         0                15597 1022 4076 1.993\n",
            "\n",
            "09:36:06 | time:16045s total_exs:591232 total_steps:36952 epochs:4.76 time_left:797s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.1     1   816  3351   .6399      89.11 32.85  336             32768  1.496    .3127 25.57  1.52 .0004 204.5   840   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.571      .6282         0                15618 1021 4191 2.054\n",
            "\n",
            "09:36:16 | time:16055s total_exs:591568 total_steps:36973 epochs:4.77 time_left:788s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.7     1 812.2  3313   .6310      93.18 32.63  336             32768  1.784    .3127 24.99 1.579 .0004 199.9 815.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.852      .6180   .002976                15639 1012 4128 2.04\n",
            "\n",
            "09:36:27 | time:16066s total_exs:591904 total_steps:36994 epochs:4.77 time_left:779s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   178.4     1 800.5  3265   .5952      78.31 32.63  336             32768  2.299    .3127 25.25 1.606 .0004   202 823.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.983      .6182         0                15660 1002 4089 2.039\n",
            "\n",
            "09:36:37 | time:16076s total_exs:592232 total_steps:37014 epochs:4.77 time_left:770s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.1     1 814.3  3331   .6189      83.33 32.72  328             32768  2.346    .3127 26.12 1.592 .0004   209 854.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.916      .6188   .003049                15680 1023 4186 2.02\n",
            "\n",
            "09:36:47 | time:16086s total_exs:592560 total_steps:37035 epochs:4.77 time_left:761s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.4     1 812.3  3273   .6341      86.85 32.23  328             32768  1.764    .3042 25.93   1.7 .0004 207.4 835.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.476      .6075         0                15701 1020 4108 2.076\n",
            "\n",
            "09:36:57 | time:16096s total_exs:592896 total_steps:37056 epochs:4.78 time_left:752s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     209     1   840  3424   .6756        104 32.61  336             32768  1.855    .3127 27.15  1.63 .0004 217.2 885.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.101      .6128         0                15722 1057 4309 2.038\n",
            "\n",
            "09:37:07 | time:16106s total_exs:593224 total_steps:37076 epochs:4.78 time_left:744s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   198.1     1 813.1  3292   .6341       96.5 32.39  328             32768   1.52    .3127 27.77  1.63 .0004 222.1 899.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.102      .6146         0                15742 1035 4191 1.999\n",
            "\n",
            "09:37:18 | time:16117s total_exs:593552 total_steps:37097 epochs:4.78 time_left:735s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.2     1 820.2  3264   .6494      87.66 31.83  328             32768  1.703    .3127 27.27 1.661 .0004 218.2 868.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.263      .6002         0                15763 1038 4132 2.051\n",
            "\n",
            "09:37:28 | time:16127s total_exs:593888 total_steps:37118 epochs:4.78 time_left:726s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     199     1 818.9  3371   .6458      96.62 32.93  336             32768  1.713    .3127 25.88 1.617 .0004   207   852   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.039      .6134         0                15784 1026 4223 2.058\n",
            "\n",
            "09:37:38 | time:16137s total_exs:594216 total_steps:37138 epochs:4.79 time_left:717s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.1     1 820.5  3321   .6402      90.52 32.38  328             32768  2.501    .3127  28.4 1.562 .0004 227.2 919.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.769      .6189   .003049                15804 1048 4240 1.999\n",
            "\n",
            "09:37:48 | time:16147s total_exs:594544 total_steps:37159 epochs:4.79 time_left:708s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.9     1 822.1  3271   .6433      95.09 31.83  328             32768  1.818    .3127 28.41 1.645 .0004 227.3 904.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.181      .6057         0                15825 1049 4175 2.051\n",
            "\n",
            "09:37:59 | time:16158s total_exs:594880 total_steps:37180 epochs:4.79 time_left:699s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.2     1 808.2  3280   .6399      96.16 32.47  336             32768  1.697    .3127 26.44 1.493 .0004 211.5 858.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.449      .6394   .002976                15846 1020 4138 2.029\n",
            "\n",
            "09:38:09 | time:16168s total_exs:595216 total_steps:37201 epochs:4.80 time_left:690s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.5     1   809  3306   .6280      87.35 32.69  336             32768  1.953    .3127 25.57 1.619 .0004 204.6 836.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.048      .6120         0                15867 1014 4142 2.043\n",
            "\n",
            "09:38:19 | time:16178s total_exs:595552 total_steps:37222 epochs:4.80 time_left:681s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.1     1 816.4  3312   .6429      87.08 32.46  336             32768  1.883    .3127 25.27 1.615 .0004 202.2 820.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.027      .6211         0                15888 1019 4133 2.029\n",
            "\n",
            "09:38:29 | time:16188s total_exs:595872 total_steps:37242 epochs:4.80 time_left:672s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.1     1 810.6  3230   .6375      89.77 31.88  320             32768  2.281    .3127 26.89 1.618 .0004 215.1 857.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.041      .6165         0                15908 1026 4087 1.993\n",
            "\n",
            "09:38:40 | time:16199s total_exs:596208 total_steps:37263 epochs:4.80 time_left:663s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.5     1 819.6  3345   .6369      89.01 32.65  336             32768  1.965    .3127 25.66 1.544 .0004 205.3 837.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.684      .6296         0                15929 1025 4183 2.041\n",
            "\n",
            "09:38:50 | time:16209s total_exs:596544 total_steps:37284 epochs:4.81 time_left:654s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.6     1 817.3  3322   .6458      90.45 32.51  336             32768  2.044    .3127 25.91 1.583 .0004 207.3 842.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.87      .6203   .002976                15950 1025 4164 2.032\n",
            "\n",
            "09:39:00 | time:16219s total_exs:596872 total_steps:37304 epochs:4.81 time_left:645s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   213.8     1   828  3369   .6585      110.3 32.55  328             32768   1.84    .3127 26.82 1.558 .0004 214.6 873.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.749      .6250         0                15970 1043 4242 2.01\n",
            "\n",
            "09:39:10 | time:16229s total_exs:597192 total_steps:37324 epochs:4.81 time_left:636s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.3     1 817.4  3264   .6406      87.15 31.95  320             32768  1.969    .3127 26.88 1.605 .0004   215 858.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.976      .6123         0                15990 1032 4123 2.034\n",
            "\n",
            "09:39:21 | time:16239s total_exs:597520 total_steps:37345 epochs:4.81 time_left:628s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.4     1 806.2  3251   .6189      87.57 32.25  328             32768  1.598    .3168 24.84 1.501 .0004 198.7 801.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.485      .6300         0                16011 1005 4052 2.078\n",
            "\n",
            "09:39:31 | time:16249s total_exs:597848 total_steps:37365 epochs:4.82 time_left:619s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   180.5     1   797  3270   .6037      80.84 32.82  328             32768  1.919    .3168 24.96  1.53 .0004 199.7 819.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 4.617      .6299         0                16031 996.7 4089 2.024\n",
            "\n",
            "09:39:41 | time:16260s total_exs:598176 total_steps:37386 epochs:4.82 time_left:610s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.9     1 815.1  3340   .6250      82.98 32.78  328             32768    1.7    .3042  26.3 1.614 .0004 210.4 862.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 5.021      .6130         0                16052 1026 4203 2.11\n",
            "\n",
            "09:39:51 | time:16270s total_exs:598512 total_steps:37407 epochs:4.82 time_left:601s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     185     1 812.7  3297   .6190      83.37 32.46  336             32768  1.953    .3127 26.12 1.527 .0004 208.9 847.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.602      .6266         0                16073 1022 4145 2.029\n",
            "\n",
            "09:40:01 | time:16280s total_exs:598832 total_steps:37427 epochs:4.82 time_left:592s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   205.6     1 831.1  3305   .6719      101.7 31.81  320             32768  1.731    .3127  28.6 1.616 .0004 228.8 909.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.034      .6175   .003125                16093 1060 4215 1.989\n",
            "\n",
            "09:40:11 | time:16290s total_exs:599168 total_steps:37448 epochs:4.83 time_left:583s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.4     1 806.7  3305   .6190       82.6 32.77  336             32768  1.626    .3128 24.98 1.554 .0004 199.8 818.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.729      .6270   .002976                16114 1007 4124 2.049\n",
            "\n",
            "09:40:19 | Overflow: setting loss scale to 16384.0\n",
            "09:40:21 | time:16300s total_exs:599488 total_steps:37468 epochs:4.83 time_left:574s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     191 .9500 816.8  3248   .6344      88.86 31.81  320             28672  2.028    .3168 26.83 1.588 .0004 214.7 853.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.893      .6253   .003125                16134 1031 4102 1.989\n",
            "\n",
            "09:40:32 | time:16311s total_exs:599824 total_steps:37489 epochs:4.83 time_left:565s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.7     1 817.4  3332   .6369      91.53 32.62  336             16384  2.533    .3127 26.45  1.59 .0004 211.6 862.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.905      .6183         0                16155 1029 4195 2.039\n",
            "\n",
            "09:40:42 | time:16321s total_exs:600144 total_steps:37509 epochs:4.84 time_left:557s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   199.5     1 828.9  3314   .6625      95.86 31.98  320             16384   3.08    .3128 26.35 1.542 .0004 210.8 842.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.675      .6312         0                16175 1040 4157 1.999\n",
            "\n",
            "09:40:52 | time:16331s total_exs:600480 total_steps:37530 epochs:4.84 time_left:547s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.6     1 809.2  3295   .6339      87.45 32.57  336             16384  2.268    .3127 26.41 1.575 .0004 211.3 860.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.831      .6206         0                16196 1020 4155 2.036\n",
            "\n",
            "09:41:02 | time:16341s total_exs:600808 total_steps:37550 epochs:4.84 time_left:539s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   205.8     1 816.1  3322   .6402      103.8 32.57  328             16384  2.296    .3128 26.55  1.59 .0004 212.4 864.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.902      .6149         0                16216 1028 4187 2.01\n",
            "\n",
            "09:41:12 | time:16351s total_exs:601128 total_steps:37570 epochs:4.84 time_left:530s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.1     1 823.4  3277   .6469      92.14 31.84  320             16384  1.739    .3127 26.95 1.604 .0004 215.6 858.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.975      .6133   .003125                16236 1039 4135 2.028\n",
            "\n",
            "09:41:22 | time:16361s total_exs:601448 total_steps:37590 epochs:4.85 time_left:521s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.7     1 819.6  3260   .6375      86.26 31.82  320             16384  1.711    .3128 26.82 1.499 .0004 214.6 853.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.476      .6311         0                16256 1034 4114 2.025\n",
            "\n",
            "09:41:32 | time:16371s total_exs:601776 total_steps:37611 epochs:4.85 time_left:512s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     184     1 804.9  3261   .6189       83.4 32.42  328             16384  1.748    .3128 25.72 1.513 .0004 205.7 833.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.538      .6305         0                16277 1011 4095 2.088\n",
            "\n",
            "09:41:43 | time:16382s total_exs:602104 total_steps:37631 epochs:4.85 time_left:503s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.4     1 813.7  3304   .6341      92.73 32.48  328             16384  2.437    .3128 27.95  1.55 .0004 223.6 907.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.714      .6287         0                16297 1037 4212 2.007\n",
            "\n",
            "09:41:53 | time:16392s total_exs:602424 total_steps:37651 epochs:4.85 time_left:495s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   217.1     1 839.1  3354   .6719      112.2 31.98  320             16384  2.751    .3128 26.86 1.538 .0004 214.9   859   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.656      .6302   .003125                16317 1054 4213 2.036\n",
            "\n",
            "09:42:03 | time:16402s total_exs:602752 total_steps:37672 epochs:4.86 time_left:486s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   182.6     1 800.7  3256   .6159      82.54 32.53  328             16384  1.611    .3127  25.4 1.575 .0004 203.2 826.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.829      .6248   .003049                16338 1004 4082 2.095\n",
            "\n",
            "09:42:13 | time:16412s total_exs:603072 total_steps:37692 epochs:4.86 time_left:477s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     190     1 804.4  3209   .6094      89.43 31.92  320             16384  2.003    .3128  26.3 1.518 .0004 210.4 839.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.561      .6304         0                16358 1015 4049 1.995\n",
            "\n",
            "09:42:23 | time:16422s total_exs:603400 total_steps:37712 epochs:4.86 time_left:468s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.1     1 815.5  3301   .6280      84.12 32.38  328             16384  2.156    .3128 25.46 1.603 .0004 203.7 824.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.968      .6178         0                16378 1019 4125 1.999\n",
            "\n",
            "09:42:33 | time:16432s total_exs:603728 total_steps:37733 epochs:4.86 time_left:459s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.9     1 806.4  3261   .6189      88.06 32.35  328             16384  1.912    .3127 25.25 1.503 .0004   202 816.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.497      .6371         0                16399 1008 4078 2.085\n",
            "\n",
            "09:42:43 | time:16442s total_exs:604056 total_steps:37753 epochs:4.87 time_left:451s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.5     1 806.4  3292   .6250      89.73 32.65  328             16384  1.667    .3168 26.41 1.528 .0004 211.3 862.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.607      .6332         0                16419 1018 4154 2.016\n",
            "\n",
            "09:42:53 | time:16452s total_exs:604376 total_steps:37773 epochs:4.87 time_left:442s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   183.1     1   811  3234   .6250      81.73  31.9  320             16384  1.652    .3168  26.2 1.595 .0004 209.6 835.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.93      .6178         0                16439 1021 4070 2.03\n",
            "\n",
            "09:43:03 | time:16462s total_exs:604704 total_steps:37794 epochs:4.87 time_left:433s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   177.8     1 809.3  3317   .6250      76.66 32.79  328             16384  1.699    .3042 24.23 1.563 .0004 193.8 794.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.773      .6278   .003049                16460 1003 4112 2.111\n",
            "\n",
            "09:43:13 | time:16472s total_exs:605032 total_steps:37814 epochs:4.87 time_left:424s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   181.6     1 808.5  3309   .6128      80.55 32.74  328             16384  1.609    .3231 25.51  1.58 .0004 204.1 835.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.854      .6266   .003049                16480 1013 4144 2.02\n",
            "\n",
            "09:43:24 | time:16482s total_exs:605360 total_steps:37835 epochs:4.88 time_left:415s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.5     1   813  3261   .6402      91.88 32.09  328             16384  1.744    .3128 25.14 1.486 .0004 201.1 806.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.42      .6358   .003049                16501 1014 4068 2.065\n",
            "\n",
            "09:43:34 | time:16493s total_exs:605688 total_steps:37855 epochs:4.88 time_left:406s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     185     1 802.4  3292   .6220       84.7 32.82  328             16384  1.492    .3128 25.79 1.548 .0004 206.3 846.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.704      .6261         0                16521 1009 4139 2.024\n",
            "\n",
            "09:43:44 | time:16503s total_exs:606016 total_steps:37876 epochs:4.88 time_left:397s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.7     1   819  3266   .6311      88.35 31.91  328             16384  2.121    .3128  26.2 1.583 .0004 209.6 835.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.869      .6164         0                16542 1029 4102 2.055\n",
            "\n",
            "09:43:54 | time:16513s total_exs:606336 total_steps:37896 epochs:4.89 time_left:389s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     189     1 813.4  3236   .6375      87.35 31.83  320             16384  1.496    .3128 26.95 1.536 .0004 215.6 857.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.646      .6322         0                16562 1029 4094 1.989\n",
            "\n",
            "09:44:04 | time:16523s total_exs:606664 total_steps:37916 epochs:4.89 time_left:380s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.6     1 825.9  3377   .6463      93.37 32.71  328             16384   2.67    .3128 26.67 1.579 .0004 213.4 872.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.848      .6297   .003049                16582 1039 4249 2.017\n",
            "\n",
            "09:44:14 | time:16533s total_exs:606976 total_steps:37936 epochs:4.89 time_left:371s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   186.2     1 803.7  3124   .6250      85.69 31.09  312             16384  1.586    .3128 27.43 1.607 .0004 219.5   853   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.989      .6179         0                16602 1023 3977 2.005\n",
            "\n",
            "09:44:24 | time:16543s total_exs:607312 total_steps:37957 epochs:4.89 time_left:362s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.5     1   816  3323   .6458      87.52 32.58  336             16384  1.843    .3128 25.12 1.571 .0004   201 818.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.809      .6261         0                16623 1017 4141 2.036\n",
            "\n",
            "09:44:34 | time:16553s total_exs:607632 total_steps:37977 epochs:4.90 time_left:354s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.8     1 806.4  3221   .6281      86.96 31.96  320             16384    1.6    .3168 26.47 1.573 .0004 211.8 845.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.82      .6274         0                16643 1018 4067 1.998\n",
            "\n",
            "09:44:41 | Overflow: setting loss scale to 8192.0\n",
            "09:44:45 | time:16563s total_exs:607960 total_steps:37997 epochs:4.90 time_left:345s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.4 .9500 812.1  3301   .6463      89.87 32.51  328             13107  1.571    .3128 26.65 1.541 .0004 213.2 866.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.668      .6297         0                16663 1025 4167 2.007\n",
            "\n",
            "09:44:55 | time:16574s total_exs:608280 total_steps:38017 epochs:4.90 time_left:336s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   197.3     1 818.6  3272   .6281      94.98 31.98  320              8192  1.488    .3128 26.98 1.553 .0004 215.9 862.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.726      .6237         0                16683 1034 4135 2.038\n",
            "\n",
            "09:45:05 | time:16584s total_exs:608608 total_steps:38038 epochs:4.90 time_left:327s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.5     1 813.2  3271   .6311      88.87 32.18  328              8192  1.566    .3128  26.6 1.563 .0004 212.8 855.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.771      .6214         0                16704 1026 4127 2.073\n",
            "\n",
            "09:45:15 | time:16594s total_exs:608928 total_steps:38058 epochs:4.91 time_left:318s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.6     1 810.2  3225   .6281      89.31 31.84  320              8192   1.65    .3168 27.22 1.606 .0004 217.8 866.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.98      .6204         0                16724 1028 4092 1.991\n",
            "\n",
            "09:45:25 | time:16604s total_exs:609256 total_steps:38078 epochs:4.91 time_left:309s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.7     1 818.9  3326   .6463      90.36 32.49  328              8192  1.891    .3128 24.94 1.557 .0004 199.5 810.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.745      .6268   .003049                16744 1018 4136 2.004\n",
            "\n",
            "09:45:35 | time:16614s total_exs:609576 total_steps:38098 epochs:4.91 time_left:301s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.1     1 811.2  3240   .6250      90.74 31.96  320              8192  1.621    .3128 27.17 1.528 .0004 217.4 868.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.609      .6327         0                16764 1029 4109 2.034\n",
            "\n",
            "09:45:45 | time:16624s total_exs:609904 total_steps:38119 epochs:4.91 time_left:292s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.8     1 816.2  3249   .6341      92.77 31.84  328              8192  2.089    .3128 27.23 1.611 .0004 217.8 867.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.008      .6213   .003049                16785 1034 4116 2.052\n",
            "\n",
            "09:45:55 | time:16634s total_exs:610232 total_steps:38139 epochs:4.92 time_left:283s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.5     1 819.1  3318   .6372      94.09 32.41  328              8192  1.675    .3124 25.54 1.558 .0004 204.3 827.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.749      .6252         0                16805 1023 4146 2.001\n",
            "\n",
            "09:46:06 | time:16645s total_exs:610560 total_steps:38160 epochs:4.92 time_left:274s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.4     1 826.4  3315   .6616      93.15  32.1  328              8192  2.465    .3124 25.02 1.568 .0004 200.1 802.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.799      .6288         0                16826 1026 4118 2.067\n",
            "\n",
            "09:46:16 | time:16655s total_exs:610896 total_steps:38181 epochs:4.92 time_left:265s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.2     1 820.9  3336   .6518      93.63 32.51  336              8192  1.584    .3164 26.16 1.596 .0004 209.3 850.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.933      .6199         0                16847 1030 4186 2.032\n",
            "\n",
            "09:46:26 | time:16665s total_exs:611224 total_steps:38201 epochs:4.92 time_left:256s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   178.7     1 805.7  3303   .6128      78.01 32.79  328              8192  1.589    .3124 25.39 1.559 .0004 203.1 832.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.754      .6280         0                16867 1009 4135 2.024\n",
            "\n",
            "09:46:36 | time:16675s total_exs:611552 total_steps:38222 epochs:4.93 time_left:247s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   201.7     1 816.3  3291   .6433      99.68 32.25  328              8192  1.677    .3124 26.25 1.682 .0004   210 846.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.378      .6030         0                16888 1026 4137 2.077\n",
            "\n",
            "09:46:46 | time:16685s total_exs:611880 total_steps:38242 epochs:4.93 time_left:238s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.9     1 810.4  3287   .6311      89.59 32.45  328              8192   1.56    .3124 26.44 1.602 .0004 211.5 857.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.964      .6103         0                16908 1022 4145 2.003\n",
            "\n",
            "09:46:57 | time:16696s total_exs:612208 total_steps:38263 epochs:4.93 time_left:229s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.5     1 802.5  3213   .6250      90.22 32.03  328              8192  1.798    .3124 25.61 1.518 .0004 204.9 820.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.564      .6304   .006098                16929 1007 4034 2.065\n",
            "\n",
            "09:47:07 | time:16706s total_exs:612536 total_steps:38283 epochs:4.93 time_left:220s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "     219     1 831.9  3410   .6738        115 32.79  328              8192  1.942    .3124  26.5 1.613 .0004   212 868.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.016      .6115   .003049                16949 1044 4278 2.022\n",
            "\n",
            "09:47:17 | time:16716s total_exs:612864 total_steps:38304 epochs:4.94 time_left:211s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   175.7     1 788.9  3154   .5884      77.04 31.98  328              8192  1.728    .3164 25.46 1.517 .0004 203.7 814.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 4.56      .6284   .003049                16970 992.6 3968 2.06\n",
            "\n",
            "09:47:27 | time:16726s total_exs:613192 total_steps:38324 epochs:4.94 time_left:202s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.1     1 803.1  3282   .6250      83.67  32.7  328              8192   1.64    .3124 26.32 1.527 .0004 210.6 860.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.603      .6320         0                16990 1014 4143 2.016\n",
            "\n",
            "09:47:37 | time:16736s total_exs:613520 total_steps:38345 epochs:4.94 time_left:193s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   204.1     1 807.6  3250   .6280      103.1  32.2  328              8192  2.085    .3164 25.76 1.506 .0004 206.1 829.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.507      .6319         0                17011 1014 4080 2.074\n",
            "\n",
            "09:47:47 | time:16746s total_exs:613856 total_steps:38366 epochs:4.95 time_left:184s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   181.4     1 806.1  3336   .6131       80.6 33.11  336              8192  2.032    .3124 25.18 1.553 .0004 201.5 833.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.727      .6296         0                17032 1008 4170 2.069\n",
            "\n",
            "09:47:58 | time:16756s total_exs:614176 total_steps:38386 epochs:4.95 time_left:176s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.4     1 811.2  3244   .6344      92.02 31.99  320              8192  3.181    .3124 27.62 1.576 .0004 220.9 883.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.835      .6167   .003125                17052 1032 4128    2\n",
            "\n",
            "09:48:08 | time:16767s total_exs:614496 total_steps:38406 epochs:4.95 time_left:167s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.1     1   807  3197   .6188      91.26 31.69  320              8192  1.512    .3164 26.47 1.552 .0004 211.8 838.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.723      .6243         0                17072 1019 4036 1.981\n",
            "\n",
            "09:48:18 | time:16777s total_exs:614824 total_steps:38426 epochs:4.95 time_left:158s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.9     1 819.2  3314   .6372      88.51 32.36  328              8192  1.548    .3124 26.22 1.555 .0004 209.8 848.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.735      .6223   .003049                17092 1029 4162 2.001\n",
            "\n",
            "09:48:28 | time:16787s total_exs:615144 total_steps:38446 epochs:4.96 time_left:149s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.6     1 827.3  3309   .6562      91.17    32  320              8192  1.545    .3164 26.97 1.508 .0004 215.8   863   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 4.518      .6297         0                17112 1043 4172 2.04\n",
            "\n",
            "09:48:38 | time:16797s total_exs:615472 total_steps:38467 epochs:4.96 time_left:140s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   193.5     1 809.2  3260   .6311      92.31 32.23  328              8192  1.806    .3164 26.09 1.566 .0004 208.8   841   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.787      .6165         0                17133 1018 4101 2.076\n",
            "\n",
            "09:48:48 | time:16807s total_exs:615808 total_steps:38488 epochs:4.96 time_left:131s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   184.2     1 813.2  3333   .6280      82.51 32.79  336              8192  1.798    .3164 25.51 1.514 .0004 204.1 836.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.546      .6319         0                17154 1017 4169 2.049\n",
            "\n",
            "09:48:58 | time:16817s total_exs:616136 total_steps:38508 epochs:4.96 time_left:122s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   191.5     1 823.1  3371   .6463      88.64 32.76  328              8192   1.69    .3124  26.3 1.473 .0004 210.4 861.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.362      .6381         0                17174 1034 4233 2.021\n",
            "\n",
            "09:49:09 | time:16828s total_exs:616464 total_steps:38529 epochs:4.97 time_left:113s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   194.3     1 810.5  3216   .6280      93.02 31.75  328              8192  1.716    .3124 26.95 1.576 .0004 215.6 855.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.834      .6194         0                17195 1026 4072 2.043\n",
            "\n",
            "09:49:19 | time:16838s total_exs:616792 total_steps:38549 epochs:4.97 time_left:104s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   187.9     1 816.2  3334   .6402       85.9 32.68  328              8192  1.691    .3124 25.94 1.572 .0004 207.5 847.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.818      .6246         0                17215 1024 4182 2.017\n",
            "\n",
            "09:49:29 | time:16848s total_exs:617112 total_steps:38569 epochs:4.97 time_left:95s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   195.3     1 812.1  3229   .6312      93.75  31.8  320              8192  1.638    .3164 26.15 1.521 .0004 209.2 831.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.576      .6397   .003125                17235 1021 4060 2.026\n",
            "\n",
            "09:49:39 | time:16858s total_exs:617440 total_steps:38590 epochs:4.97 time_left:87s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   192.3     1 825.5  3311   .6433       89.1 32.09  328              8192  1.618    .3124 25.97 1.564 .0004 207.8 833.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.778      .6244         0                17256 1033 4144 2.067\n",
            "\n",
            "09:49:49 | time:16868s total_exs:617768 total_steps:38610 epochs:4.98 time_left:78s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   185.7     1 808.1  3314   .6220       84.7  32.8  328              8192  1.643    .3124 25.69 1.544 .0004 205.5 842.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.685      .6215         0                17276 1014 4157 2.023\n",
            "\n",
            "09:49:59 | time:16878s total_exs:618096 total_steps:38631 epochs:4.98 time_left:69s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   206.5     1 836.8  3337   .6616      101.9 31.91  328              8192  1.574    .3124 27.16 1.572 .0004 217.3 866.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.816      .6231         0                17297 1054 4204 2.055\n",
            "\n",
            "09:50:09 | time:16888s total_exs:618424 total_steps:38651 epochs:4.98 time_left:60s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.7     1 814.3  3327   .6311      86.89 32.69  328              8192  1.689    .3124 26.12 1.561 .0004   209 853.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.764      .6239         0                17317 1023 4181 2.018\n",
            "\n",
            "09:50:20 | time:16899s total_exs:618752 total_steps:38672 epochs:4.99 time_left:51s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   196.4     1 818.8  3281   .6463      94.03 32.05  328              8192  2.008    .3164 26.86 1.633 .0004 214.9 860.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 5.121      .6065         0                17338 1034 4141 2.066\n",
            "\n",
            "09:50:30 | time:16909s total_exs:619080 total_steps:38692 epochs:4.99 time_left:42s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   189.8     1   819  3359   .6372       87.4 32.81  328              8192  1.547    .3124 26.43 1.609 .0004 211.5 867.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.999      .6167         0                17358 1031 4227 2.026\n",
            "\n",
            "09:50:40 | time:16919s total_exs:619408 total_steps:38713 epochs:4.99 time_left:33s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   205.6     1 828.7  3341   .6555        102 32.26  328              8192  1.764    .3124 27.13  1.55 .0004   217 875.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.712      .6256         0                17379 1046 4217 2.079\n",
            "\n",
            "09:50:50 | time:16929s total_exs:619728 total_steps:38733 epochs:4.99 time_left:24s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   199.5     1 821.4  3263   .6500      96.79 31.78  320              8192  2.276    .3124    28 1.534 .0004   224 889.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.636      .6271         0                17399 1045 4153 1.987\n",
            "\n",
            "09:51:00 | time:16939s total_exs:620056 total_steps:38753 epochs:5.00 time_left:15s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.1     1 810.8  3307   .6311      86.79 32.63  328              8192  1.748    .3124 26.31 1.524 .0004 210.5 858.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.592      .6282         0                17419 1021 4165 2.014\n",
            "\n",
            "09:51:10 | time:16949s total_exs:620384 total_steps:38774 epochs:5.00 time_left:6s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   188.8     1 814.7  3302   .6250      86.98 32.43  328              8192  1.598    .3124 24.51 1.518 .0004 196.1 794.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.563      .6307         0                17440 1011 4097 2.088\n",
            "\n",
            "09:51:17 | time:16956s total_exs:620616 total_steps:38788 epochs:5.00 time_left:0s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss    lr  ltpb  ltps  \\\n",
            "   190.3     1 816.2  3343   .6293      88.28 32.76  232              8192  1.723    .3124 26.01 1.566 .0004 208.1 852.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 4.788      .6194    .00431                17454 1024 4195 2.006\n",
            "\n",
            "09:51:17 | num_epochs completed:5.0 time elapsed:16956.273809194565s\n",
            "09:51:17 | \u001b[33mOverriding opt[\"dict_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict (previously: /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model.dict)\u001b[0m\n",
            "09:51:17 | \u001b[33mOverriding opt[\"multitask_weights\"] to (1.0, 3.0, 3.0, 3.0) (previously: [1.0, 3.0, 3.0, 3.0])\u001b[0m\n",
            "09:51:17 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: fr_finetuned,fromfile_datatype_extension: True,checkpoint_activations: False,world_logs: ,save_format: conversations,log_keep_fields: all,verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "09:51:17 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--init-opt /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt --allow-missing-init-opts True --task blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --dynamic-batching full --eval-batchsize 8 --num-epochs -1 --validation-every-n-secs 900.0 --save-after-valid True --validation-every-n-epochs -1.0 --validation-patience 20 --distributed-world-size 8 --port 61337 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --chosen-topic-delimiter \n",
            " --num-topics 5 --train-experiencer-only False --remove-political-convos False --teacher-model /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model --task-loss-coeff 1.0 --encoder-loss-coeff 24.0 --hidden-loss-coeff 5.0 --pred-loss-coeff 8.0 --embedding-loss-coeff 0.35 --self-attn-loss-coeff 0.6 --enc-dec-attn-loss-coeff 3.0 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --inference beam --force-fp16-tokens False --learningrate 0.0004 --gpu 0 --bpe-vocab /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json --bpe-merge /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt --bpe-add-prefix-space True --max-lr-steps -1 --parlai-home /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI --show-advanced-args False --numthreads 1 --rank 0\u001b[0m\n",
            "09:51:17 | Using CUDA\n",
            "09:51:17 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model.dict\n",
            "09:51:18 | num words = 8008\n",
            "09:51:25 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "09:51:25 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model\n",
            "09:51:46 | creating task(s): fromfile:parlaiformat\n",
            "09:51:46 | Loading ParlAI text data: fr_finetuned_valid.txt\n",
            "09:51:46 | running eval: valid\n",
            "09:52:05 | eval completed in 18.46s\n",
            "09:52:05 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
            "   217.6 846.8 11209   .6908      110.8 104.9 1934    .2172 27.35 1.887 .0004 216.8  2870       0          0 6.599      .5853   \n",
            "    token_em  total_train_updates  tpb   tps  \n",
            "    .0005171                13577 1064 14079\n",
            "\u001b[0m\n",
            "09:52:05 | creating task(s): fromfile:parlaiformat\n",
            "09:52:05 | Loading ParlAI text data: fr_finetuned_test.txt\n",
            "09:52:05 | running eval: test\n",
            "09:52:24 | eval completed in 18.84s\n",
            "09:52:24 | \u001b[1mtest:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
            "   220.1 837.1 11076   .6908      113.2 103.6 1950    .2172 27.57 1.914 .0004 215.9  2857       0          0 6.78      .5748   \n",
            "    token_em  total_train_updates  tpb   tps  \n",
            "    .0005128                13577 1053 13932\n",
            "\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'clen': AverageMetric(217.6),\n",
              "  'ctpb': GlobalAverageMetric(846.8),\n",
              "  'ctps': GlobalTimerMetric(1.121e+04),\n",
              "  'ctrunc': AverageMetric(0.6908),\n",
              "  'ctrunclen': AverageMetric(110.8),\n",
              "  'exps': GlobalTimerMetric(104.9),\n",
              "  'exs': SumMetric(1934),\n",
              "  'gpu_mem': GlobalAverageMetric(0.2172),\n",
              "  'llen': AverageMetric(27.35),\n",
              "  'loss': AverageMetric(1.887),\n",
              "  'lr': GlobalAverageMetric(0.0004),\n",
              "  'ltpb': GlobalAverageMetric(216.8),\n",
              "  'ltps': GlobalTimerMetric(2870),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'ppl': PPLMetric(6.599),\n",
              "  'token_acc': AverageMetric(0.5853),\n",
              "  'token_em': AverageMetric(0.0005171),\n",
              "  'total_train_updates': GlobalFixedMetric(1.358e+04),\n",
              "  'tpb': GlobalAverageMetric(1064),\n",
              "  'tps': GlobalTimerMetric(1.408e+04)},\n",
              " {'clen': AverageMetric(220.1),\n",
              "  'ctpb': GlobalAverageMetric(837.1),\n",
              "  'ctps': GlobalTimerMetric(1.108e+04),\n",
              "  'ctrunc': AverageMetric(0.6908),\n",
              "  'ctrunclen': AverageMetric(113.2),\n",
              "  'exps': GlobalTimerMetric(103.6),\n",
              "  'exs': SumMetric(1950),\n",
              "  'gpu_mem': GlobalAverageMetric(0.2172),\n",
              "  'llen': AverageMetric(27.57),\n",
              "  'loss': AverageMetric(1.914),\n",
              "  'lr': GlobalAverageMetric(0.0004),\n",
              "  'ltpb': GlobalAverageMetric(215.9),\n",
              "  'ltps': GlobalTimerMetric(2857),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'ppl': PPLMetric(6.78),\n",
              "  'token_acc': AverageMetric(0.5748),\n",
              "  'token_em': AverageMetric(0.0005128),\n",
              "  'total_train_updates': GlobalFixedMetric(1.358e+04),\n",
              "  'tpb': GlobalAverageMetric(1053),\n",
              "  'tps': GlobalTimerMetric(1.393e+04)})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    # task= \"blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues\",\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model= \"transformer/generator\",\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "\n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file= dict_file,\n",
        "\n",
        "    # arguments we get from the pretrained model.\n",
        "    n_heads= 32, n_layers= 2, n_positions= 128, n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    dropout= 0.1, log_every_n_secs= 10,\n",
        "    multitask_weights= \"1,3,3,3\",\n",
        "    attention_dropout= 0.0,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "\n",
        "    variant= \"prelayernorm\",\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    relu_dropout= 0.0, model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "    # save_after_valid= True,\n",
        "\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 5,\n",
        "    verbose = True,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    batchsize= 8, fp16= True, fp16_impl= \"mem_efficient\",\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\",\n",
        "    validation_metric_mode= \"min\" # vmm= \"min\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFFM5mP1n-0P",
        "outputId": "4b77479c-abd4-4912-81a3-8d90bed38f2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colabs/blender-models/finetuned-400m/model\n",
            "10:39:14 | \u001b[33mOverriding opt[\"dict_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict (previously: /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model.dict)\u001b[0m\n",
            "10:39:14 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "10:39:14 | Using CUDA\n",
            "10:39:14 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model.dict\n",
            "10:39:14 | num words = 8008\n",
            "10:39:29 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "10:39:29 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model\n",
            "10:39:52 | creating task(s): fromfile:parlaiformat\n",
            "10:39:52 | Loading ParlAI text data: fr_finetuned_valid.txt\n",
            "10:39:52 | Opt:\n",
            "10:39:52 |     activation: gelu\n",
            "10:39:52 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "10:39:52 |     adam_eps: 1e-08\n",
            "10:39:52 |     add_p1_after_newln: False\n",
            "10:39:52 |     aggregate_micro: False\n",
            "10:39:52 |     allow_missing_init_opts: False\n",
            "10:39:52 |     attention_dropout: 0.0\n",
            "10:39:52 |     batchsize: 8\n",
            "10:39:52 |     beam_block_full_context: True\n",
            "10:39:52 |     beam_block_list_filename: None\n",
            "10:39:52 |     beam_block_ngram: -1\n",
            "10:39:52 |     beam_context_block_ngram: -1\n",
            "10:39:52 |     beam_delay: 30\n",
            "10:39:52 |     beam_length_penalty: 0.65\n",
            "10:39:52 |     beam_min_length: 1\n",
            "10:39:52 |     beam_size: 1\n",
            "10:39:52 |     betas: '[0.9, 0.999]'\n",
            "10:39:52 |     bpe_add_prefix_space: None\n",
            "10:39:52 |     bpe_debug: False\n",
            "10:39:52 |     bpe_dropout: None\n",
            "10:39:52 |     bpe_merge: /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model.dict-merges.txt\n",
            "10:39:52 |     bpe_vocab: /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model.dict-vocab.json\n",
            "10:39:52 |     checkpoint_activations: False\n",
            "10:39:52 |     compute_tokenized_bleu: False\n",
            "10:39:52 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "10:39:52 |     datatype: train\n",
            "10:39:52 |     delimiter: '  '\n",
            "10:39:52 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "10:39:52 |     dict_endtoken: __end__\n",
            "10:39:52 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model.dict\n",
            "10:39:52 |     dict_include_test: False\n",
            "10:39:52 |     dict_include_valid: False\n",
            "10:39:52 |     dict_initpath: None\n",
            "10:39:52 |     dict_language: english\n",
            "10:39:52 |     dict_loaded: True\n",
            "10:39:52 |     dict_lower: False\n",
            "10:39:52 |     dict_max_ngram_size: -1\n",
            "10:39:52 |     dict_maxexs: -1\n",
            "10:39:52 |     dict_maxtokens: -1\n",
            "10:39:52 |     dict_minfreq: 0\n",
            "10:39:52 |     dict_nulltoken: __null__\n",
            "10:39:52 |     dict_starttoken: __start__\n",
            "10:39:52 |     dict_textfields: text,labels\n",
            "10:39:52 |     dict_tokenizer: bytelevelbpe\n",
            "10:39:52 |     dict_unktoken: __unk__\n",
            "10:39:52 |     display_add_fields: \n",
            "10:39:52 |     display_examples: False\n",
            "10:39:52 |     download_path: None\n",
            "10:39:52 |     dropout: 0.1\n",
            "10:39:52 |     dynamic_batching: None\n",
            "10:39:52 |     embedding_projection: random\n",
            "10:39:52 |     embedding_size: 1280\n",
            "10:39:52 |     embedding_type: random\n",
            "10:39:52 |     embeddings_scale: True\n",
            "10:39:52 |     eval_batchsize: None\n",
            "10:39:52 |     eval_dynamic_batching: None\n",
            "10:39:52 |     evaltask: None\n",
            "10:39:52 |     ffn_size: 5120\n",
            "10:39:52 |     final_extra_opt: \n",
            "10:39:52 |     force_fp16_tokens: True\n",
            "10:39:52 |     fp16: True\n",
            "10:39:52 |     fp16_impl: mem_efficient\n",
            "10:39:52 |     fromfile_datapath: fr_finetuned\n",
            "10:39:52 |     fromfile_datatype_extension: True\n",
            "10:39:52 |     gpu: -1\n",
            "10:39:52 |     gradient_clip: 0.1\n",
            "10:39:52 |     hide_labels: False\n",
            "10:39:52 |     history_add_global_end_token: end\n",
            "10:39:52 |     history_reversed: False\n",
            "10:39:52 |     history_size: -1\n",
            "10:39:52 |     image_cropsize: 224\n",
            "10:39:52 |     image_mode: raw\n",
            "10:39:52 |     image_size: 256\n",
            "10:39:52 |     inference: greedy\n",
            "10:39:52 |     init_model: zoo:blender/blender_400Mdistill/model\n",
            "10:39:52 |     init_opt: None\n",
            "10:39:52 |     interactive_mode: False\n",
            "10:39:52 |     invsqrt_lr_decay_gamma: -1\n",
            "10:39:52 |     is_debug: False\n",
            "10:39:52 |     label_truncate: 128\n",
            "10:39:52 |     learn_positional_embeddings: False\n",
            "10:39:52 |     learningrate: 7e-06\n",
            "10:39:52 |     log_every_n_secs: 10.0\n",
            "10:39:52 |     log_every_n_steps: 50\n",
            "10:39:52 |     log_keep_fields: all\n",
            "10:39:52 |     loglevel: info\n",
            "10:39:52 |     lr_scheduler: reduceonplateau\n",
            "10:39:52 |     lr_scheduler_decay: 0.5\n",
            "10:39:52 |     lr_scheduler_patience: 3\n",
            "10:39:52 |     max_train_steps: -1\n",
            "10:39:52 |     max_train_time: -1\n",
            "10:39:52 |     metrics: default\n",
            "10:39:52 |     model: transformer/generator\n",
            "10:39:52 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-400m/model\n",
            "10:39:52 |     model_parallel: False\n",
            "10:39:52 |     momentum: 0\n",
            "10:39:52 |     multitask_weights: '[1.0, 3.0, 3.0, 3.0]'\n",
            "10:39:52 |     mutators: None\n",
            "10:39:52 |     n_decoder_layers: 12\n",
            "10:39:52 |     n_encoder_layers: 2\n",
            "10:39:52 |     n_heads: 32\n",
            "10:39:52 |     n_layers: 2\n",
            "10:39:52 |     n_positions: 128\n",
            "10:39:52 |     n_segments: 0\n",
            "10:39:52 |     nesterov: True\n",
            "10:39:52 |     no_cuda: False\n",
            "10:39:52 |     num_epochs: 5.0\n",
            "10:39:52 |     num_examples: 20\n",
            "10:39:52 |     num_workers: 0\n",
            "10:39:52 |     nus: [0.7]\n",
            "10:39:52 |     optimizer: mem_eff_adam\n",
            "10:39:52 |     output_scaling: 1.0\n",
            "10:39:52 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'fr_finetuned', 'fromfile_datatype_extension': True, 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-400m/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict', 'num_examples': '20', 'skip_generation': False}\"\n",
            "10:39:52 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "10:39:52 |     person_tokens: False\n",
            "10:39:52 |     rank_candidates: False\n",
            "10:39:52 |     relu_dropout: 0.0\n",
            "10:39:52 |     save_after_valid: False\n",
            "10:39:52 |     save_every_n_secs: -1\n",
            "10:39:52 |     save_format: conversations\n",
            "10:39:52 |     share_word_embeddings: True\n",
            "10:39:52 |     short_final_eval: False\n",
            "10:39:52 |     skip_generation: False\n",
            "10:39:52 |     special_tok_lst: None\n",
            "10:39:52 |     split_lines: False\n",
            "10:39:52 |     starttime: Mar30_11-32\n",
            "10:39:52 |     task: fromfile:parlaiformat\n",
            "10:39:52 |     temperature: 1.0\n",
            "10:39:52 |     tensorboard_log: False\n",
            "10:39:52 |     tensorboard_logdir: None\n",
            "10:39:52 |     text_truncate: 128\n",
            "10:39:52 |     topk: 10\n",
            "10:39:52 |     topp: 0.9\n",
            "10:39:52 |     truncate: 128\n",
            "10:39:52 |     update_freq: 2\n",
            "10:39:52 |     use_reply: label\n",
            "10:39:52 |     validation_cutoff: 1.0\n",
            "10:39:52 |     validation_every_n_epochs: 0.25\n",
            "10:39:52 |     validation_every_n_secs: -1\n",
            "10:39:52 |     validation_every_n_steps: -1\n",
            "10:39:52 |     validation_max_exs: -1\n",
            "10:39:52 |     validation_metric: ppl\n",
            "10:39:52 |     validation_metric_mode: min\n",
            "10:39:52 |     validation_patience: 10\n",
            "10:39:52 |     validation_share_agent: False\n",
            "10:39:52 |     variant: prelayernorm\n",
            "10:39:52 |     verbose: False\n",
            "10:39:52 |     wandb_entity: None\n",
            "10:39:52 |     wandb_log: False\n",
            "10:39:52 |     wandb_name: None\n",
            "10:39:52 |     wandb_project: None\n",
            "10:39:52 |     warmup_rate: 0.0001\n",
            "10:39:52 |     warmup_updates: 100\n",
            "10:39:52 |     weight_decay: None\n",
            "10:39:52 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finetuned_valid.txt- - -\u001b[0;0m\n",
            "\u001b[0msalut ! comment vas-tu ce soir ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: je le fais bien . juste se détendre avec mes deux chiens.\u001b[0;0m\n",
            "\u001b[0;95m     model: je vais bien . Comment allez-vous ?\u001b[0;0m\n",
            "\u001b[0mgénial . dans mon temps libre je fais du bénévolat.\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est bien. quel genre de travail bénévole faites-vous?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un peu ennuyeux. je suis une femme .\u001b[0;0m\n",
            "\u001b[0mje travaille dans un refuge pour sans-abri dans ma ville.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Bien pour vous . vous aimez les voitures anciennes? j'ai deux anciennes mustangs.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis dans un groupe de rock.\u001b[0;0m\n",
            "\u001b[0mcool . pas vraiment dans les voitures. mon travail de jour est dur.\u001b[0;0m\n",
            "\u001b[1;94m    labels: les voitures sont mon truc. voitures anciennes . j'adore travailler sur cela. est-ce que tu aimes ça ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis d'accord . je suis un peu plus âgé de mon travail.\u001b[0;0m\n",
            "\u001b[0moui, j'aime la foule, faire connaissance avec les gens.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je n'ai pas pensé à l'aspect foule de la lutte. je n'aime pas les foules.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d'entendre ça . je suis toujours avec mon travail.\u001b[0;0m\n",
            "\u001b[0mcompris . quand j'ai besoin d'être seul, je m'entraîne plusieurs fois par semaine.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je suis d'accord . travailler est un excellent moyen de faire ses doléances. Aimez-vous la musique country ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis d'accord . je ne suis pas très bon dans ce domaine.\u001b[0;0m\n",
            "\u001b[0mun petit peu . je peux admirer taylor swift.\u001b[0;0m\n",
            "\u001b[1;94m    labels: lol. elle est plus pop maintenant. ses vieux trucs étaient country. j'aime le country classique.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis dans le groupe de rock.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finetuned_valid.txt- - -\u001b[0;0m\n",
            "\u001b[0msalut comment allez-vous ? je vais bien et vous?\u001b[0;0m\n",
            "\u001b[1;94m    labels: je vais bien. sirotant juste du thé. que faites-vous pour le travail?\u001b[0;0m\n",
            "\u001b[0;95m     model: je vais bien . Comment allez-vous ?\u001b[0;0m\n",
            "\u001b[0mje faisais de l'aide à domicile mais maintenant je suis handicapé.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je suis désolé de l'entendre . Qu'est-il arrivé\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un enseignant de première année.\u001b[0;0m\n",
            "\u001b[0mj'ai obtenu mon diplôme et obtenu mon permis de travail un certain temps et est devenu je vais.\u001b[0;0m\n",
            "\u001b[1;94m    labels: alors que faites-vous maintenant pour vous amuser? J'aime lire .\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis d'accord . je suis un peu ennuyeux.\u001b[0;0m\n",
            "\u001b[0mj'ai des chiens et je les promène. et un chat.\u001b[0;0m\n",
            "\u001b[1;94m    labels: quel genre de chiens avez-vous?\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai un chien, mais je ne suis pas un grand fan de la nourriture.\u001b[0;0m\n",
            "\u001b[0mj'aime lire sur michael jackson et la psychologie aujourd'hui.\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'aime lire . j'ai une grande bibliothèque chez moi.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis d'accord . je ne suis pas très bon dans ce domaine.\u001b[0;0m\n",
            "\u001b[0mc'est très agréable j'aime lire et lire des revues.\u001b[0;0m\n",
            "\u001b[1;94m    labels: j'ai du mal à entendre donc la lecture est mon endroit heureux.\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai une grande collection de timbres.\u001b[0;0m\n",
            "\u001b[0mcool ça va. ma maman était très dure avec moi.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je travaille comme guide d'excursion de musée de malentendants.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis d'accord . je ne suis pas très bon dans ce domaine.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finetuned_valid.txt- - -\u001b[0;0m\n",
            "\u001b[0msalut ! vous aimez les tortues?\u001b[0;0m\n",
            "\u001b[1;94m    labels: en fait, personnellement, je préfère le chat.\u001b[0;0m\n",
            "\u001b[0;95m     model: j'aime les animaux, mais je ne suis pas très bon dans ce domaine.\u001b[0;0m\n",
            "\u001b[0mj'ai une tortue son nom est rapide. les chatons sont sympas aussi, tho!\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est un nom adorable pour une tortue. j'ai 2 chats\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai un chat nommé vache, il est une grande collection de timbres.\u001b[0;0m\n",
            "\u001b[0mquels sont vos noms de chatons?\u001b[0;0m\n",
            "\u001b[1;94m    labels: neige et hiver, du nom de ma saison préférée\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai un chat nommé vache, il est une grande collection de timbres.\u001b[0;0m\n",
            "\u001b[0mJ'aime ça ! je vais à l'école maternelle.\u001b[0;0m\n",
            "\u001b[1;94m    labels: oh tu es si jeune!\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d'entendre ça . je suis sûr que je ne peux pas y mettre\u001b[0;0m\n",
            "\u001b[0mquel âge avez-vous ? j'ai fêté mes quatre ans au jour de mon anniversaire!\u001b[0;0m\n",
            "\u001b[1;94m    labels: je suis une vieille femme. j'ai gagné une médaille d'or en 1993\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai une grande collection de timbres, mais je ne suis pas très bon dans ce domaine.\u001b[0;0m\n",
            "\u001b[0mHou la la ! ! tu as gagné l'or! êtes-vous riche ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: non . je dépense tout mon argent sur des chapeaux! j'en ai plus d'un millier\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un enseignant de première année, j'ai une grande école\u001b[0;0m\n"
          ]
        }
      ],
      "source": [
        "print(f'{finetuned_model_path}/model')\n",
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat',\n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    dict_file= dict_file,\n",
        "\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmP8Jegtnhg8"
      },
      "outputs": [],
      "source": [
        "!pip install twython "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4aAFF79n-0Q"
      },
      "outputs": [],
      "source": [
        "test_set_length = sum(len(d) for d in dialogs_test)\n",
        "model_path = f'{finetuned_model_path}/model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JrI43Tpn-0Q",
        "outputId": "9b8ddb4e-bc08-4b2a-e100-8a66be5a50e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ],
      "source": [
        "# another_model_path = f'{mydrive_path}finetuned-beam_prmts_infrnc'\n",
        "\n",
        "!(parlai display_model \\\n",
        "    --task 'fromfile:parlaiformat' \\\n",
        "    --fromfile-datapath 'fr_finetuned' \\\n",
        "    --datatype test \\\n",
        "    --fromfile-datatype-extension True \\\n",
        "    --model-file $model_path \\\n",
        "    --dict-file $dict_file \\\n",
        "    --num-examples $test_set_length \\\n",
        "    --skip-generation False) \\\n",
        "    > \"08-finetuned-400m-5epochs-testset-output.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfRpKAjjn-0R"
      },
      "outputs": [],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat',\n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    datatype= \"test\",\n",
        "    num_examples=test_set_length,\n",
        "    skip_generation=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smmkjy6FtqLv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPBQeKBvtttq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi1hQxccuKkx"
      },
      "source": [
        "## Fine-tuning (400M model + topk parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJwE1KIYuKky"
      },
      "source": [
        "### Convert dataset to ParlAI  format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JgF3_9BuKky",
        "outputId": "6d51af8f-7573-4d0c-d3aa-64af499b5d17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Xpersona'...\n",
            "remote: Enumerating objects: 282, done.\u001b[K\n",
            "remote: Counting objects: 100% (282/282), done.\u001b[K\n",
            "remote: Compressing objects: 100% (201/201), done.\u001b[K\n",
            "remote: Total 282 (delta 96), reused 241 (delta 78), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (282/282), 45.00 MiB | 11.64 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n",
            "Checking out files: 100% (218/218), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HLTCHKUST/Xpersona.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y0ali8PuKky"
      },
      "source": [
        "[All XPersona datasets](https://github.com/HLTCHKUST/Xpersona/tree/master/dataset) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi8mpNZ1uKky"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_train_corrected.json','r') as f:\n",
        "   train_data = json.load(f)\n",
        "\n",
        "dialogs_train = pd.DataFrame(train_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_valid_human_annotated.json','r') as f:\n",
        "   valid_data = json.load(f)\n",
        "\n",
        "dialogs_valid = pd.DataFrame(valid_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_test_human_annotated.json','r') as f:\n",
        "   text_data = json.load(f)\n",
        "\n",
        "dialogs_test = pd.DataFrame(text_data)['dialogue'].tolist()\n",
        "\n",
        "# dialogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2gRFM87uKky",
        "outputId": "50a4da90-1d10-4aef-a87b-994857023421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15776427\n",
            "257114\n",
            "262425\n"
          ]
        }
      ],
      "source": [
        "# pd.read_json(f,orient='records', encoding='utf-8',lines=True)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# dialogs = pd.read_json('XPersona/dataset/Fr_persona_split_train_corrected.json')['dialogue'].tolist()\n",
        "\n",
        "\n",
        "text = \"\"\n",
        "\n",
        "def transfer_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, each_two_turn in enumerate(d):\n",
        "    u1 = each_two_turn[0]\n",
        "    u2 = each_two_turn[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "\n",
        "# d = dialogs[0]\n",
        "# print(d)\n",
        "# print(transfer_dialog(d))\n",
        "\n",
        "data_train = \"\"\n",
        "for d in dialogs_train:\n",
        "  data_train += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_train = open(\"fr_finetuned_train.txt\",\"w\")\n",
        "print(file_train.write(data_train))\n",
        "\n",
        "data_valid = \"\"\n",
        "for d in dialogs_valid:\n",
        "  data_valid += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_valid = open(\"fr_finetuned_valid.txt\",\"w\")\n",
        "print(file_valid.write(data_valid))\n",
        "\n",
        "data_test = \"\"\n",
        "for d in dialogs_test:\n",
        "  data_test += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_test = open(\"fr_finetuned_test.txt\",\"w\")\n",
        "print(file_test.write(data_test))\n",
        "\n",
        "# print(len(data_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBdvyDjCPyOS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHGqVr7SuKkz"
      },
      "source": [
        "### Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6JXGnBHP0iN",
        "outputId": "c1901cff-6a70-4fd1-d474-61029eaf75e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "124122\n",
            "248244\n"
          ]
        }
      ],
      "source": [
        "double_lines = []\n",
        "with open('fr_finetuned_train.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    double_lines.extend(lines)\n",
        "    double_lines.extend(lines)\n",
        "\n",
        "print(len(lines))\n",
        "print(len(double_lines))\n",
        "with open('fr_finetuned_train.txt', 'w') as ff:\n",
        "    ff.writelines(double_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roFeyZCAYZ0n",
        "outputId": "d6664f48-774f-4657-ebc7-078de9dcb5e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"text:salut comment vas-tu ? Je me prépare à faire du guépard pour rester en forme.\\tlabels:vous devez être très rapide. la chasse est l'un de mes passe-temps préférés.\\n\",\n",
              " \"text:je suis ! pour mon hobby j'aime faire la mise en conserve ou un peu tailler.\\tlabels:je remodèle aussi des maisons quand je ne suis pas à la chasse à l'arc.\\n\",\n",
              " \"text:c'est bien. quand j'étais au lycée, je me suis placé 6ème au 100m dash!\\tlabels:c'est génial . avez-vous une saison ou une période préférée de l'année?\\n\",\n",
              " \"text:Non . mais j'ai une viande préférée car c'est tout ce que je mange exclusivement.\\tlabels:quelle est votre viande préférée à manger?\\n\",\n",
              " \"text:je devrais dire sa côte de bœuf. avez-vous des aliments préférés?\\tlabels:j'aime le poulet ou les macaronis et le fromage.\\n\",\n",
              " \"text:avez-vous prévu quelque chose pour aujourd'hui? je pense que je vais faire de la mise en conserve.\\tlabels:je vais regarder le football. que conservez-vous?\\n\",\n",
              " \"text:je pense que je vais pouvoir un peu de confiture. jouez-vous aussi pour le plaisir?\\tlabels:si j'ai le temps en dehors des maisons de chasse et de rénovation. ce qui n'est pas grand chose!\\tepisode_done:True\\n\",\n",
              " \"text:Bonjour comment allez-vous aujourd'hui ?\\tlabels:je passe du temps avec mes 4 soeurs que faites-vous\\n\",\n",
              " \"text:wow, quatre sœurs. je regarde juste le jeu des trônes.\\tlabels:c'est un bon spectacle je regarde ça en buvant du thé glacé\\n\",\n",
              " \"text:je suis d'accord . Comment gagnez-vous votre vie ?\\tlabels:je suis un chercheur je recherche le fait que les sirènes sont réelles\\n\"]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2SSCxxzuKkz"
      },
      "outputs": [],
      "source": [
        "finetuned_model_path = f'{mydrive_path}finetuned-400m-topk-double'\n",
        "init_model = 'zoo:blender/blender_400Mdistill/model'\n",
        "dict_file  = 'zoo:blender/blender_400Mdistill/model.dict'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SocCsjcGuKkz",
        "outputId": "400f7b7c-c81a-48e8-b487-40e4f8be69ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13:32:05 | building dictionary first...\n",
            "13:32:05 | No model with opt yet at: /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk-double/model(.opt)\n",
            "13:32:05 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: download_path: None,verbose: True,is_debug: False,datapath: /usr/local/lib/python3.7/dist-packages/data,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,world_logs: ,save_format: conversations,log_keep_fields: all,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: fr_finetuned,fromfile_datatype_extension: True,checkpoint_activations: False,interactive_mode: False\u001b[0m\n",
            "13:32:05 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--init-opt /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt --allow-missing-init-opts True --task blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --dynamic-batching full --eval-batchsize 8 --num-epochs -1 --validation-every-n-secs 900.0 --save-after-valid True --validation-every-n-epochs -1.0 --validation-patience 20 --distributed-world-size 8 --port 61337 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --chosen-topic-delimiter \n",
            " --num-topics 5 --train-experiencer-only False --remove-political-convos False --teacher-model /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model --task-loss-coeff 1.0 --encoder-loss-coeff 24.0 --hidden-loss-coeff 5.0 --pred-loss-coeff 8.0 --embedding-loss-coeff 0.35 --self-attn-loss-coeff 0.6 --enc-dec-attn-loss-coeff 3.0 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --beam-length-penalty 0.65 --inference beam --topk 10 --temperature 1.0 --learningrate 0.0004 --gpu 0 --bpe-vocab /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json --bpe-merge /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt --bpe-add-prefix-space True --max-lr-steps -1 --parlai-home /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI --show-advanced-args False --numthreads 1 --rank 0 --dict-loaded True\u001b[0m\n",
            "13:32:05 | Using CUDA\n",
            "13:32:05 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict\n",
            "13:32:05 | num words = 8008\n",
            "13:32:11 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "13:32:11 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model\n",
            "13:32:27 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
            "13:32:27 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
            "13:32:28 | Opt:\n",
            "13:32:28 |     activation: gelu\n",
            "13:32:28 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "13:32:28 |     adam_eps: 1e-08\n",
            "13:32:28 |     add_p1_after_newln: False\n",
            "13:32:28 |     aggregate_micro: False\n",
            "13:32:28 |     allow_missing_init_opts: False\n",
            "13:32:28 |     attention_dropout: 0.0\n",
            "13:32:28 |     batchsize: 8\n",
            "13:32:28 |     beam_block_full_context: True\n",
            "13:32:28 |     beam_block_list_filename: None\n",
            "13:32:28 |     beam_block_ngram: -1\n",
            "13:32:28 |     beam_context_block_ngram: -1\n",
            "13:32:28 |     beam_delay: 30\n",
            "13:32:28 |     beam_length_penalty: 1.03\n",
            "13:32:28 |     beam_min_length: 1\n",
            "13:32:28 |     beam_size: 1\n",
            "13:32:28 |     betas: '(0.9, 0.999)'\n",
            "13:32:28 |     bpe_add_prefix_space: None\n",
            "13:32:28 |     bpe_debug: False\n",
            "13:32:28 |     bpe_dropout: None\n",
            "13:32:28 |     bpe_merge: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict-merges.txt\n",
            "13:32:28 |     bpe_vocab: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict-vocab.json\n",
            "13:32:28 |     checkpoint_activations: False\n",
            "13:32:28 |     compute_tokenized_bleu: False\n",
            "13:32:28 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "13:32:28 |     datatype: train\n",
            "13:32:28 |     delimiter: '  '\n",
            "13:32:28 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "13:32:28 |     dict_endtoken: __end__\n",
            "13:32:28 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict\n",
            "13:32:28 |     dict_include_test: False\n",
            "13:32:28 |     dict_include_valid: False\n",
            "13:32:28 |     dict_initpath: None\n",
            "13:32:28 |     dict_language: english\n",
            "13:32:28 |     dict_loaded: True\n",
            "13:32:28 |     dict_lower: False\n",
            "13:32:28 |     dict_max_ngram_size: -1\n",
            "13:32:28 |     dict_maxexs: -1\n",
            "13:32:28 |     dict_maxtokens: -1\n",
            "13:32:28 |     dict_minfreq: 0\n",
            "13:32:28 |     dict_nulltoken: __null__\n",
            "13:32:28 |     dict_starttoken: __start__\n",
            "13:32:28 |     dict_textfields: text,labels\n",
            "13:32:28 |     dict_tokenizer: bytelevelbpe\n",
            "13:32:28 |     dict_unktoken: __unk__\n",
            "13:32:28 |     display_examples: False\n",
            "13:32:28 |     download_path: None\n",
            "13:32:28 |     dropout: 0.1\n",
            "13:32:28 |     dynamic_batching: None\n",
            "13:32:28 |     embedding_projection: random\n",
            "13:32:28 |     embedding_size: 1280\n",
            "13:32:28 |     embedding_type: random\n",
            "13:32:28 |     embeddings_scale: True\n",
            "13:32:28 |     eval_batchsize: None\n",
            "13:32:28 |     eval_dynamic_batching: None\n",
            "13:32:28 |     evaltask: None\n",
            "13:32:28 |     ffn_size: 5120\n",
            "13:32:28 |     final_extra_opt: \n",
            "13:32:28 |     force_fp16_tokens: False\n",
            "13:32:28 |     fp16: True\n",
            "13:32:28 |     fp16_impl: mem_efficient\n",
            "13:32:28 |     fromfile_datapath: fr_finetuned\n",
            "13:32:28 |     fromfile_datatype_extension: True\n",
            "13:32:28 |     gpu: -1\n",
            "13:32:28 |     gradient_clip: 0.1\n",
            "13:32:28 |     hide_labels: False\n",
            "13:32:28 |     history_add_global_end_token: end\n",
            "13:32:28 |     history_reversed: False\n",
            "13:32:28 |     history_size: -1\n",
            "13:32:28 |     image_cropsize: 224\n",
            "13:32:28 |     image_mode: raw\n",
            "13:32:28 |     image_size: 256\n",
            "13:32:28 |     inference: topk\n",
            "13:32:28 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model\n",
            "13:32:28 |     init_opt: None\n",
            "13:32:28 |     interactive_mode: False\n",
            "13:32:28 |     invsqrt_lr_decay_gamma: -1\n",
            "13:32:28 |     is_debug: False\n",
            "13:32:28 |     label_truncate: 128\n",
            "13:32:28 |     learn_positional_embeddings: False\n",
            "13:32:28 |     learningrate: 7e-06\n",
            "13:32:28 |     load_from_checkpoint: True\n",
            "13:32:28 |     log_every_n_secs: 10.0\n",
            "13:32:28 |     log_every_n_steps: 50\n",
            "13:32:28 |     log_keep_fields: all\n",
            "13:32:28 |     loglevel: info\n",
            "13:32:28 |     lr_scheduler: reduceonplateau\n",
            "13:32:28 |     lr_scheduler_decay: 0.5\n",
            "13:32:28 |     lr_scheduler_patience: 3\n",
            "13:32:28 |     max_train_steps: -1\n",
            "13:32:28 |     max_train_time: -1\n",
            "13:32:28 |     metrics: default\n",
            "13:32:28 |     model: transformer/generator\n",
            "13:32:28 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk-double/model\n",
            "13:32:28 |     model_parallel: False\n",
            "13:32:28 |     momentum: 0\n",
            "13:32:28 |     multitask_weights: '(1.0, 3.0, 3.0, 3.0)'\n",
            "13:32:28 |     mutators: None\n",
            "13:32:28 |     n_decoder_layers: 12\n",
            "13:32:28 |     n_encoder_layers: 2\n",
            "13:32:28 |     n_heads: 32\n",
            "13:32:28 |     n_layers: 2\n",
            "13:32:28 |     n_positions: 128\n",
            "13:32:28 |     n_segments: 0\n",
            "13:32:28 |     nesterov: True\n",
            "13:32:28 |     no_cuda: False\n",
            "13:32:28 |     num_epochs: 5.0\n",
            "13:32:28 |     num_workers: 0\n",
            "13:32:28 |     nus: (0.7,)\n",
            "13:32:28 |     optimizer: mem_eff_adam\n",
            "13:32:28 |     output_scaling: 1.0\n",
            "13:32:28 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'fr_finetuned', 'fromfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk-double/model', 'init_model': 'zoo:blender/blender_400Mdistill/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict', 'n_heads': 32, 'n_layers': 2, 'n_positions': 128, 'n_encoder_layers': 2, 'n_decoder_layers': 12, 'embedding_size': 1280, 'ffn_size': 5120, 'label_truncate': 128, 'text_truncate': 128, 'truncate': 128, 'dropout': 0.1, 'log_every_n_secs': 10.0, 'multitask_weights': (1.0, 3.0, 3.0, 3.0), 'attention_dropout': 0.0, 'activation': 'gelu', 'history_add_global_end_token': 'end', 'delimiter': '  ', 'dict_tokenizer': 'bytelevelbpe', 'variant': 'prelayernorm', 'optimizer': 'mem_eff_adam', 'learningrate': 7e-06, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'relu_dropout': 0.0, 'model_parallel': False, 'warmup_updates': 100, 'update_freq': 2, 'gradient_clip': 0.1, 'validation_every_n_epochs': 0.25, 'num_epochs': 5.0, 'verbose': True, 'batchsize': 8, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'validation_patience': 10, 'validation_metric': 'ppl', 'validation_metric_mode': 'min', 'inference': 'topk', 'temperature': 0.7, 'topk': 30, 'beam_length_penalty': 1.03}\"\n",
            "13:32:28 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "13:32:28 |     person_tokens: False\n",
            "13:32:28 |     rank_candidates: False\n",
            "13:32:28 |     relu_dropout: 0.0\n",
            "13:32:28 |     save_after_valid: False\n",
            "13:32:28 |     save_every_n_secs: -1\n",
            "13:32:28 |     save_format: conversations\n",
            "13:32:28 |     share_word_embeddings: True\n",
            "13:32:28 |     short_final_eval: False\n",
            "13:32:28 |     skip_generation: True\n",
            "13:32:28 |     special_tok_lst: None\n",
            "13:32:28 |     split_lines: False\n",
            "13:32:28 |     starttime: Apr01_13-32\n",
            "13:32:28 |     task: fromfile:parlaiformat\n",
            "13:32:28 |     temperature: 0.7\n",
            "13:32:28 |     tensorboard_log: False\n",
            "13:32:28 |     tensorboard_logdir: None\n",
            "13:32:28 |     text_truncate: 128\n",
            "13:32:28 |     topk: 30\n",
            "13:32:28 |     topp: 0.9\n",
            "13:32:28 |     truncate: 128\n",
            "13:32:28 |     update_freq: 2\n",
            "13:32:28 |     use_reply: label\n",
            "13:32:28 |     validation_cutoff: 1.0\n",
            "13:32:28 |     validation_every_n_epochs: 0.25\n",
            "13:32:28 |     validation_every_n_secs: -1\n",
            "13:32:28 |     validation_every_n_steps: -1\n",
            "13:32:28 |     validation_max_exs: -1\n",
            "13:32:28 |     validation_metric: ppl\n",
            "13:32:28 |     validation_metric_mode: min\n",
            "13:32:28 |     validation_patience: 10\n",
            "13:32:28 |     validation_share_agent: False\n",
            "13:32:28 |     variant: prelayernorm\n",
            "13:32:28 |     verbose: True\n",
            "13:32:28 |     wandb_entity: None\n",
            "13:32:28 |     wandb_log: False\n",
            "13:32:28 |     wandb_name: None\n",
            "13:32:28 |     wandb_project: None\n",
            "13:32:28 |     warmup_rate: 0.0001\n",
            "13:32:28 |     warmup_updates: 100\n",
            "13:32:28 |     weight_decay: None\n",
            "13:32:28 |     world_logs: \n",
            "13:32:29 | creating task(s): fromfile:parlaiformat\n",
            "13:32:29 | Loading ParlAI text data: fr_finetuned_train.txt\n",
            "13:32:32 | training...\n",
            "13:32:32 | Overflow: setting loss scale to 65536.0\n",
            "13:32:34 | Overflow: setting loss scale to 32768.0\n",
            "13:32:42 | time:10s total_exs:456 total_steps:28 epochs:0.00 time_left:27332s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n",
            "   182.3 .9286 807.3  4585   .6162      81.43 45.43  456             37449   17.1    .6082  25.9 5.796 1.961e-06 207.2  1177   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0  329      .1124         0                   28 1015 5761 2.811\n",
            "\n",
            "13:32:52 | time:20s total_exs:920 total_steps:57 epochs:0.00 time_left:27180s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   199.1     1 820.4  4745   .6487      96.55 46.26  464             32768  16.58    .5692 26.45 5.716 3.99e-06 211.6  1224   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 303.6      .1141         0                   57 1032 5968 2.91\n",
            "\n",
            "13:33:02 | time:30s total_exs:1376 total_steps:86 epochs:0.01 time_left:27188s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.8     1 808.9  4612   .6294      81.64 45.61  456             32768   11.1    .5692 26.06 5.369 6.02e-06 208.5  1189   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 214.7      .1188         0                   86 1017 5801 2.905\n",
            "\n",
            "13:33:12 | time:40s total_exs:1840 total_steps:115 epochs:0.01 time_left:27085s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   192.2     1 812.2  4701   .6401      90.63 46.31  464             32768   8.14    .5692  25.5 4.994 6.93e-06   204  1181   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 147.6      .1536         0                  115 1016 5882 2.901\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-64ced2bbeff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mbeam_length_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.03\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \"\"\"\n\u001b[1;32m    999\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_train_log\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m             \u001b[0;31m# we've already done what we need in these\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mtrain_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    905\u001b[0m                 \u001b[0;31m# do one example / batch of examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m                     \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparley\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopTrainException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m                     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Stopping from {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/worlds.py\u001b[0m in \u001b[0;36mparley\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0;31m# The agent acts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m             \u001b[0mbatch_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m             \u001b[0;31m# We possibly execute this action in the world.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/worlds.py\u001b[0m in \u001b[0;36mbatch_act\u001b[0;34m(self, agent_idx, batch_observation)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_act'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0mbatch_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_observation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0;31m# Store the actions locally in each world.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworlds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36mbatch_act\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m   2236\u001b[0m             \u001b[0;31m# register the start of updates for later counting when they occur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ups'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalTimerMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m             \u001b[0moom_sync\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36mupdate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2374\u001b[0m             )\n\u001b[1;32m   2375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2376\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2378\u001b[0m         \u001b[0;31m# keep track up number of steps, compute warmup factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/utils/fp16.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \"\"\"\n\u001b[1;32m    529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unscale_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/utils/fp16.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0mp_data_fp32\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_data_fp32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    # task= \"blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues\",\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model= \"transformer/generator\",\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "\n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file= dict_file,\n",
        "\n",
        "    # arguments we get from the pretrained model.\n",
        "    n_heads= 32, n_layers= 2, n_positions= 128, n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    dropout= 0.1, log_every_n_secs= 10,\n",
        "    multitask_weights= \"1,3,3,3\",\n",
        "    attention_dropout= 0.0,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "\n",
        "    variant= \"prelayernorm\",\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    relu_dropout= 0.0, model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "    # save_after_valid= True,\n",
        "\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 5,\n",
        "    verbose = True,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    batchsize= 8, fp16= True, fp16_impl= \"mem_efficient\",\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\",\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    inference = 'topk', \n",
        "    temperature = 0.7, \n",
        "    topk=30, \n",
        "    beam_length_penalty=1.03\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcAAzveAuKkz",
        "outputId": "e3c6c778-9e94-43c8-ef52-a75f5718fdb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model\n",
            "09:48:09 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "09:48:09 | Using CUDA\n",
            "09:48:09 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model.dict\n",
            "09:48:09 | num words = 8008\n",
            "09:48:15 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "09:48:15 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model\n",
            "09:48:35 | creating task(s): fromfile:parlaiformat\n",
            "09:48:35 | Loading ParlAI text data: fr_finetuned_valid.txt\n",
            "09:48:35 | Opt:\n",
            "09:48:35 |     activation: gelu\n",
            "09:48:35 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "09:48:35 |     adam_eps: 1e-08\n",
            "09:48:35 |     add_p1_after_newln: False\n",
            "09:48:35 |     aggregate_micro: False\n",
            "09:48:35 |     allow_missing_init_opts: False\n",
            "09:48:35 |     attention_dropout: 0.0\n",
            "09:48:35 |     batchsize: 8\n",
            "09:48:35 |     beam_block_full_context: True\n",
            "09:48:35 |     beam_block_list_filename: None\n",
            "09:48:35 |     beam_block_ngram: -1\n",
            "09:48:35 |     beam_context_block_ngram: -1\n",
            "09:48:35 |     beam_delay: 30\n",
            "09:48:35 |     beam_length_penalty: 1.03\n",
            "09:48:35 |     beam_min_length: 1\n",
            "09:48:35 |     beam_size: 1\n",
            "09:48:35 |     betas: '[0.9, 0.999]'\n",
            "09:48:35 |     bpe_add_prefix_space: None\n",
            "09:48:35 |     bpe_debug: False\n",
            "09:48:35 |     bpe_dropout: None\n",
            "09:48:35 |     bpe_merge: /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model.dict-merges.txt\n",
            "09:48:35 |     bpe_vocab: /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model.dict-vocab.json\n",
            "09:48:35 |     checkpoint_activations: False\n",
            "09:48:35 |     compute_tokenized_bleu: False\n",
            "09:48:35 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "09:48:35 |     datatype: train\n",
            "09:48:35 |     delimiter: '  '\n",
            "09:48:35 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "09:48:35 |     dict_endtoken: __end__\n",
            "09:48:35 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model.dict\n",
            "09:48:35 |     dict_include_test: False\n",
            "09:48:35 |     dict_include_valid: False\n",
            "09:48:35 |     dict_initpath: None\n",
            "09:48:35 |     dict_language: english\n",
            "09:48:35 |     dict_loaded: True\n",
            "09:48:35 |     dict_lower: False\n",
            "09:48:35 |     dict_max_ngram_size: -1\n",
            "09:48:35 |     dict_maxexs: -1\n",
            "09:48:35 |     dict_maxtokens: -1\n",
            "09:48:35 |     dict_minfreq: 0\n",
            "09:48:35 |     dict_nulltoken: __null__\n",
            "09:48:35 |     dict_starttoken: __start__\n",
            "09:48:35 |     dict_textfields: text,labels\n",
            "09:48:35 |     dict_tokenizer: bytelevelbpe\n",
            "09:48:35 |     dict_unktoken: __unk__\n",
            "09:48:35 |     display_add_fields: \n",
            "09:48:35 |     display_examples: False\n",
            "09:48:35 |     download_path: None\n",
            "09:48:35 |     dropout: 0.1\n",
            "09:48:35 |     dynamic_batching: None\n",
            "09:48:35 |     embedding_projection: random\n",
            "09:48:35 |     embedding_size: 1280\n",
            "09:48:35 |     embedding_type: random\n",
            "09:48:35 |     embeddings_scale: True\n",
            "09:48:35 |     eval_batchsize: None\n",
            "09:48:35 |     eval_dynamic_batching: None\n",
            "09:48:35 |     evaltask: None\n",
            "09:48:35 |     ffn_size: 5120\n",
            "09:48:35 |     final_extra_opt: \n",
            "09:48:35 |     force_fp16_tokens: True\n",
            "09:48:35 |     fp16: True\n",
            "09:48:35 |     fp16_impl: mem_efficient\n",
            "09:48:35 |     fromfile_datapath: fr_finetuned\n",
            "09:48:35 |     fromfile_datatype_extension: True\n",
            "09:48:35 |     gpu: -1\n",
            "09:48:35 |     gradient_clip: 0.1\n",
            "09:48:35 |     hide_labels: False\n",
            "09:48:35 |     history_add_global_end_token: end\n",
            "09:48:35 |     history_reversed: False\n",
            "09:48:35 |     history_size: -1\n",
            "09:48:35 |     image_cropsize: 224\n",
            "09:48:35 |     image_mode: raw\n",
            "09:48:35 |     image_size: 256\n",
            "09:48:35 |     inference: topk\n",
            "09:48:35 |     init_model: zoo:blender/blender_400Mdistill/model\n",
            "09:48:35 |     init_opt: None\n",
            "09:48:35 |     interactive_mode: False\n",
            "09:48:35 |     invsqrt_lr_decay_gamma: -1\n",
            "09:48:35 |     is_debug: False\n",
            "09:48:35 |     label_truncate: 128\n",
            "09:48:35 |     learn_positional_embeddings: False\n",
            "09:48:35 |     learningrate: 7e-06\n",
            "09:48:35 |     log_every_n_secs: 10.0\n",
            "09:48:35 |     log_every_n_steps: 50\n",
            "09:48:35 |     log_keep_fields: all\n",
            "09:48:35 |     loglevel: info\n",
            "09:48:35 |     lr_scheduler: reduceonplateau\n",
            "09:48:35 |     lr_scheduler_decay: 0.5\n",
            "09:48:35 |     lr_scheduler_patience: 3\n",
            "09:48:35 |     max_train_steps: -1\n",
            "09:48:35 |     max_train_time: -1\n",
            "09:48:35 |     metrics: default\n",
            "09:48:35 |     model: transformer/generator\n",
            "09:48:35 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model\n",
            "09:48:35 |     model_parallel: False\n",
            "09:48:35 |     momentum: 0\n",
            "09:48:35 |     multitask_weights: '[1.0, 3.0, 3.0, 3.0]'\n",
            "09:48:35 |     mutators: None\n",
            "09:48:35 |     n_decoder_layers: 12\n",
            "09:48:35 |     n_encoder_layers: 2\n",
            "09:48:35 |     n_heads: 32\n",
            "09:48:35 |     n_layers: 2\n",
            "09:48:35 |     n_positions: 128\n",
            "09:48:35 |     n_segments: 0\n",
            "09:48:35 |     nesterov: True\n",
            "09:48:35 |     no_cuda: False\n",
            "09:48:35 |     num_epochs: 5.0\n",
            "09:48:35 |     num_examples: 20\n",
            "09:48:35 |     num_workers: 0\n",
            "09:48:35 |     nus: [0.7]\n",
            "09:48:35 |     optimizer: mem_eff_adam\n",
            "09:48:35 |     output_scaling: 1.0\n",
            "09:48:35 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'fr_finetuned', 'fromfile_datatype_extension': True, 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict', 'num_examples': '20', 'skip_generation': False, 'inference': 'topk', 'temperature': 0.7, 'topk': 30, 'beam_length_penalty': 1.03}\"\n",
            "09:48:35 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "09:48:35 |     person_tokens: False\n",
            "09:48:35 |     rank_candidates: False\n",
            "09:48:35 |     relu_dropout: 0.0\n",
            "09:48:35 |     save_after_valid: False\n",
            "09:48:35 |     save_every_n_secs: -1\n",
            "09:48:35 |     save_format: conversations\n",
            "09:48:35 |     share_word_embeddings: True\n",
            "09:48:35 |     short_final_eval: False\n",
            "09:48:35 |     skip_generation: False\n",
            "09:48:35 |     special_tok_lst: None\n",
            "09:48:35 |     split_lines: False\n",
            "09:48:35 |     starttime: Mar31_11-31\n",
            "09:48:35 |     task: fromfile:parlaiformat\n",
            "09:48:35 |     temperature: 0.7\n",
            "09:48:35 |     tensorboard_log: False\n",
            "09:48:35 |     tensorboard_logdir: None\n",
            "09:48:35 |     text_truncate: 128\n",
            "09:48:35 |     topk: 30\n",
            "09:48:35 |     topp: 0.9\n",
            "09:48:35 |     truncate: 128\n",
            "09:48:35 |     update_freq: 2\n",
            "09:48:35 |     use_reply: label\n",
            "09:48:35 |     validation_cutoff: 1.0\n",
            "09:48:35 |     validation_every_n_epochs: 0.25\n",
            "09:48:35 |     validation_every_n_secs: -1\n",
            "09:48:35 |     validation_every_n_steps: -1\n",
            "09:48:35 |     validation_max_exs: -1\n",
            "09:48:35 |     validation_metric: ppl\n",
            "09:48:35 |     validation_metric_mode: min\n",
            "09:48:35 |     validation_patience: 10\n",
            "09:48:35 |     validation_share_agent: False\n",
            "09:48:35 |     variant: prelayernorm\n",
            "09:48:35 |     verbose: False\n",
            "09:48:35 |     wandb_entity: None\n",
            "09:48:35 |     wandb_log: False\n",
            "09:48:35 |     wandb_name: None\n",
            "09:48:35 |     wandb_project: None\n",
            "09:48:35 |     warmup_rate: 0.0001\n",
            "09:48:35 |     warmup_updates: 100\n",
            "09:48:35 |     weight_decay: None\n",
            "09:48:35 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finetuned_valid.txt- - -\u001b[0;0m\n",
            "\u001b[0msalut ! comment vas-tu ce soir ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: je le fais bien . juste se détendre avec mes deux chiens.\u001b[0;0m\n",
            "\u001b[0;95m     model: Je vais bien. toi ?\u001b[0;0m\n",
            "\u001b[0mgénial . dans mon temps libre je fais du bénévolat.\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est bien. quel genre de travail bénévole faites-vous?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis une bonneur que je pourrais vous?\u001b[0;0m\n",
            "\u001b[0mje travaille dans un refuge pour sans-abri dans ma ville.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Bien pour vous . vous aimez les voitures anciennes? j'ai deux anciennes mustangs.\u001b[0;0m\n",
            "\u001b[0;95m     model: je n'ai pas la nager. j'aime pas vraiment une fiction.\u001b[0;0m\n",
            "\u001b[0mcool . pas vraiment dans les voitures. mon travail de jour est dur.\u001b[0;0m\n",
            "\u001b[1;94m    labels: les voitures sont mon truc. voitures anciennes . j'adore travailler sur cela. est-ce que tu aimes ça ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je n'ai pas de terminer le temps.\u001b[0;0m\n",
            "\u001b[0moui, j'aime la foule, faire connaissance avec les gens.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je n'ai pas pensé à l'aspect foule de la lutte. je n'aime pas les foules.\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai une bonne chose. pas de ma maman est à la maison.\u001b[0;0m\n",
            "\u001b[0mcompris . quand j'ai besoin d'être seul, je m'entraîne plusieurs fois par semaine.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je suis d'accord . travailler est un excellent moyen de faire ses doléances. Aimez-vous la musique country ?\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai un film film de la musique. elle est mon père sur le faire.\u001b[0;0m\n",
            "\u001b[0mun petit peu . je peux admirer taylor swift.\u001b[0;0m\n",
            "\u001b[1;94m    labels: lol. elle est plus pop maintenant. ses vieux trucs étaient country. j'aime le country classique.\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai commencé à un récemment.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finetuned_valid.txt- - -\u001b[0;0m\n",
            "\u001b[0msalut comment allez-vous ? je vais bien et vous?\u001b[0;0m\n",
            "\u001b[1;94m    labels: je vais bien. sirotant juste du thé. que faites-vous pour le travail?\u001b[0;0m\n",
            "\u001b[0;95m     model: je vais bien . j'ai une grande de mes deux chiens.\u001b[0;0m\n",
            "\u001b[0mje faisais de l'aide à domicile mais maintenant je suis handicapé.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je suis désolé de l'entendre . Qu'est-il arrivé\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un fabneur et c'est une bonne journée.\u001b[0;0m\n",
            "\u001b[0mj'ai obtenu mon diplôme et obtenu mon permis de travail un certain temps et est devenu je vais.\u001b[0;0m\n",
            "\u001b[1;94m    labels: alors que faites-vous maintenant pour vous amuser? J'aime lire .\u001b[0;0m\n",
            "\u001b[0;95m     model: pas de terminer des frères.\u001b[0;0m\n",
            "\u001b[0mj'ai des chiens et je les promène. et un chat.\u001b[0;0m\n",
            "\u001b[1;94m    labels: quel genre de chiens avez-vous?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un peu de l'argent pour la plupart.\u001b[0;0m\n",
            "\u001b[0mj'aime lire sur michael jackson et la psychologie aujourd'hui.\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'aime lire . j'ai une grande bibliothèque chez moi.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui, ça a l'air génial .\u001b[0;0m\n",
            "\u001b[0mc'est très agréable j'aime lire et lire des revues.\u001b[0;0m\n",
            "\u001b[1;94m    labels: j'ai du mal à entendre donc la lecture est mon endroit heureux.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis dans les gens. je ne vais pas dépétarien aussi.\u001b[0;0m\n",
            "\u001b[0mcool ça va. ma maman était très dure avec moi.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je travaille comme guide d'excursion de musée de malentendants.\u001b[0;0m\n",
            "\u001b[0;95m     model: c'est cool . je suis allé à la musique. je suis uneur.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finetuned_valid.txt- - -\u001b[0;0m\n",
            "\u001b[0msalut ! vous aimez les tortues?\u001b[0;0m\n",
            "\u001b[1;94m    labels: en fait, personnellement, je préfère le chat.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne suis pas aller à la maison. je suis allez-vous ?\u001b[0;0m\n",
            "\u001b[0mj'ai une tortue son nom est rapide. les chatons sont sympas aussi, tho!\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est un nom adorable pour une tortue. j'ai 2 chats\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai une petit ami.\u001b[0;0m\n",
            "\u001b[0mquels sont vos noms de chatons?\u001b[0;0m\n",
            "\u001b[1;94m    labels: neige et hiver, du nom de ma saison préférée\u001b[0;0m\n",
            "\u001b[0;95m     model: je parie que je suis dormir de la guités de l'automne.\u001b[0;0m\n",
            "\u001b[0mJ'aime ça ! je vais à l'école maternelle.\u001b[0;0m\n",
            "\u001b[1;94m    labels: oh tu es si jeune!\u001b[0;0m\n",
            "\u001b[0;95m     model: il y a une fois. vous avez les choses.\u001b[0;0m\n",
            "\u001b[0mquel âge avez-vous ? j'ai fêté mes quatre ans au jour de mon anniversaire!\u001b[0;0m\n",
            "\u001b[1;94m    labels: je suis une vieille femme. j'ai gagné une médaille d'or en 1993\u001b[0;0m\n",
            "\u001b[0;95m     model: c'est bien, je suis désolé .\u001b[0;0m\n",
            "\u001b[0mHou la la ! ! tu as gagné l'or! êtes-vous riche ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: non . je dépense tout mon argent sur des chapeaux! j'en ai plus d'un millier\u001b[0;0m\n",
            "\u001b[0;95m     model: je n'ai pas, mais j'aime faire des animaux deux.\u001b[0;0m\n"
          ]
        }
      ],
      "source": [
        "print(f'{finetuned_model_path}/model')\n",
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat',\n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    dict_file= dict_file,\n",
        "\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    inference = 'topk', \n",
        "    temperature = 0.7, \n",
        "    topk=30, \n",
        "    beam_length_penalty=1.03\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJRxOsMOuKk0",
        "outputId": "58755077-ea3b-4984-83b5-87ee33c24c9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting twython\n",
            "  Downloading twython-3.9.1-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython) (2.27.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.2.0)\n",
            "Installing collected packages: twython\n",
            "Successfully installed twython-3.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install twython "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlbD3mTHuKk0"
      },
      "outputs": [],
      "source": [
        "test_set_length = sum(len(d) for d in dialogs_test)\n",
        "model_path = f'{finetuned_model_path}/model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FdC9Nd2uKk0"
      },
      "outputs": [],
      "source": [
        "# another_model_path = f'{mydrive_path}finetuned-beam_prmts_infrnc'\n",
        "\n",
        "!(parlai display_model \\\n",
        "    --task 'fromfile:parlaiformat' \\\n",
        "    --fromfile-datapath 'fr_finetuned' \\\n",
        "    --datatype test \\\n",
        "    --fromfile-datatype-extension True \\\n",
        "    --model-file $model_path \\\n",
        "    --dict-file $dict_file \\\n",
        "    --num-examples $test_set_length \\\n",
        "    --skip-generation False \\\n",
        "    --inference 'topk' \\\n",
        "    --temperature 0.7 \\\n",
        "    --topk 30 \\\n",
        "    --beam-length-penalty 1.03) \\\n",
        "    > \"09-finetuned-400m-topk-5epochs-testset-output.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtwY67cwuKk0"
      },
      "outputs": [],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat',\n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    datatype= \"test\",\n",
        "    num_examples=test_set_length,\n",
        "    skip_generation=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AEgvstY4yGc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6TKGyXv4z9I"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "182pIsvG41gu"
      },
      "source": [
        "## Fine-tuning (400M model + translated BST & ED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRCSb08J41gu"
      },
      "source": [
        "### Convert dataset to ParlAI  format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_HZqVYa41gu",
        "outputId": "6d51af8f-7573-4d0c-d3aa-64af499b5d17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Xpersona'...\n",
            "remote: Enumerating objects: 282, done.\u001b[K\n",
            "remote: Counting objects: 100% (282/282), done.\u001b[K\n",
            "remote: Compressing objects: 100% (201/201), done.\u001b[K\n",
            "remote: Total 282 (delta 96), reused 241 (delta 78), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (282/282), 45.00 MiB | 11.64 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n",
            "Checking out files: 100% (218/218), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HLTCHKUST/Xpersona.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmBfhD7Q41gv"
      },
      "source": [
        "[All XPersona datasets](https://github.com/HLTCHKUST/Xpersona/tree/master/dataset) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElG42b-P41gv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_train_corrected.json','r') as f:\n",
        "   train_data = json.load(f)\n",
        "\n",
        "dialogs_train = pd.DataFrame(train_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_valid_human_annotated.json','r') as f:\n",
        "   valid_data = json.load(f)\n",
        "\n",
        "dialogs_valid = pd.DataFrame(valid_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_test_human_annotated.json','r') as f:\n",
        "   text_data = json.load(f)\n",
        "\n",
        "dialogs_test = pd.DataFrame(text_data)['dialogue'].tolist()\n",
        "\n",
        "# dialogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J27g-0eF41gv",
        "outputId": "50a4da90-1d10-4aef-a87b-994857023421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15776427\n",
            "257114\n",
            "262425\n"
          ]
        }
      ],
      "source": [
        "# pd.read_json(f,orient='records', encoding='utf-8',lines=True)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# dialogs = pd.read_json('XPersona/dataset/Fr_persona_split_train_corrected.json')['dialogue'].tolist()\n",
        "\n",
        "\n",
        "text = \"\"\n",
        "\n",
        "def transfer_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, each_two_turn in enumerate(d):\n",
        "    u1 = each_two_turn[0]\n",
        "    u2 = each_two_turn[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "\n",
        "# d = dialogs[0]\n",
        "# print(d)\n",
        "# print(transfer_dialog(d))\n",
        "\n",
        "data_train = \"\"\n",
        "for d in dialogs_train:\n",
        "  data_train += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_train = open(\"fr_finetuned_train.txt\",\"w\")\n",
        "print(file_train.write(data_train))\n",
        "\n",
        "data_valid = \"\"\n",
        "for d in dialogs_valid:\n",
        "  data_valid += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_valid = open(\"fr_finetuned_valid.txt\",\"w\")\n",
        "print(file_valid.write(data_valid))\n",
        "\n",
        "data_test = \"\"\n",
        "for d in dialogs_test:\n",
        "  data_test += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_test = open(\"fr_finetuned_test.txt\",\"w\")\n",
        "print(file_test.write(data_test))\n",
        "\n",
        "# print(len(data_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njr5h8lQ5BoA"
      },
      "source": [
        "### Convert BST & ED to ParlAI format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3VzPHl09Zn1"
      },
      "outputs": [],
      "source": [
        "def transfer_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, each_two_turn in enumerate(d):\n",
        "    u1 = each_two_turn[0]\n",
        "    u2 = each_two_turn[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEXkQmh5NZCb"
      },
      "outputs": [],
      "source": [
        "def convert_list_of_turns_to_parlai_format(turns):\n",
        "    dialogs = []\n",
        "    dialogs_parlai_format = \"\"\n",
        "    if len(turns) % 2 == 0:\n",
        "        if len(turns) > 3:\n",
        "            # [1 2 3 4] => [1 2 3 4]\n",
        "            dialogs.append(turns)\n",
        "            # [1 2 3 4] => [2 3]\n",
        "            dialogs.append(turns[1:len(turns)-1])\n",
        "        else:\n",
        "            dialogs.append(turns)\n",
        "    else:\n",
        "        # [1 2 3 4 5] => [2,3,4,5]\n",
        "        dialogs.append(turns[1:])\n",
        "        # [1 2 3 4 5] => [1,2,3,4]\n",
        "        dialogs.append(turns[0:len(turns)-1])\n",
        "\n",
        "    for dialog in dialogs:\n",
        "        iter_dialog = iter(dialog)\n",
        "        dialog_pairs = [[x, next(iter_dialog)] for x in iter_dialog]\n",
        "        dialogs_parlai_format += transfer_dialog(dialog_pairs)\n",
        "        \n",
        "    return dialogs_parlai_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHOiHfsUbAhi",
        "outputId": "e57763c4-1162-479d-a09a-c1480796c9f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finished!\n"
          ]
        }
      ],
      "source": [
        "mydrive_datasets_path = '/content/drive/MyDrive/colabs/aliae-workspace/datasets/'\n",
        "fr_bst_turns = []\n",
        "with open(f\"{mydrive_datasets_path}bst_utterance_fr.txt\") as file:\n",
        "    fr_bst_turns = file.readlines()\n",
        "\n",
        "en_bst_turns = []\n",
        "with open(f\"{mydrive_datasets_path}bst_all_utterances.txt\") as file:\n",
        "    en_bst_turns = file.readlines()\n",
        "\n",
        "for i in range(0, len(fr_bst_turns)):\n",
        "    parts = fr_bst_turns[i].split(\":\")\n",
        "    if len(parts) < 2: \n",
        "        fr_bst_turns[i] = fr_bst_turns[i].replace(\"[\",\"\")\n",
        "        fr_bst_turns[i] = en_bst_turns[i].split(\":\")[0] + \" : \" + fr_bst_turns[i]\n",
        "        # print(fr_bst_turns[i])\n",
        "\n",
        "with open(f\"{mydrive_datasets_path}bst_utterance_fr_v1.txt\", \"w\") as file:\n",
        "    file.writelines(fr_bst_turns)\n",
        "print(\"finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlKl8fdILX6E",
        "outputId": "72a680ab-edac-4e61-b417-1f3a2937b635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84169 12078 10973\n",
            "/content/drive/MyDrive/colabs/aliae-workspace/datasets/ed_french_train.txt\n",
            "/content/drive/MyDrive/colabs/aliae-workspace/datasets/ed_french_valid.txt\n",
            "/content/drive/MyDrive/colabs/aliae-workspace/datasets/ed_french_test.txt\n"
          ]
        }
      ],
      "source": [
        "mydrive_datasets_path = '/content/drive/MyDrive/colabs/aliae-workspace/datasets/'\n",
        "en_ed_turns = []\n",
        "with open(f\"{mydrive_datasets_path}ed_utterance_fr_v2.txt\") as file:\n",
        "    en_ed_turns = file.readlines()\n",
        "\n",
        "def split_a_line(item):\n",
        "    id_n_index, text = item.split('] ', maxsplit = 1)\n",
        "    id_n_index = id_n_index.replace(' ','')\n",
        "    dialog_id, index = id_n_index.split(',')\n",
        "    dialog_id = dialog_id.replace('[','').replace('(','')\n",
        "    index = index.replace(']','')\n",
        "    text = text.replace('\\n','')\n",
        "    return [dialog_id, index, text]\n",
        "\n",
        "\n",
        "def ed_create_parlai_files(turns_grouped, dataset_section):\n",
        "    parlai_format = \"\"\n",
        "    for i, groupped_turns in enumerate(turns_grouped):\n",
        "        dialog = [turn[2] for turn in groupped_turns]\n",
        "        parlai_format += convert_list_of_turns_to_parlai_format(dialog)\n",
        "    \n",
        "    dataset_name = \"ed_french\"\n",
        "    with open(f\"{mydrive_datasets_path}{dataset_name}_{dataset_section}.txt\", 'w') as file:\n",
        "        file.write(parlai_format)\n",
        "        print(f\"{mydrive_datasets_path}{dataset_name}_{dataset_section}.txt\")\n",
        "\n",
        "turns_train = [split_a_line(s) for s in en_ed_turns[:84169]]\n",
        "turns_valid = [split_a_line(s) for s in en_ed_turns[84169:84169 + 12078]]\n",
        "turns_test = [split_a_line(s) for s in en_ed_turns[84169 + 12078:]]\n",
        "print(len(turns_train), len(turns_valid), len(turns_test))\n",
        "\n",
        "from itertools import groupby\n",
        "turns_train_grouped = [list(item) for key, item in groupby(turns_train, lambda x: x[0])]\n",
        "ed_create_parlai_files(turns_train_grouped, 'train')\n",
        "\n",
        "turns_valid_grouped = [list(item) for key, item in groupby(turns_valid, lambda x: x[0])]\n",
        "ed_create_parlai_files(turns_valid_grouped, 'valid')\n",
        "\n",
        "turns_test_grouped = [list(item) for key, item in groupby(turns_test, lambda x: x[0])]\n",
        "ed_create_parlai_files(turns_test_grouped, 'test')\n",
        "\n",
        "# print(len(turns_train_grouped), len(turns_valid_grouped), len(turns_test_grouped))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHyLe-Wm5P-K",
        "outputId": "50581bf2-0a5b-4d2d-d2a2-b5e1e1b26f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "91118\n",
            "/content/drive/MyDrive/colabs/aliae-workspace/datasets/bst_french_train.txt\n",
            "/content/drive/MyDrive/colabs/aliae-workspace/datasets/bst_french_valid.txt\n",
            "/content/drive/MyDrive/colabs/aliae-workspace/datasets/bst_french_test.txt\n"
          ]
        }
      ],
      "source": [
        "mydrive_datasets_path = '/content/drive/MyDrive/colabs/aliae-workspace/datasets/'\n",
        "bst_turns = []\n",
        "with open(f\"{mydrive_datasets_path}bst_utterance_fr_v1.txt\") as file:\n",
        "    bst_turns = file.readlines()\n",
        "\n",
        "dialog_index = 1\n",
        "dialog = []\n",
        "dialogs = []\n",
        "parlai_format_train = \"\"\n",
        "parlai_format_valid = \"\"\n",
        "parlai_format_test = \"\"\n",
        "dialog_counter = 0\n",
        "print(len(bst_turns))\n",
        "for i in range(0, len(bst_turns)):\n",
        "    parts = bst_turns[i].split(\":\")\n",
        "    text = parts[1][1:].replace(\"\\n\",\"\")\n",
        "\n",
        "    if parts[0].lower().startswith(f\"[d{dialog_index}-\"):\n",
        "        dialog.append(text)\n",
        "    else:\n",
        "        if (dialog_index <= 4819):\n",
        "            parlai_format_train += convert_list_of_turns_to_parlai_format(dialog)\n",
        "        elif (dialog_index <= 4819 + 1009):\n",
        "            parlai_format_valid += convert_list_of_turns_to_parlai_format(dialog)\n",
        "        else:\n",
        "            parlai_format_test += convert_list_of_turns_to_parlai_format(dialog)\n",
        "\n",
        "        # dialogs.append([dialog])\n",
        "        dialog_index = dialog_index + 1\n",
        "        dialog = [text]\n",
        "        \n",
        "dataset_name = \"bst_french\"\n",
        "with open(f\"{mydrive_datasets_path}{dataset_name}_train.txt\", 'w') as file:\n",
        "    file.write(parlai_format_train)\n",
        "    print(f\"{mydrive_datasets_path}{dataset_name}_train.txt\")\n",
        "\n",
        "with open(f\"{mydrive_datasets_path}{dataset_name}_valid.txt\", 'w') as file:\n",
        "    file.write(parlai_format_valid)\n",
        "    print(f\"{mydrive_datasets_path}{dataset_name}_valid.txt\")\n",
        "\n",
        "with open(f\"{mydrive_datasets_path}{dataset_name}_test.txt\", 'w') as file:\n",
        "    file.write(parlai_format_test)\n",
        "    print(f\"{mydrive_datasets_path}{dataset_name}_test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYjeogWWOwvt"
      },
      "outputs": [],
      "source": [
        "bst_file_path = f\"{mydrive_datasets_path}bst_french\" \n",
        "!parlai display_data --task fromfile:parlaiformat --fromfile_datapath $bst_file_path --fromfile-datatype-extension true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbqCha4sJ1i9"
      },
      "source": [
        "### 3 New Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LkYZI5oIqoK"
      },
      "source": [
        "#### French ED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAjlghLmp5wd"
      },
      "outputs": [],
      "source": [
        "# ed_file_path = f\"{mydrive_datasets_path}ed_french\"\n",
        "# !parlai display_data --task fromfile:parlaiformat --fromfile_datapath $ed_file_path --fromfile-datatype-extension true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDKRNWNve2yO"
      },
      "outputs": [],
      "source": [
        "# !(parlai display_data \\\n",
        "#     --task fr_empathetic_dialogues \\\n",
        "#     --datapath /content/drive/MyDrive/colabs/aliae-workspace/datasets/fr_empathetic_dialogues/ \\\n",
        "#     --datatype train \\\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1JfjUux3lS7",
        "outputId": "4f5b1f40-15a5-4662-ed5e-bb94818836c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/dataset_french_ed/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/test.txt' -> '/content/dataset_french_ed/test.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/train.txt' -> '/content/dataset_french_ed/train.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/valid.txt' -> '/content/dataset_french_ed/valid.txt'\n",
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/agents.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/agents.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/build.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/build.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/__init__.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/__init__.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/LICENSE_DOCUMENTATION' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/LICENSE_DOCUMENTATION'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/README.md' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/README.md'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test/empathetic_dialogues_test.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test/empathetic_dialogues_test.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test/empathetic_dialogues_train.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test/empathetic_dialogues_train.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test/empathetic_dialogues_valid.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test/empathetic_dialogues_valid.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/worlds.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/worlds.py'\n"
          ]
        }
      ],
      "source": [
        "googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/\"\n",
        "data_path = \"/content/dataset_french_ed/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "!cp -rv $googledrive_data_path* $data_path\n",
        "\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_empathetic_dialogues/'\n",
        "!mkdir $task_path'french_empathetic_dialogues'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/* $task_path'french_empathetic_dialogues/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVBqI4-LQ_M7",
        "outputId": "bd673f20-492d-406c-cd30-2c2d21f575e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:Avez-vous subi \n",
            "1707224\n",
            "text:Je me souviens \n",
            "11562550\n",
            "text:Tu vas bien mai\n",
            "1785683\n"
          ]
        }
      ],
      "source": [
        "# convert to lowercase\n",
        "import glob\n",
        "txt_files = glob.glob(f'{data_path}*.txt')\n",
        "for file in txt_files:\n",
        "    with open(file, 'r') as f:\n",
        "        text = f.read()\n",
        "        print(text[:20])\n",
        "    with open(file, 'w') as f:\n",
        "        result = f.write(text.lower())\n",
        "        print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFKFf0IbXVlr"
      },
      "outputs": [],
      "source": [
        "!parlai display_data --task fromfile:parlaiformat --fromfile_datapath $data_path\"train.txt\" --verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG1ibjGYKNui"
      },
      "outputs": [],
      "source": [
        "# Build the dictionary \n",
        "!parlai build_dict --task fromfile:parlaiformat --fromfile_datapath $data_path\"train.txt\" --dict-file $data_path\"model.dict\" --dict_language french"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54aQM-2UPmOf"
      },
      "outputs": [],
      "source": [
        "!parlai display_data --task french_empathetic_dialogues \n",
        "# --datapath $data_path\n",
        "# fromfile:parlaiformat --fromfile_datapath $ed_file_path --fromfile-datatype-extension true "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuHsEs85PmM8"
      },
      "outputs": [],
      "source": [
        "# data_path  = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/fr_empathetic_dialogues/\"\n",
        "# !parlai build_dict --task fromfile:parlaiformat --fromfile_datapath $d_path\"train.txt\" \\\n",
        "#     --dict-file $d_path\"model.dict\" \\\n",
        "#     --dict_language french\n",
        "\n",
        "# !rm /content/drive/MyDrive/colabs/aliae-workspace/datasets/fr_empathetic_dialogues/models/blender/reddit_3B/Reddit3B_v0.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjurrxzVjnDF"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/train_400M-ed/\"\n",
        "# !rm -R $model_path\n",
        "!mkdir $model_path\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    task= \"french_empathetic_dialogues\",\n",
        "    model= \"transformer/generator\",\n",
        "    model_file= f\"{model_path}model\",\n",
        "    init_model= \"zoo:blender/blender_400Mdistill/model\",\n",
        "    dict_file= f\"zoo:blender/blender_400Mdistill/model.dict\",\n",
        "    # datapath= data_path,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    n_heads= 32, n_layers= 2, n_positions= 128, n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    dropout= 0.1, log_every_n_secs= 10,\n",
        "    # multitask_weights= \"1,3,3,3\",\n",
        "    attention_dropout= 0.0,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "\n",
        "    variant= \"prelayernorm\",\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    relu_dropout= 0.0, model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "    # save_after_valid= True,\n",
        "\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 2,\n",
        "    verbose = True,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    batchsize= 8, fp16= True, fp16_impl= \"mem_efficient\",\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\",\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    inference = 'topk', \n",
        "    temperature = 0.7, \n",
        "    topk=30, \n",
        "    beam_length_penalty=1.03\n",
        ")\n",
        "\n",
        "# TrainModel.main(\n",
        "#     # task='fromfile:parlaiformat', \n",
        "#     # fromfile_datapath='fr_finetuned',\n",
        "#     # fromfile_datatype_extension=True,\n",
        "\n",
        "#     # model= \"transformer/generator\",\n",
        "#     # model_file= f'{finetuned_model_path}/model',\n",
        "\n",
        "#     # initialize with a pretrained model\n",
        "#     # init_model= init_model,\n",
        "#     # dict_file= dict_file,\n",
        "\n",
        "#     # arguments we get from the pretrained model.\n",
        "#     n_heads= 32, n_layers= 2, n_positions= 128, n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "#     embedding_size= 1280, ffn_size= 5120,\n",
        "#     label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "#     dropout= 0.1, log_every_n_secs= 10,\n",
        "#     multitask_weights= \"1,3,3,3\",\n",
        "#     attention_dropout= 0.0,\n",
        "#     activation= \"gelu\",\n",
        "#     history_add_global_end_token= \"end\", \n",
        "#     delimiter= '  ', \n",
        "#     dict_tokenizer= \"bytelevelbpe\",\n",
        "\n",
        "#     variant= \"prelayernorm\",\n",
        "\n",
        "#     # some training arguments, specific to this fine-tuning\n",
        "#     optimizer= \"mem_eff_adam\",\n",
        "#     lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "#     relu_dropout= 0.0, model_parallel= False,\n",
        "#     warmup_updates= 100,\n",
        "#     update_freq= 2,\n",
        "#     gradient_clip= 0.1, \n",
        "#     # save_after_valid= True,\n",
        "\n",
        "#     validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "#     num_epochs = 5,\n",
        "#     verbose = True,\n",
        "    \n",
        "#     # depend on your gpu\n",
        "#     batchsize= 8, fp16= True, fp16_impl= \"mem_efficient\",\n",
        "\n",
        "#     # speeds up validation\n",
        "#     skip_generation= True,\n",
        "#     vp= 10,\n",
        "#     validation_metric= \"ppl\",\n",
        "#     validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "#     # customized parameters\n",
        "#     inference = 'topk', \n",
        "#     temperature = 0.7, \n",
        "#     topk=30, \n",
        "#     beam_length_penalty=1.03\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOd58oEFedis"
      },
      "outputs": [],
      "source": [
        "# !wget https://huggingface.co/facebook/blenderbot-90M/blob/main/merges.txt\n",
        "# !wget https://huggingface.co/facebook/blenderbot-90M/blob/main/vocab.json\n",
        "\n",
        "# !mkdir blenderbot-3B\n",
        "# !wget -P blenderbot-3B/ https://huggingface.co/facebook/blenderbot-3B/blob/main/merges.txt\n",
        "# !wget -P blenderbot-3B/ https://huggingface.co/facebook/blenderbot-3B/blob/main/vocab.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjg_qxlh1hyP"
      },
      "outputs": [],
      "source": [
        "!parlai display_data --task french_empathetic_dialogues --datapath $data_path\n",
        "# fromfile:parlaiformat --fromfile_datapath $ed_file_path --fromfile-datatype-extension true "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqi4HSAvNAKj"
      },
      "outputs": [],
      "source": [
        "# !rm /usr/local/lib/python3.7/dist-packages/data/models/msc/blender3B_1024/model_v0.1.tar.gz\n",
        "# !parlai display_model -mf zoo:msc/blender3B_1024/model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2A13EIruA-_G"
      },
      "outputs": [],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    model_file=f\"{model_path}model\",\n",
        "    dict_file= f\"{data_path}/model.dict\",\n",
        "    task= \"french_empathetic_dialogues\",\n",
        "    datapath= data_path,\n",
        "    skip_generation= False \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMS2Z7r1DBKm"
      },
      "outputs": [],
      "source": [
        "!parlai train_model help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEhPwssqnUeI"
      },
      "source": [
        "#### French XPersona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km7IjM8fn1Yr",
        "outputId": "4798d7e9-1cbe-49ce-8786-98d3e540075d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/dataset_french_xpersona/': No such file or directory\n",
            "Cloning into 'Xpersona'...\n",
            "remote: Enumerating objects: 282, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 282 (delta 5), reused 4 (delta 4), pack-reused 275\u001b[K\n",
            "Receiving objects: 100% (282/282), 45.00 MiB | 23.61 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n",
            "15776427\n",
            "257114\n",
            "262425\n"
          ]
        }
      ],
      "source": [
        "data_path = \"/content/dataset_french_xpersona/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "!git clone https://github.com/HLTCHKUST/Xpersona.git\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_train_corrected.json','r') as f:\n",
        "   train_data = json.load(f)\n",
        "\n",
        "dialogs_train = pd.DataFrame(train_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_valid_human_annotated.json','r') as f:\n",
        "   valid_data = json.load(f)\n",
        "\n",
        "dialogs_valid = pd.DataFrame(valid_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_test_human_annotated.json','r') as f:\n",
        "   text_data = json.load(f)\n",
        "\n",
        "dialogs_test = pd.DataFrame(text_data)['dialogue'].tolist()\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "text = \"\"\n",
        "def transfer_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, each_two_turn in enumerate(d):\n",
        "    u1 = each_two_turn[0]\n",
        "    u2 = each_two_turn[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "\n",
        "data_train = \"\"\n",
        "for d in dialogs_train:\n",
        "  data_train += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_train = open(f\"{data_path}train.txt\",\"w\")\n",
        "print(file_train.write(data_train))\n",
        "\n",
        "data_valid = \"\"\n",
        "for d in dialogs_valid:\n",
        "  data_valid += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_valid = open(f\"{data_path}valid.txt\",\"w\")\n",
        "print(file_valid.write(data_valid))\n",
        "\n",
        "data_test = \"\"\n",
        "for d in dialogs_test:\n",
        "  data_test += transfer_dialog(d)\n",
        "\n",
        "# save to file\n",
        "file_test = open(f\"{data_path}test.txt\",\"w\")\n",
        "print(file_test.write(data_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au-j3mf3noBA",
        "outputId": "bbc858d6-6414-41a1-f160-8a726f86e5b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/agents.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/agents.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/build.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/build.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/__init__.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/__init__.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/LICENSE_DOCUMENTATION' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/LICENSE_DOCUMENTATION'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/README.md' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/README.md'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test/empathetic_dialogues_test.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test/empathetic_dialogues_test.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test/empathetic_dialogues_train.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test/empathetic_dialogues_train.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test/empathetic_dialogues_valid.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test/empathetic_dialogues_valid.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/worlds.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/worlds.py'\n"
          ]
        }
      ],
      "source": [
        "# !rm -R $data_path\n",
        "# !mkdir $data_path\n",
        "# !cp -rv $googledrive_data_path* $data_path\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "# !ls /usr/local/lib/python3.7/dist-packages/parlai/tasks/\n",
        "# !cd /usr/local/lib/python3.7/dist-packages/parlai/tasks/\n",
        "!rm -R $task_path'french_xpersona/'\n",
        "!mkdir $task_path'french_xpersona'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/* $task_path'french_xpersona/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOAeoXPBqRs-"
      },
      "outputs": [],
      "source": [
        "!parlai display_data --task fromfile:parlaiformat --fromfile_datapath $data_path\"train.txt\" --verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifeg5NQHqSAu"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/train_400M-xpersona/\"\n",
        "# !rm -R $model_path\n",
        "!mkdir $model_path\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    task= \"french_xpersona\",\n",
        "    model= \"transformer/generator\",\n",
        "    model_file= f\"{model_path}model\",\n",
        "    init_model= \"zoo:blender/blender_400Mdistill/model\",\n",
        "    dict_file= f\"zoo:blender/blender_400Mdistill/model.dict\",\n",
        "    # datapath= data_path,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    n_heads= 32, n_layers= 2, n_positions= 128, n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    dropout= 0.1, log_every_n_secs= 10,\n",
        "    # multitask_weights= \"1,3,3,3\",\n",
        "    attention_dropout= 0.0,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "\n",
        "    variant= \"prelayernorm\",\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    relu_dropout= 0.0, model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "    # save_after_valid= True,\n",
        "\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 2,\n",
        "    verbose = True,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    batchsize= 8, fp16= True, fp16_impl= \"mem_efficient\",\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\",\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    inference = 'topk', \n",
        "    temperature = 0.7, \n",
        "    topk=30, \n",
        "    beam_length_penalty=1.03\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4OAiZworYOo"
      },
      "outputs": [],
      "source": [
        "!parlai display_data --task french_xpersona \n",
        "# --datapath $data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXKzmI7RrXyQ"
      },
      "outputs": [],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    model_file=f\"{model_path}model\",\n",
        "    task= \"french_xpersona\",\n",
        "    datapath= data_path,\n",
        "    skip_generation= False \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IET8vKuvI54h"
      },
      "source": [
        "#### French BST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAB6i9JnI8AX",
        "outputId": "6915cc85-cc78-450e-a435-2d19345cf427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/dataset_french_bst/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/test.txt' -> '/content/dataset_french_bst/test.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/train.txt' -> '/content/dataset_french_bst/train.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/valid.txt' -> '/content/dataset_french_bst/valid.txt'\n",
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/agents.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/agents.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/build.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/build.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/__init__.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/__init__.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/LICENSE_DOCUMENTATION' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/LICENSE_DOCUMENTATION'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/README.md' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/README.md'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test/empathetic_dialogues_test.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test/empathetic_dialogues_test.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test/empathetic_dialogues_train.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test/empathetic_dialogues_train.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test/empathetic_dialogues_valid.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test/empathetic_dialogues_valid.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/worlds.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/worlds.py'\n"
          ]
        }
      ],
      "source": [
        "googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/\"\n",
        "data_path = \"/content/dataset_french_bst/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "!cp -rv $googledrive_data_path* $data_path\n",
        "\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_blended_skill_talk/'\n",
        "!mkdir $task_path'french_blended_skill_talk'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/* $task_path'french_blended_skill_talk/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy-7AHNI5G9k",
        "outputId": "36c1f1ce-da9f-46f6-a364-5970ed27785c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset_french_bst/\n",
            "text:Wow, je ne suis\n",
            "2225052\n",
            "text:J'aime la musiq\n",
            "10591463\n",
            "text:Cela semble dan\n",
            "2236484\n"
          ]
        }
      ],
      "source": [
        "# convert to lowercase\n",
        "print(data_path)\n",
        "import glob\n",
        "txt_files = glob.glob(f'{data_path}*.txt')\n",
        "for file in txt_files:\n",
        "    with open(file, 'r') as f:\n",
        "        text = f.read()\n",
        "        print(text[:20])\n",
        "    with open(file, 'w') as f:\n",
        "        result = f.write(text.lower())\n",
        "        print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwGoxeps5wuJ"
      },
      "outputs": [],
      "source": [
        "!parlai display_data --task fromfile:parlaiformat --fromfile_datapath $data_path\"train.txt\" --verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6ceqmvY5amK"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/train_400M-bst/\"\n",
        "# !rm -R $model_path\n",
        "!mkdir $model_path\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    task= \"french_blended_skill_talk\",\n",
        "    model= \"transformer/generator\",\n",
        "    model_file= f\"{model_path}model\",\n",
        "    init_model= \"zoo:blender/blender_400Mdistill/model\",\n",
        "    dict_file= f\"zoo:blender/blender_400Mdistill/model.dict\",\n",
        "    datapath= data_path,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    n_heads= 32, n_layers= 2, n_positions= 128, n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    dropout= 0.1, log_every_n_secs= 10,\n",
        "    # multitask_weights= \"1,3,3,3\",\n",
        "    attention_dropout= 0.0,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "\n",
        "    variant= \"prelayernorm\",\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    relu_dropout= 0.0, model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "    # save_after_valid= True,\n",
        "\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 2,\n",
        "    verbose = True,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    batchsize= 8, fp16= True, fp16_impl= \"mem_efficient\",\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\",\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    inference = 'topk', \n",
        "    temperature = 0.7, \n",
        "    topk=30, \n",
        "    beam_length_penalty=1.03\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpxH1jt56GC-",
        "outputId": "9a21003c-bc5a-44c7-b11f-aa2d2146d86f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n",
            "15:59:42 | Opt:\n",
            "15:59:42 |     allow_missing_init_opts: False\n",
            "15:59:42 |     batchsize: 1\n",
            "15:59:42 |     datapath: /content/dataset_french_bst/\n",
            "15:59:42 |     datatype: train:ordered\n",
            "15:59:42 |     dict_class: None\n",
            "15:59:42 |     display_add_fields: \n",
            "15:59:42 |     download_path: None\n",
            "15:59:42 |     dynamic_batching: None\n",
            "15:59:42 |     hide_labels: False\n",
            "15:59:42 |     ignore_agent_reply: True\n",
            "15:59:42 |     image_cropsize: 224\n",
            "15:59:42 |     image_mode: raw\n",
            "15:59:42 |     image_size: 256\n",
            "15:59:42 |     init_model: None\n",
            "15:59:42 |     init_opt: None\n",
            "15:59:42 |     is_debug: False\n",
            "15:59:42 |     loglevel: info\n",
            "15:59:42 |     max_display_len: 1000\n",
            "15:59:42 |     model: None\n",
            "15:59:42 |     model_file: None\n",
            "15:59:42 |     multitask_weights: [1]\n",
            "15:59:42 |     mutators: None\n",
            "15:59:42 |     num_examples: 10\n",
            "15:59:42 |     override: \"{'task': 'french_blended_skill_talk', 'datapath': '/content/dataset_french_bst/'}\"\n",
            "15:59:42 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "15:59:42 |     starttime: Apr22_15-59\n",
            "15:59:42 |     task: french_blended_skill_talk\n",
            "15:59:42 |     verbose: False\n",
            "15:59:42 | creating task(s): french_blended_skill_talk\n",
            "15:59:42 | Loading ParlAI text data: /content/dataset_french_bst/train.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk - - -\u001b[0;0m\n",
            "\u001b[0mj'aime la musique live, c'est pourquoi j'essaie d'aller aux concerts.\u001b[0;0m\n",
            "   \u001b[1;94mmoi aussi. qu'est-ce que tu aimes ?\u001b[0;0m\n",
            "\u001b[0mj'aime jouer la comédie, j'espère être un acteur, et vous ?\u001b[0;0m\n",
            "   \u001b[1;94mc'est bon. vous avez des enfants ?\u001b[0;0m\n",
            "\u001b[0mnon, mais un jour.\u001b[0;0m\n",
            "   \u001b[1;94mc'est bien. j'ai 2\u001b[0;0m\n",
            "\u001b[0mlorsque j'aurai terminé mes études, je compte fonder une famille.\u001b[0;0m\n",
            "   \u001b[1;94mc'est génial ! tu seras prête\u001b[0;0m\n",
            "\u001b[0mje l'espère, quel âge ont vos enfants ?\u001b[0;0m\n",
            "   \u001b[1;94m5 & 7. ils me prennent beaucoup de temps.\u001b[0;0m\n",
            "\u001b[0mj'imagine. je suis sûr qu'ils sont de grands enfants.\u001b[0;0m\n",
            "   \u001b[1;94mheureusement, ils aiment les fleurs tout autant que moi. nous passons beaucoup de temps dans le jardin.\u001b[0;0m\n",
            "\u001b[0mj'aimerais avoir plus de temps pour faire ce genre de choses. l'école de médecine est épuisante. \u001b[0;0m\n",
            "   \u001b[1;94mon dirait bien. as-tu trouvé un travail d'actrice, cependant ?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk - - -\u001b[0;0m\n",
            "\u001b[0mmoi aussi. qu'est-ce que tu aimes ?\u001b[0;0m\n",
            "   \u001b[1;94mj'aime jouer la comédie, j'espère être un acteur, et vous ?\u001b[0;0m\n",
            "\u001b[0mc'est bon. vous avez des enfants ?\u001b[0;0m\n",
            "   \u001b[1;94mnon, mais un jour.\u001b[0;0m\n",
            "\u001b[0mc'est bien. j'ai 2\u001b[0;0m\n",
            "   \u001b[1;94mlorsque j'aurai terminé mes études, je compte fonder une famille.\u001b[0;0m\n",
            "15:59:43 | loaded 9638 episodes with a total of 59700 examples\n"
          ]
        }
      ],
      "source": [
        "!parlai display_data --task french_blended_skill_talk \n",
        "# --datapath $data_path\n",
        "# fromfile:parlaiformat --fromfile_datapath $ed_file_path --fromfile-datatype-extension true "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slJrbV8p539t",
        "outputId": "3ab0eb44-d353-4652-c5ed-c9a106ea41af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16:01:00 | \u001b[33mOverriding opt[\"dict_file\"] to /content/dataset_french_bst//model.dict (previously: /content/dataset_french_bst/models/blender/blender_400Mdistill/model.dict)\u001b[0m\n",
            "16:01:00 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "16:01:00 | Using CUDA\n",
            "16:01:00 | loading dictionary from /content/train_400M-bst/model.dict\n",
            "16:01:00 | num words = 8008\n",
            "16:01:05 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "16:01:05 | Loading existing model params from /content/train_400M-bst/model\n",
            "16:01:11 | creating task(s): french_blended_skill_talk\n",
            "16:01:11 | Loading ParlAI text data: /content/dataset_french_bst/valid.txt\n",
            "16:01:11 | Opt:\n",
            "16:01:11 |     activation: gelu\n",
            "16:01:11 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "16:01:11 |     adam_eps: 1e-08\n",
            "16:01:11 |     add_p1_after_newln: False\n",
            "16:01:11 |     aggregate_micro: False\n",
            "16:01:11 |     allow_missing_init_opts: False\n",
            "16:01:11 |     attention_dropout: 0.0\n",
            "16:01:11 |     batchsize: 8\n",
            "16:01:11 |     beam_block_full_context: True\n",
            "16:01:11 |     beam_block_list_filename: None\n",
            "16:01:11 |     beam_block_ngram: -1\n",
            "16:01:11 |     beam_context_block_ngram: -1\n",
            "16:01:11 |     beam_delay: 30\n",
            "16:01:11 |     beam_length_penalty: 1.03\n",
            "16:01:11 |     beam_min_length: 1\n",
            "16:01:11 |     beam_size: 1\n",
            "16:01:11 |     betas: '[0.9, 0.999]'\n",
            "16:01:11 |     bpe_add_prefix_space: None\n",
            "16:01:11 |     bpe_debug: False\n",
            "16:01:11 |     bpe_dropout: None\n",
            "16:01:11 |     bpe_merge: /content/train_400M-bst/model.dict-merges.txt\n",
            "16:01:11 |     bpe_vocab: /content/train_400M-bst/model.dict-vocab.json\n",
            "16:01:11 |     checkpoint_activations: False\n",
            "16:01:11 |     compute_tokenized_bleu: False\n",
            "16:01:11 |     datapath: /content/dataset_french_bst/\n",
            "16:01:11 |     datatype: train\n",
            "16:01:11 |     delimiter: '  '\n",
            "16:01:11 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "16:01:11 |     dict_endtoken: __end__\n",
            "16:01:11 |     dict_file: /content/train_400M-bst/model.dict\n",
            "16:01:11 |     dict_include_test: False\n",
            "16:01:11 |     dict_include_valid: False\n",
            "16:01:11 |     dict_initpath: None\n",
            "16:01:11 |     dict_language: english\n",
            "16:01:11 |     dict_loaded: True\n",
            "16:01:11 |     dict_lower: False\n",
            "16:01:11 |     dict_max_ngram_size: -1\n",
            "16:01:11 |     dict_maxexs: -1\n",
            "16:01:11 |     dict_maxtokens: -1\n",
            "16:01:11 |     dict_minfreq: 0\n",
            "16:01:11 |     dict_nulltoken: __null__\n",
            "16:01:11 |     dict_starttoken: __start__\n",
            "16:01:11 |     dict_textfields: text,labels\n",
            "16:01:11 |     dict_tokenizer: bytelevelbpe\n",
            "16:01:11 |     dict_unktoken: __unk__\n",
            "16:01:11 |     display_add_fields: \n",
            "16:01:11 |     display_examples: False\n",
            "16:01:11 |     download_path: None\n",
            "16:01:11 |     dropout: 0.1\n",
            "16:01:11 |     dynamic_batching: None\n",
            "16:01:11 |     embedding_projection: random\n",
            "16:01:11 |     embedding_size: 1280\n",
            "16:01:11 |     embedding_type: random\n",
            "16:01:11 |     embeddings_scale: True\n",
            "16:01:11 |     eval_batchsize: None\n",
            "16:01:11 |     eval_dynamic_batching: None\n",
            "16:01:11 |     evaltask: None\n",
            "16:01:11 |     ffn_size: 5120\n",
            "16:01:11 |     final_extra_opt: \n",
            "16:01:11 |     force_fp16_tokens: True\n",
            "16:01:11 |     fp16: True\n",
            "16:01:11 |     fp16_impl: mem_efficient\n",
            "16:01:11 |     gpu: -1\n",
            "16:01:11 |     gradient_clip: 0.1\n",
            "16:01:11 |     hide_labels: False\n",
            "16:01:11 |     history_add_global_end_token: end\n",
            "16:01:11 |     history_reversed: False\n",
            "16:01:11 |     history_size: -1\n",
            "16:01:11 |     image_cropsize: 224\n",
            "16:01:11 |     image_mode: raw\n",
            "16:01:11 |     image_size: 256\n",
            "16:01:11 |     inference: topk\n",
            "16:01:11 |     init_model: /content/dataset_french_bst/models/blender/blender_400Mdistill/model\n",
            "16:01:11 |     init_opt: None\n",
            "16:01:11 |     interactive_mode: False\n",
            "16:01:11 |     invsqrt_lr_decay_gamma: -1\n",
            "16:01:11 |     is_debug: False\n",
            "16:01:11 |     label_truncate: 128\n",
            "16:01:11 |     learn_positional_embeddings: False\n",
            "16:01:11 |     learningrate: 7e-06\n",
            "16:01:11 |     log_every_n_secs: 10.0\n",
            "16:01:11 |     log_every_n_steps: 50\n",
            "16:01:11 |     log_keep_fields: all\n",
            "16:01:11 |     loglevel: info\n",
            "16:01:11 |     lr_scheduler: reduceonplateau\n",
            "16:01:11 |     lr_scheduler_decay: 0.5\n",
            "16:01:11 |     lr_scheduler_patience: 3\n",
            "16:01:11 |     max_train_steps: -1\n",
            "16:01:11 |     max_train_time: -1\n",
            "16:01:11 |     metrics: default\n",
            "16:01:11 |     model: transformer/generator\n",
            "16:01:11 |     model_file: /content/train_400M-bst/model\n",
            "16:01:11 |     model_parallel: False\n",
            "16:01:11 |     momentum: 0\n",
            "16:01:11 |     multitask_weights: [1]\n",
            "16:01:11 |     mutators: None\n",
            "16:01:11 |     n_decoder_layers: 12\n",
            "16:01:11 |     n_encoder_layers: 2\n",
            "16:01:11 |     n_heads: 32\n",
            "16:01:11 |     n_layers: 2\n",
            "16:01:11 |     n_positions: 128\n",
            "16:01:11 |     n_segments: 0\n",
            "16:01:11 |     nesterov: True\n",
            "16:01:11 |     no_cuda: False\n",
            "16:01:11 |     num_epochs: 2.0\n",
            "16:01:11 |     num_examples: 10\n",
            "16:01:11 |     num_workers: 0\n",
            "16:01:11 |     nus: [0.7]\n",
            "16:01:11 |     optimizer: mem_eff_adam\n",
            "16:01:11 |     output_scaling: 1.0\n",
            "16:01:11 |     override: \"{'model_file': '/content/train_400M-bst/model', 'dict_file': '/content/dataset_french_bst//model.dict', 'task': 'french_blended_skill_talk', 'datapath': '/content/dataset_french_bst/', 'skip_generation': False}\"\n",
            "16:01:11 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "16:01:11 |     person_tokens: False\n",
            "16:01:11 |     rank_candidates: False\n",
            "16:01:11 |     relu_dropout: 0.0\n",
            "16:01:11 |     save_after_valid: False\n",
            "16:01:11 |     save_every_n_secs: -1\n",
            "16:01:11 |     save_format: conversations\n",
            "16:01:11 |     share_word_embeddings: True\n",
            "16:01:11 |     short_final_eval: False\n",
            "16:01:11 |     skip_generation: False\n",
            "16:01:11 |     special_tok_lst: None\n",
            "16:01:11 |     split_lines: False\n",
            "16:01:11 |     starttime: Apr22_15-46\n",
            "16:01:11 |     task: french_blended_skill_talk\n",
            "16:01:11 |     temperature: 0.7\n",
            "16:01:11 |     tensorboard_log: False\n",
            "16:01:11 |     tensorboard_logdir: None\n",
            "16:01:11 |     text_truncate: 128\n",
            "16:01:11 |     topk: 30\n",
            "16:01:11 |     topp: 0.9\n",
            "16:01:11 |     truncate: 128\n",
            "16:01:11 |     update_freq: 2\n",
            "16:01:11 |     use_reply: label\n",
            "16:01:11 |     validation_cutoff: 1.0\n",
            "16:01:11 |     validation_every_n_epochs: 0.25\n",
            "16:01:11 |     validation_every_n_secs: -1\n",
            "16:01:11 |     validation_every_n_steps: -1\n",
            "16:01:11 |     validation_max_exs: -1\n",
            "16:01:11 |     validation_metric: ppl\n",
            "16:01:11 |     validation_metric_mode: min\n",
            "16:01:11 |     validation_patience: 10\n",
            "16:01:11 |     validation_share_agent: False\n",
            "16:01:11 |     variant: prelayernorm\n",
            "16:01:11 |     verbose: False\n",
            "16:01:11 |     wandb_entity: None\n",
            "16:01:11 |     wandb_log: False\n",
            "16:01:11 |     wandb_name: None\n",
            "16:01:11 |     wandb_project: None\n",
            "16:01:11 |     warmup_rate: 0.0001\n",
            "16:01:11 |     warmup_updates: 100\n",
            "16:01:11 |     weight_decay: None\n",
            "16:01:11 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk- - -\u001b[0;0m\n",
            "\u001b[0mcela semble dangereux. cela vaut-il la peine de faire un travail aussi dangereux ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: wekk, c'est bon si vous êtes bien formé.  il y a trois niveaux \u001b[0;0m\n",
            "\u001b[0;95m     model: cela va-vous aussi dangereux, mais sont une çais, mais les les dansir.\u001b[0;0m\n",
            "\u001b[0ma quel niveau êtes-vous ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: j'ai reçu une formation sur le tas lorsque j'ai commencé à travailler.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui, je suis le mes dans le pas, je les dans niveaux.  à la rés l'appelle pour le cela.  je l'ai dans.\u001b[0;0m\n",
            "\u001b[0mc'est génial ! depuis combien de temps faites-vous ce travail ? \u001b[0;0m\n",
            "\u001b[1;94m    labels: depuis un bon nombre d'années maintenant.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne suis, je suis, les mes de cilantre que mais ?\u001b[0;0m\n",
            "\u001b[0mon dirait que ça peut être un travail dangereux parfois.\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est vrai, surtout si vous ne prenez pas les mesures appropriées.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui, j'ai oui. c'est avec dans'ils.\u001b[0;0m\n",
            "\u001b[0meh bien, vous êtes entré dans un métier qui vous garantira toujours du travail.\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est vrai, surtout dans cette économie\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis, je suis de jouer d'eness. c'est vous, je suis le très que vous étérés pas à les mesureuxes.\u001b[0;0m\n",
            "\u001b[0mj'ai moi-même un emploi assez stable, mais surtout du travail de bureau.\u001b[0;0m\n",
            "\u001b[1;94m    labels: est-ce que ça vous plaît ?\u001b[0;0m\n",
            "\u001b[0;95m     model: c'est bon j'ai mais. je ne dans le mesure.\u001b[0;0m\n",
            "\u001b[0mje le fais vraiment, mais parfois j'aimerais être plus actif et debout.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je vous comprends. s'étirer le long des couloirs\u001b[0;0m\n",
            "\u001b[0;95m     model: c'est une aussi à tout une.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk- - -\u001b[0;0m\n",
            "\u001b[0mwekk, c'est bon si vous êtes bien formé.  il y a trois niveaux \u001b[0;0m\n",
            "\u001b[1;94m    labels: a quel niveau êtes-vous ?\u001b[0;0m\n",
            "\u001b[0;95m     model: oui, vous.\u001b[0;0m\n",
            "\u001b[0mj'ai reçu une formation sur le tas lorsque j'ai commencé à travailler.\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est génial ! depuis combien de temps faites-vous ce travail ? \u001b[0;0m\n",
            "\u001b[0;95m     model: c'est une êtes de vous il ?\u001b[0;0m\n",
            "\u001b[0mdepuis un bon nombre d'années maintenant.\u001b[0;0m\n",
            "\u001b[1;94m    labels: on dirait que ça peut être un travail dangereux parfois.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui ! pas, vous foie en bon namére ?\u001b[0;0m\n"
          ]
        }
      ],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    model_file=f\"{model_path}model\",\n",
        "    dict_file= f\"{data_path}/model.dict\",\n",
        "    task= \"french_blended_skill_talk\",\n",
        "    datapath= data_path,\n",
        "    skip_generation= False \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Vhz_W7AmgJ"
      },
      "source": [
        "#### Multi-task finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxWG4g7gCLbf"
      },
      "outputs": [],
      "source": [
        "data_path_xpersona = \"/content/dataset_french_xpersona/\"\n",
        "data_path_ed = \"/content/dataset_french_ed/\"\n",
        "data_path_bst = \"/content/dataset_french_bst/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rfgTImWkE9a",
        "outputId": "b38403bf-495a-458a-8647-65a011d3ecaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-topk//model' -> '/content/finetuned-multitask-400m-topk/model'\n",
            "'/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-topk//model.dict' -> '/content/finetuned-multitask-400m-topk/model.dict'\n",
            "'/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-topk//model.dict-merges.txt' -> '/content/finetuned-multitask-400m-topk/model.dict-merges.txt'\n",
            "'/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-topk//model.dict.opt' -> '/content/finetuned-multitask-400m-topk/model.dict.opt'\n",
            "'/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-topk//model.dict-vocab.json' -> '/content/finetuned-multitask-400m-topk/model.dict-vocab.json'\n",
            "'/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-topk//model.opt' -> '/content/finetuned-multitask-400m-topk/model.opt'\n",
            "'/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-topk//model.trainstats' -> '/content/finetuned-multitask-400m-topk/model.trainstats'\n"
          ]
        }
      ],
      "source": [
        "# !parlai display_data --task french_xpersona\n",
        "# !parlai display_data --task french_empathetic_dialouges\n",
        "# !parlai display_data --task french_blended_skill_talk\n",
        "\n",
        "# drive_model_path = \"/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-topk/\"\n",
        "# local_model_path = \"/content/finetuned-multitask-400m-topk\"\n",
        "# # !mkdir $local_model_path\n",
        "# !cp -ruv $drive_model_path/* $local_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTZuNEF_1JFK",
        "outputId": "602229b6-d290-420f-95f1-d484563849f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ],
      "source": [
        "with open(f\"{data_path_bst}test.txt\") as f:\n",
        "    test_lines = f.readlines()\n",
        "\n",
        "count = len(test_lines)\n",
        "!parlai display_data --task fromfile:parlaiformat --fromfile_datapath $data_path_bst\"test.txt\" --verbose --num-examples $count > \"french_bst_test_set.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMyldrAzAlsj",
        "outputId": "3372ce9b-caea-4270-fb07-ec5402c307c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21:25:51 | building dictionary first...\n",
            "21:25:51 | \u001b[33mOverriding opt[\"multitask_weights\"] to (1.0, 3.0, 3.0) (previously: [1.0, 3.0, 3.0])\u001b[0m\n",
            "21:25:51 | \u001b[33mOverriding opt[\"init_model\"] to zoo:blender/blender_400Mdistill/model (previously: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model)\u001b[0m\n",
            "21:25:51 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,world_logs: ,save_format: conversations,log_keep_fields: all,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,checkpoint_activations: False,verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "21:25:51 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--init-opt /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt --allow-missing-init-opts True --task blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 8 --dynamic-batching full --eval-batchsize 8 --num-epochs -1 --validation-every-n-secs 900.0 --save-after-valid True --validation-every-n-epochs -1.0 --validation-patience 20 --distributed-world-size 8 --port 61337 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --chosen-topic-delimiter \n",
            " --num-topics 5 --train-experiencer-only False --remove-political-convos False --teacher-model /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model --task-loss-coeff 1.0 --encoder-loss-coeff 24.0 --hidden-loss-coeff 5.0 --pred-loss-coeff 8.0 --embedding-loss-coeff 0.35 --self-attn-loss-coeff 0.6 --enc-dec-attn-loss-coeff 3.0 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --inference beam --force-fp16-tokens False --learningrate 0.0004 --gpu 0 --bpe-vocab /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json --bpe-merge /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt --bpe-add-prefix-space True --max-lr-steps -1 --parlai-home /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI --show-advanced-args False --numthreads 1 --rank 0\u001b[0m\n",
            "21:25:51 | Using CUDA\n",
            "21:25:51 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model.dict\n",
            "21:25:51 | num words = 8008\n",
            "21:25:58 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "21:25:58 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model\n",
            "21:26:13 | Opt:\n",
            "21:26:13 |     activation: gelu\n",
            "21:26:13 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "21:26:13 |     adam_eps: 1e-08\n",
            "21:26:13 |     add_p1_after_newln: False\n",
            "21:26:13 |     aggregate_micro: False\n",
            "21:26:13 |     allow_missing_init_opts: False\n",
            "21:26:13 |     attention_dropout: 0.0\n",
            "21:26:13 |     batchsize: 32\n",
            "21:26:13 |     beam_block_full_context: True\n",
            "21:26:13 |     beam_block_list_filename: None\n",
            "21:26:13 |     beam_block_ngram: -1\n",
            "21:26:13 |     beam_context_block_ngram: -1\n",
            "21:26:13 |     beam_delay: 30\n",
            "21:26:13 |     beam_length_penalty: 0.65\n",
            "21:26:13 |     beam_min_length: 1\n",
            "21:26:13 |     beam_size: 1\n",
            "21:26:13 |     betas: '[0.9, 0.999]'\n",
            "21:26:13 |     bpe_add_prefix_space: None\n",
            "21:26:13 |     bpe_debug: False\n",
            "21:26:13 |     bpe_dropout: None\n",
            "21:26:13 |     bpe_merge: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model.dict-merges.txt\n",
            "21:26:13 |     bpe_vocab: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model.dict-vocab.json\n",
            "21:26:13 |     checkpoint_activations: False\n",
            "21:26:13 |     compute_tokenized_bleu: False\n",
            "21:26:13 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "21:26:13 |     datatype: train\n",
            "21:26:13 |     delimiter: '  '\n",
            "21:26:13 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "21:26:13 |     dict_endtoken: __end__\n",
            "21:26:13 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model.dict\n",
            "21:26:13 |     dict_include_test: False\n",
            "21:26:13 |     dict_include_valid: False\n",
            "21:26:13 |     dict_initpath: None\n",
            "21:26:13 |     dict_language: english\n",
            "21:26:13 |     dict_loaded: True\n",
            "21:26:13 |     dict_lower: False\n",
            "21:26:13 |     dict_max_ngram_size: -1\n",
            "21:26:13 |     dict_maxexs: -1\n",
            "21:26:13 |     dict_maxtokens: -1\n",
            "21:26:13 |     dict_minfreq: 0\n",
            "21:26:13 |     dict_nulltoken: __null__\n",
            "21:26:13 |     dict_starttoken: __start__\n",
            "21:26:13 |     dict_textfields: text,labels\n",
            "21:26:13 |     dict_tokenizer: bytelevelbpe\n",
            "21:26:13 |     dict_unktoken: __unk__\n",
            "21:26:13 |     display_examples: False\n",
            "21:26:13 |     download_path: None\n",
            "21:26:13 |     dropout: 0.1\n",
            "21:26:13 |     dynamic_batching: None\n",
            "21:26:13 |     embedding_projection: random\n",
            "21:26:13 |     embedding_size: 1280\n",
            "21:26:13 |     embedding_type: random\n",
            "21:26:13 |     embeddings_scale: True\n",
            "21:26:13 |     eval_batchsize: None\n",
            "21:26:13 |     eval_dynamic_batching: None\n",
            "21:26:13 |     evaltask: None\n",
            "21:26:13 |     ffn_size: 5120\n",
            "21:26:13 |     final_extra_opt: \n",
            "21:26:13 |     force_fp16_tokens: True\n",
            "21:26:13 |     fp16: True\n",
            "21:26:13 |     fp16_impl: mem_efficient\n",
            "21:26:13 |     gpu: -1\n",
            "21:26:13 |     gradient_clip: 0.1\n",
            "21:26:13 |     hide_labels: False\n",
            "21:26:13 |     history_add_global_end_token: end\n",
            "21:26:13 |     history_reversed: False\n",
            "21:26:13 |     history_size: -1\n",
            "21:26:13 |     image_cropsize: 224\n",
            "21:26:13 |     image_mode: raw\n",
            "21:26:13 |     image_size: 256\n",
            "21:26:13 |     inference: greedy\n",
            "21:26:13 |     init_model: zoo:blender/blender_400Mdistill/model\n",
            "21:26:13 |     init_opt: None\n",
            "21:26:13 |     interactive_mode: False\n",
            "21:26:13 |     invsqrt_lr_decay_gamma: -1\n",
            "21:26:13 |     is_debug: False\n",
            "21:26:13 |     label_truncate: 128\n",
            "21:26:13 |     learn_positional_embeddings: False\n",
            "21:26:13 |     learningrate: 7e-06\n",
            "21:26:13 |     load_from_checkpoint: True\n",
            "21:26:13 |     log_every_n_secs: 10.0\n",
            "21:26:13 |     log_every_n_steps: 50\n",
            "21:26:13 |     log_keep_fields: all\n",
            "21:26:13 |     loglevel: info\n",
            "21:26:13 |     lr_scheduler: reduceonplateau\n",
            "21:26:13 |     lr_scheduler_decay: 0.5\n",
            "21:26:13 |     lr_scheduler_patience: 3\n",
            "21:26:13 |     max_train_steps: -1\n",
            "21:26:13 |     max_train_time: -1\n",
            "21:26:13 |     metrics: default\n",
            "21:26:13 |     model: transformer/generator\n",
            "21:26:13 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model\n",
            "21:26:13 |     model_parallel: False\n",
            "21:26:13 |     momentum: 0\n",
            "21:26:13 |     multitask_weights: '(1.0, 3.0, 3.0)'\n",
            "21:26:13 |     mutators: None\n",
            "21:26:13 |     n_decoder_layers: 12\n",
            "21:26:13 |     n_encoder_layers: 2\n",
            "21:26:13 |     n_heads: 32\n",
            "21:26:13 |     n_layers: 2\n",
            "21:26:13 |     n_positions: 128\n",
            "21:26:13 |     n_segments: 0\n",
            "21:26:13 |     nesterov: True\n",
            "21:26:13 |     no_cuda: False\n",
            "21:26:13 |     num_epochs: 5.0\n",
            "21:26:13 |     num_workers: 0\n",
            "21:26:13 |     nus: [0.7]\n",
            "21:26:13 |     optimizer: mem_eff_adam\n",
            "21:26:13 |     output_scaling: 1.0\n",
            "21:26:13 |     override: \"{'task': 'french_blended_skill_talk,french_xpersona,french_empathetic_dialogues', 'multitask_weights': (1.0, 3.0, 3.0), 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model', 'init_model': 'zoo:blender/blender_400Mdistill/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict', 'n_heads': 32, 'n_layers': 2, 'n_positions': 128, 'n_encoder_layers': 2, 'n_decoder_layers': 12, 'embedding_size': 1280, 'ffn_size': 5120, 'label_truncate': 128, 'text_truncate': 128, 'truncate': 128, 'dropout': 0.1, 'log_every_n_secs': 10.0, 'attention_dropout': 0.0, 'activation': 'gelu', 'history_add_global_end_token': 'end', 'delimiter': '  ', 'dict_tokenizer': 'bytelevelbpe', 'variant': 'prelayernorm', 'optimizer': 'mem_eff_adam', 'learningrate': 7e-06, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'relu_dropout': 0.0, 'model_parallel': False, 'warmup_updates': 100, 'update_freq': 2, 'gradient_clip': 0.1, 'validation_every_n_epochs': 0.25, 'num_epochs': 5.0, 'verbose': True, 'batchsize': 32, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'validation_patience': 10, 'validation_metric': 'ppl', 'validation_metric_mode': 'min'}\"\n",
            "21:26:13 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "21:26:13 |     person_tokens: False\n",
            "21:26:13 |     rank_candidates: False\n",
            "21:26:13 |     relu_dropout: 0.0\n",
            "21:26:13 |     save_after_valid: False\n",
            "21:26:13 |     save_every_n_secs: -1\n",
            "21:26:13 |     save_format: conversations\n",
            "21:26:13 |     share_word_embeddings: True\n",
            "21:26:13 |     short_final_eval: False\n",
            "21:26:13 |     skip_generation: True\n",
            "21:26:13 |     special_tok_lst: None\n",
            "21:26:13 |     split_lines: False\n",
            "21:26:13 |     starttime: Apr27_08-14\n",
            "21:26:13 |     task: french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\n",
            "21:26:13 |     temperature: 1.0\n",
            "21:26:13 |     tensorboard_log: False\n",
            "21:26:13 |     tensorboard_logdir: None\n",
            "21:26:13 |     text_truncate: 128\n",
            "21:26:13 |     topk: 10\n",
            "21:26:13 |     topp: 0.9\n",
            "21:26:13 |     truncate: 128\n",
            "21:26:13 |     update_freq: 2\n",
            "21:26:13 |     use_reply: label\n",
            "21:26:13 |     validation_cutoff: 1.0\n",
            "21:26:13 |     validation_every_n_epochs: 0.25\n",
            "21:26:13 |     validation_every_n_secs: -1\n",
            "21:26:13 |     validation_every_n_steps: -1\n",
            "21:26:13 |     validation_max_exs: -1\n",
            "21:26:13 |     validation_metric: ppl\n",
            "21:26:13 |     validation_metric_mode: min\n",
            "21:26:13 |     validation_patience: 10\n",
            "21:26:13 |     validation_share_agent: False\n",
            "21:26:13 |     variant: prelayernorm\n",
            "21:26:13 |     verbose: True\n",
            "21:26:13 |     wandb_entity: None\n",
            "21:26:13 |     wandb_log: False\n",
            "21:26:13 |     wandb_name: None\n",
            "21:26:13 |     wandb_project: None\n",
            "21:26:13 |     warmup_rate: 0.0001\n",
            "21:26:13 |     warmup_updates: 100\n",
            "21:26:13 |     weight_decay: None\n",
            "21:26:13 |     world_logs: \n",
            "21:26:14 | creating task(s): french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\n",
            "21:26:14 | Loading ParlAI text data: /content/dataset_french_bst/train.txt\n",
            "21:26:15 | Loading ParlAI text data: /content/dataset_french_xpersona/train.txt\n",
            "21:26:16 | Loading ParlAI text data: /content/dataset_french_ed/train.txt\n",
            "21:26:17 | training...\n",
            "21:26:18 | time:44512s total_exs:1242336 total_steps:19411 epochs:5.00 time_left:0s\n",
            "                                clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   all                         31.93   879  2155       0          0 78.42   32    .6435    26 2.314 6.93e-06   806  1976   \n",
            "   french_blended_skill_talk    47.5                   0          0          6          28.83 2.321                        \n",
            "   french_empathetic_dialogues 32.55                   0          0         11          26.09 2.742                        \n",
            "   french_xpersona             15.73                   0          0         15          23.07 1.881                        \n",
            "                                ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   all                               0          0 10.75      .4982         0                18449 1685 4132    0  \n",
            "   french_blended_skill_talk         0          0 10.18      .4798         0                                      \n",
            "   french_empathetic_dialogues       0          0 15.51      .4425         0                                      \n",
            "   french_xpersona                   0          0 6.561      .5723         0\n",
            "\n",
            "21:26:18 | num_epochs completed:5.0 time elapsed:44511.70118331909s\n",
            "21:26:35 | \u001b[33mOverriding opt[\"multitask_weights\"] to (1.0, 3.0, 3.0) (previously: [1.0, 3.0, 3.0])\u001b[0m\n",
            "21:26:35 | \u001b[33mOverriding opt[\"dict_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict (previously: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model.dict)\u001b[0m\n",
            "21:26:35 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,world_logs: ,save_format: conversations,log_keep_fields: all,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,checkpoint_activations: False,verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "21:26:35 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--init-opt /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt --allow-missing-init-opts True --task blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 8 --dynamic-batching full --eval-batchsize 8 --num-epochs -1 --validation-every-n-secs 900.0 --save-after-valid True --validation-every-n-epochs -1.0 --validation-patience 20 --distributed-world-size 8 --port 61337 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --chosen-topic-delimiter \n",
            " --num-topics 5 --train-experiencer-only False --remove-political-convos False --teacher-model /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model --task-loss-coeff 1.0 --encoder-loss-coeff 24.0 --hidden-loss-coeff 5.0 --pred-loss-coeff 8.0 --embedding-loss-coeff 0.35 --self-attn-loss-coeff 0.6 --enc-dec-attn-loss-coeff 3.0 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --inference beam --force-fp16-tokens False --learningrate 0.0004 --gpu 0 --bpe-vocab /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json --bpe-merge /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt --bpe-add-prefix-space True --max-lr-steps -1 --parlai-home /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI --show-advanced-args False --numthreads 1 --rank 0\u001b[0m\n",
            "21:26:35 | Using CUDA\n",
            "21:26:35 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model.dict\n",
            "21:26:35 | num words = 8008\n",
            "21:26:43 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "21:26:43 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model\n",
            "21:27:03 | creating task(s): french_blended_skill_talk\n",
            "21:27:03 | Loading ParlAI text data: /content/dataset_french_bst/valid.txt\n",
            "21:27:04 | creating task(s): french_xpersona\n",
            "21:27:04 | Loading ParlAI text data: /content/dataset_french_xpersona/valid.txt\n",
            "21:27:04 | creating task(s): french_empathetic_dialogues\n",
            "21:27:04 | Loading ParlAI text data: /content/dataset_french_ed/valid.txt\n",
            "21:27:04 | running eval: valid\n",
            "21:31:42 | eval completed in 278.09s\n",
            "21:31:42 | \u001b[1mvalid:\n",
            "                                clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   all                         178.9  3442  9364   .5210      84.95 86.16 23718    .4361 33.84 2.374 6.93e-06  1174  3193   \n",
            "   french_blended_skill_talk     245               .7019      136.3       12476          37.07 2.404                        \n",
            "   french_empathetic_dialogues 74.47               .1722      8.068        9308          37.12 2.419                        \n",
            "   french_xpersona             217.2               .6887      110.5        1934          27.33 2.298                        \n",
            "                                 ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  \n",
            "   all                          .001441      .0447 10.75      .4932  .0002082                18449 4615 12558  \n",
            "   french_blended_skill_talk   .0002405    .003687 11.06      .4895         0                                  \n",
            "   french_empathetic_dialogues  .004083      .1304 11.24      .4799  .0001074                                  \n",
            "   french_xpersona                    0          0 9.956      .5101  .0005171\n",
            "\u001b[0m\n",
            "21:31:42 | creating task(s): french_blended_skill_talk\n",
            "21:31:42 | Loading ParlAI text data: /content/dataset_french_bst/test.txt\n",
            "21:31:42 | creating task(s): french_xpersona\n",
            "21:31:42 | Loading ParlAI text data: /content/dataset_french_xpersona/test.txt\n",
            "21:31:43 | creating task(s): french_empathetic_dialogues\n",
            "21:31:43 | Loading ParlAI text data: /content/dataset_french_ed/test.txt\n",
            "21:31:43 | running eval: test\n",
            "21:36:09 | eval completed in 266.56s\n",
            "21:36:09 | \u001b[1mtest:\n",
            "                                clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   all                         182.5  3442  9381   .5275      88.05 86.48 22497    .4362 34.86   2.4 6.93e-06  1208  3292   \n",
            "   french_blended_skill_talk   249.3               .6999      140.8       12121          38.08  2.41                        \n",
            "   french_empathetic_dialogues  78.6               .1924      10.43        8426          38.97 2.469                        \n",
            "   french_xpersona             219.7               .6903      112.9        1950          27.55  2.32                        \n",
            "                                 ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  \n",
            "   all                          .001863     .05611 11.04      .4882         0                18449 4650 12672  \n",
            "   french_blended_skill_talk   .0002475      .0132 11.14      .4874         0                                  \n",
            "   french_empathetic_dialogues  .005341      .1551 11.81      .4729         0                                  \n",
            "   french_xpersona                    0          0 10.18      .5043         0\n",
            "\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'clen': MacroAverageMetric(178.9),\n",
              "  'ctpb': GlobalAverageMetric(3442),\n",
              "  'ctps': GlobalTimerMetric(9364),\n",
              "  'ctrunc': MacroAverageMetric(0.521),\n",
              "  'ctrunclen': MacroAverageMetric(84.95),\n",
              "  'exps': GlobalTimerMetric(86.16),\n",
              "  'exs': SumMetric(2.372e+04),\n",
              "  'french_blended_skill_talk/clen': AverageMetric(245),\n",
              "  'french_blended_skill_talk/ctrunc': AverageMetric(0.7019),\n",
              "  'french_blended_skill_talk/ctrunclen': AverageMetric(136.3),\n",
              "  'french_blended_skill_talk/exs': SumMetric(1.248e+04),\n",
              "  'french_blended_skill_talk/llen': AverageMetric(37.07),\n",
              "  'french_blended_skill_talk/loss': AverageMetric(2.404),\n",
              "  'french_blended_skill_talk/ltrunc': AverageMetric(0.0002405),\n",
              "  'french_blended_skill_talk/ltrunclen': AverageMetric(0.003687),\n",
              "  'french_blended_skill_talk/ppl': PPLMetric(11.06),\n",
              "  'french_blended_skill_talk/token_acc': AverageMetric(0.4895),\n",
              "  'french_blended_skill_talk/token_em': AverageMetric(0),\n",
              "  'french_empathetic_dialogues/clen': AverageMetric(74.47),\n",
              "  'french_empathetic_dialogues/ctrunc': AverageMetric(0.1722),\n",
              "  'french_empathetic_dialogues/ctrunclen': AverageMetric(8.068),\n",
              "  'french_empathetic_dialogues/exs': SumMetric(9308),\n",
              "  'french_empathetic_dialogues/llen': AverageMetric(37.12),\n",
              "  'french_empathetic_dialogues/loss': AverageMetric(2.419),\n",
              "  'french_empathetic_dialogues/ltrunc': AverageMetric(0.004083),\n",
              "  'french_empathetic_dialogues/ltrunclen': AverageMetric(0.1304),\n",
              "  'french_empathetic_dialogues/ppl': PPLMetric(11.24),\n",
              "  'french_empathetic_dialogues/token_acc': AverageMetric(0.4799),\n",
              "  'french_empathetic_dialogues/token_em': AverageMetric(0.0001074),\n",
              "  'french_xpersona/clen': AverageMetric(217.2),\n",
              "  'french_xpersona/ctrunc': AverageMetric(0.6887),\n",
              "  'french_xpersona/ctrunclen': AverageMetric(110.5),\n",
              "  'french_xpersona/exs': SumMetric(1934),\n",
              "  'french_xpersona/llen': AverageMetric(27.33),\n",
              "  'french_xpersona/loss': AverageMetric(2.298),\n",
              "  'french_xpersona/ltrunc': AverageMetric(0),\n",
              "  'french_xpersona/ltrunclen': AverageMetric(0),\n",
              "  'french_xpersona/ppl': PPLMetric(9.956),\n",
              "  'french_xpersona/token_acc': AverageMetric(0.5101),\n",
              "  'french_xpersona/token_em': AverageMetric(0.0005171),\n",
              "  'gpu_mem': GlobalAverageMetric(0.4361),\n",
              "  'llen': MacroAverageMetric(33.84),\n",
              "  'loss': MacroAverageMetric(2.374),\n",
              "  'lr': GlobalAverageMetric(6.93e-06),\n",
              "  'ltpb': GlobalAverageMetric(1174),\n",
              "  'ltps': GlobalTimerMetric(3193),\n",
              "  'ltrunc': MacroAverageMetric(0.001441),\n",
              "  'ltrunclen': MacroAverageMetric(0.0447),\n",
              "  'ppl': MacroAverageMetric(10.75),\n",
              "  'token_acc': MacroAverageMetric(0.4932),\n",
              "  'token_em': MacroAverageMetric(0.0002082),\n",
              "  'total_train_updates': GlobalFixedMetric(1.845e+04),\n",
              "  'tpb': GlobalAverageMetric(4615),\n",
              "  'tps': GlobalTimerMetric(1.256e+04)},\n",
              " {'clen': MacroAverageMetric(182.5),\n",
              "  'ctpb': GlobalAverageMetric(3442),\n",
              "  'ctps': GlobalTimerMetric(9381),\n",
              "  'ctrunc': MacroAverageMetric(0.5275),\n",
              "  'ctrunclen': MacroAverageMetric(88.05),\n",
              "  'exps': GlobalTimerMetric(86.48),\n",
              "  'exs': SumMetric(2.25e+04),\n",
              "  'french_blended_skill_talk/clen': AverageMetric(249.3),\n",
              "  'french_blended_skill_talk/ctrunc': AverageMetric(0.6999),\n",
              "  'french_blended_skill_talk/ctrunclen': AverageMetric(140.8),\n",
              "  'french_blended_skill_talk/exs': SumMetric(1.212e+04),\n",
              "  'french_blended_skill_talk/llen': AverageMetric(38.08),\n",
              "  'french_blended_skill_talk/loss': AverageMetric(2.41),\n",
              "  'french_blended_skill_talk/ltrunc': AverageMetric(0.0002475),\n",
              "  'french_blended_skill_talk/ltrunclen': AverageMetric(0.0132),\n",
              "  'french_blended_skill_talk/ppl': PPLMetric(11.14),\n",
              "  'french_blended_skill_talk/token_acc': AverageMetric(0.4874),\n",
              "  'french_blended_skill_talk/token_em': AverageMetric(0),\n",
              "  'french_empathetic_dialogues/clen': AverageMetric(78.6),\n",
              "  'french_empathetic_dialogues/ctrunc': AverageMetric(0.1924),\n",
              "  'french_empathetic_dialogues/ctrunclen': AverageMetric(10.43),\n",
              "  'french_empathetic_dialogues/exs': SumMetric(8426),\n",
              "  'french_empathetic_dialogues/llen': AverageMetric(38.97),\n",
              "  'french_empathetic_dialogues/loss': AverageMetric(2.469),\n",
              "  'french_empathetic_dialogues/ltrunc': AverageMetric(0.005341),\n",
              "  'french_empathetic_dialogues/ltrunclen': AverageMetric(0.1551),\n",
              "  'french_empathetic_dialogues/ppl': PPLMetric(11.81),\n",
              "  'french_empathetic_dialogues/token_acc': AverageMetric(0.4729),\n",
              "  'french_empathetic_dialogues/token_em': AverageMetric(0),\n",
              "  'french_xpersona/clen': AverageMetric(219.7),\n",
              "  'french_xpersona/ctrunc': AverageMetric(0.6903),\n",
              "  'french_xpersona/ctrunclen': AverageMetric(112.9),\n",
              "  'french_xpersona/exs': SumMetric(1950),\n",
              "  'french_xpersona/llen': AverageMetric(27.55),\n",
              "  'french_xpersona/loss': AverageMetric(2.32),\n",
              "  'french_xpersona/ltrunc': AverageMetric(0),\n",
              "  'french_xpersona/ltrunclen': AverageMetric(0),\n",
              "  'french_xpersona/ppl': PPLMetric(10.18),\n",
              "  'french_xpersona/token_acc': AverageMetric(0.5043),\n",
              "  'french_xpersona/token_em': AverageMetric(0),\n",
              "  'gpu_mem': GlobalAverageMetric(0.4362),\n",
              "  'llen': MacroAverageMetric(34.86),\n",
              "  'loss': MacroAverageMetric(2.4),\n",
              "  'lr': GlobalAverageMetric(6.93e-06),\n",
              "  'ltpb': GlobalAverageMetric(1208),\n",
              "  'ltps': GlobalTimerMetric(3292),\n",
              "  'ltrunc': MacroAverageMetric(0.001863),\n",
              "  'ltrunclen': MacroAverageMetric(0.05611),\n",
              "  'ppl': MacroAverageMetric(11.04),\n",
              "  'token_acc': MacroAverageMetric(0.4882),\n",
              "  'token_em': MacroAverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(1.845e+04),\n",
              "  'tpb': GlobalAverageMetric(4650),\n",
              "  'tps': GlobalTimerMetric(1.267e+04)})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model_path = \"/content/finetuned-multitask-400m-topk/\"\n",
        "model_path = \"/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/\"\n",
        "# !rm -R $model_path\n",
        "# !mkdir $model_path\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "    multitask_weights= \"1,3,3\",\n",
        "\n",
        "    model= \"transformer/generator\",\n",
        "    model_file= f\"{model_path}model\",\n",
        "    init_model= \"zoo:blender/blender_400Mdistill/model\",\n",
        "    dict_file= f\"zoo:blender/blender_400Mdistill/model.dict\",\n",
        "    # datapath= data_path,\n",
        "\n",
        "    # depend on your gpu\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 3,\n",
        "    log_every_n_secs= 60,\n",
        "    verbose = True,\n",
        "    attention_dropout= 0.0, \n",
        "    batchsize= 32, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    \n",
        "    # arguments we get from the pretrained model. \"from recipes page for 2.7B model\" \n",
        "    embedding_size= 2560, ffn_size= 10240,\n",
        "    variant= \"prelayernorm\",\n",
        "    n_heads= 32, n_positions= 128, \n",
        "    n_encoder_layers= 2, n_decoder_layers= 24,\n",
        "    \n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "    dropout= 0.1,\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    optimizer= \"adam\",\n",
        "    relu_dropout= 0.0, \n",
        "    model_parallel= True,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "    # save_after_valid= True,\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    # inference = 'topk', \n",
        "    # temperature = 0.7, \n",
        "    # topk=30, \n",
        "    # beam_length_penalty=1.03\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "nu_En98q00T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbfBss3Q96Pr",
        "outputId": "e3503d61-485e-4674-d6d2-902d7c3c425b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m//model\n"
          ]
        }
      ],
      "source": [
        "model_path = \"/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/\"\n",
        "print(f'{model_path}/model')\n",
        "from parlai.scripts.display_model import DisplayModel\n",
        "\n",
        "# class MyClass:\n",
        "#     task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "#     model_file= f\"{model_path}model\",\n",
        "#     datatype= \"test\",\n",
        "#     num_examples=20,\n",
        "#     skip_generation=False\n",
        "\n",
        "# p1 = MyClass()\n",
        "\n",
        "# DisplayModel.main(\n",
        "#     \"--task\", \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "#     \"--model-file\", f\"{model_path}model\",\n",
        "#     \"--datatype\", \"test\",\n",
        "#     \"--num-examples\", \"20\",\n",
        "#     \"--skip-generation\", \"False\"\n",
        "# )\n",
        "\n",
        "\n",
        "# dict_param = {\n",
        "#     \"task\": \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "#     \"model_file\": f\"{model_path}model\",\n",
        "#     \"datatype\": \"test\",\n",
        "#     \"num_examples\": 20,\n",
        "#     \"skip_generation\" : False\n",
        "# }\n",
        "# display(**dict_param)\n",
        "\n",
        "DisplayModel.main(\n",
        "    task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "    # model= \"transformer/generator\",\n",
        "    model_file= f\"{model_path}model\",\n",
        "    # init_model= \"zoo:blender/blender_400Mdistill/model\",\n",
        "    # dict_file= f\"zoo:blender/blender_400Mdistill/model.dict\",\n",
        "\n",
        "    datatype= \"test\",\n",
        "    # fromfile_datatype_extension= True,\n",
        "\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    beam_size = 1,\n",
        "    inference = 'topk', # Generation algorithm. Choices: beam, topk, greedy, delayedbeam, nucleus\n",
        "    topp= 0.3,\n",
        "    # temperature = 0.7, # Temperature to add during decoding. Default 1.0\n",
        "    # beam_length_penalty=0.8 # Applies a length penalty. Set to 0 for no penalty. Default: 0.65.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "grid = [\n",
        "    # {\n",
        "    #     'task': ['french_blended_skill_talk'], \n",
        "    #     'model_file': [f'{model_path}model'],\n",
        "    #     'datatype': ['valid'],\n",
        "    #     'num_examples' : [20],\n",
        "    #     'skip_generation': [False],\n",
        "    #     'report_filename' : ['report.json'],\n",
        "    #     'world_logs' : ['world-log.json'],\n",
        "    #     'display_examples' : [True],\n",
        "\n",
        "    #     'beam_context_block_ngram': [2,3],\n",
        "    #     'beam_block_ngram': [2,3],\n",
        "    #     'beam_min_length': [10, 20, 30],\n",
        "    #     'beam_size': [10, 20],\n",
        "    #     'beam_length_penalty': [0.7, 0.8, 1],\n",
        "    #     'topp': [0.3, 0.9],\n",
        "     \n",
        "    #     'inference': ['beam', 'greedy'],\n",
        "    #     'temperature': [0.5, 0.8],\n",
        "    # },\n",
        "    {\n",
        "        'task': ['french_blended_skill_talk'], \n",
        "        'model_file': [f'{model_path}model'],\n",
        "        'datatype': ['valid'],\n",
        "        'num_examples' : [100],\n",
        "        'skip_generation': [False],\n",
        "\n",
        "        'beam_context_block_ngram': [2,3],\n",
        "        'beam_block_ngram': [2,3],\n",
        "        'beam_min_length': [10, 20, 30],\n",
        "        'beam_size': [10, 20],\n",
        "        'beam_length_penalty': [0.7, 0.8, 1],\n",
        "        'topp': [0.3, 0.9],\n",
        "     \n",
        "        'inference': ['topk'],   \n",
        "        'temperature': [0.5, 0.8],\n",
        "        'topk': [10,20, 30]\n",
        "    }\n",
        "]\n",
        "print(len(list( ParameterGrid(grid))))\n",
        "\n",
        "from parlai.scripts.display_model import DisplayModel\n",
        "from parlai.scripts.eval_model import EvalModel\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for param in list(ParameterGrid(grid)):\n",
        "    print(param)\n",
        "    // #     'world_logs' : ['world-log.json'],\n",
        "    #     'display_examples' : [True],\n",
        "    result = EvalModel.main(**param)\n",
        "    print(round(time.time() - start_time, 2), \"\\n\")\n",
        "    print(result)\n",
        "    # break"
      ],
      "metadata": {
        "id": "9xqdNcjdmlw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4276d05c-12b2-4a95-c8d2-5f70882b96eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "864\n",
            "{'beam_block_ngram': 2, 'beam_context_block_ngram': 2, 'beam_length_penalty': 0.7, 'beam_min_length': 10, 'beam_size': 10, 'datatype': 'valid', 'inference': 'topk', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model', 'num_examples': 100, 'skip_generation': False, 'task': 'french_blended_skill_talk', 'temperature': 0.5, 'topk': 10, 'topp': 0.3}\n",
            "15:12:50 | \u001b[33mOverriding opt[\"datatype\"] to valid (previously: train)\u001b[0m\n",
            "15:12:50 | \u001b[33mOverriding opt[\"beam_block_ngram\"] to 2 (previously: -1)\u001b[0m\n",
            "15:12:50 | \u001b[33mOverriding opt[\"beam_context_block_ngram\"] to 2 (previously: -1)\u001b[0m\n",
            "15:12:50 | \u001b[33mOverriding opt[\"beam_length_penalty\"] to 0.7 (previously: 0.65)\u001b[0m\n",
            "15:12:50 | \u001b[33mOverriding opt[\"beam_min_length\"] to 10 (previously: 1)\u001b[0m\n",
            "15:12:50 | \u001b[33mOverriding opt[\"beam_size\"] to 10 (previously: 1)\u001b[0m\n",
            "15:12:50 | \u001b[33mOverriding opt[\"inference\"] to topk (previously: greedy)\u001b[0m\n",
            "15:12:50 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "15:12:50 | \u001b[33mOverriding opt[\"task\"] to french_blended_skill_talk (previously: french_blended_skill_talk,french_xpersona,french_empathetic_dialogues)\u001b[0m\n",
            "15:12:50 | \u001b[33mOverriding opt[\"temperature\"] to 0.5 (previously: 1.0)\u001b[0m\n",
            "15:12:50 | \u001b[33mOverriding opt[\"topp\"] to 0.3 (previously: 0.9)\u001b[0m\n",
            "15:12:50 | Using CUDA\n",
            "15:12:50 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model.dict\n",
            "15:12:50 | num words = 8008\n",
            "15:12:56 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "15:12:56 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model\n",
            "15:13:00 | Opt:\n",
            "15:13:00 |     activation: gelu\n",
            "15:13:00 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "15:13:00 |     adam_eps: 1e-08\n",
            "15:13:00 |     add_p1_after_newln: False\n",
            "15:13:00 |     aggregate_micro: False\n",
            "15:13:00 |     allow_missing_init_opts: False\n",
            "15:13:00 |     area_under_curve_class: None\n",
            "15:13:00 |     area_under_curve_digits: -1\n",
            "15:13:00 |     attention_dropout: 0.0\n",
            "15:13:00 |     batchsize: 32\n",
            "15:13:00 |     beam_block_full_context: True\n",
            "15:13:00 |     beam_block_list_filename: None\n",
            "15:13:00 |     beam_block_ngram: 2\n",
            "15:13:00 |     beam_context_block_ngram: 2\n",
            "15:13:00 |     beam_delay: 30\n",
            "15:13:00 |     beam_length_penalty: 0.7\n",
            "15:13:00 |     beam_min_length: 10\n",
            "15:13:00 |     beam_size: 10\n",
            "15:13:00 |     betas: '[0.9, 0.999]'\n",
            "15:13:00 |     bpe_add_prefix_space: None\n",
            "15:13:00 |     bpe_debug: False\n",
            "15:13:00 |     bpe_dropout: None\n",
            "15:13:00 |     bpe_merge: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model.dict-merges.txt\n",
            "15:13:00 |     bpe_vocab: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model.dict-vocab.json\n",
            "15:13:00 |     checkpoint_activations: False\n",
            "15:13:00 |     compute_tokenized_bleu: False\n",
            "15:13:00 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "15:13:00 |     datatype: valid\n",
            "15:13:00 |     delimiter: '  '\n",
            "15:13:00 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "15:13:00 |     dict_endtoken: __end__\n",
            "15:13:00 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model.dict\n",
            "15:13:00 |     dict_include_test: False\n",
            "15:13:00 |     dict_include_valid: False\n",
            "15:13:00 |     dict_initpath: None\n",
            "15:13:00 |     dict_language: english\n",
            "15:13:00 |     dict_loaded: True\n",
            "15:13:00 |     dict_lower: False\n",
            "15:13:00 |     dict_max_ngram_size: -1\n",
            "15:13:00 |     dict_maxexs: -1\n",
            "15:13:00 |     dict_maxtokens: -1\n",
            "15:13:00 |     dict_minfreq: 0\n",
            "15:13:00 |     dict_nulltoken: __null__\n",
            "15:13:00 |     dict_starttoken: __start__\n",
            "15:13:00 |     dict_textfields: text,labels\n",
            "15:13:00 |     dict_tokenizer: bytelevelbpe\n",
            "15:13:00 |     dict_unktoken: __unk__\n",
            "15:13:00 |     display_examples: False\n",
            "15:13:00 |     download_path: None\n",
            "15:13:00 |     dropout: 0.1\n",
            "15:13:00 |     dynamic_batching: None\n",
            "15:13:00 |     embedding_projection: random\n",
            "15:13:00 |     embedding_size: 1280\n",
            "15:13:00 |     embedding_type: random\n",
            "15:13:00 |     embeddings_scale: True\n",
            "15:13:00 |     eval_batchsize: None\n",
            "15:13:00 |     eval_dynamic_batching: None\n",
            "15:13:00 |     evaltask: None\n",
            "15:13:00 |     ffn_size: 5120\n",
            "15:13:00 |     final_extra_opt: \n",
            "15:13:00 |     force_fp16_tokens: True\n",
            "15:13:00 |     fp16: True\n",
            "15:13:00 |     fp16_impl: mem_efficient\n",
            "15:13:00 |     gpu: -1\n",
            "15:13:00 |     gradient_clip: 0.1\n",
            "15:13:00 |     hide_labels: False\n",
            "15:13:00 |     history_add_global_end_token: end\n",
            "15:13:00 |     history_reversed: False\n",
            "15:13:00 |     history_size: -1\n",
            "15:13:00 |     image_cropsize: 224\n",
            "15:13:00 |     image_mode: raw\n",
            "15:13:00 |     image_size: 256\n",
            "15:13:00 |     inference: topk\n",
            "15:13:00 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model\n",
            "15:13:00 |     init_opt: None\n",
            "15:13:00 |     interactive_mode: False\n",
            "15:13:00 |     invsqrt_lr_decay_gamma: -1\n",
            "15:13:00 |     is_debug: False\n",
            "15:13:00 |     label_truncate: 128\n",
            "15:13:00 |     learn_positional_embeddings: False\n",
            "15:13:00 |     learningrate: 7e-06\n",
            "15:13:00 |     log_every_n_secs: 10.0\n",
            "15:13:00 |     log_every_n_steps: 50\n",
            "15:13:00 |     log_keep_fields: all\n",
            "15:13:00 |     loglevel: info\n",
            "15:13:00 |     lr_scheduler: reduceonplateau\n",
            "15:13:00 |     lr_scheduler_decay: 0.5\n",
            "15:13:00 |     lr_scheduler_patience: 3\n",
            "15:13:00 |     max_train_steps: -1\n",
            "15:13:00 |     max_train_time: -1\n",
            "15:13:00 |     metrics: default\n",
            "15:13:00 |     model: transformer/generator\n",
            "15:13:00 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model\n",
            "15:13:00 |     model_parallel: False\n",
            "15:13:00 |     momentum: 0\n",
            "15:13:00 |     multitask_weights: '[1.0, 3.0, 3.0]'\n",
            "15:13:00 |     mutators: None\n",
            "15:13:00 |     n_decoder_layers: 12\n",
            "15:13:00 |     n_encoder_layers: 2\n",
            "15:13:00 |     n_heads: 32\n",
            "15:13:00 |     n_layers: 2\n",
            "15:13:00 |     n_positions: 128\n",
            "15:13:00 |     n_segments: 0\n",
            "15:13:00 |     nesterov: True\n",
            "15:13:00 |     no_cuda: False\n",
            "15:13:00 |     num_epochs: 5.0\n",
            "15:13:00 |     num_examples: 100\n",
            "15:13:00 |     num_workers: 0\n",
            "15:13:00 |     nus: [0.7]\n",
            "15:13:00 |     optimizer: mem_eff_adam\n",
            "15:13:00 |     output_scaling: 1.0\n",
            "15:13:00 |     override: \"{'datatype': 'valid', 'beam_block_ngram': 2, 'beam_context_block_ngram': 2, 'beam_length_penalty': 0.7, 'beam_min_length': 10, 'beam_size': 10, 'inference': 'topk', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m/model', 'num_examples': 100, 'skip_generation': False, 'task': 'french_blended_skill_talk', 'temperature': 0.5, 'topk': 10, 'topp': 0.3}\"\n",
            "15:13:00 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "15:13:00 |     person_tokens: False\n",
            "15:13:00 |     rank_candidates: False\n",
            "15:13:00 |     relu_dropout: 0.0\n",
            "15:13:00 |     report_filename: \n",
            "15:13:00 |     save_after_valid: False\n",
            "15:13:00 |     save_every_n_secs: -1\n",
            "15:13:00 |     save_format: conversations\n",
            "15:13:00 |     share_word_embeddings: True\n",
            "15:13:00 |     short_final_eval: False\n",
            "15:13:00 |     skip_generation: False\n",
            "15:13:00 |     special_tok_lst: None\n",
            "15:13:00 |     split_lines: False\n",
            "15:13:00 |     starttime: Apr27_08-14\n",
            "15:13:00 |     task: french_blended_skill_talk\n",
            "15:13:00 |     temperature: 0.5\n",
            "15:13:00 |     tensorboard_log: False\n",
            "15:13:00 |     tensorboard_logdir: None\n",
            "15:13:00 |     text_truncate: 128\n",
            "15:13:00 |     topk: 10\n",
            "15:13:00 |     topp: 0.3\n",
            "15:13:00 |     truncate: 128\n",
            "15:13:00 |     update_freq: 2\n",
            "15:13:00 |     use_reply: label\n",
            "15:13:00 |     validation_cutoff: 1.0\n",
            "15:13:00 |     validation_every_n_epochs: 0.25\n",
            "15:13:00 |     validation_every_n_secs: -1\n",
            "15:13:00 |     validation_every_n_steps: -1\n",
            "15:13:00 |     validation_max_exs: -1\n",
            "15:13:00 |     validation_metric: ppl\n",
            "15:13:00 |     validation_metric_mode: min\n",
            "15:13:00 |     validation_patience: 10\n",
            "15:13:00 |     validation_share_agent: False\n",
            "15:13:00 |     variant: prelayernorm\n",
            "15:13:00 |     verbose: False\n",
            "15:13:00 |     wandb_entity: None\n",
            "15:13:00 |     wandb_log: False\n",
            "15:13:00 |     wandb_name: None\n",
            "15:13:00 |     wandb_project: None\n",
            "15:13:00 |     warmup_rate: 0.0001\n",
            "15:13:00 |     warmup_updates: 100\n",
            "15:13:00 |     weight_decay: None\n",
            "15:13:00 |     world_logs: \n",
            "15:13:01 | Evaluating task french_blended_skill_talk using datatype valid.\n",
            "15:13:01 | creating task(s): french_blended_skill_talk\n",
            "15:13:01 | Loading ParlAI text data: /content/dataset_french_bst/valid.txt\n",
            "15:13:12 | 12.0% complete (12 / 100), 0:00:10 elapsed, 0:01:18 eta\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gen_n_toks  gpu_mem  llen  loss  ltpb  ltps  \\\n",
            "           0 8.612e-09   175   105 118.5   .6667         70 1.128   12 .05591       20.17   .09944    26 2.379    26 29.33   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  tpb   tps  \n",
            "         0          0 10.8      .5224         0  131 147.8\n",
            "15:13:22 | 23.0% complete (23 / 100), 0:00:21 elapsed, 0:01:12 eta\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gen_n_toks  gpu_mem  llen  loss  ltpb  ltps  \\\n",
            "           0 7.638e-09 181.5 101.6 109.2   .6087      79.91 1.074   23 .0738       21.26   .09947 30.13 2.346 30.13 32.37   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em   tpb   tps  \n",
            "         0          0 10.44      .5137         0 131.7 141.6\n",
            "15:13:33 | 33.0% complete (33 / 100), 0:00:31 elapsed, 0:01:05 eta\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gen_n_toks  gpu_mem  llen  loss  ltpb  ltps  \\\n",
            "           0 4.099e-06 199.5 107.9 111.5   .6667      91.64 1.033   33 .08136       22.15   .09956 28.39 2.367 28.39 29.34   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em   tpb   tps  \n",
            "         0          0 10.67      .5176         0 136.3 140.8\n",
            "15:13:44 | 44.0% complete (44 / 100), 0:00:42 elapsed, 0:00:54 eta\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gen_n_toks  gpu_mem  llen  loss  ltpb  ltps  \\\n",
            "           0 3.076e-06 183.4 104.5 107.7   .6136      78.84  1.03   44 .07179       22.34    .1005 27.73 2.486 27.73 28.56   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em   tpb   tps  \n",
            "         0          0 12.01      .4893         0 132.2 136.2\n",
            "15:13:54 | 56.0% complete (56 / 100), 0:00:53 elapsed, 0:00:42 eta\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gen_n_toks  gpu_mem  llen  loss  ltpb  ltps  \\\n",
            "           0 2.419e-06 187.2 105.3 111.2   .6250      81.98 1.056   56 .06591       22.66   .09944 29.95 2.407 29.95 31.63   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em   tpb   tps  \n",
            "         0          0 11.1      .4937         0 135.2 142.8\n",
            "15:14:05 | 65.0% complete (65 / 100), 0:01:03 elapsed, 0:00:34 eta\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gen_n_toks  gpu_mem  llen  loss  ltpb  ltps  \\\n",
            "           0 2.084e-06 208.6 106.9 108.7   .6462      101.7 1.016   65 .06593       23.25    .1006 30.52 2.358 30.52 31.02   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em   tpb   tps  \n",
            "         0          0 10.57      .4975         0 137.5 139.7\n",
            "15:14:15 | 75.0% complete (75 / 100), 0:01:14 elapsed, 0:00:25 eta\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gen_n_toks  gpu_mem  llen  loss  ltpb  ltps  \\\n",
            "           0 1.809e-06 212.1 105.5 106.9   .6400      106.6 1.014   75 .07283       23.25    .1027 32.47 2.332 32.47 32.91   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em   tpb   tps  \n",
            "         0          0 10.3      .5010         0 137.9 139.8\n",
            "15:14:26 | 85.0% complete (85 / 100), 0:01:24 elapsed, 0:00:15 eta\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gen_n_toks  gpu_mem  llen  loss  ltpb  ltps  \\\n",
            "           0 1.596e-06 223.3 106.1 106.6   .6588      117.2 1.005   85 .06573       23.19    .1033 33.18 2.341 33.18 33.33   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em   tpb   tps  \n",
            "         0          0 10.39      .5000         0 139.3 139.9\n",
            "15:14:36 | 96.0% complete (96 / 100), 0:01:35 elapsed, 0:00:04 eta\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gen_n_toks  gpu_mem  llen  loss  ltpb  ltps  \\\n",
            "           0 1.414e-06 222.9 106.3 107.1   .6667      116.6 1.007   96 .07125       23.42    .1023 34.57 2.365 34.57 34.82   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em   tpb   tps  \n",
            "         0          0 10.64      .4977         0 140.9 141.9\n",
            "15:14:42 | \u001b[1mReport for french_blended_skill_talk:\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gen_n_toks  gpu_mem  llen  loss  ltpb  ltps  \\\n",
            "           0 1.358e-06 226.9 106.9 105.8   .6700      120.1 .9896  100 .06983       23.37    .1009 34.21 2.366 34.21 33.86   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em   tpb   tps  \n",
            "         0          0 10.66      .4978         0 141.1 139.6\u001b[0m\n",
            "15:14:42 | Finished evaluating tasks ['french_blended_skill_talk'] using datatype valid\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gen_n_toks  gpu_mem  llen  loss  ltpb  ltps  \\\n",
            "           0 1.358e-06 226.9 106.9 105.8   .6700      120.1 .9896  100 .06983       23.37    .1009 34.21 2.366 34.21 33.86   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em   tpb   tps  \n",
            "         0          0 10.66      .4978         0 141.1 139.6\n",
            "112.17 \n",
            "\n",
            "{'exs': SumMetric(100), 'accuracy': ExactMatchMetric(0), 'f1': F1Metric(0.06983), 'bleu-4': BleuMetric(1.358e-06), 'clen': AverageMetric(226.9), 'ctrunc': AverageMetric(0.67), 'ctrunclen': AverageMetric(120.1), 'llen': AverageMetric(34.21), 'ltrunc': AverageMetric(0), 'ltrunclen': AverageMetric(0), 'loss': AverageMetric(2.366), 'ppl': PPLMetric(10.66), 'token_acc': AverageMetric(0.4978), 'token_em': AverageMetric(0), 'gen_n_toks': AverageMetric(23.37), 'exps': GlobalTimerMetric(0.9896), 'ltpb': GlobalAverageMetric(34.21), 'ltps': GlobalTimerMetric(33.86), 'ctpb': GlobalAverageMetric(106.9), 'ctps': GlobalTimerMetric(105.8), 'tpb': GlobalAverageMetric(141.1), 'tps': GlobalTimerMetric(139.6), 'gpu_mem': GlobalAverageMetric(0.1009)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7K77cxZVmrsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPH4WwAhmufI",
        "outputId": "949893d7-605c-462c-a7b3-323b3f5e9629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14: Similarity Score for a dialog: 0.3493618220090866\n",
            "27: Similarity Score for a dialog: 0.3425620687859399\n",
            "56: Similarity Score for a dialog: 0.28388609417847227\n",
            "81: Similarity Score for a dialog: 0.2417905932292342\n",
            "110: Similarity Score for a dialog: 0.2467489690968284\n",
            "139: Similarity Score for a dialog: 0.2414254258003305\n",
            "168: Similarity Score for a dialog: 0.2554341968058086\n",
            "197: Similarity Score for a dialog: 0.24380125400299826\n",
            "226: Similarity Score for a dialog: 0.24595733880996704\n",
            "251: Similarity Score for a dialog: 0.24300135046121527\n",
            "280: Similarity Score for a dialog: 0.23911622361115673\n",
            "305: Similarity Score for a dialog: 0.23095074716351321\n",
            "334: Similarity Score for a dialog: 0.2292991311599811\n",
            "359: Similarity Score for a dialog: 0.23180824858618193\n",
            "372: Similarity Score for a dialog: 0.2290712069099148\n",
            "381: Similarity Score for a dialog: 0.22634249314179886\n",
            "394: Similarity Score for a dialog: 0.2252393566856259\n",
            "403: Similarity Score for a dialog: 0.2236574177351809\n",
            "432: Similarity Score for a dialog: 0.22283298855360884\n",
            "457: Similarity Score for a dialog: 0.2274638787250627\n",
            "486: Similarity Score for a dialog: 0.22884657179825327\n",
            "511: Similarity Score for a dialog: 0.22746567891501798\n",
            "540: Similarity Score for a dialog: 0.22604352032335906\n",
            "565: Similarity Score for a dialog: 0.22311906396027872\n",
            "594: Similarity Score for a dialog: 0.21913727219183962\n",
            "619: Similarity Score for a dialog: 0.22050505123982495\n",
            "648: Similarity Score for a dialog: 0.21876032805691162\n",
            "673: Similarity Score for a dialog: 0.22166474037056352\n",
            "698: Similarity Score for a dialog: 0.22239478298329882\n",
            "719: Similarity Score for a dialog: 0.22172040515983035\n",
            "748: Similarity Score for a dialog: 0.2214784326652686\n",
            "773: Similarity Score for a dialog: 0.2201375113299457\n",
            "802: Similarity Score for a dialog: 0.22516119140417465\n",
            "827: Similarity Score for a dialog: 0.22416545218558767\n",
            "856: Similarity Score for a dialog: 0.22681158314486152\n",
            "881: Similarity Score for a dialog: 0.2261380366629587\n",
            "910: Similarity Score for a dialog: 0.22372885136843818\n",
            "935: Similarity Score for a dialog: 0.22288626636068026\n",
            "964: Similarity Score for a dialog: 0.22242048116208152\n",
            "989: Similarity Score for a dialog: 0.22095346504024097\n",
            "1018: Similarity Score for a dialog: 0.22071689236529018\n",
            "1047: Similarity Score for a dialog: 0.21784895642971\n",
            "1080: Similarity Score for a dialog: 0.2202459425235597\n",
            "1109: Similarity Score for a dialog: 0.22031697383227197\n",
            "1138: Similarity Score for a dialog: 0.21874481610899424\n",
            "1163: Similarity Score for a dialog: 0.21888472993991204\n",
            "1192: Similarity Score for a dialog: 0.21912935202233882\n",
            "1217: Similarity Score for a dialog: 0.22280793494637102\n",
            "1246: Similarity Score for a dialog: 0.224771682942907\n",
            "1271: Similarity Score for a dialog: 0.22605953723484395\n",
            "1300: Similarity Score for a dialog: 0.22892018864615657\n",
            "1325: Similarity Score for a dialog: 0.22967564109070548\n",
            "1354: Similarity Score for a dialog: 0.230900450741038\n",
            "1379: Similarity Score for a dialog: 0.23323969742709613\n",
            "1408: Similarity Score for a dialog: 0.23444302351513444\n",
            "1433: Similarity Score for a dialog: 0.23337048965411775\n",
            "1442: Similarity Score for a dialog: 0.2337101782465445\n",
            "1451: Similarity Score for a dialog: 0.23415495723720778\n",
            "1480: Similarity Score for a dialog: 0.23465118152769596\n",
            "1509: Similarity Score for a dialog: 0.23693769223495456\n",
            "1538: Similarity Score for a dialog: 0.2367029849892935\n",
            "1563: Similarity Score for a dialog: 0.2360430177291261\n",
            "1592: Similarity Score for a dialog: 0.2382326536318263\n",
            "1617: Similarity Score for a dialog: 0.23892711590889334\n",
            "1646: Similarity Score for a dialog: 0.23754969633871106\n",
            "1671: Similarity Score for a dialog: 0.23618480355122048\n",
            "1688: Similarity Score for a dialog: 0.23561592577621707\n",
            "1701: Similarity Score for a dialog: 0.2354978476457462\n",
            "1730: Similarity Score for a dialog: 0.23361937961397836\n",
            "1755: Similarity Score for a dialog: 0.2328506245750104\n",
            "1784: Similarity Score for a dialog: 0.23386834751128593\n",
            "1809: Similarity Score for a dialog: 0.23536160685892762\n",
            "1834: Similarity Score for a dialog: 0.2363261619240654\n",
            "1855: Similarity Score for a dialog: 0.23709279645418105\n",
            "1884: Similarity Score for a dialog: 0.23781108330713202\n",
            "1909: Similarity Score for a dialog: 0.23750929543362984\n",
            "1938: Similarity Score for a dialog: 0.23710871464672775\n",
            "1963: Similarity Score for a dialog: 0.23610589177331934\n",
            "1984: Similarity Score for a dialog: 0.23551775518984414\n",
            "2005: Similarity Score for a dialog: 0.2343193515748775\n",
            "2034: Similarity Score for a dialog: 0.23502804856206497\n",
            "2059: Similarity Score for a dialog: 0.23652150979397274\n",
            "2088: Similarity Score for a dialog: 0.23663955269403666\n",
            "2113: Similarity Score for a dialog: 0.23727556254978724\n",
            "2142: Similarity Score for a dialog: 0.23886368591519236\n",
            "2167: Similarity Score for a dialog: 0.23816092152884963\n",
            "2180: Similarity Score for a dialog: 0.2379484719575475\n",
            "2193: Similarity Score for a dialog: 0.2381961865728788\n",
            "2222: Similarity Score for a dialog: 0.23923358456528143\n",
            "2247: Similarity Score for a dialog: 0.2394978603900031\n",
            "2276: Similarity Score for a dialog: 0.23978147622354506\n",
            "2301: Similarity Score for a dialog: 0.2396558432706765\n",
            "2330: Similarity Score for a dialog: 0.23983905422501267\n",
            "2355: Similarity Score for a dialog: 0.2393174094704141\n",
            "2384: Similarity Score for a dialog: 0.23906860669609972\n",
            "2409: Similarity Score for a dialog: 0.2384016886739533\n",
            "2422: Similarity Score for a dialog: 0.23873517037564537\n",
            "2435: Similarity Score for a dialog: 0.23936092030798267\n",
            "2464: Similarity Score for a dialog: 0.23975777739306559\n",
            "2489: Similarity Score for a dialog: 0.24015631129261245\n",
            "2518: Similarity Score for a dialog: 0.23997188563928132\n",
            "2543: Similarity Score for a dialog: 0.2394271417385281\n",
            "2576: Similarity Score for a dialog: 0.23962860092556823\n",
            "2605: Similarity Score for a dialog: 0.23939182658414968\n",
            "2634: Similarity Score for a dialog: 0.23984873559955644\n",
            "2659: Similarity Score for a dialog: 0.24114078098211397\n",
            "2688: Similarity Score for a dialog: 0.24117609956835168\n",
            "2713: Similarity Score for a dialog: 0.24149102529373728\n",
            "2742: Similarity Score for a dialog: 0.2420732515676114\n",
            "2767: Similarity Score for a dialog: 0.24144247459681858\n",
            "2800: Similarity Score for a dialog: 0.24246117575329357\n",
            "2829: Similarity Score for a dialog: 0.2420929517460001\n",
            "2858: Similarity Score for a dialog: 0.2415726771483298\n",
            "2883: Similarity Score for a dialog: 0.24100137293650561\n",
            "2912: Similarity Score for a dialog: 0.24015247501166803\n",
            "2937: Similarity Score for a dialog: 0.23976859172526482\n",
            "2966: Similarity Score for a dialog: 0.23891062808822616\n",
            "2995: Similarity Score for a dialog: 0.23731165123301454\n",
            "3024: Similarity Score for a dialog: 0.23627185621817676\n",
            "3049: Similarity Score for a dialog: 0.23515265363447704\n",
            "3078: Similarity Score for a dialog: 0.23586941436562386\n",
            "3103: Similarity Score for a dialog: 0.23589802514037603\n",
            "3132: Similarity Score for a dialog: 0.23540114759434147\n",
            "3157: Similarity Score for a dialog: 0.23579871570703737\n",
            "3190: Similarity Score for a dialog: 0.23561660637119797\n",
            "3223: Similarity Score for a dialog: 0.23513842397279316\n",
            "3252: Similarity Score for a dialog: 0.2349614693278261\n",
            "3277: Similarity Score for a dialog: 0.234328004036354\n",
            "3306: Similarity Score for a dialog: 0.23436967182389032\n",
            "3331: Similarity Score for a dialog: 0.23483508366849576\n",
            "3360: Similarity Score for a dialog: 0.23582281371103692\n",
            "3385: Similarity Score for a dialog: 0.23604143103035463\n",
            "3398: Similarity Score for a dialog: 0.23650399395441374\n",
            "3407: Similarity Score for a dialog: 0.23683413020443167\n",
            "3416: Similarity Score for a dialog: 0.23667468449461584\n",
            "3421: Similarity Score for a dialog: 0.23719158450848538\n",
            "3450: Similarity Score for a dialog: 0.23727564031395285\n",
            "3475: Similarity Score for a dialog: 0.23741585486819466\n",
            "3488: Similarity Score for a dialog: 0.2372641241864739\n",
            "3501: Similarity Score for a dialog: 0.23714818068703594\n",
            "3530: Similarity Score for a dialog: 0.23737556380461375\n",
            "3555: Similarity Score for a dialog: 0.23645098316232374\n",
            "3584: Similarity Score for a dialog: 0.236567157208184\n",
            "3609: Similarity Score for a dialog: 0.2362932076252301\n",
            "3638: Similarity Score for a dialog: 0.2366918953683321\n",
            "3663: Similarity Score for a dialog: 0.2370171844610013\n",
            "3692: Similarity Score for a dialog: 0.23776264297077696\n",
            "3717: Similarity Score for a dialog: 0.23758353237771873\n",
            "3738: Similarity Score for a dialog: 0.2367344055384945\n",
            "3755: Similarity Score for a dialog: 0.23632134170583274\n",
            "3780: Similarity Score for a dialog: 0.2367434278017706\n",
            "3801: Similarity Score for a dialog: 0.2365744968627346\n",
            "3830: Similarity Score for a dialog: 0.23729083905421683\n",
            "3855: Similarity Score for a dialog: 0.23770413869291898\n",
            "3884: Similarity Score for a dialog: 0.23823739543241734\n",
            "3909: Similarity Score for a dialog: 0.238639054648356\n",
            "3926: Similarity Score for a dialog: 0.23886640513392832\n",
            "3943: Similarity Score for a dialog: 0.23952886380093938\n",
            "3972: Similarity Score for a dialog: 0.24014609850322208\n",
            "3997: Similarity Score for a dialog: 0.24102305156508616\n",
            "4026: Similarity Score for a dialog: 0.2411711371427685\n",
            "4055: Similarity Score for a dialog: 0.24120859109970608\n",
            "4084: Similarity Score for a dialog: 0.24146824416388338\n",
            "4109: Similarity Score for a dialog: 0.2407528434992365\n",
            "4134: Similarity Score for a dialog: 0.24055984311935014\n",
            "4155: Similarity Score for a dialog: 0.2407389147933162\n",
            "4176: Similarity Score for a dialog: 0.24166096879423765\n",
            "4193: Similarity Score for a dialog: 0.24197498160990577\n",
            "4218: Similarity Score for a dialog: 0.24246102754166396\n",
            "4243: Similarity Score for a dialog: 0.24212798828915333\n",
            "4272: Similarity Score for a dialog: 0.2420839233165988\n",
            "4301: Similarity Score for a dialog: 0.24225434959061004\n",
            "4330: Similarity Score for a dialog: 0.24235310439432325\n",
            "4355: Similarity Score for a dialog: 0.24253044199344503\n",
            "4372: Similarity Score for a dialog: 0.24218044506918107\n",
            "4389: Similarity Score for a dialog: 0.24264206897301258\n",
            "4418: Similarity Score for a dialog: 0.24176680362931716\n",
            "4443: Similarity Score for a dialog: 0.24161648603929547\n",
            "4472: Similarity Score for a dialog: 0.24225231451095122\n",
            "4497: Similarity Score for a dialog: 0.24236556320123512\n",
            "4526: Similarity Score for a dialog: 0.24247294039874173\n",
            "4551: Similarity Score for a dialog: 0.24250040612245713\n",
            "4580: Similarity Score for a dialog: 0.24278515361164782\n",
            "4605: Similarity Score for a dialog: 0.24339487739698878\n",
            "4634: Similarity Score for a dialog: 0.24323796705064185\n",
            "4659: Similarity Score for a dialog: 0.24307420038742159\n",
            "4668: Similarity Score for a dialog: 0.24297195894894566\n",
            "4677: Similarity Score for a dialog: 0.24273583593256673\n",
            "4706: Similarity Score for a dialog: 0.2432781112050654\n",
            "4731: Similarity Score for a dialog: 0.24328669841142214\n",
            "4744: Similarity Score for a dialog: 0.24309514819859437\n",
            "4753: Similarity Score for a dialog: 0.24313963515852152\n",
            "4782: Similarity Score for a dialog: 0.2432508766886418\n",
            "4807: Similarity Score for a dialog: 0.24336946692779352\n",
            "4836: Similarity Score for a dialog: 0.24389694066129425\n",
            "4865: Similarity Score for a dialog: 0.24373973097591597\n",
            "4894: Similarity Score for a dialog: 0.24396155939853573\n",
            "4919: Similarity Score for a dialog: 0.24385186672529283\n",
            "4932: Similarity Score for a dialog: 0.24386155953696878\n",
            "4945: Similarity Score for a dialog: 0.2441183750155965\n",
            "4974: Similarity Score for a dialog: 0.24464135679183918\n",
            "4999: Similarity Score for a dialog: 0.24452311417475964\n",
            "5028: Similarity Score for a dialog: 0.24443319922987475\n",
            "5053: Similarity Score for a dialog: 0.24445013728097192\n",
            "5082: Similarity Score for a dialog: 0.24420068614322266\n",
            "5107: Similarity Score for a dialog: 0.2438873134737266\n",
            "5136: Similarity Score for a dialog: 0.24416132967175866\n",
            "5161: Similarity Score for a dialog: 0.24417853440271572\n",
            "5190: Similarity Score for a dialog: 0.24394979022434324\n",
            "5215: Similarity Score for a dialog: 0.24358913143010685\n",
            "5244: Similarity Score for a dialog: 0.2433534686278324\n",
            "5269: Similarity Score for a dialog: 0.24340735877764672\n",
            "5298: Similarity Score for a dialog: 0.2434462881654372\n",
            "5323: Similarity Score for a dialog: 0.243584288016737\n",
            "5352: Similarity Score for a dialog: 0.24373147711810086\n",
            "5377: Similarity Score for a dialog: 0.24416120244532039\n",
            "5406: Similarity Score for a dialog: 0.24440078623664604\n",
            "5431: Similarity Score for a dialog: 0.2443224985896684\n",
            "5460: Similarity Score for a dialog: 0.24385609133787095\n",
            "5485: Similarity Score for a dialog: 0.24379189943664212\n",
            "5498: Similarity Score for a dialog: 0.24369246366320912\n",
            "5507: Similarity Score for a dialog: 0.2434606971923662\n",
            "5536: Similarity Score for a dialog: 0.2431614733741887\n",
            "5561: Similarity Score for a dialog: 0.24285617965632833\n",
            "5590: Similarity Score for a dialog: 0.24237907956175272\n",
            "5615: Similarity Score for a dialog: 0.24207615117937636\n",
            "5644: Similarity Score for a dialog: 0.24199089083092018\n",
            "5673: Similarity Score for a dialog: 0.24217115435749292\n",
            "5686: Similarity Score for a dialog: 0.2426123044334161\n",
            "5695: Similarity Score for a dialog: 0.24294398539858356\n",
            "5724: Similarity Score for a dialog: 0.24339285295109606\n",
            "5749: Similarity Score for a dialog: 0.2435797066859685\n",
            "5778: Similarity Score for a dialog: 0.24325880167768124\n",
            "5803: Similarity Score for a dialog: 0.24320280602320826\n",
            "5832: Similarity Score for a dialog: 0.24324550357381147\n",
            "5857: Similarity Score for a dialog: 0.24292237555494223\n",
            "5886: Similarity Score for a dialog: 0.243154932840372\n",
            "5911: Similarity Score for a dialog: 0.24314782103370033\n",
            "5940: Similarity Score for a dialog: 0.24328160195168613\n",
            "5965: Similarity Score for a dialog: 0.24367252708797724\n",
            "5994: Similarity Score for a dialog: 0.24333400664756577\n",
            "6019: Similarity Score for a dialog: 0.24299729537850434\n",
            "6028: Similarity Score for a dialog: 0.2432244893268997\n",
            "6037: Similarity Score for a dialog: 0.243634360107761\n",
            "6066: Similarity Score for a dialog: 0.2434775142669473\n",
            "6091: Similarity Score for a dialog: 0.24342700098825953\n",
            "6120: Similarity Score for a dialog: 0.2427384775966264\n",
            "6145: Similarity Score for a dialog: 0.2422957479347617\n",
            "6174: Similarity Score for a dialog: 0.24241306660910528\n",
            "6199: Similarity Score for a dialog: 0.2427958379529657\n",
            "6228: Similarity Score for a dialog: 0.24267128682555164\n",
            "6253: Similarity Score for a dialog: 0.24270862388082698\n",
            "6282: Similarity Score for a dialog: 0.2425522160486612\n",
            "6307: Similarity Score for a dialog: 0.24303425865215858\n",
            "6340: Similarity Score for a dialog: 0.2429385648712359\n",
            "6369: Similarity Score for a dialog: 0.24296225164209884\n",
            "6398: Similarity Score for a dialog: 0.2425590493658092\n",
            "6423: Similarity Score for a dialog: 0.24191666552289626\n",
            "6452: Similarity Score for a dialog: 0.24161293202069611\n",
            "6477: Similarity Score for a dialog: 0.2409453675129909\n",
            "6506: Similarity Score for a dialog: 0.2409163689441745\n",
            "6531: Similarity Score for a dialog: 0.2406728206910977\n",
            "6560: Similarity Score for a dialog: 0.2407341772214406\n",
            "6585: Similarity Score for a dialog: 0.24090593101239982\n",
            "6614: Similarity Score for a dialog: 0.24138776901599152\n",
            "6639: Similarity Score for a dialog: 0.24181984093331194\n",
            "6668: Similarity Score for a dialog: 0.24199832838492877\n",
            "6693: Similarity Score for a dialog: 0.2420472918346631\n",
            "6718: Similarity Score for a dialog: 0.24178477013321975\n",
            "6739: Similarity Score for a dialog: 0.24195965484610088\n",
            "6768: Similarity Score for a dialog: 0.24192123846967634\n",
            "6793: Similarity Score for a dialog: 0.24180350543252763\n",
            "6822: Similarity Score for a dialog: 0.24180683592460614\n",
            "6847: Similarity Score for a dialog: 0.24181022186998113\n",
            "6876: Similarity Score for a dialog: 0.24147555733331988\n",
            "6905: Similarity Score for a dialog: 0.2414117919292921\n",
            "6934: Similarity Score for a dialog: 0.24112258876727016\n",
            "6959: Similarity Score for a dialog: 0.24113416910881566\n",
            "6988: Similarity Score for a dialog: 0.24108894498928876\n",
            "7013: Similarity Score for a dialog: 0.24099858083199233\n",
            "7042: Similarity Score for a dialog: 0.2408881722536381\n",
            "7067: Similarity Score for a dialog: 0.24067752381005206\n",
            "7096: Similarity Score for a dialog: 0.24057027913423412\n",
            "7121: Similarity Score for a dialog: 0.24037386394073174\n",
            "7150: Similarity Score for a dialog: 0.240617855969734\n",
            "7175: Similarity Score for a dialog: 0.24033919880728793\n",
            "7212: Similarity Score for a dialog: 0.24000565247802713\n",
            "7245: Similarity Score for a dialog: 0.23987379553861615\n",
            "7274: Similarity Score for a dialog: 0.239218903096413\n",
            "7299: Similarity Score for a dialog: 0.23873724716572603\n",
            "7328: Similarity Score for a dialog: 0.23899523282227825\n",
            "7353: Similarity Score for a dialog: 0.2388861119809918\n",
            "7382: Similarity Score for a dialog: 0.23887792210190756\n",
            "7407: Similarity Score for a dialog: 0.23899905902567567\n",
            "7436: Similarity Score for a dialog: 0.2386004291107192\n",
            "7461: Similarity Score for a dialog: 0.23885567827294704\n",
            "7490: Similarity Score for a dialog: 0.2390034570469479\n",
            "7515: Similarity Score for a dialog: 0.23939234769240103\n",
            "7532: Similarity Score for a dialog: 0.23952204451626996\n",
            "7549: Similarity Score for a dialog: 0.23967467538238812\n",
            "7578: Similarity Score for a dialog: 0.23986786759975315\n",
            "7603: Similarity Score for a dialog: 0.24024084602586665\n",
            "7632: Similarity Score for a dialog: 0.2408747813782253\n",
            "7657: Similarity Score for a dialog: 0.24113853387873926\n",
            "7678: Similarity Score for a dialog: 0.2411105746952332\n",
            "7699: Similarity Score for a dialog: 0.24086580030079136\n",
            "7728: Similarity Score for a dialog: 0.24100213924875474\n",
            "7753: Similarity Score for a dialog: 0.24079157611996407\n",
            "7782: Similarity Score for a dialog: 0.24072883854440272\n",
            "7807: Similarity Score for a dialog: 0.24051850236331423\n",
            "7836: Similarity Score for a dialog: 0.24040264286982702\n",
            "7861: Similarity Score for a dialog: 0.24049303750976156\n",
            "7890: Similarity Score for a dialog: 0.2403577898383278\n",
            "7915: Similarity Score for a dialog: 0.24025097491488112\n",
            "7944: Similarity Score for a dialog: 0.23983041888500214\n",
            "7969: Similarity Score for a dialog: 0.23960225415939626\n",
            "7998: Similarity Score for a dialog: 0.23965207273130854\n",
            "8023: Similarity Score for a dialog: 0.23965489485319927\n",
            "8052: Similarity Score for a dialog: 0.23964562651801435\n",
            "8077: Similarity Score for a dialog: 0.2396478587615102\n",
            "8106: Similarity Score for a dialog: 0.23972720552439614\n",
            "8131: Similarity Score for a dialog: 0.23974814206334114\n",
            "8160: Similarity Score for a dialog: 0.23984203045802874\n",
            "8185: Similarity Score for a dialog: 0.23967177659109243\n",
            "8214: Similarity Score for a dialog: 0.23974796810418225\n",
            "8239: Similarity Score for a dialog: 0.2396586874319676\n",
            "8268: Similarity Score for a dialog: 0.2394254493446456\n",
            "8293: Similarity Score for a dialog: 0.2393501445049809\n",
            "8322: Similarity Score for a dialog: 0.2393402502303193\n",
            "8347: Similarity Score for a dialog: 0.2392122909096894\n",
            "8376: Similarity Score for a dialog: 0.23925712571533075\n",
            "8401: Similarity Score for a dialog: 0.23895614377220187\n",
            "8430: Similarity Score for a dialog: 0.23899892773617196\n",
            "8455: Similarity Score for a dialog: 0.23886902910322025\n",
            "8464: Similarity Score for a dialog: 0.23896038449890233\n",
            "8473: Similarity Score for a dialog: 0.23900166462640066\n",
            "8502: Similarity Score for a dialog: 0.23880459672391022\n",
            "8531: Similarity Score for a dialog: 0.2386701506521169\n",
            "8560: Similarity Score for a dialog: 0.23869943919053468\n",
            "8589: Similarity Score for a dialog: 0.2383630843733772\n",
            "8618: Similarity Score for a dialog: 0.23854376091914262\n",
            "8643: Similarity Score for a dialog: 0.23831750371274477\n",
            "8672: Similarity Score for a dialog: 0.2381044492233711\n",
            "8697: Similarity Score for a dialog: 0.2379900571364551\n",
            "8726: Similarity Score for a dialog: 0.2381249079840677\n",
            "8751: Similarity Score for a dialog: 0.2381598327054574\n",
            "8780: Similarity Score for a dialog: 0.2384570482230691\n",
            "8805: Similarity Score for a dialog: 0.23884325257544328\n",
            "8834: Similarity Score for a dialog: 0.23927817355032086\n",
            "8863: Similarity Score for a dialog: 0.23938257482074315\n",
            "8892: Similarity Score for a dialog: 0.2394636677449904\n",
            "8917: Similarity Score for a dialog: 0.23905018203344847\n",
            "8946: Similarity Score for a dialog: 0.2388530465474776\n",
            "8971: Similarity Score for a dialog: 0.23876559120752003\n",
            "9000: Similarity Score for a dialog: 0.23915865398843486\n",
            "9025: Similarity Score for a dialog: 0.23899611513254085\n",
            "9058: Similarity Score for a dialog: 0.23917299233161268\n",
            "9091: Similarity Score for a dialog: 0.23938689051424544\n",
            "9120: Similarity Score for a dialog: 0.2395516123592051\n",
            "9149: Similarity Score for a dialog: 0.2394751854672235\n",
            "9178: Similarity Score for a dialog: 0.23970897059059806\n",
            "9203: Similarity Score for a dialog: 0.23962374918006127\n",
            "9216: Similarity Score for a dialog: 0.23948901692025074\n",
            "9229: Similarity Score for a dialog: 0.23938760741659595\n",
            "9258: Similarity Score for a dialog: 0.23922043418881933\n",
            "9287: Similarity Score for a dialog: 0.23915035797784823\n",
            "9308: Similarity Score for a dialog: 0.23924825555305054\n",
            "9329: Similarity Score for a dialog: 0.23916695220873843\n",
            "9358: Similarity Score for a dialog: 0.2390831270995898\n",
            "9383: Similarity Score for a dialog: 0.2390911784279222\n",
            "9412: Similarity Score for a dialog: 0.23908425797533273\n",
            "9441: Similarity Score for a dialog: 0.23919450159708605\n",
            "9470: Similarity Score for a dialog: 0.23930783838524924\n",
            "9495: Similarity Score for a dialog: 0.23922226519039302\n",
            "9524: Similarity Score for a dialog: 0.23903910015828556\n",
            "9549: Similarity Score for a dialog: 0.2392500314229016\n",
            "9566: Similarity Score for a dialog: 0.23946447283867667\n",
            "9583: Similarity Score for a dialog: 0.23988706563404405\n",
            "9592: Similarity Score for a dialog: 0.24001926439693003\n",
            "9597: Similarity Score for a dialog: 0.2401305955545795\n",
            "9606: Similarity Score for a dialog: 0.24014515768394268\n",
            "9611: Similarity Score for a dialog: 0.24019484666008276\n",
            "9640: Similarity Score for a dialog: 0.2401773816489536\n",
            "9665: Similarity Score for a dialog: 0.24028245442037707\n",
            "9694: Similarity Score for a dialog: 0.24035839783067772\n",
            "9719: Similarity Score for a dialog: 0.24023132740488817\n",
            "9748: Similarity Score for a dialog: 0.24025703280955707\n",
            "9773: Similarity Score for a dialog: 0.24037846757680761\n",
            "9802: Similarity Score for a dialog: 0.24006544140198394\n",
            "9827: Similarity Score for a dialog: 0.2399487286671933\n",
            "9856: Similarity Score for a dialog: 0.24013571876735063\n",
            "9881: Similarity Score for a dialog: 0.24031642087752111\n",
            "9910: Similarity Score for a dialog: 0.24011564890486228\n",
            "9935: Similarity Score for a dialog: 0.240180904558592\n",
            "9964: Similarity Score for a dialog: 0.24013301845588572\n",
            "9989: Similarity Score for a dialog: 0.24042037351382256\n",
            "10018: Similarity Score for a dialog: 0.24038893829393243\n",
            "10047: Similarity Score for a dialog: 0.24039700215661544\n",
            "10060: Similarity Score for a dialog: 0.24043959677944096\n",
            "10069: Similarity Score for a dialog: 0.2406928881323404\n",
            "10098: Similarity Score for a dialog: 0.24076965245927118\n",
            "10123: Similarity Score for a dialog: 0.24068623059362185\n",
            "10152: Similarity Score for a dialog: 0.2408245226759479\n",
            "10177: Similarity Score for a dialog: 0.24072482395206873\n",
            "10206: Similarity Score for a dialog: 0.24072649521180947\n",
            "10235: Similarity Score for a dialog: 0.24068915325408669\n",
            "10264: Similarity Score for a dialog: 0.24078015939353326\n",
            "10289: Similarity Score for a dialog: 0.24073642531567405\n",
            "10318: Similarity Score for a dialog: 0.2407292224732893\n",
            "10343: Similarity Score for a dialog: 0.24081591777758443\n",
            "10372: Similarity Score for a dialog: 0.24093821721128292\n",
            "10397: Similarity Score for a dialog: 0.24099893639484046\n",
            "10426: Similarity Score for a dialog: 0.24113668392549237\n",
            "10451: Similarity Score for a dialog: 0.24114433739898333\n",
            "10480: Similarity Score for a dialog: 0.24168738769348835\n",
            "10505: Similarity Score for a dialog: 0.24195834624371002\n",
            "10534: Similarity Score for a dialog: 0.24196493383644244\n",
            "10559: Similarity Score for a dialog: 0.2420573434638218\n",
            "10588: Similarity Score for a dialog: 0.2418581251424162\n",
            "10613: Similarity Score for a dialog: 0.24170753853406543\n",
            "10646: Similarity Score for a dialog: 0.24189863613805573\n",
            "10675: Similarity Score for a dialog: 0.2420171178759667\n",
            "10704: Similarity Score for a dialog: 0.24247394751602344\n",
            "10729: Similarity Score for a dialog: 0.24250962247592814\n",
            "10758: Similarity Score for a dialog: 0.2423181432499709\n",
            "10783: Similarity Score for a dialog: 0.24226308180393227\n",
            "10812: Similarity Score for a dialog: 0.2423261454892637\n",
            "10837: Similarity Score for a dialog: 0.24240308190734225\n",
            "10866: Similarity Score for a dialog: 0.24263639970364984\n",
            "10891: Similarity Score for a dialog: 0.242481863766265\n",
            "10920: Similarity Score for a dialog: 0.24258184138922645\n",
            "10945: Similarity Score for a dialog: 0.24276667238401228\n",
            "10974: Similarity Score for a dialog: 0.24284432097712647\n",
            "10999: Similarity Score for a dialog: 0.2428353932315313\n",
            "11028: Similarity Score for a dialog: 0.2427410358561659\n",
            "11053: Similarity Score for a dialog: 0.24284137551078183\n",
            "11082: Similarity Score for a dialog: 0.24288223287199814\n",
            "11107: Similarity Score for a dialog: 0.24305385370414867\n",
            "11136: Similarity Score for a dialog: 0.2430112528713925\n",
            "11161: Similarity Score for a dialog: 0.2430457615000722\n",
            "11190: Similarity Score for a dialog: 0.24291294300339705\n",
            "11215: Similarity Score for a dialog: 0.24270711887078864\n",
            "11244: Similarity Score for a dialog: 0.24265084057868422\n",
            "11269: Similarity Score for a dialog: 0.24267324858345435\n",
            "11298: Similarity Score for a dialog: 0.24284951345701328\n",
            "11323: Similarity Score for a dialog: 0.24307348244679708\n",
            "11352: Similarity Score for a dialog: 0.24301229795385706\n",
            "11377: Similarity Score for a dialog: 0.24321816759057918\n",
            "11406: Similarity Score for a dialog: 0.24316120158241938\n",
            "11431: Similarity Score for a dialog: 0.24317632776936712\n",
            "11460: Similarity Score for a dialog: 0.24313448723696532\n",
            "11485: Similarity Score for a dialog: 0.24333570603971227\n",
            "11514: Similarity Score for a dialog: 0.24337317151524562\n",
            "11539: Similarity Score for a dialog: 0.24313782914346796\n",
            "11568: Similarity Score for a dialog: 0.24308903401596227\n",
            "11593: Similarity Score for a dialog: 0.24318573901787588\n",
            "11622: Similarity Score for a dialog: 0.24319425256680002\n",
            "11647: Similarity Score for a dialog: 0.24319654224297996\n",
            "11676: Similarity Score for a dialog: 0.24296023566441846\n",
            "11705: Similarity Score for a dialog: 0.24297109290435487\n",
            "11734: Similarity Score for a dialog: 0.24302304180292067\n",
            "11759: Similarity Score for a dialog: 0.24286546227349118\n",
            "11788: Similarity Score for a dialog: 0.2427475423595505\n",
            "11813: Similarity Score for a dialog: 0.24280487561245317\n",
            "11838: Similarity Score for a dialog: 0.24265903700601557\n",
            "11859: Similarity Score for a dialog: 0.242655387656909\n",
            "11880: Similarity Score for a dialog: 0.24251867684300077\n",
            "11901: Similarity Score for a dialog: 0.2426188588429056\n",
            "11910: Similarity Score for a dialog: 0.24269541792574148\n",
            "11919: Similarity Score for a dialog: 0.2427144115214951\n",
            "11948: Similarity Score for a dialog: 0.24260898431964245\n",
            "11973: Similarity Score for a dialog: 0.2426442453088511\n",
            "12002: Similarity Score for a dialog: 0.24258061248887922\n",
            "12027: Similarity Score for a dialog: 0.2424564742985771\n",
            "12056: Similarity Score for a dialog: 0.24283643667225968\n",
            "12081: Similarity Score for a dialog: 0.24289008280291877\n",
            "12110: Similarity Score for a dialog: 0.24308313418854632\n",
            "12139: Similarity Score for a dialog: 0.24333591808517693\n",
            "12168: Similarity Score for a dialog: 0.2431792017050773\n",
            "12193: Similarity Score for a dialog: 0.24283931501522615\n",
            "12222: Similarity Score for a dialog: 0.2427096246140286\n",
            "12247: Similarity Score for a dialog: 0.24275427306087385\n",
            "12264: Similarity Score for a dialog: 0.2430327271957909\n",
            "12281: Similarity Score for a dialog: 0.24343469259092362\n",
            "12310: Similarity Score for a dialog: 0.24348850542220224\n",
            "12335: Similarity Score for a dialog: 0.2437017287680558\n",
            "12356: Similarity Score for a dialog: 0.24382041319526954\n",
            "12377: Similarity Score for a dialog: 0.24359722870655465\n",
            "12406: Similarity Score for a dialog: 0.24373924330301933\n",
            "12431: Similarity Score for a dialog: 0.2437491487507064\n",
            "12460: Similarity Score for a dialog: 0.24412370918115125\n",
            "12485: Similarity Score for a dialog: 0.24440879013393074\n",
            "12494: Similarity Score for a dialog: 0.24437533522701907\n",
            "12499: Similarity Score for a dialog: 0.24441074643614172\n",
            "12516: Similarity Score for a dialog: 0.2443767648081203\n",
            "12533: Similarity Score for a dialog: 0.2443470894609674\n",
            "12562: Similarity Score for a dialog: 0.2445604420585924\n",
            "12587: Similarity Score for a dialog: 0.24479976245283766\n",
            "12616: Similarity Score for a dialog: 0.244924834787403\n",
            "12645: Similarity Score for a dialog: 0.2448870792143491\n",
            "12678: Similarity Score for a dialog: 0.24490675312667254\n",
            "12711: Similarity Score for a dialog: 0.2446994197906001\n",
            "12740: Similarity Score for a dialog: 0.24497371180701294\n",
            "12765: Similarity Score for a dialog: 0.2450194209121503\n",
            "12794: Similarity Score for a dialog: 0.24544111663142865\n",
            "12819: Similarity Score for a dialog: 0.2455445093360765\n",
            "12848: Similarity Score for a dialog: 0.2455699398403846\n",
            "12873: Similarity Score for a dialog: 0.2455743104243533\n",
            "12902: Similarity Score for a dialog: 0.2457016887267671\n",
            "12927: Similarity Score for a dialog: 0.2457831505925999\n",
            "12956: Similarity Score for a dialog: 0.24580375805968815\n",
            "12981: Similarity Score for a dialog: 0.2456006357993517\n",
            "12990: Similarity Score for a dialog: 0.24568074556270567\n",
            "12999: Similarity Score for a dialog: 0.24570896960544344\n",
            "13028: Similarity Score for a dialog: 0.24558107539970928\n",
            "13053: Similarity Score for a dialog: 0.24559028065292554\n",
            "13082: Similarity Score for a dialog: 0.24545956302059543\n",
            "13107: Similarity Score for a dialog: 0.24516361495219596\n",
            "13136: Similarity Score for a dialog: 0.24506543390334787\n",
            "13161: Similarity Score for a dialog: 0.2449217138335236\n",
            "13190: Similarity Score for a dialog: 0.24495586691450122\n",
            "13215: Similarity Score for a dialog: 0.24480772445127416\n",
            "13248: Similarity Score for a dialog: 0.24503639730900564\n",
            "13277: Similarity Score for a dialog: 0.24511552748949916\n",
            "13306: Similarity Score for a dialog: 0.2448953950076244\n",
            "13331: Similarity Score for a dialog: 0.24487174505430118\n",
            "13360: Similarity Score for a dialog: 0.24482448589641237\n",
            "13389: Similarity Score for a dialog: 0.24501942442130617\n",
            "13422: Similarity Score for a dialog: 0.2449284791410554\n",
            "13451: Similarity Score for a dialog: 0.24477520113561851\n",
            "13480: Similarity Score for a dialog: 0.24472263587769316\n",
            "13509: Similarity Score for a dialog: 0.24471816687653838\n",
            "13538: Similarity Score for a dialog: 0.24467053738245706\n",
            "13563: Similarity Score for a dialog: 0.24464050654857605\n",
            "13592: Similarity Score for a dialog: 0.24455735541118528\n",
            "13617: Similarity Score for a dialog: 0.24436715184159985\n",
            "13638: Similarity Score for a dialog: 0.24445966148071824\n",
            "13655: Similarity Score for a dialog: 0.2445320476705447\n",
            "13684: Similarity Score for a dialog: 0.24460484429240195\n",
            "13709: Similarity Score for a dialog: 0.2445320554758936\n",
            "13738: Similarity Score for a dialog: 0.2445994653661425\n",
            "13763: Similarity Score for a dialog: 0.24466295446989256\n",
            "13792: Similarity Score for a dialog: 0.24472309875187043\n",
            "13817: Similarity Score for a dialog: 0.24466046280544404\n",
            "13846: Similarity Score for a dialog: 0.2445777383568835\n",
            "13871: Similarity Score for a dialog: 0.24411499886289978\n",
            "13900: Similarity Score for a dialog: 0.24396598759096413\n",
            "13929: Similarity Score for a dialog: 0.2436530025498034\n",
            "13958: Similarity Score for a dialog: 0.2434917023307384\n",
            "13983: Similarity Score for a dialog: 0.24345541578748306\n",
            "14012: Similarity Score for a dialog: 0.2434743705306056\n",
            "14037: Similarity Score for a dialog: 0.24335375879733787\n",
            "14066: Similarity Score for a dialog: 0.24332518450054222\n",
            "14091: Similarity Score for a dialog: 0.24309402493501364\n",
            "14120: Similarity Score for a dialog: 0.2431274015339199\n",
            "14145: Similarity Score for a dialog: 0.24294064485159905\n",
            "14174: Similarity Score for a dialog: 0.24287047154761074\n",
            "14199: Similarity Score for a dialog: 0.2427593381974133\n",
            "14228: Similarity Score for a dialog: 0.24310639445564228\n",
            "14253: Similarity Score for a dialog: 0.24308098362914882\n",
            "14282: Similarity Score for a dialog: 0.24298291636985941\n",
            "14307: Similarity Score for a dialog: 0.24287652286394978\n",
            "14336: Similarity Score for a dialog: 0.2428610706765423\n",
            "14361: Similarity Score for a dialog: 0.24274643403342994\n",
            "14390: Similarity Score for a dialog: 0.2428317372975421\n",
            "14415: Similarity Score for a dialog: 0.24286440794415065\n",
            "14444: Similarity Score for a dialog: 0.24273110703908723\n",
            "14473: Similarity Score for a dialog: 0.24264705995004543\n",
            "14502: Similarity Score for a dialog: 0.24273711980182044\n",
            "14527: Similarity Score for a dialog: 0.24270036643875342\n",
            "14556: Similarity Score for a dialog: 0.24269707481080413\n",
            "14581: Similarity Score for a dialog: 0.24274896991925962\n",
            "14610: Similarity Score for a dialog: 0.24288473500912622\n",
            "14635: Similarity Score for a dialog: 0.2427640537522358\n",
            "14664: Similarity Score for a dialog: 0.24265974309211352\n",
            "14689: Similarity Score for a dialog: 0.24254145861539192\n",
            "14718: Similarity Score for a dialog: 0.2424871301781797\n",
            "14743: Similarity Score for a dialog: 0.24239992467228497\n",
            "14772: Similarity Score for a dialog: 0.24246710827668605\n",
            "14797: Similarity Score for a dialog: 0.2424151877136495\n",
            "14826: Similarity Score for a dialog: 0.24244308880747834\n",
            "14851: Similarity Score for a dialog: 0.24236786718970485\n",
            "14880: Similarity Score for a dialog: 0.24251845736746916\n",
            "14909: Similarity Score for a dialog: 0.24256209901971862\n",
            "14938: Similarity Score for a dialog: 0.2424668180704372\n",
            "14963: Similarity Score for a dialog: 0.24226602736552233\n",
            "14996: Similarity Score for a dialog: 0.24226695697151318\n",
            "15025: Similarity Score for a dialog: 0.2423384468779329\n",
            "15034: Similarity Score for a dialog: 0.24239197772463983\n",
            "15043: Similarity Score for a dialog: 0.24247607919176206\n",
            "15060: Similarity Score for a dialog: 0.24247538915890923\n",
            "15077: Similarity Score for a dialog: 0.24252847023141144\n",
            "15106: Similarity Score for a dialog: 0.24265582454792456\n",
            "15131: Similarity Score for a dialog: 0.24273221977452197\n",
            "15159: Similarity Score for a dialog: 0.2423661393319287\n",
            "15184: Similarity Score for a dialog: 0.24214150291740416\n",
            "15213: Similarity Score for a dialog: 0.24217854237984024\n",
            "15238: Similarity Score for a dialog: 0.24224149432286166\n",
            "15267: Similarity Score for a dialog: 0.24235033919877852\n",
            "15292: Similarity Score for a dialog: 0.24242804360866982\n",
            "15321: Similarity Score for a dialog: 0.24260017884636126\n",
            "15350: Similarity Score for a dialog: 0.24261002942357604\n",
            "15379: Similarity Score for a dialog: 0.2429139342051912\n",
            "15404: Similarity Score for a dialog: 0.2430297118308155\n",
            "15433: Similarity Score for a dialog: 0.24298422344814094\n",
            "15458: Similarity Score for a dialog: 0.24282876064754647\n",
            "15487: Similarity Score for a dialog: 0.24291672879400508\n",
            "15512: Similarity Score for a dialog: 0.24290286261262983\n",
            "15541: Similarity Score for a dialog: 0.2430323049006809\n",
            "15566: Similarity Score for a dialog: 0.24311868334873854\n",
            "15595: Similarity Score for a dialog: 0.2430770641744827\n",
            "15624: Similarity Score for a dialog: 0.24298998144714257\n",
            "15645: Similarity Score for a dialog: 0.24292519724320588\n",
            "15662: Similarity Score for a dialog: 0.24281679011904617\n",
            "15691: Similarity Score for a dialog: 0.24285556423052929\n",
            "15716: Similarity Score for a dialog: 0.2429450485598586\n",
            "15745: Similarity Score for a dialog: 0.24282747774785954\n",
            "15770: Similarity Score for a dialog: 0.24268400319317326\n",
            "15799: Similarity Score for a dialog: 0.24275769743795073\n",
            "15824: Similarity Score for a dialog: 0.24271115601718768\n",
            "15853: Similarity Score for a dialog: 0.24252915019868737\n",
            "15878: Similarity Score for a dialog: 0.2424692956375527\n",
            "15907: Similarity Score for a dialog: 0.24223719128360752\n",
            "15932: Similarity Score for a dialog: 0.24225127231901417\n",
            "15961: Similarity Score for a dialog: 0.24232463424107378\n",
            "15986: Similarity Score for a dialog: 0.2424105803018634\n",
            "16015: Similarity Score for a dialog: 0.2425038569950278\n",
            "16040: Similarity Score for a dialog: 0.24243943025425682\n",
            "16069: Similarity Score for a dialog: 0.24242728564087895\n",
            "16094: Similarity Score for a dialog: 0.24237602208792788\n",
            "16123: Similarity Score for a dialog: 0.2423060940962985\n",
            "16148: Similarity Score for a dialog: 0.24225924520923747\n",
            "16177: Similarity Score for a dialog: 0.24203119882767488\n",
            "16202: Similarity Score for a dialog: 0.24186431354623705\n",
            "16231: Similarity Score for a dialog: 0.24188033528518504\n",
            "16256: Similarity Score for a dialog: 0.24207983131728364\n",
            "16285: Similarity Score for a dialog: 0.2421138051523854\n",
            "16310: Similarity Score for a dialog: 0.24201049268234764\n",
            "16339: Similarity Score for a dialog: 0.2420896791393517\n",
            "16364: Similarity Score for a dialog: 0.2419861305434217\n",
            "16381: Similarity Score for a dialog: 0.2420896265914378\n",
            "16398: Similarity Score for a dialog: 0.24227570262820303\n",
            "16427: Similarity Score for a dialog: 0.24219404948896073\n",
            "16452: Similarity Score for a dialog: 0.2423402267244809\n",
            "16481: Similarity Score for a dialog: 0.24227205361298407\n",
            "16506: Similarity Score for a dialog: 0.24227840885948776\n",
            "16539: Similarity Score for a dialog: 0.24201065774973732\n",
            "16572: Similarity Score for a dialog: 0.2420008804131999\n",
            "16601: Similarity Score for a dialog: 0.2419621910285611\n",
            "16630: Similarity Score for a dialog: 0.24175199894180932\n",
            "16659: Similarity Score for a dialog: 0.24167082320641453\n",
            "16684: Similarity Score for a dialog: 0.2417034532947535\n",
            "16713: Similarity Score for a dialog: 0.2416898042198495\n",
            "16738: Similarity Score for a dialog: 0.24159575127769578\n",
            "16767: Similarity Score for a dialog: 0.2416356209894149\n",
            "16792: Similarity Score for a dialog: 0.24153055002257376\n",
            "16821: Similarity Score for a dialog: 0.2415302769378434\n",
            "16846: Similarity Score for a dialog: 0.2413916979257704\n",
            "16875: Similarity Score for a dialog: 0.24146865530114084\n",
            "16900: Similarity Score for a dialog: 0.24158166681249607\n",
            "16929: Similarity Score for a dialog: 0.2414506198065685\n",
            "16954: Similarity Score for a dialog: 0.24120920910714874\n",
            "16983: Similarity Score for a dialog: 0.2412153991299602\n",
            "17008: Similarity Score for a dialog: 0.2412219965544007\n",
            "17037: Similarity Score for a dialog: 0.24128810623440022\n",
            "17062: Similarity Score for a dialog: 0.24128384695104438\n",
            "17091: Similarity Score for a dialog: 0.2412534970550911\n",
            "17116: Similarity Score for a dialog: 0.2412847975918146\n",
            "17145: Similarity Score for a dialog: 0.24117907781291809\n",
            "17170: Similarity Score for a dialog: 0.2411026413112534\n",
            "17199: Similarity Score for a dialog: 0.24141123624538344\n",
            "17224: Similarity Score for a dialog: 0.24169626932554747\n",
            "17253: Similarity Score for a dialog: 0.24162602390219254\n",
            "17278: Similarity Score for a dialog: 0.24162549311880593\n",
            "17307: Similarity Score for a dialog: 0.24166195686039074\n",
            "17332: Similarity Score for a dialog: 0.2416330703212192\n",
            "17361: Similarity Score for a dialog: 0.2414145590738578\n",
            "17390: Similarity Score for a dialog: 0.24117152801239927\n",
            "17419: Similarity Score for a dialog: 0.24103956535110152\n",
            "17444: Similarity Score for a dialog: 0.2412101343987938\n",
            "17473: Similarity Score for a dialog: 0.241292916397225\n",
            "17498: Similarity Score for a dialog: 0.2413516142609331\n",
            "17527: Similarity Score for a dialog: 0.24133430101016778\n",
            "17552: Similarity Score for a dialog: 0.24118242429420547\n",
            "17581: Similarity Score for a dialog: 0.24114215400103017\n",
            "17606: Similarity Score for a dialog: 0.2411076997900287\n",
            "17635: Similarity Score for a dialog: 0.24111430344427617\n",
            "17660: Similarity Score for a dialog: 0.2410271020256585\n",
            "17689: Similarity Score for a dialog: 0.24098651851879022\n",
            "17714: Similarity Score for a dialog: 0.24095480272214748\n",
            "17743: Similarity Score for a dialog: 0.240755444603005\n",
            "17768: Similarity Score for a dialog: 0.24066924598728867\n",
            "17797: Similarity Score for a dialog: 0.24079966102013622\n",
            "17822: Similarity Score for a dialog: 0.24080774320325155\n",
            "17851: Similarity Score for a dialog: 0.24067744061313917\n",
            "17876: Similarity Score for a dialog: 0.24059514629945516\n",
            "17905: Similarity Score for a dialog: 0.2404831530741201\n",
            "17930: Similarity Score for a dialog: 0.24045257001108497\n",
            "17959: Similarity Score for a dialog: 0.24051806758439723\n",
            "17984: Similarity Score for a dialog: 0.2404941690620035\n",
            "18013: Similarity Score for a dialog: 0.240533495541631\n",
            "18038: Similarity Score for a dialog: 0.24060369909245782\n",
            "18067: Similarity Score for a dialog: 0.24061503558001815\n",
            "18092: Similarity Score for a dialog: 0.240692961160689\n",
            "18121: Similarity Score for a dialog: 0.24062591114858792\n",
            "18146: Similarity Score for a dialog: 0.24053993055766085\n",
            "18175: Similarity Score for a dialog: 0.24049815789537568\n",
            "18200: Similarity Score for a dialog: 0.2402863575365551\n",
            "18229: Similarity Score for a dialog: 0.24033137983586184\n",
            "18254: Similarity Score for a dialog: 0.24030002620585342\n",
            "18283: Similarity Score for a dialog: 0.24026522745546183\n",
            "18308: Similarity Score for a dialog: 0.24027725091849622\n",
            "18337: Similarity Score for a dialog: 0.2403446172711902\n",
            "18362: Similarity Score for a dialog: 0.24043936605847832\n",
            "18391: Similarity Score for a dialog: 0.24046641535334273\n",
            "18416: Similarity Score for a dialog: 0.24057710779840494\n",
            "18445: Similarity Score for a dialog: 0.24048661902858234\n",
            "18470: Similarity Score for a dialog: 0.24036938989703657\n",
            "18499: Similarity Score for a dialog: 0.24029001179218268\n",
            "18524: Similarity Score for a dialog: 0.240184772556446\n",
            "18553: Similarity Score for a dialog: 0.24034813022822474\n",
            "18578: Similarity Score for a dialog: 0.24039562901978415\n",
            "18607: Similarity Score for a dialog: 0.24026268359770572\n",
            "18632: Similarity Score for a dialog: 0.24027154015494878\n",
            "18661: Similarity Score for a dialog: 0.24016898832104294\n",
            "18686: Similarity Score for a dialog: 0.2401101612562674\n",
            "18715: Similarity Score for a dialog: 0.2401264869820628\n",
            "18740: Similarity Score for a dialog: 0.2401545978488101\n",
            "18769: Similarity Score for a dialog: 0.24009841198008322\n",
            "18794: Similarity Score for a dialog: 0.2401463149278533\n",
            "18803: Similarity Score for a dialog: 0.2401788916236231\n",
            "18812: Similarity Score for a dialog: 0.24021366320427254\n",
            "18841: Similarity Score for a dialog: 0.24012221582161916\n",
            "18866: Similarity Score for a dialog: 0.24024045128134142\n",
            "18895: Similarity Score for a dialog: 0.2402254248420841\n",
            "18924: Similarity Score for a dialog: 0.24029083177048005\n",
            "18953: Similarity Score for a dialog: 0.2402950845103666\n",
            "18978: Similarity Score for a dialog: 0.2403987607887234\n",
            "19007: Similarity Score for a dialog: 0.24031390457125804\n",
            "19032: Similarity Score for a dialog: 0.24033711305623984\n",
            "19061: Similarity Score for a dialog: 0.2402804299459777\n",
            "19086: Similarity Score for a dialog: 0.24024847672065688\n",
            "19115: Similarity Score for a dialog: 0.24016657666445937\n",
            "19140: Similarity Score for a dialog: 0.24013218355739652\n",
            "19169: Similarity Score for a dialog: 0.24032635999914637\n",
            "19194: Similarity Score for a dialog: 0.24031462352074814\n",
            "19223: Similarity Score for a dialog: 0.2401874789283752\n",
            "19252: Similarity Score for a dialog: 0.24010653925539496\n",
            "19281: Similarity Score for a dialog: 0.2400219774266193\n",
            "19306: Similarity Score for a dialog: 0.24000869484162682\n",
            "19335: Similarity Score for a dialog: 0.2399448528878827\n",
            "19360: Similarity Score for a dialog: 0.24000367077184384\n",
            "19389: Similarity Score for a dialog: 0.24022015274748157\n",
            "19414: Similarity Score for a dialog: 0.24035592611363749\n",
            "19443: Similarity Score for a dialog: 0.24026151922940586\n",
            "19468: Similarity Score for a dialog: 0.2402406517464208\n",
            "19497: Similarity Score for a dialog: 0.2402484418576479\n",
            "19522: Similarity Score for a dialog: 0.24021830535202007\n",
            "19551: Similarity Score for a dialog: 0.24019458549212927\n",
            "19576: Similarity Score for a dialog: 0.24017385820770096\n",
            "19605: Similarity Score for a dialog: 0.24004733694054373\n",
            "19630: Similarity Score for a dialog: 0.2399222554413751\n",
            "19659: Similarity Score for a dialog: 0.23993842836636953\n",
            "19684: Similarity Score for a dialog: 0.24009628195712854\n",
            "19713: Similarity Score for a dialog: 0.24002441568733554\n",
            "19742: Similarity Score for a dialog: 0.24013461063928054\n",
            "19771: Similarity Score for a dialog: 0.24001026307378687\n",
            "19796: Similarity Score for a dialog: 0.24011888859086733\n",
            "19825: Similarity Score for a dialog: 0.24010933632776726\n",
            "19850: Similarity Score for a dialog: 0.24014196641712146\n",
            "19879: Similarity Score for a dialog: 0.2401855361432811\n",
            "19904: Similarity Score for a dialog: 0.24023830891127604\n",
            "19933: Similarity Score for a dialog: 0.24033267506014536\n",
            "19958: Similarity Score for a dialog: 0.24037767291440434\n",
            "19987: Similarity Score for a dialog: 0.24034533053905052\n",
            "20012: Similarity Score for a dialog: 0.24033520143414244\n",
            "20041: Similarity Score for a dialog: 0.2403456604084337\n",
            "20066: Similarity Score for a dialog: 0.2403426309343174\n",
            "20095: Similarity Score for a dialog: 0.2403410713904196\n",
            "20120: Similarity Score for a dialog: 0.2403574955509186\n",
            "20149: Similarity Score for a dialog: 0.24033816615072462\n",
            "20174: Similarity Score for a dialog: 0.24024840522386368\n",
            "20203: Similarity Score for a dialog: 0.24032370222329094\n",
            "20232: Similarity Score for a dialog: 0.2404919995626317\n",
            "20261: Similarity Score for a dialog: 0.24049183425691467\n",
            "20286: Similarity Score for a dialog: 0.24043072752393455\n",
            "20315: Similarity Score for a dialog: 0.24044017129592574\n",
            "20340: Similarity Score for a dialog: 0.24038085144526164\n",
            "20349: Similarity Score for a dialog: 0.24044638761215253\n",
            "20354: Similarity Score for a dialog: 0.24045498611239674\n",
            "20383: Similarity Score for a dialog: 0.24045020165118638\n",
            "20408: Similarity Score for a dialog: 0.24045647521537447\n",
            "20417: Similarity Score for a dialog: 0.24047576485877195\n",
            "20422: Similarity Score for a dialog: 0.24051828143178708\n",
            "20451: Similarity Score for a dialog: 0.24040184521470656\n",
            "20480: Similarity Score for a dialog: 0.24022023928108935\n",
            "20509: Similarity Score for a dialog: 0.24023993599638954\n",
            "20538: Similarity Score for a dialog: 0.24036004711911377\n",
            "20567: Similarity Score for a dialog: 0.24022063281647665\n",
            "20596: Similarity Score for a dialog: 0.24022352325809987\n",
            "20625: Similarity Score for a dialog: 0.24017872790026668\n",
            "20650: Similarity Score for a dialog: 0.24026126951689875\n",
            "20679: Similarity Score for a dialog: 0.24014469934997507\n",
            "20708: Similarity Score for a dialog: 0.2401329481577819\n",
            "20737: Similarity Score for a dialog: 0.24018716403703808\n",
            "20762: Similarity Score for a dialog: 0.24028688497175996\n",
            "20791: Similarity Score for a dialog: 0.24021562612809358\n",
            "20820: Similarity Score for a dialog: 0.24017461344165517\n",
            "20849: Similarity Score for a dialog: 0.24020534275160782\n",
            "20878: Similarity Score for a dialog: 0.24006627284742235\n",
            "20907: Similarity Score for a dialog: 0.23993773285331974\n",
            "20932: Similarity Score for a dialog: 0.23980940623573374\n",
            "20961: Similarity Score for a dialog: 0.23972586745764313\n",
            "20986: Similarity Score for a dialog: 0.23973660053504176\n",
            "21007: Similarity Score for a dialog: 0.23973076974340948\n",
            "21028: Similarity Score for a dialog: 0.2396971730584556\n",
            "21057: Similarity Score for a dialog: 0.2398018333738413\n",
            "21082: Similarity Score for a dialog: 0.23972142201764515\n",
            "21107: Similarity Score for a dialog: 0.23957107730334481\n",
            "21132: Similarity Score for a dialog: 0.23953589621019428\n",
            "21153: Similarity Score for a dialog: 0.2394500673721716\n",
            "21170: Similarity Score for a dialog: 0.2393587785161158\n",
            "21199: Similarity Score for a dialog: 0.23927462100137256\n",
            "21224: Similarity Score for a dialog: 0.23929320704943224\n",
            "21253: Similarity Score for a dialog: 0.23937073958225033\n",
            "21278: Similarity Score for a dialog: 0.23938172224519894\n",
            "21307: Similarity Score for a dialog: 0.23931504770478873\n",
            "21332: Similarity Score for a dialog: 0.23939018339671264\n",
            "21361: Similarity Score for a dialog: 0.23930382042506293\n",
            "21386: Similarity Score for a dialog: 0.23920701927018936\n",
            "21415: Similarity Score for a dialog: 0.2391774528092166\n",
            "21440: Similarity Score for a dialog: 0.2391896955708236\n",
            "21477: Similarity Score for a dialog: 0.23922532121258464\n",
            "21514: Similarity Score for a dialog: 0.23944182504519418\n",
            "21543: Similarity Score for a dialog: 0.23952591459108744\n",
            "21568: Similarity Score for a dialog: 0.23944763746072162\n",
            "21597: Similarity Score for a dialog: 0.2396347454411845\n",
            "21626: Similarity Score for a dialog: 0.2395936834301872\n",
            "21655: Similarity Score for a dialog: 0.23962305985873353\n",
            "21680: Similarity Score for a dialog: 0.2395421216748973\n",
            "21709: Similarity Score for a dialog: 0.23940491518476031\n",
            "21734: Similarity Score for a dialog: 0.23928939545263073\n",
            "21763: Similarity Score for a dialog: 0.23920738398654007\n",
            "21788: Similarity Score for a dialog: 0.2392478658425493\n",
            "21817: Similarity Score for a dialog: 0.23922639915858687\n",
            "21846: Similarity Score for a dialog: 0.23931366002310045\n",
            "21875: Similarity Score for a dialog: 0.23932728098085296\n",
            "21904: Similarity Score for a dialog: 0.23938908174502196\n",
            "21933: Similarity Score for a dialog: 0.23937539459231424\n",
            "21958: Similarity Score for a dialog: 0.23935193275832883\n",
            "21987: Similarity Score for a dialog: 0.23921164549325918\n",
            "22016: Similarity Score for a dialog: 0.2390986732432062\n",
            "22045: Similarity Score for a dialog: 0.23909707313177994\n",
            "22074: Similarity Score for a dialog: 0.2389886648342125\n",
            "22103: Similarity Score for a dialog: 0.2390047221167324\n",
            "22128: Similarity Score for a dialog: 0.23890584260206912\n",
            "22157: Similarity Score for a dialog: 0.23887863450609687\n",
            "22182: Similarity Score for a dialog: 0.23885672430231691\n",
            "22211: Similarity Score for a dialog: 0.2389675533856623\n",
            "22236: Similarity Score for a dialog: 0.23896364028987252\n",
            "22265: Similarity Score for a dialog: 0.23897276774095963\n",
            "22290: Similarity Score for a dialog: 0.23898191249898254\n",
            "22319: Similarity Score for a dialog: 0.23881300645047157\n",
            "22344: Similarity Score for a dialog: 0.23903743902487834\n",
            "22377: Similarity Score for a dialog: 0.23924858174205407\n",
            "22410: Similarity Score for a dialog: 0.23931887118113496\n",
            "22439: Similarity Score for a dialog: 0.23931893929427342\n",
            "22468: Similarity Score for a dialog: 0.23950447281619952\n",
            "22481: Similarity Score for a dialog: 0.23953167977962583\n",
            "22494: Similarity Score for a dialog: 0.23959883621613753\n",
            "22523: Similarity Score for a dialog: 0.23969238374576124\n",
            "22552: Similarity Score for a dialog: 0.2398742458428239\n",
            "22581: Similarity Score for a dialog: 0.23994793356738556\n",
            "22606: Similarity Score for a dialog: 0.240078756610697\n",
            "22635: Similarity Score for a dialog: 0.24007633256782795\n",
            "22660: Similarity Score for a dialog: 0.2401341566326887\n",
            "22689: Similarity Score for a dialog: 0.24023367303989993\n",
            "22714: Similarity Score for a dialog: 0.2403699301111157\n",
            "22743: Similarity Score for a dialog: 0.24028597297682677\n",
            "22768: Similarity Score for a dialog: 0.24024759918629474\n",
            "22797: Similarity Score for a dialog: 0.24021178865801018\n",
            "22822: Similarity Score for a dialog: 0.24032880981406757\n",
            "22831: Similarity Score for a dialog: 0.24036744290228176\n",
            "22836: Similarity Score for a dialog: 0.2403913956080114\n",
            "22865: Similarity Score for a dialog: 0.240384397494769\n",
            "22894: Similarity Score for a dialog: 0.24047409508501028\n",
            "22923: Similarity Score for a dialog: 0.24052292981845058\n",
            "22948: Similarity Score for a dialog: 0.24058162753599066\n",
            "22977: Similarity Score for a dialog: 0.2405923100725196\n",
            "23006: Similarity Score for a dialog: 0.24062113419076628\n",
            "23035: Similarity Score for a dialog: 0.24056413425042644\n",
            "23060: Similarity Score for a dialog: 0.24062613662023646\n",
            "23089: Similarity Score for a dialog: 0.24062927906909068\n",
            "23114: Similarity Score for a dialog: 0.24069013952456456\n",
            "23143: Similarity Score for a dialog: 0.2408318395632045\n",
            "23168: Similarity Score for a dialog: 0.24087950145772397\n",
            "23197: Similarity Score for a dialog: 0.24080051837813535\n",
            "23222: Similarity Score for a dialog: 0.24066590959524395\n",
            "23251: Similarity Score for a dialog: 0.24061165375443896\n",
            "23276: Similarity Score for a dialog: 0.24059424651491226\n",
            "23309: Similarity Score for a dialog: 0.2405554585189326\n",
            "23338: Similarity Score for a dialog: 0.24060780872502024\n",
            "23367: Similarity Score for a dialog: 0.2405734607453352\n",
            "23392: Similarity Score for a dialog: 0.24060874407162428\n",
            "23425: Similarity Score for a dialog: 0.2405052180098073\n",
            "23454: Similarity Score for a dialog: 0.24049986502066956\n",
            "23483: Similarity Score for a dialog: 0.24048269511311027\n",
            "23508: Similarity Score for a dialog: 0.2404532058999965\n",
            "23541: Similarity Score for a dialog: 0.24036065467194223\n",
            "23574: Similarity Score for a dialog: 0.2403538287466483\n",
            "23603: Similarity Score for a dialog: 0.2403548493847014\n",
            "23632: Similarity Score for a dialog: 0.24037873599516144\n",
            "23661: Similarity Score for a dialog: 0.24050171858903366\n",
            "23690: Similarity Score for a dialog: 0.2406092932720483\n",
            "23719: Similarity Score for a dialog: 0.24068585207393625\n",
            "23744: Similarity Score for a dialog: 0.2408443790257676\n",
            "23773: Similarity Score for a dialog: 0.24084041211864082\n",
            "23798: Similarity Score for a dialog: 0.24093125225199438\n",
            "23827: Similarity Score for a dialog: 0.24081157726607533\n",
            "23852: Similarity Score for a dialog: 0.2408138385865942\n",
            "23873: Similarity Score for a dialog: 0.24079354172060502\n",
            "23890: Similarity Score for a dialog: 0.24075488999304182\n",
            "23919: Similarity Score for a dialog: 0.24077410970980545\n",
            "23944: Similarity Score for a dialog: 0.2408311294357045\n",
            "23973: Similarity Score for a dialog: 0.24071252015878258\n",
            "23998: Similarity Score for a dialog: 0.24063171687988608\n",
            "24027: Similarity Score for a dialog: 0.24071079620557298\n",
            "24056: Similarity Score for a dialog: 0.24089499589933633\n",
            "24069: Similarity Score for a dialog: 0.24094402638922816\n",
            "24078: Similarity Score for a dialog: 0.24093933489508174\n",
            "24115: Similarity Score for a dialog: 0.24079833322338637\n",
            "24148: Similarity Score for a dialog: 0.2406793926212404\n",
            "24177: Similarity Score for a dialog: 0.24058065450209798\n",
            "24206: Similarity Score for a dialog: 0.2404935362179109\n",
            "24235: Similarity Score for a dialog: 0.240472263073361\n",
            "24260: Similarity Score for a dialog: 0.24047929385068714\n",
            "24289: Similarity Score for a dialog: 0.24059808321387605\n",
            "24314: Similarity Score for a dialog: 0.24054742614129979\n",
            "24343: Similarity Score for a dialog: 0.240433861768363\n",
            "24372: Similarity Score for a dialog: 0.24047439254070552\n",
            "24401: Similarity Score for a dialog: 0.24043946881640252\n",
            "24426: Similarity Score for a dialog: 0.24033036322582085\n",
            "24455: Similarity Score for a dialog: 0.2404225998354778\n",
            "24480: Similarity Score for a dialog: 0.2404911626030247\n",
            "24509: Similarity Score for a dialog: 0.24060213991382506\n",
            "24534: Similarity Score for a dialog: 0.24056215067467504\n",
            "24563: Similarity Score for a dialog: 0.24081192005100005\n",
            "24588: Similarity Score for a dialog: 0.2408996672255547\n",
            "24601: Similarity Score for a dialog: 0.24098061755189176\n",
            "24610: Similarity Score for a dialog: 0.2410596697611317\n",
            "24639: Similarity Score for a dialog: 0.24111877512730023\n",
            "24668: Similarity Score for a dialog: 0.24125494339670608\n",
            "24697: Similarity Score for a dialog: 0.2413598473040826\n",
            "24722: Similarity Score for a dialog: 0.2414299487425041\n",
            "24751: Similarity Score for a dialog: 0.2414672488228911\n",
            "24776: Similarity Score for a dialog: 0.24157585200688367\n",
            "24805: Similarity Score for a dialog: 0.24149144050785773\n",
            "24830: Similarity Score for a dialog: 0.2415035319640408\n",
            "24859: Similarity Score for a dialog: 0.24146322465520895\n",
            "24884: Similarity Score for a dialog: 0.24141860940352436\n",
            "24913: Similarity Score for a dialog: 0.24124727307882277\n",
            "24938: Similarity Score for a dialog: 0.241167724176463\n",
            "24967: Similarity Score for a dialog: 0.24112337179430313\n",
            "24992: Similarity Score for a dialog: 0.24107870003316254\n",
            "25021: Similarity Score for a dialog: 0.24096055680902168\n",
            "25046: Similarity Score for a dialog: 0.24092813180389092\n",
            "25075: Similarity Score for a dialog: 0.24098725537793445\n",
            "25100: Similarity Score for a dialog: 0.24100923695482523\n",
            "25117: Similarity Score for a dialog: 0.24090831498227522\n",
            "25134: Similarity Score for a dialog: 0.2408802265982883\n",
            "25163: Similarity Score for a dialog: 0.2409179343658831\n",
            "25188: Similarity Score for a dialog: 0.24097779223846902\n",
            "25217: Similarity Score for a dialog: 0.24095196979646874\n",
            "25242: Similarity Score for a dialog: 0.2408869353303119\n",
            "25271: Similarity Score for a dialog: 0.24081169383849474\n",
            "25300: Similarity Score for a dialog: 0.2408614412341123\n",
            "25329: Similarity Score for a dialog: 0.2408577460907831\n",
            "25354: Similarity Score for a dialog: 0.24096664617306432\n",
            "25383: Similarity Score for a dialog: 0.24097046107220882\n",
            "25412: Similarity Score for a dialog: 0.24109578154944394\n",
            "25441: Similarity Score for a dialog: 0.24110981688128622\n",
            "25466: Similarity Score for a dialog: 0.24120396399992672\n",
            "25495: Similarity Score for a dialog: 0.24125122088253795\n",
            "25520: Similarity Score for a dialog: 0.24123156702759985\n",
            "25549: Similarity Score for a dialog: 0.24118732932729248\n",
            "25574: Similarity Score for a dialog: 0.24111424028450348\n",
            "25603: Similarity Score for a dialog: 0.2409875389493799\n",
            "25628: Similarity Score for a dialog: 0.24097002274175386\n",
            "25657: Similarity Score for a dialog: 0.24097932926429727\n",
            "25682: Similarity Score for a dialog: 0.24091071934366987\n",
            "25691: Similarity Score for a dialog: 0.24093032313037724\n",
            "25700: Similarity Score for a dialog: 0.24095024920872019\n",
            "25721: Similarity Score for a dialog: 0.24089484332092756\n",
            "25738: Similarity Score for a dialog: 0.2409564081581479\n",
            "25759: Similarity Score for a dialog: 0.24099145442314623\n",
            "25776: Similarity Score for a dialog: 0.2410473843517213\n",
            "25805: Similarity Score for a dialog: 0.2410777962100939\n",
            "25830: Similarity Score for a dialog: 0.24107987934818442\n",
            "25843: Similarity Score for a dialog: 0.24111230321106616\n",
            "25856: Similarity Score for a dialog: 0.24114935764857837\n",
            "25885: Similarity Score for a dialog: 0.24099768968971852\n",
            "25910: Similarity Score for a dialog: 0.24093713974393993\n",
            "25939: Similarity Score for a dialog: 0.24088061776115993\n",
            "25964: Similarity Score for a dialog: 0.24077104770423546\n",
            "25993: Similarity Score for a dialog: 0.2407017568522148\n",
            "26022: Similarity Score for a dialog: 0.24070965454565268\n",
            "26051: Similarity Score for a dialog: 0.2407195632460419\n",
            "26076: Similarity Score for a dialog: 0.24063151531573296\n",
            "26109: Similarity Score for a dialog: 0.24060208466772254\n",
            "26138: Similarity Score for a dialog: 0.24055587434531245\n",
            "26167: Similarity Score for a dialog: 0.24052910135391897\n",
            "26192: Similarity Score for a dialog: 0.2406135108817077\n",
            "26201: Similarity Score for a dialog: 0.2406428567601945\n",
            "26206: Similarity Score for a dialog: 0.24064761628251913\n",
            "26235: Similarity Score for a dialog: 0.24071912613259158\n",
            "26260: Similarity Score for a dialog: 0.24066372992362708\n",
            "26269: Similarity Score for a dialog: 0.24061696992747958\n",
            "26278: Similarity Score for a dialog: 0.2405915879533494\n",
            "26307: Similarity Score for a dialog: 0.24059535526268705\n",
            "26335: Similarity Score for a dialog: 0.24069004694002094\n",
            "26356: Similarity Score for a dialog: 0.24065419070701238\n",
            "26373: Similarity Score for a dialog: 0.2406745888841901\n",
            "26402: Similarity Score for a dialog: 0.2406295363879268\n",
            "26427: Similarity Score for a dialog: 0.24057507189778787\n",
            "26440: Similarity Score for a dialog: 0.24059627728458322\n",
            "26453: Similarity Score for a dialog: 0.24060160626008253\n",
            "26474: Similarity Score for a dialog: 0.24063792457979283\n",
            "26491: Similarity Score for a dialog: 0.24065149211276773\n",
            "26520: Similarity Score for a dialog: 0.24052359749026658\n",
            "26545: Similarity Score for a dialog: 0.2404541157620673\n",
            "26574: Similarity Score for a dialog: 0.24027092017157403\n",
            "26599: Similarity Score for a dialog: 0.2401841056606944\n",
            "26628: Similarity Score for a dialog: 0.2402276841942148\n",
            "26657: Similarity Score for a dialog: 0.24016408378012544\n",
            "26670: Similarity Score for a dialog: 0.24016921935372734\n",
            "26683: Similarity Score for a dialog: 0.24020552531862246\n",
            "26712: Similarity Score for a dialog: 0.2401958001534907\n",
            "26741: Similarity Score for a dialog: 0.24032074442398924\n",
            "26770: Similarity Score for a dialog: 0.24017095855220238\n",
            "26799: Similarity Score for a dialog: 0.2400424480539086\n",
            "26812: Similarity Score for a dialog: 0.24005945499613968\n",
            "26825: Similarity Score for a dialog: 0.24002958976054614\n",
            "26854: Similarity Score for a dialog: 0.24016297299429737\n",
            "26879: Similarity Score for a dialog: 0.2402692310531889\n",
            "26908: Similarity Score for a dialog: 0.240210458687804\n",
            "26933: Similarity Score for a dialog: 0.24015913635837488\n",
            "26962: Similarity Score for a dialog: 0.2401405436535478\n",
            "26987: Similarity Score for a dialog: 0.24012015835268466\n",
            "27016: Similarity Score for a dialog: 0.2402192932666114\n",
            "27041: Similarity Score for a dialog: 0.24035527642739926\n",
            "27070: Similarity Score for a dialog: 0.24038289533824253\n",
            "27095: Similarity Score for a dialog: 0.24039023957201378\n",
            "27124: Similarity Score for a dialog: 0.24039105981645367\n",
            "27149: Similarity Score for a dialog: 0.24041544694806738\n",
            "27162: Similarity Score for a dialog: 0.2403747554056049\n",
            "27171: Similarity Score for a dialog: 0.2403665615216819\n",
            "27200: Similarity Score for a dialog: 0.24046321528526649\n",
            "27225: Similarity Score for a dialog: 0.24054679885615804\n",
            "27254: Similarity Score for a dialog: 0.24075345302153492\n",
            "27279: Similarity Score for a dialog: 0.24088707805679807\n",
            "27308: Similarity Score for a dialog: 0.2408835848705903\n",
            "27333: Similarity Score for a dialog: 0.24088060744341408\n",
            "27362: Similarity Score for a dialog: 0.24088021506682983\n",
            "27387: Similarity Score for a dialog: 0.24083270595914255\n",
            "27416: Similarity Score for a dialog: 0.24106910692649872\n",
            "27441: Similarity Score for a dialog: 0.2411683380892225\n",
            "27454: Similarity Score for a dialog: 0.24122069135061108\n",
            "27463: Similarity Score for a dialog: 0.2412444017641599\n",
            "27504: Similarity Score for a dialog: 0.24118012097553573\n",
            "27541: Similarity Score for a dialog: 0.24107244090169522\n",
            "27570: Similarity Score for a dialog: 0.2410726818355383\n",
            "27595: Similarity Score for a dialog: 0.24101377231315643\n",
            "27624: Similarity Score for a dialog: 0.24105030678719477\n",
            "27649: Similarity Score for a dialog: 0.24102790393177043\n",
            "27670: Similarity Score for a dialog: 0.2409262687644634\n",
            "27691: Similarity Score for a dialog: 0.24090390589825003\n",
            "27720: Similarity Score for a dialog: 0.24094128278941748\n",
            "27745: Similarity Score for a dialog: 0.24098438615092937\n",
            "27774: Similarity Score for a dialog: 0.24096319655790116\n",
            "27799: Similarity Score for a dialog: 0.2408521451760873\n",
            "27828: Similarity Score for a dialog: 0.24087544243758946\n",
            "27853: Similarity Score for a dialog: 0.24093636233612928\n",
            "27882: Similarity Score for a dialog: 0.2409606272838173\n",
            "27907: Similarity Score for a dialog: 0.24098725656579428\n",
            "27924: Similarity Score for a dialog: 0.24100348644526576\n",
            "27941: Similarity Score for a dialog: 0.2410609439585763\n",
            "27970: Similarity Score for a dialog: 0.24108268445409914\n",
            "27995: Similarity Score for a dialog: 0.2411943560428588\n",
            "28024: Similarity Score for a dialog: 0.24119215972528674\n",
            "28049: Similarity Score for a dialog: 0.24123681378244116\n",
            "28078: Similarity Score for a dialog: 0.24117235949823454\n",
            "28103: Similarity Score for a dialog: 0.2411064413249085\n",
            "28132: Similarity Score for a dialog: 0.241113411010998\n",
            "28157: Similarity Score for a dialog: 0.241135757611814\n",
            "28186: Similarity Score for a dialog: 0.2411015321284267\n",
            "28215: Similarity Score for a dialog: 0.2410766013183154\n",
            "28244: Similarity Score for a dialog: 0.24117254382115308\n",
            "28269: Similarity Score for a dialog: 0.241197886931217\n",
            "28298: Similarity Score for a dialog: 0.24118579069048812\n",
            "28327: Similarity Score for a dialog: 0.2411741845409392\n",
            "28356: Similarity Score for a dialog: 0.2411232385048144\n",
            "28381: Similarity Score for a dialog: 0.24115490978208898\n",
            "28410: Similarity Score for a dialog: 0.24100173441393793\n",
            "28435: Similarity Score for a dialog: 0.2408421175372941\n",
            "28464: Similarity Score for a dialog: 0.24069526401825442\n",
            "28489: Similarity Score for a dialog: 0.2407465725564861\n",
            "28518: Similarity Score for a dialog: 0.24070517043212822\n",
            "28543: Similarity Score for a dialog: 0.2406645977453182\n",
            "28572: Similarity Score for a dialog: 0.24065322131564745\n",
            "28597: Similarity Score for a dialog: 0.24070498364210313\n",
            "28626: Similarity Score for a dialog: 0.24066549080906594\n",
            "28651: Similarity Score for a dialog: 0.24057334786308174\n",
            "28680: Similarity Score for a dialog: 0.24043321935861928\n",
            "28705: Similarity Score for a dialog: 0.24032468238198443\n",
            "28734: Similarity Score for a dialog: 0.24032775530634895\n",
            "28759: Similarity Score for a dialog: 0.24036486632519005\n",
            "28788: Similarity Score for a dialog: 0.24039084481046197\n",
            "28813: Similarity Score for a dialog: 0.24031373560902686\n",
            "28842: Similarity Score for a dialog: 0.24033886645047964\n",
            "28867: Similarity Score for a dialog: 0.24037592303890168\n",
            "28896: Similarity Score for a dialog: 0.2403925412024396\n",
            "28921: Similarity Score for a dialog: 0.2403961953416227\n",
            "28950: Similarity Score for a dialog: 0.24039903194501008\n",
            "28975: Similarity Score for a dialog: 0.24030158594925366\n",
            "29004: Similarity Score for a dialog: 0.24028510455122418\n",
            "29029: Similarity Score for a dialog: 0.24028594221152105\n",
            "29058: Similarity Score for a dialog: 0.24041283700470747\n",
            "29087: Similarity Score for a dialog: 0.24041788135859046\n",
            "29116: Similarity Score for a dialog: 0.24046424699702496\n",
            "29141: Similarity Score for a dialog: 0.2405024420927483\n",
            "29170: Similarity Score for a dialog: 0.24047759345588102\n",
            "29195: Similarity Score for a dialog: 0.24052287560719873\n",
            "29224: Similarity Score for a dialog: 0.24052095838471985\n",
            "29249: Similarity Score for a dialog: 0.24054737030776882\n",
            "29278: Similarity Score for a dialog: 0.24048127285454962\n",
            "29303: Similarity Score for a dialog: 0.24040694532256235\n",
            "29332: Similarity Score for a dialog: 0.24039063742202674\n",
            "29357: Similarity Score for a dialog: 0.24040712004379225\n",
            "29378: Similarity Score for a dialog: 0.24047722195915092\n",
            "29399: Similarity Score for a dialog: 0.2405037206082341\n",
            "29428: Similarity Score for a dialog: 0.2405092901909108\n",
            "29453: Similarity Score for a dialog: 0.24051226687314745\n",
            "29482: Similarity Score for a dialog: 0.2405043496317426\n",
            "29507: Similarity Score for a dialog: 0.2403976708054461\n",
            "29536: Similarity Score for a dialog: 0.24034981436992425\n",
            "29561: Similarity Score for a dialog: 0.24036429713874724\n",
            "29590: Similarity Score for a dialog: 0.24053163078045384\n",
            "29615: Similarity Score for a dialog: 0.24053728074845068\n",
            "29636: Similarity Score for a dialog: 0.2405563863857581\n",
            "29653: Similarity Score for a dialog: 0.24060067534802806\n",
            "29682: Similarity Score for a dialog: 0.24062805037381155\n",
            "29707: Similarity Score for a dialog: 0.24071091270998396\n",
            "29736: Similarity Score for a dialog: 0.24063680177784225\n",
            "29761: Similarity Score for a dialog: 0.2406673208414515\n",
            "29790: Similarity Score for a dialog: 0.2406787949634063\n",
            "29815: Similarity Score for a dialog: 0.24071648618404057\n",
            "29844: Similarity Score for a dialog: 0.240823285140194\n",
            "29869: Similarity Score for a dialog: 0.240684942373638\n",
            "29898: Similarity Score for a dialog: 0.24063859025168452\n",
            "29923: Similarity Score for a dialog: 0.24062014575605437\n",
            "29952: Similarity Score for a dialog: 0.24062924415930967\n",
            "29977: Similarity Score for a dialog: 0.2406339127120887\n",
            "30006: Similarity Score for a dialog: 0.2406421179198572\n",
            "30035: Similarity Score for a dialog: 0.24068207501071015\n",
            "30064: Similarity Score for a dialog: 0.2407361789215797\n",
            "30089: Similarity Score for a dialog: 0.24069984234533756\n",
            "30118: Similarity Score for a dialog: 0.24057385937233822\n",
            "30143: Similarity Score for a dialog: 0.24046197672454245\n",
            "30172: Similarity Score for a dialog: 0.2405501274476832\n",
            "30197: Similarity Score for a dialog: 0.24047264919013064\n",
            "30226: Similarity Score for a dialog: 0.2403740862369376\n",
            "30251: Similarity Score for a dialog: 0.2404428861807343\n",
            "30280: Similarity Score for a dialog: 0.24038443778327115\n",
            "30305: Similarity Score for a dialog: 0.24023000296719402\n",
            "30334: Similarity Score for a dialog: 0.24029048547617368\n",
            "30359: Similarity Score for a dialog: 0.24028305779549444\n",
            "30388: Similarity Score for a dialog: 0.24022466433556472\n",
            "30413: Similarity Score for a dialog: 0.2402117157932409\n",
            "30442: Similarity Score for a dialog: 0.24029857258955686\n",
            "30471: Similarity Score for a dialog: 0.24035219406899944\n",
            "30500: Similarity Score for a dialog: 0.24035959296369178\n",
            "30529: Similarity Score for a dialog: 0.24028945319441727\n",
            "30546: Similarity Score for a dialog: 0.24030245276805656\n",
            "30563: Similarity Score for a dialog: 0.24028269594505186\n",
            "30592: Similarity Score for a dialog: 0.24027852384188245\n",
            "30621: Similarity Score for a dialog: 0.24018347746254143\n",
            "30650: Similarity Score for a dialog: 0.24004540609874567\n",
            "30675: Similarity Score for a dialog: 0.23996491151837593\n",
            "30704: Similarity Score for a dialog: 0.2398869767055347\n",
            "30729: Similarity Score for a dialog: 0.23978519441585508\n",
            "30758: Similarity Score for a dialog: 0.23991175837676615\n",
            "30783: Similarity Score for a dialog: 0.23993814428845942\n",
            "30812: Similarity Score for a dialog: 0.23991969859165543\n",
            "30837: Similarity Score for a dialog: 0.239920199635074\n",
            "30866: Similarity Score for a dialog: 0.23991458418231504\n",
            "30891: Similarity Score for a dialog: 0.23981040685864083\n",
            "30920: Similarity Score for a dialog: 0.2397311023487301\n",
            "30945: Similarity Score for a dialog: 0.23970554282113837\n",
            "30974: Similarity Score for a dialog: 0.2396436228712394\n",
            "30999: Similarity Score for a dialog: 0.23962760585430976\n",
            "31028: Similarity Score for a dialog: 0.23958078484712395\n",
            "31053: Similarity Score for a dialog: 0.23961895220337903\n",
            "31082: Similarity Score for a dialog: 0.23953364969439617\n",
            "31107: Similarity Score for a dialog: 0.239518010496795\n",
            "31136: Similarity Score for a dialog: 0.23942384386520643\n",
            "31161: Similarity Score for a dialog: 0.23939392375091442\n",
            "31190: Similarity Score for a dialog: 0.23938182444848086\n",
            "31215: Similarity Score for a dialog: 0.23934294452745403\n",
            "31244: Similarity Score for a dialog: 0.23929324142492875\n",
            "31269: Similarity Score for a dialog: 0.23922297392263409\n",
            "31298: Similarity Score for a dialog: 0.23927603757631483\n",
            "31323: Similarity Score for a dialog: 0.23925824323486622\n",
            "31336: Similarity Score for a dialog: 0.239314974070977\n",
            "31349: Similarity Score for a dialog: 0.23936566738204754\n",
            "31378: Similarity Score for a dialog: 0.23922814691517189\n",
            "31407: Similarity Score for a dialog: 0.23915649553088394\n",
            "31436: Similarity Score for a dialog: 0.2391827695978307\n",
            "31461: Similarity Score for a dialog: 0.23925510039047482\n",
            "31490: Similarity Score for a dialog: 0.23928847843449508\n",
            "31515: Similarity Score for a dialog: 0.2392634447645273\n",
            "31544: Similarity Score for a dialog: 0.23922695752991271\n",
            "31569: Similarity Score for a dialog: 0.23920883724237554\n",
            "31598: Similarity Score for a dialog: 0.2390880673827936\n",
            "31623: Similarity Score for a dialog: 0.23905689687936899\n",
            "31632: Similarity Score for a dialog: 0.23906335265985204\n",
            "31637: Similarity Score for a dialog: 0.23905332990833728\n",
            "31650: Similarity Score for a dialog: 0.2390326115112547\n",
            "31663: Similarity Score for a dialog: 0.23902754777398097\n",
            "31692: Similarity Score for a dialog: 0.23910788410311196\n",
            "31717: Similarity Score for a dialog: 0.239141749105799\n",
            "31750: Similarity Score for a dialog: 0.23911849536912014\n",
            "31783: Similarity Score for a dialog: 0.23914318507621246\n",
            "31812: Similarity Score for a dialog: 0.23914534947789232\n",
            "31837: Similarity Score for a dialog: 0.23914298191149946\n",
            "31866: Similarity Score for a dialog: 0.23913293008613282\n",
            "31891: Similarity Score for a dialog: 0.23913538267982387\n",
            "31920: Similarity Score for a dialog: 0.23918949519181493\n",
            "31945: Similarity Score for a dialog: 0.2391030064183992\n",
            "31974: Similarity Score for a dialog: 0.23902952188008483\n",
            "31999: Similarity Score for a dialog: 0.23900056634692976\n",
            "32028: Similarity Score for a dialog: 0.23896419381014214\n",
            "32053: Similarity Score for a dialog: 0.23888875776258112\n",
            "32082: Similarity Score for a dialog: 0.23880032971805107\n",
            "32107: Similarity Score for a dialog: 0.23886497753387004\n",
            "32136: Similarity Score for a dialog: 0.2388622574506683\n",
            "32161: Similarity Score for a dialog: 0.23886850851574096\n",
            "32190: Similarity Score for a dialog: 0.23883895894153945\n",
            "32215: Similarity Score for a dialog: 0.23890020743422702\n",
            "32232: Similarity Score for a dialog: 0.23890679578083576\n",
            "32245: Similarity Score for a dialog: 0.238881933170551\n",
            "32274: Similarity Score for a dialog: 0.23876855180610082\n",
            "32299: Similarity Score for a dialog: 0.23871842597150086\n",
            "32328: Similarity Score for a dialog: 0.2387878936264446\n",
            "32353: Similarity Score for a dialog: 0.23870857307550747\n",
            "32382: Similarity Score for a dialog: 0.23868404691836068\n",
            "32407: Similarity Score for a dialog: 0.23868673027435428\n",
            "32436: Similarity Score for a dialog: 0.23872080754369604\n",
            "32465: Similarity Score for a dialog: 0.23859698724859923\n",
            "32494: Similarity Score for a dialog: 0.2385622989959602\n",
            "32519: Similarity Score for a dialog: 0.23864528895560114\n",
            "32548: Similarity Score for a dialog: 0.23865411232162978\n",
            "32573: Similarity Score for a dialog: 0.23869064778727972\n",
            "32602: Similarity Score for a dialog: 0.23867557892518693\n",
            "32627: Similarity Score for a dialog: 0.23865173768236814\n",
            "32656: Similarity Score for a dialog: 0.23878839729662407\n",
            "32681: Similarity Score for a dialog: 0.2388664108786713\n",
            "32710: Similarity Score for a dialog: 0.2388769053988558\n",
            "32735: Similarity Score for a dialog: 0.23885627780826366\n",
            "32768: Similarity Score for a dialog: 0.23884398955565042\n",
            "32797: Similarity Score for a dialog: 0.23882885006767726\n",
            "32826: Similarity Score for a dialog: 0.2388540598367611\n",
            "32851: Similarity Score for a dialog: 0.23890187250026937\n",
            "32880: Similarity Score for a dialog: 0.23883125442545683\n",
            "32909: Similarity Score for a dialog: 0.23872048100971707\n",
            "32938: Similarity Score for a dialog: 0.23874867072407624\n",
            "32963: Similarity Score for a dialog: 0.23873649799701302\n",
            "32992: Similarity Score for a dialog: 0.23870517998087615\n",
            "33021: Similarity Score for a dialog: 0.23873558266739484\n",
            "33030: Similarity Score for a dialog: 0.23875400065935937\n",
            "33035: Similarity Score for a dialog: 0.23880012291034927\n",
            "33064: Similarity Score for a dialog: 0.2387637474286853\n",
            "33089: Similarity Score for a dialog: 0.23876971422962087\n",
            "33118: Similarity Score for a dialog: 0.23878566963494588\n",
            "33147: Similarity Score for a dialog: 0.23879455931346644\n",
            "33176: Similarity Score for a dialog: 0.23890970304293815\n",
            "33201: Similarity Score for a dialog: 0.2389467535728068\n",
            "33230: Similarity Score for a dialog: 0.23892547518044174\n",
            "33255: Similarity Score for a dialog: 0.2389004137131354\n",
            "33284: Similarity Score for a dialog: 0.23897359985436195\n",
            "33313: Similarity Score for a dialog: 0.23906758703000983\n",
            "33322: Similarity Score for a dialog: 0.23913617700917808\n",
            "33331: Similarity Score for a dialog: 0.23918001235575725\n",
            "33360: Similarity Score for a dialog: 0.2391608422963287\n",
            "33385: Similarity Score for a dialog: 0.23917183099341383\n",
            "33414: Similarity Score for a dialog: 0.23912296222865304\n",
            "33439: Similarity Score for a dialog: 0.23913819955479845\n",
            "33468: Similarity Score for a dialog: 0.23910273454176476\n",
            "33493: Similarity Score for a dialog: 0.23908574507748187\n",
            "33522: Similarity Score for a dialog: 0.23919982050502284\n",
            "33547: Similarity Score for a dialog: 0.23923328291456955\n",
            "33576: Similarity Score for a dialog: 0.23921794543429964\n",
            "33601: Similarity Score for a dialog: 0.23918400884987306\n",
            "33618: Similarity Score for a dialog: 0.23925191646872324\n",
            "33635: Similarity Score for a dialog: 0.23927516141697974\n",
            "33664: Similarity Score for a dialog: 0.23938770285245142\n",
            "33693: Similarity Score for a dialog: 0.23935262032428117\n",
            "33722: Similarity Score for a dialog: 0.23946877293961488\n",
            "33747: Similarity Score for a dialog: 0.23951028478563804\n",
            "33776: Similarity Score for a dialog: 0.23959867148417485\n",
            "33801: Similarity Score for a dialog: 0.23960857566191912\n",
            "33830: Similarity Score for a dialog: 0.23958429795242717\n",
            "33855: Similarity Score for a dialog: 0.23955189444688849\n",
            "33884: Similarity Score for a dialog: 0.23952261666888633\n",
            "33909: Similarity Score for a dialog: 0.23947433437480722\n",
            "33938: Similarity Score for a dialog: 0.23948015625728117\n",
            "33963: Similarity Score for a dialog: 0.23953009678176262\n",
            "33992: Similarity Score for a dialog: 0.2397398554471765\n",
            "34017: Similarity Score for a dialog: 0.23994350251632923\n",
            "34046: Similarity Score for a dialog: 0.23992756779474533\n",
            "34071: Similarity Score for a dialog: 0.23981563901375297\n",
            "34104: Similarity Score for a dialog: 0.23986463666526783\n",
            "34133: Similarity Score for a dialog: 0.23994225466164484\n",
            "34162: Similarity Score for a dialog: 0.23987575899755723\n",
            "34187: Similarity Score for a dialog: 0.23988133776122908\n",
            "34216: Similarity Score for a dialog: 0.2398009066110586\n",
            "34245: Similarity Score for a dialog: 0.2397831723398864\n",
            "34274: Similarity Score for a dialog: 0.2397270168705567\n",
            "34303: Similarity Score for a dialog: 0.23970841216033562\n",
            "34332: Similarity Score for a dialog: 0.2396295234694198\n",
            "34357: Similarity Score for a dialog: 0.23960642634187124\n",
            "34386: Similarity Score for a dialog: 0.23956342110197926\n",
            "34411: Similarity Score for a dialog: 0.23953086435953036\n",
            "34440: Similarity Score for a dialog: 0.23946082804862934\n",
            "34469: Similarity Score for a dialog: 0.23940460130554894\n",
            "34478: Similarity Score for a dialog: 0.23944803809933543\n",
            "34483: Similarity Score for a dialog: 0.23943054840287475\n",
            "34512: Similarity Score for a dialog: 0.23945024191983486\n",
            "34541: Similarity Score for a dialog: 0.23951535716005476\n",
            "34570: Similarity Score for a dialog: 0.239476871065454\n",
            "34595: Similarity Score for a dialog: 0.23933667330689037\n",
            "34624: Similarity Score for a dialog: 0.2393384748579696\n",
            "34649: Similarity Score for a dialog: 0.23934342989315704\n",
            "34678: Similarity Score for a dialog: 0.23921656832470484\n",
            "34703: Similarity Score for a dialog: 0.2391880010695443\n",
            "34716: Similarity Score for a dialog: 0.23920744613285913\n",
            "34725: Similarity Score for a dialog: 0.23921574278564003\n",
            "34758: Similarity Score for a dialog: 0.23921736906505456\n",
            "34787: Similarity Score for a dialog: 0.23919478252667886\n",
            "34808: Similarity Score for a dialog: 0.23919415992825263\n",
            "34829: Similarity Score for a dialog: 0.23919293248761486\n",
            "34858: Similarity Score for a dialog: 0.2391535590321244\n",
            "34883: Similarity Score for a dialog: 0.23917306917186898\n",
            "34912: Similarity Score for a dialog: 0.23907539495612154\n",
            "34937: Similarity Score for a dialog: 0.23913963296232402\n",
            "34966: Similarity Score for a dialog: 0.2390856362061931\n",
            "34991: Similarity Score for a dialog: 0.23914866497096196\n",
            "35020: Similarity Score for a dialog: 0.23912785307438292\n",
            "35045: Similarity Score for a dialog: 0.23921931615513614\n",
            "35074: Similarity Score for a dialog: 0.239173483610963\n",
            "35099: Similarity Score for a dialog: 0.23909516513428547\n",
            "35124: Similarity Score for a dialog: 0.23917290192306262\n",
            "35149: Similarity Score for a dialog: 0.2390871784601779\n",
            "35178: Similarity Score for a dialog: 0.23915989991491698\n",
            "35203: Similarity Score for a dialog: 0.23917982101739496\n",
            "35232: Similarity Score for a dialog: 0.23907875685218918\n",
            "35257: Similarity Score for a dialog: 0.23907010191293745\n",
            "35286: Similarity Score for a dialog: 0.23902790636991403\n",
            "35315: Similarity Score for a dialog: 0.2390672258262092\n",
            "35344: Similarity Score for a dialog: 0.23902439122913782\n",
            "35369: Similarity Score for a dialog: 0.23895125585377464\n",
            "35398: Similarity Score for a dialog: 0.23897904591371827\n",
            "35423: Similarity Score for a dialog: 0.23897991810639477\n",
            "35452: Similarity Score for a dialog: 0.23897050627563668\n",
            "35477: Similarity Score for a dialog: 0.2389687474339362\n",
            "35506: Similarity Score for a dialog: 0.23897304078765394\n",
            "35531: Similarity Score for a dialog: 0.238963695298864\n",
            "35560: Similarity Score for a dialog: 0.23897754674516636\n",
            "35585: Similarity Score for a dialog: 0.23906786389382464\n",
            "35614: Similarity Score for a dialog: 0.2391803909519907\n",
            "35639: Similarity Score for a dialog: 0.23918548023177386\n",
            "35668: Similarity Score for a dialog: 0.23923849752033233\n",
            "35693: Similarity Score for a dialog: 0.2392875907189746\n",
            "35722: Similarity Score for a dialog: 0.23923719774317403\n",
            "35747: Similarity Score for a dialog: 0.23923751922046052\n",
            "35776: Similarity Score for a dialog: 0.23918626761925074\n",
            "35801: Similarity Score for a dialog: 0.23921335953778064\n",
            "35830: Similarity Score for a dialog: 0.23918495068300746\n",
            "35855: Similarity Score for a dialog: 0.23913479970334636\n",
            "35884: Similarity Score for a dialog: 0.2390285615377074\n",
            "35913: Similarity Score for a dialog: 0.2390873518240139\n",
            "35926: Similarity Score for a dialog: 0.2390955625765498\n",
            "35939: Similarity Score for a dialog: 0.23916401084697161\n",
            "35968: Similarity Score for a dialog: 0.23912970723663476\n",
            "35993: Similarity Score for a dialog: 0.2391023476203692\n",
            "36002: Similarity Score for a dialog: 0.23907583748359815\n",
            "36011: Similarity Score for a dialog: 0.23911826402201264\n",
            "36028: Similarity Score for a dialog: 0.23910731435528615\n",
            "36041: Similarity Score for a dialog: 0.23919395780435646\n",
            "36050: Similarity Score for a dialog: 0.23917441474494056\n",
            "36059: Similarity Score for a dialog: 0.23918904063436178\n",
            "36088: Similarity Score for a dialog: 0.23913847073888675\n",
            "36113: Similarity Score for a dialog: 0.23912423052435508\n",
            "36142: Similarity Score for a dialog: 0.23911177039221157\n",
            "36167: Similarity Score for a dialog: 0.2391138273358846\n",
            "36196: Similarity Score for a dialog: 0.23909959787509202\n",
            "36221: Similarity Score for a dialog: 0.23906443595550825\n",
            "36250: Similarity Score for a dialog: 0.2389895222574454\n",
            "36275: Similarity Score for a dialog: 0.2389416706299471\n",
            "36304: Similarity Score for a dialog: 0.23892307364680726\n",
            "36329: Similarity Score for a dialog: 0.23892051236899303\n",
            "36358: Similarity Score for a dialog: 0.23883799106641196\n",
            "36383: Similarity Score for a dialog: 0.23883207874856263\n",
            "36412: Similarity Score for a dialog: 0.23885742455960676\n",
            "36437: Similarity Score for a dialog: 0.2388626350346694\n",
            "36466: Similarity Score for a dialog: 0.23884990185194524\n",
            "36491: Similarity Score for a dialog: 0.2388990974341792\n",
            "36520: Similarity Score for a dialog: 0.23887531095474276\n",
            "36545: Similarity Score for a dialog: 0.2388375884140258\n",
            "36574: Similarity Score for a dialog: 0.23892100208385428\n",
            "36599: Similarity Score for a dialog: 0.23900346538736902\n",
            "36628: Similarity Score for a dialog: 0.23892896616021342\n",
            "36653: Similarity Score for a dialog: 0.23891669695704557\n",
            "36682: Similarity Score for a dialog: 0.23886119619028465\n",
            "36707: Similarity Score for a dialog: 0.23881443667890515\n",
            "36736: Similarity Score for a dialog: 0.23881287373936672\n",
            "36765: Similarity Score for a dialog: 0.23886987643724605\n",
            "36794: Similarity Score for a dialog: 0.23889715223453928\n",
            "36819: Similarity Score for a dialog: 0.23890116969133482\n",
            "36848: Similarity Score for a dialog: 0.23885916272524174\n",
            "36873: Similarity Score for a dialog: 0.23874412414678683\n",
            "36902: Similarity Score for a dialog: 0.23874691112100013\n",
            "36927: Similarity Score for a dialog: 0.23878856747253785\n",
            "36956: Similarity Score for a dialog: 0.2388100584659235\n",
            "36981: Similarity Score for a dialog: 0.23880489763381393\n",
            "37014: Similarity Score for a dialog: 0.23885070201407438\n",
            "37043: Similarity Score for a dialog: 0.23884808602016874\n",
            "37052: Similarity Score for a dialog: 0.238838234096206\n",
            "37057: Similarity Score for a dialog: 0.23880967897190566\n",
            "37086: Similarity Score for a dialog: 0.23888690763969833\n",
            "37111: Similarity Score for a dialog: 0.23884341393948066\n",
            "37140: Similarity Score for a dialog: 0.2389064714646279\n",
            "37169: Similarity Score for a dialog: 0.23889849898255244\n",
            "37198: Similarity Score for a dialog: 0.23889212514228506\n",
            "37227: Similarity Score for a dialog: 0.23889559233128443\n",
            "37256: Similarity Score for a dialog: 0.23890732812868593\n",
            "37281: Similarity Score for a dialog: 0.23890316060759298\n",
            "37310: Similarity Score for a dialog: 0.23893432842772272\n",
            "37335: Similarity Score for a dialog: 0.23890289824006455\n",
            "37364: Similarity Score for a dialog: 0.23892900175614623\n",
            "37389: Similarity Score for a dialog: 0.2389788807942336\n",
            "37418: Similarity Score for a dialog: 0.23897291953124714\n",
            "37443: Similarity Score for a dialog: 0.23895841363632928\n",
            "37472: Similarity Score for a dialog: 0.23895036838212955\n",
            "37497: Similarity Score for a dialog: 0.23897407989679986\n",
            "37526: Similarity Score for a dialog: 0.23896306907342074\n",
            "37551: Similarity Score for a dialog: 0.23893507494771765\n",
            "37580: Similarity Score for a dialog: 0.2390015040691812\n",
            "37605: Similarity Score for a dialog: 0.2390116840475716\n",
            "37614: Similarity Score for a dialog: 0.2390361318664792\n",
            "37619: Similarity Score for a dialog: 0.23902184975489266\n",
            "37648: Similarity Score for a dialog: 0.23906795356549165\n",
            "37673: Similarity Score for a dialog: 0.23906348021115173\n",
            "37694: Similarity Score for a dialog: 0.2390552098949544\n",
            "37715: Similarity Score for a dialog: 0.23908294649233788\n",
            "37744: Similarity Score for a dialog: 0.23911750887972566\n",
            "37769: Similarity Score for a dialog: 0.23915530959319378\n",
            "37798: Similarity Score for a dialog: 0.239111843931185\n",
            "37823: Similarity Score for a dialog: 0.23901301800953154\n",
            "37848: Similarity Score for a dialog: 0.23906359232009614\n",
            "37873: Similarity Score for a dialog: 0.2390886092150704\n",
            "37902: Similarity Score for a dialog: 0.23910991098574405\n",
            "37927: Similarity Score for a dialog: 0.23914123991373018\n",
            "37956: Similarity Score for a dialog: 0.2391761589098901\n",
            "37981: Similarity Score for a dialog: 0.23920494391635247\n",
            "38010: Similarity Score for a dialog: 0.23921499252811249\n",
            "38035: Similarity Score for a dialog: 0.23923810779441243\n",
            "38064: Similarity Score for a dialog: 0.23925739916587277\n",
            "38089: Similarity Score for a dialog: 0.2392176672794555\n",
            "38118: Similarity Score for a dialog: 0.23928700890299046\n",
            "38143: Similarity Score for a dialog: 0.239361427398876\n",
            "38172: Similarity Score for a dialog: 0.2393344384749315\n",
            "38197: Similarity Score for a dialog: 0.23935787236373282\n",
            "38226: Similarity Score for a dialog: 0.2393359503058922\n",
            "38251: Similarity Score for a dialog: 0.23932203098268542\n",
            "38280: Similarity Score for a dialog: 0.23923911013410162\n",
            "38309: Similarity Score for a dialog: 0.23919098175556036\n",
            "38338: Similarity Score for a dialog: 0.23928475317329234\n",
            "38363: Similarity Score for a dialog: 0.23938525595599705\n",
            "38392: Similarity Score for a dialog: 0.2392853446117514\n",
            "38417: Similarity Score for a dialog: 0.23917391314408296\n",
            "38446: Similarity Score for a dialog: 0.23925703324478834\n",
            "38471: Similarity Score for a dialog: 0.2392117213080348\n",
            "38500: Similarity Score for a dialog: 0.23917996403058664\n",
            "38525: Similarity Score for a dialog: 0.23911773279753962\n",
            "38554: Similarity Score for a dialog: 0.23907912327799327\n",
            "38579: Similarity Score for a dialog: 0.2390343718426039\n",
            "38608: Similarity Score for a dialog: 0.2390224898470328\n",
            "38633: Similarity Score for a dialog: 0.23905835663937394\n",
            "38662: Similarity Score for a dialog: 0.23900546483909293\n",
            "38691: Similarity Score for a dialog: 0.23900422045699324\n",
            "38720: Similarity Score for a dialog: 0.23891575874228033\n",
            "38749: Similarity Score for a dialog: 0.23885588749364065\n",
            "38778: Similarity Score for a dialog: 0.23878025663739305\n",
            "38803: Similarity Score for a dialog: 0.23877072723518175\n",
            "38832: Similarity Score for a dialog: 0.23874507156032265\n",
            "38857: Similarity Score for a dialog: 0.23874067118993628\n",
            "38886: Similarity Score for a dialog: 0.23870153199041172\n",
            "38911: Similarity Score for a dialog: 0.23862349539694944\n",
            "38940: Similarity Score for a dialog: 0.23868533316693677\n",
            "38965: Similarity Score for a dialog: 0.23873127882201792\n",
            "38994: Similarity Score for a dialog: 0.23873730210054844\n",
            "39019: Similarity Score for a dialog: 0.23875085716502747\n",
            "39048: Similarity Score for a dialog: 0.2386438740247917\n",
            "39073: Similarity Score for a dialog: 0.238600387352931\n",
            "39082: Similarity Score for a dialog: 0.2386318779058259\n",
            "39087: Similarity Score for a dialog: 0.23862045253923928\n",
            "39116: Similarity Score for a dialog: 0.23867725369644516\n",
            "39141: Similarity Score for a dialog: 0.238592192286317\n",
            "39170: Similarity Score for a dialog: 0.23858847930619922\n",
            "39195: Similarity Score for a dialog: 0.2385372141748123\n",
            "39224: Similarity Score for a dialog: 0.23853117620476716\n",
            "39249: Similarity Score for a dialog: 0.23851148917133\n",
            "39278: Similarity Score for a dialog: 0.23848230144429766\n",
            "39303: Similarity Score for a dialog: 0.23845445692719938\n",
            "39332: Similarity Score for a dialog: 0.23846609929044402\n",
            "39357: Similarity Score for a dialog: 0.23839448038138605\n",
            "39386: Similarity Score for a dialog: 0.2383648259382482\n",
            "39411: Similarity Score for a dialog: 0.23825673198476777\n",
            "39440: Similarity Score for a dialog: 0.23824757381289494\n",
            "39465: Similarity Score for a dialog: 0.23820259724463827\n",
            "39494: Similarity Score for a dialog: 0.2382192475360122\n",
            "39519: Similarity Score for a dialog: 0.23824282263127244\n",
            "39548: Similarity Score for a dialog: 0.238293999399885\n",
            "39573: Similarity Score for a dialog: 0.23834222620265463\n",
            "39602: Similarity Score for a dialog: 0.23839121349405906\n",
            "39627: Similarity Score for a dialog: 0.2383427836341169\n",
            "39656: Similarity Score for a dialog: 0.2383165234055072\n",
            "39681: Similarity Score for a dialog: 0.23828835285409736\n",
            "39710: Similarity Score for a dialog: 0.23825548437282218\n",
            "39735: Similarity Score for a dialog: 0.2381704523091336\n",
            "39764: Similarity Score for a dialog: 0.238079095207344\n",
            "39789: Similarity Score for a dialog: 0.23804668428775705\n",
            "39818: Similarity Score for a dialog: 0.23802014898045074\n",
            "39843: Similarity Score for a dialog: 0.23804996430736447\n",
            "39872: Similarity Score for a dialog: 0.2380636278019714\n",
            "39897: Similarity Score for a dialog: 0.23804073649334656\n",
            "39926: Similarity Score for a dialog: 0.23801568310160717\n",
            "39951: Similarity Score for a dialog: 0.23801299697154338\n",
            "39980: Similarity Score for a dialog: 0.23802735610023387\n",
            "40009: Similarity Score for a dialog: 0.2379367103537081\n",
            "40026: Similarity Score for a dialog: 0.23790934621857426\n",
            "40043: Similarity Score for a dialog: 0.23787597713225864\n",
            "40072: Similarity Score for a dialog: 0.23787989231150347\n",
            "40097: Similarity Score for a dialog: 0.23785042679749577\n",
            "40126: Similarity Score for a dialog: 0.23787916991121397\n",
            "40151: Similarity Score for a dialog: 0.23788315056391313\n",
            "40180: Similarity Score for a dialog: 0.23791575506687812\n",
            "40205: Similarity Score for a dialog: 0.2379669976060672\n",
            "40234: Similarity Score for a dialog: 0.2379527144629803\n",
            "40259: Similarity Score for a dialog: 0.23787385009528234\n",
            "40280: Similarity Score for a dialog: 0.23782955764382274\n",
            "40301: Similarity Score for a dialog: 0.23785846914538655\n",
            "40330: Similarity Score for a dialog: 0.2378546934856891\n",
            "40355: Similarity Score for a dialog: 0.23783623948811863\n",
            "40384: Similarity Score for a dialog: 0.23790320320176844\n",
            "40409: Similarity Score for a dialog: 0.23782422857234295\n",
            "40434: Similarity Score for a dialog: 0.23790530874691448\n",
            "40455: Similarity Score for a dialog: 0.23796111859474972\n",
            "40484: Similarity Score for a dialog: 0.23796303591986243\n",
            "40513: Similarity Score for a dialog: 0.23796160854241863\n",
            "40542: Similarity Score for a dialog: 0.23793109267466334\n",
            "40567: Similarity Score for a dialog: 0.23785718675194165\n",
            "40596: Similarity Score for a dialog: 0.2378558076827589\n",
            "40621: Similarity Score for a dialog: 0.2379069835508068\n",
            "40650: Similarity Score for a dialog: 0.23788219764893026\n",
            "40675: Similarity Score for a dialog: 0.2379037634820482\n",
            "40704: Similarity Score for a dialog: 0.23784254572349878\n",
            "40733: Similarity Score for a dialog: 0.2377749760056569\n",
            "40762: Similarity Score for a dialog: 0.2378019155516703\n",
            "40787: Similarity Score for a dialog: 0.23782703318115558\n",
            "40816: Similarity Score for a dialog: 0.2377390134191733\n",
            "40845: Similarity Score for a dialog: 0.23768398336586805\n",
            "40874: Similarity Score for a dialog: 0.23762037749189396\n",
            "40899: Similarity Score for a dialog: 0.23764401936715532\n",
            "40928: Similarity Score for a dialog: 0.2376450602133664\n",
            "40953: Similarity Score for a dialog: 0.23764891553623144\n",
            "40982: Similarity Score for a dialog: 0.23760406288065414\n",
            "41007: Similarity Score for a dialog: 0.23761677808043158\n",
            "41028: Similarity Score for a dialog: 0.23764614407544987\n",
            "41045: Similarity Score for a dialog: 0.23766307650576263\n",
            "41074: Similarity Score for a dialog: 0.23761083335227298\n",
            "41103: Similarity Score for a dialog: 0.23762468812176554\n",
            "41132: Similarity Score for a dialog: 0.23769350796937472\n",
            "41157: Similarity Score for a dialog: 0.23776989064332918\n",
            "41186: Similarity Score for a dialog: 0.2376888260328445\n",
            "41211: Similarity Score for a dialog: 0.23776813961895993\n",
            "41240: Similarity Score for a dialog: 0.23776299384887417\n",
            "41265: Similarity Score for a dialog: 0.23773234533370918\n",
            "41294: Similarity Score for a dialog: 0.23778194307753117\n",
            "41319: Similarity Score for a dialog: 0.23777501544204854\n",
            "41348: Similarity Score for a dialog: 0.23776930615694905\n",
            "41373: Similarity Score for a dialog: 0.23771587852142773\n",
            "41402: Similarity Score for a dialog: 0.23765171287713194\n",
            "41427: Similarity Score for a dialog: 0.23754857791628584\n",
            "41456: Similarity Score for a dialog: 0.23757776530711047\n",
            "41481: Similarity Score for a dialog: 0.2375437286753187\n",
            "41510: Similarity Score for a dialog: 0.23755139247011167\n",
            "41535: Similarity Score for a dialog: 0.23753957306579682\n",
            "41564: Similarity Score for a dialog: 0.23758074966479778\n",
            "41589: Similarity Score for a dialog: 0.23757231773658316\n",
            "41618: Similarity Score for a dialog: 0.2375380289408711\n",
            "41643: Similarity Score for a dialog: 0.23753815867033856\n",
            "41672: Similarity Score for a dialog: 0.2376175903263619\n",
            "41697: Similarity Score for a dialog: 0.23752661986419896\n",
            "41726: Similarity Score for a dialog: 0.2375147932492353\n",
            "41751: Similarity Score for a dialog: 0.237386963381787\n",
            "41780: Similarity Score for a dialog: 0.23735454126611835\n",
            "41809: Similarity Score for a dialog: 0.2373782379331175\n",
            "41838: Similarity Score for a dialog: 0.2373969300169905\n",
            "41863: Similarity Score for a dialog: 0.2374402147732722\n",
            "41880: Similarity Score for a dialog: 0.23744560962821104\n",
            "41893: Similarity Score for a dialog: 0.23744074034137205\n",
            "41914: Similarity Score for a dialog: 0.23745587472369847\n",
            "41935: Similarity Score for a dialog: 0.23748340040322155\n",
            "41964: Similarity Score for a dialog: 0.2374952704124306\n",
            "41993: Similarity Score for a dialog: 0.23750866501629747\n",
            "42022: Similarity Score for a dialog: 0.2375376800448394\n",
            "42047: Similarity Score for a dialog: 0.23754439336408922\n",
            "42056: Similarity Score for a dialog: 0.23758092615124576\n",
            "42061: Similarity Score for a dialog: 0.2375695461495332\n",
            "42090: Similarity Score for a dialog: 0.23755323813659496\n",
            "42115: Similarity Score for a dialog: 0.2375485298161549\n",
            "42136: Similarity Score for a dialog: 0.23746001986515705\n",
            "42153: Similarity Score for a dialog: 0.23751018984992395\n",
            "42182: Similarity Score for a dialog: 0.23750794808140815\n",
            "42207: Similarity Score for a dialog: 0.23744513150776\n",
            "42228: Similarity Score for a dialog: 0.23747647867644667\n",
            "42245: Similarity Score for a dialog: 0.2374604349091785\n",
            "42274: Similarity Score for a dialog: 0.2374989016364488\n",
            "42303: Similarity Score for a dialog: 0.23746370205260794\n",
            "42332: Similarity Score for a dialog: 0.237450861521057\n",
            "42357: Similarity Score for a dialog: 0.23745750959023507\n",
            "42386: Similarity Score for a dialog: 0.23747524214865787\n",
            "42411: Similarity Score for a dialog: 0.23753679135860817\n",
            "42440: Similarity Score for a dialog: 0.2375276082308333\n",
            "42465: Similarity Score for a dialog: 0.23749856371959527\n",
            "42494: Similarity Score for a dialog: 0.23758691447457975\n",
            "42519: Similarity Score for a dialog: 0.2376058142636028\n",
            "42548: Similarity Score for a dialog: 0.23759229568721854\n",
            "42573: Similarity Score for a dialog: 0.2375551653221791\n",
            "42602: Similarity Score for a dialog: 0.2375851039686168\n",
            "42627: Similarity Score for a dialog: 0.23757004023665038\n",
            "42656: Similarity Score for a dialog: 0.2376112482574597\n",
            "42681: Similarity Score for a dialog: 0.23764824578716032\n",
            "42710: Similarity Score for a dialog: 0.23767991178733452\n",
            "42735: Similarity Score for a dialog: 0.23766708299631428\n",
            "42764: Similarity Score for a dialog: 0.2375732615135353\n",
            "42789: Similarity Score for a dialog: 0.23748885910232753\n",
            "42818: Similarity Score for a dialog: 0.23741955900264425\n",
            "42843: Similarity Score for a dialog: 0.23742396953525477\n",
            "42872: Similarity Score for a dialog: 0.2373822955794042\n",
            "42897: Similarity Score for a dialog: 0.23742738380758535\n",
            "42914: Similarity Score for a dialog: 0.23743784629002726\n",
            "42931: Similarity Score for a dialog: 0.23748032854736645\n",
            "42964: Similarity Score for a dialog: 0.23745234372424287\n",
            "42993: Similarity Score for a dialog: 0.23744055003914805\n",
            "43022: Similarity Score for a dialog: 0.23741433525453925\n",
            "43047: Similarity Score for a dialog: 0.2373893761499611\n",
            "43076: Similarity Score for a dialog: 0.2374976391764488\n",
            "43101: Similarity Score for a dialog: 0.2374957555693304\n",
            "43130: Similarity Score for a dialog: 0.23753671701026755\n",
            "43155: Similarity Score for a dialog: 0.23752299268606342\n",
            "43184: Similarity Score for a dialog: 0.23746522909347698\n",
            "43209: Similarity Score for a dialog: 0.2375045775221755\n",
            "43238: Similarity Score for a dialog: 0.23742956145324823\n",
            "43263: Similarity Score for a dialog: 0.2373725563142591\n",
            "43292: Similarity Score for a dialog: 0.23739331585774892\n",
            "43317: Similarity Score for a dialog: 0.2373723664449866\n",
            "43346: Similarity Score for a dialog: 0.23742482652220365\n",
            "43371: Similarity Score for a dialog: 0.23746590350521102\n",
            "43400: Similarity Score for a dialog: 0.237478688259865\n",
            "43425: Similarity Score for a dialog: 0.2374299720140597\n",
            "43442: Similarity Score for a dialog: 0.23747584032458627\n",
            "43455: Similarity Score for a dialog: 0.237477298414602\n",
            "43484: Similarity Score for a dialog: 0.23749635661418408\n",
            "43509: Similarity Score for a dialog: 0.2374908247933159\n",
            "43538: Similarity Score for a dialog: 0.2374896548510362\n",
            "43563: Similarity Score for a dialog: 0.23751158504909192\n",
            "43592: Similarity Score for a dialog: 0.23747416252985123\n",
            "43621: Similarity Score for a dialog: 0.2374331873463112\n",
            "43634: Similarity Score for a dialog: 0.23746494608383303\n",
            "43647: Similarity Score for a dialog: 0.2375655414389883\n",
            "43676: Similarity Score for a dialog: 0.23759918608863306\n",
            "43701: Similarity Score for a dialog: 0.237591157579354\n",
            "43730: Similarity Score for a dialog: 0.23759563933430128\n",
            "43755: Similarity Score for a dialog: 0.2375949679379661\n",
            "43784: Similarity Score for a dialog: 0.23762042836355515\n",
            "43809: Similarity Score for a dialog: 0.23761936254336843\n",
            "43838: Similarity Score for a dialog: 0.2376000048673258\n",
            "43867: Similarity Score for a dialog: 0.23761605122594817\n",
            "43896: Similarity Score for a dialog: 0.23764668181456924\n",
            "43921: Similarity Score for a dialog: 0.23763055756413115\n",
            "43946: Similarity Score for a dialog: 0.2376087299876592\n",
            "43971: Similarity Score for a dialog: 0.2375757878960294\n",
            "44000: Similarity Score for a dialog: 0.23757292143302589\n",
            "44025: Similarity Score for a dialog: 0.23762160348123265\n",
            "44038: Similarity Score for a dialog: 0.2376545163567992\n",
            "44047: Similarity Score for a dialog: 0.23764784084880647\n",
            "44076: Similarity Score for a dialog: 0.23757943465386017\n",
            "44101: Similarity Score for a dialog: 0.2375400723936348\n",
            "44130: Similarity Score for a dialog: 0.23754529672882158\n",
            "44155: Similarity Score for a dialog: 0.2375547313037268\n",
            "44184: Similarity Score for a dialog: 0.2376311703328161\n",
            "44213: Similarity Score for a dialog: 0.23765667640860794\n",
            "44242: Similarity Score for a dialog: 0.23769119598481114\n",
            "44267: Similarity Score for a dialog: 0.23770744852281459\n",
            "44296: Similarity Score for a dialog: 0.2376950997914134\n",
            "44321: Similarity Score for a dialog: 0.23766530239912187\n",
            "44350: Similarity Score for a dialog: 0.2376519252127128\n",
            "44375: Similarity Score for a dialog: 0.23767280031388133\n",
            "44404: Similarity Score for a dialog: 0.23763957547607803\n",
            "44429: Similarity Score for a dialog: 0.2376082808667505\n",
            "44458: Similarity Score for a dialog: 0.23765785712676857\n",
            "44483: Similarity Score for a dialog: 0.23764298584264026\n",
            "44500: Similarity Score for a dialog: 0.23765788243349004\n",
            "44517: Similarity Score for a dialog: 0.2376153146484354\n",
            "44546: Similarity Score for a dialog: 0.23768467984212116\n",
            "44575: Similarity Score for a dialog: 0.2376892716712037\n",
            "44600: Similarity Score for a dialog: 0.23761523393214243\n",
            "44625: Similarity Score for a dialog: 0.23753786819023112\n",
            "44654: Similarity Score for a dialog: 0.2374641467297793\n",
            "44679: Similarity Score for a dialog: 0.23741124697715377\n",
            "44708: Similarity Score for a dialog: 0.23739045942100367\n",
            "44737: Similarity Score for a dialog: 0.23737254487533782\n",
            "44766: Similarity Score for a dialog: 0.2374255291888682\n",
            "44791: Similarity Score for a dialog: 0.2374653194927387\n",
            "44820: Similarity Score for a dialog: 0.23757914484028866\n",
            "44845: Similarity Score for a dialog: 0.23769026559881334\n",
            "44854: Similarity Score for a dialog: 0.23764550539797538\n",
            "44859: Similarity Score for a dialog: 0.23764026372357613\n",
            "44888: Similarity Score for a dialog: 0.23759054467810212\n",
            "44913: Similarity Score for a dialog: 0.2376308480936805\n",
            "44942: Similarity Score for a dialog: 0.2376516150617728\n",
            "44967: Similarity Score for a dialog: 0.2376502647845267\n",
            "44996: Similarity Score for a dialog: 0.23764149518922542\n",
            "45025: Similarity Score for a dialog: 0.23759722099538777\n",
            "45054: Similarity Score for a dialog: 0.23757013849012942\n",
            "45083: Similarity Score for a dialog: 0.23750861189289454\n",
            "45112: Similarity Score for a dialog: 0.23747085190881587\n",
            "45137: Similarity Score for a dialog: 0.23745160036181442\n",
            "45166: Similarity Score for a dialog: 0.23748463589630725\n",
            "45191: Similarity Score for a dialog: 0.23751864264451308\n",
            "45220: Similarity Score for a dialog: 0.237572316046804\n",
            "45245: Similarity Score for a dialog: 0.23760766759439012\n",
            "45274: Similarity Score for a dialog: 0.237625229591615\n",
            "45299: Similarity Score for a dialog: 0.23761915027382402\n",
            "45328: Similarity Score for a dialog: 0.2376269691066826\n",
            "45357: Similarity Score for a dialog: 0.23758875179788325\n",
            "45386: Similarity Score for a dialog: 0.23747779696101534\n",
            "45411: Similarity Score for a dialog: 0.23749617112749535\n",
            "45432: Similarity Score for a dialog: 0.23754050583191072\n",
            "45453: Similarity Score for a dialog: 0.2375815106444434\n",
            "45482: Similarity Score for a dialog: 0.23747440451761997\n",
            "45507: Similarity Score for a dialog: 0.2374446281484888\n",
            "45528: Similarity Score for a dialog: 0.2374685917414915\n",
            "45545: Similarity Score for a dialog: 0.23747192629395253\n",
            "45574: Similarity Score for a dialog: 0.23748131965450794\n",
            "45603: Similarity Score for a dialog: 0.2374756115144348\n",
            "45616: Similarity Score for a dialog: 0.23747953523286733\n",
            "45625: Similarity Score for a dialog: 0.23745376049367667\n",
            "45654: Similarity Score for a dialog: 0.2375047999486418\n",
            "45679: Similarity Score for a dialog: 0.23752668747358316\n",
            "45708: Similarity Score for a dialog: 0.2375452344154267\n",
            "45733: Similarity Score for a dialog: 0.2375634760872645\n",
            "45762: Similarity Score for a dialog: 0.23760206660378663\n",
            "45787: Similarity Score for a dialog: 0.2376474965228452\n",
            "45804: Similarity Score for a dialog: 0.23759308913754784\n",
            "45817: Similarity Score for a dialog: 0.23758449665527112\n",
            "45846: Similarity Score for a dialog: 0.2375879012639767\n",
            "45875: Similarity Score for a dialog: 0.23755378602000793\n",
            "45904: Similarity Score for a dialog: 0.237489262916643\n",
            "45929: Similarity Score for a dialog: 0.23748792388047937\n",
            "45958: Similarity Score for a dialog: 0.23749188365293106\n",
            "45983: Similarity Score for a dialog: 0.23754058359379387\n",
            "46012: Similarity Score for a dialog: 0.23766498036183298\n",
            "46037: Similarity Score for a dialog: 0.23767708462050152\n",
            "46066: Similarity Score for a dialog: 0.23769700121693388\n",
            "46091: Similarity Score for a dialog: 0.2377322256082218\n",
            "46120: Similarity Score for a dialog: 0.23769911681166403\n",
            "46145: Similarity Score for a dialog: 0.2376571972765313\n",
            "46174: Similarity Score for a dialog: 0.23768689375197813\n",
            "46199: Similarity Score for a dialog: 0.23767678185517624\n",
            "46228: Similarity Score for a dialog: 0.23768232421157126\n",
            "46253: Similarity Score for a dialog: 0.23767577191773853\n",
            "46282: Similarity Score for a dialog: 0.23767055561859596\n",
            "46307: Similarity Score for a dialog: 0.23772957132946934\n",
            "46336: Similarity Score for a dialog: 0.23770598699456447\n",
            "46361: Similarity Score for a dialog: 0.2376816811771657\n",
            "46390: Similarity Score for a dialog: 0.23771728475948228\n",
            "46415: Similarity Score for a dialog: 0.23766256562591306\n",
            "46452: Similarity Score for a dialog: 0.23760730111081177\n",
            "46489: Similarity Score for a dialog: 0.23755781206656476\n",
            "46518: Similarity Score for a dialog: 0.23753407147915223\n",
            "46543: Similarity Score for a dialog: 0.23752874407332417\n",
            "46572: Similarity Score for a dialog: 0.2374622627396562\n",
            "46597: Similarity Score for a dialog: 0.23745424378378183\n",
            "46626: Similarity Score for a dialog: 0.23755048124740924\n",
            "46651: Similarity Score for a dialog: 0.23759374392395277\n",
            "46680: Similarity Score for a dialog: 0.23755682247967663\n",
            "46705: Similarity Score for a dialog: 0.2374534117477637\n",
            "46734: Similarity Score for a dialog: 0.2374979990508345\n",
            "46759: Similarity Score for a dialog: 0.2374944564416234\n",
            "46788: Similarity Score for a dialog: 0.23752963173712727\n",
            "46813: Similarity Score for a dialog: 0.23755125723900344\n",
            "46842: Similarity Score for a dialog: 0.23755897864124878\n",
            "46867: Similarity Score for a dialog: 0.2375924575379093\n",
            "46896: Similarity Score for a dialog: 0.23752334358452357\n",
            "46921: Similarity Score for a dialog: 0.23750683497022038\n",
            "46950: Similarity Score for a dialog: 0.23754579871744985\n",
            "46975: Similarity Score for a dialog: 0.23751213496378307\n",
            "47004: Similarity Score for a dialog: 0.2374640713535846\n",
            "47029: Similarity Score for a dialog: 0.23741231214853578\n",
            "47058: Similarity Score for a dialog: 0.23743928864590658\n",
            "47083: Similarity Score for a dialog: 0.2374539124690744\n",
            "47112: Similarity Score for a dialog: 0.23751750338670563\n",
            "47137: Similarity Score for a dialog: 0.23750909792228878\n",
            "47166: Similarity Score for a dialog: 0.23749036244294797\n",
            "47191: Similarity Score for a dialog: 0.237492985954359\n",
            "47220: Similarity Score for a dialog: 0.23746144058775448\n",
            "47245: Similarity Score for a dialog: 0.23742882107206728\n",
            "47274: Similarity Score for a dialog: 0.2373753697804734\n",
            "47303: Similarity Score for a dialog: 0.2374210249751603\n",
            "47328: Similarity Score for a dialog: 0.23746864737593826\n",
            "47353: Similarity Score for a dialog: 0.23748536050417174\n",
            "47382: Similarity Score for a dialog: 0.23745787660178105\n",
            "47407: Similarity Score for a dialog: 0.23744144627911026\n",
            "47436: Similarity Score for a dialog: 0.23739930960767694\n",
            "47461: Similarity Score for a dialog: 0.23738114084613718\n",
            "47490: Similarity Score for a dialog: 0.23733676749135163\n",
            "47515: Similarity Score for a dialog: 0.23730999470341604\n",
            "47532: Similarity Score for a dialog: 0.2373211862076936\n",
            "47549: Similarity Score for a dialog: 0.23730596581772503\n",
            "47578: Similarity Score for a dialog: 0.2373557433590947\n",
            "47603: Similarity Score for a dialog: 0.23734921359848987\n",
            "47632: Similarity Score for a dialog: 0.23734179881924578\n",
            "47657: Similarity Score for a dialog: 0.23737593544502977\n",
            "47686: Similarity Score for a dialog: 0.23735075078774573\n",
            "47715: Similarity Score for a dialog: 0.23726721350234162\n",
            "47744: Similarity Score for a dialog: 0.2373134001207179\n",
            "47769: Similarity Score for a dialog: 0.2373771598335689\n",
            "47790: Similarity Score for a dialog: 0.2373650984091078\n",
            "47811: Similarity Score for a dialog: 0.23736410873840808\n",
            "47840: Similarity Score for a dialog: 0.23732844645455814\n",
            "47865: Similarity Score for a dialog: 0.23728551943107015\n",
            "47894: Similarity Score for a dialog: 0.2372656147456874\n",
            "47919: Similarity Score for a dialog: 0.2372580534877332\n",
            "47948: Similarity Score for a dialog: 0.23731717007989797\n",
            "47973: Similarity Score for a dialog: 0.23735644032657494\n",
            "48002: Similarity Score for a dialog: 0.23736441779953035\n",
            "48027: Similarity Score for a dialog: 0.2373566800671216\n",
            "48056: Similarity Score for a dialog: 0.2372735623183894\n",
            "48081: Similarity Score for a dialog: 0.23727383804164837\n",
            "48110: Similarity Score for a dialog: 0.23716200165279458\n",
            "48135: Similarity Score for a dialog: 0.23714925227248634\n",
            "48148: Similarity Score for a dialog: 0.23716672594203148\n",
            "48157: Similarity Score for a dialog: 0.23718382095853002\n",
            "48186: Similarity Score for a dialog: 0.23721565294299868\n",
            "48211: Similarity Score for a dialog: 0.23718776883074336\n",
            "48240: Similarity Score for a dialog: 0.23723773671175274\n",
            "48265: Similarity Score for a dialog: 0.2372480970205125\n",
            "48274: Similarity Score for a dialog: 0.23728139351935523\n",
            "48283: Similarity Score for a dialog: 0.23730227663827605\n",
            "48300: Similarity Score for a dialog: 0.23733656574440803\n",
            "48313: Similarity Score for a dialog: 0.2373422266585355\n",
            "48342: Similarity Score for a dialog: 0.23736374439770996\n",
            "48371: Similarity Score for a dialog: 0.23738338967371653\n",
            "48400: Similarity Score for a dialog: 0.23738451959356752\n",
            "48425: Similarity Score for a dialog: 0.23738330248745496\n",
            "48454: Similarity Score for a dialog: 0.2374340258269903\n",
            "48483: Similarity Score for a dialog: 0.23745896603522346\n",
            "48512: Similarity Score for a dialog: 0.2375165716638539\n",
            "48537: Similarity Score for a dialog: 0.23757499629399906\n",
            "48566: Similarity Score for a dialog: 0.23754510289437536\n",
            "48591: Similarity Score for a dialog: 0.237547317513394\n",
            "48620: Similarity Score for a dialog: 0.23754613215535533\n",
            "48645: Similarity Score for a dialog: 0.23749859355160927\n",
            "48674: Similarity Score for a dialog: 0.23758123057437544\n",
            "48699: Similarity Score for a dialog: 0.23761168634788535\n",
            "48728: Similarity Score for a dialog: 0.2375952156769314\n",
            "48753: Similarity Score for a dialog: 0.2375523764366566\n",
            "48782: Similarity Score for a dialog: 0.23756474487978832\n",
            "48807: Similarity Score for a dialog: 0.23752861127386565\n",
            "48836: Similarity Score for a dialog: 0.23749677586991355\n",
            "48861: Similarity Score for a dialog: 0.2375059696218041\n",
            "48890: Similarity Score for a dialog: 0.2374695145023214\n",
            "48915: Similarity Score for a dialog: 0.23742351517718474\n",
            "48944: Similarity Score for a dialog: 0.23737917041599338\n",
            "48969: Similarity Score for a dialog: 0.23735538832760017\n",
            "48982: Similarity Score for a dialog: 0.2373456306493737\n",
            "48991: Similarity Score for a dialog: 0.237333732491964\n",
            "49020: Similarity Score for a dialog: 0.23732223651537632\n",
            "49045: Similarity Score for a dialog: 0.2372739797677135\n",
            "49074: Similarity Score for a dialog: 0.23727789278412337\n",
            "49103: Similarity Score for a dialog: 0.23733495509096442\n",
            "49132: Similarity Score for a dialog: 0.23730902917155902\n",
            "49157: Similarity Score for a dialog: 0.23729032647877132\n",
            "49186: Similarity Score for a dialog: 0.23725373039575595\n",
            "49211: Similarity Score for a dialog: 0.2372589637561898\n",
            "49240: Similarity Score for a dialog: 0.23724014951188746\n",
            "49265: Similarity Score for a dialog: 0.23729234613741496\n",
            "49294: Similarity Score for a dialog: 0.2372988189738031\n",
            "49319: Similarity Score for a dialog: 0.2372915095079202\n",
            "49332: Similarity Score for a dialog: 0.23729957734433488\n",
            "49345: Similarity Score for a dialog: 0.23731504067952264\n",
            "49374: Similarity Score for a dialog: 0.23737367681094315\n",
            "49399: Similarity Score for a dialog: 0.23744757384226514\n",
            "49416: Similarity Score for a dialog: 0.23744698603404096\n",
            "49433: Similarity Score for a dialog: 0.23743934159852864\n",
            "49462: Similarity Score for a dialog: 0.23745750894660456\n",
            "49487: Similarity Score for a dialog: 0.2374475784959902\n",
            "49516: Similarity Score for a dialog: 0.23743503462428328\n",
            "49541: Similarity Score for a dialog: 0.23746225758560005\n",
            "49570: Similarity Score for a dialog: 0.23746600017356478\n",
            "49595: Similarity Score for a dialog: 0.23744567287591792\n",
            "49624: Similarity Score for a dialog: 0.2373929520096278\n",
            "49649: Similarity Score for a dialog: 0.23738969477996855\n",
            "49678: Similarity Score for a dialog: 0.23739418297977313\n",
            "49703: Similarity Score for a dialog: 0.23738626652127642\n",
            "49732: Similarity Score for a dialog: 0.23736566092487302\n",
            "49757: Similarity Score for a dialog: 0.23732336891143055\n",
            "49778: Similarity Score for a dialog: 0.23740772049188635\n",
            "49799: Similarity Score for a dialog: 0.23734335412286223\n",
            "49816: Similarity Score for a dialog: 0.2373512494194884\n",
            "49829: Similarity Score for a dialog: 0.23737016942926598\n",
            "49858: Similarity Score for a dialog: 0.23730237057641113\n",
            "49883: Similarity Score for a dialog: 0.23734647220487223\n",
            "49892: Similarity Score for a dialog: 0.2373709021674355\n",
            "49901: Similarity Score for a dialog: 0.23738232194769684\n",
            "49930: Similarity Score for a dialog: 0.23736770553327888\n",
            "49955: Similarity Score for a dialog: 0.23731033928537767\n",
            "49984: Similarity Score for a dialog: 0.23733057463492588\n",
            "50009: Similarity Score for a dialog: 0.2373343171999797\n",
            "50038: Similarity Score for a dialog: 0.23736123711868864\n",
            "50063: Similarity Score for a dialog: 0.23735357768789012\n",
            "50092: Similarity Score for a dialog: 0.23738084102254603\n",
            "50117: Similarity Score for a dialog: 0.2373805421763085\n",
            "50146: Similarity Score for a dialog: 0.2374114791883668\n",
            "50171: Similarity Score for a dialog: 0.23745361922852093\n",
            "50200: Similarity Score for a dialog: 0.23742046418720175\n",
            "50229: Similarity Score for a dialog: 0.23739652124372768\n",
            "50258: Similarity Score for a dialog: 0.23734954581945839\n",
            "50283: Similarity Score for a dialog: 0.23733387512920923\n",
            "50312: Similarity Score for a dialog: 0.23737332884997114\n",
            "50337: Similarity Score for a dialog: 0.23737437681209253\n",
            "50366: Similarity Score for a dialog: 0.23739148589056033\n",
            "50391: Similarity Score for a dialog: 0.2373984967776679\n",
            "50416: Similarity Score for a dialog: 0.23736948716378226\n",
            "50437: Similarity Score for a dialog: 0.23739191739064536\n",
            "Count:  1958\n",
            "Average:  0.23964630099720696\n",
            "[0.3493618220090866, 0.3425620687859399, 0.28388609417847227, 0.2417905932292342, 0.2467489690968284, 0.2414254258003305, 0.2554341968058086, 0.24380125400299826, 0.24595733880996704, 0.24300135046121527, 0.23911622361115673, 0.23095074716351321, 0.2292991311599811, 0.23180824858618193, 0.2290712069099148, 0.22634249314179886, 0.2252393566856259, 0.2236574177351809, 0.22283298855360884, 0.2274638787250627, 0.22884657179825327, 0.22746567891501798, 0.22604352032335906, 0.22311906396027872, 0.21913727219183962, 0.22050505123982495, 0.21876032805691162, 0.22166474037056352, 0.22239478298329882, 0.22172040515983035, 0.2214784326652686, 0.2201375113299457, 0.22516119140417465, 0.22416545218558767, 0.22681158314486152, 0.2261380366629587, 0.22372885136843818, 0.22288626636068026, 0.22242048116208152, 0.22095346504024097, 0.22071689236529018, 0.21784895642971, 0.2202459425235597, 0.22031697383227197, 0.21874481610899424, 0.21888472993991204, 0.21912935202233882, 0.22280793494637102, 0.224771682942907, 0.22605953723484395, 0.22892018864615657, 0.22967564109070548, 0.230900450741038, 0.23323969742709613, 0.23444302351513444, 0.23337048965411775, 0.2337101782465445, 0.23415495723720778, 0.23465118152769596, 0.23693769223495456, 0.2367029849892935, 0.2360430177291261, 0.2382326536318263, 0.23892711590889334, 0.23754969633871106, 0.23618480355122048, 0.23561592577621707, 0.2354978476457462, 0.23361937961397836, 0.2328506245750104, 0.23386834751128593, 0.23536160685892762, 0.2363261619240654, 0.23709279645418105, 0.23781108330713202, 0.23750929543362984, 0.23710871464672775, 0.23610589177331934, 0.23551775518984414, 0.2343193515748775, 0.23502804856206497, 0.23652150979397274, 0.23663955269403666, 0.23727556254978724, 0.23886368591519236, 0.23816092152884963, 0.2379484719575475, 0.2381961865728788, 0.23923358456528143, 0.2394978603900031, 0.23978147622354506, 0.2396558432706765, 0.23983905422501267, 0.2393174094704141, 0.23906860669609972, 0.2384016886739533, 0.23873517037564537, 0.23936092030798267, 0.23975777739306559, 0.24015631129261245, 0.23997188563928132, 0.2394271417385281, 0.23962860092556823, 0.23939182658414968, 0.23984873559955644, 0.24114078098211397, 0.24117609956835168, 0.24149102529373728, 0.2420732515676114, 0.24144247459681858, 0.24246117575329357, 0.2420929517460001, 0.2415726771483298, 0.24100137293650561, 0.24015247501166803, 0.23976859172526482, 0.23891062808822616, 0.23731165123301454, 0.23627185621817676, 0.23515265363447704, 0.23586941436562386, 0.23589802514037603, 0.23540114759434147, 0.23579871570703737, 0.23561660637119797, 0.23513842397279316, 0.2349614693278261, 0.234328004036354, 0.23436967182389032, 0.23483508366849576, 0.23582281371103692, 0.23604143103035463, 0.23650399395441374, 0.23683413020443167, 0.23667468449461584, 0.23719158450848538, 0.23727564031395285, 0.23741585486819466, 0.2372641241864739, 0.23714818068703594, 0.23737556380461375, 0.23645098316232374, 0.236567157208184, 0.2362932076252301, 0.2366918953683321, 0.2370171844610013, 0.23776264297077696, 0.23758353237771873, 0.2367344055384945, 0.23632134170583274, 0.2367434278017706, 0.2365744968627346, 0.23729083905421683, 0.23770413869291898, 0.23823739543241734, 0.238639054648356, 0.23886640513392832, 0.23952886380093938, 0.24014609850322208, 0.24102305156508616, 0.2411711371427685, 0.24120859109970608, 0.24146824416388338, 0.2407528434992365, 0.24055984311935014, 0.2407389147933162, 0.24166096879423765, 0.24197498160990577, 0.24246102754166396, 0.24212798828915333, 0.2420839233165988, 0.24225434959061004, 0.24235310439432325, 0.24253044199344503, 0.24218044506918107, 0.24264206897301258, 0.24176680362931716, 0.24161648603929547, 0.24225231451095122, 0.24236556320123512, 0.24247294039874173, 0.24250040612245713, 0.24278515361164782, 0.24339487739698878, 0.24323796705064185, 0.24307420038742159, 0.24297195894894566, 0.24273583593256673, 0.2432781112050654, 0.24328669841142214, 0.24309514819859437, 0.24313963515852152, 0.2432508766886418, 0.24336946692779352, 0.24389694066129425, 0.24373973097591597, 0.24396155939853573, 0.24385186672529283, 0.24386155953696878, 0.2441183750155965, 0.24464135679183918, 0.24452311417475964, 0.24443319922987475, 0.24445013728097192, 0.24420068614322266, 0.2438873134737266, 0.24416132967175866, 0.24417853440271572, 0.24394979022434324, 0.24358913143010685, 0.2433534686278324, 0.24340735877764672, 0.2434462881654372, 0.243584288016737, 0.24373147711810086, 0.24416120244532039, 0.24440078623664604, 0.2443224985896684, 0.24385609133787095, 0.24379189943664212, 0.24369246366320912, 0.2434606971923662, 0.2431614733741887, 0.24285617965632833, 0.24237907956175272, 0.24207615117937636, 0.24199089083092018, 0.24217115435749292, 0.2426123044334161, 0.24294398539858356, 0.24339285295109606, 0.2435797066859685, 0.24325880167768124, 0.24320280602320826, 0.24324550357381147, 0.24292237555494223, 0.243154932840372, 0.24314782103370033, 0.24328160195168613, 0.24367252708797724, 0.24333400664756577, 0.24299729537850434, 0.2432244893268997, 0.243634360107761, 0.2434775142669473, 0.24342700098825953, 0.2427384775966264, 0.2422957479347617, 0.24241306660910528, 0.2427958379529657, 0.24267128682555164, 0.24270862388082698, 0.2425522160486612, 0.24303425865215858, 0.2429385648712359, 0.24296225164209884, 0.2425590493658092, 0.24191666552289626, 0.24161293202069611, 0.2409453675129909, 0.2409163689441745, 0.2406728206910977, 0.2407341772214406, 0.24090593101239982, 0.24138776901599152, 0.24181984093331194, 0.24199832838492877, 0.2420472918346631, 0.24178477013321975, 0.24195965484610088, 0.24192123846967634, 0.24180350543252763, 0.24180683592460614, 0.24181022186998113, 0.24147555733331988, 0.2414117919292921, 0.24112258876727016, 0.24113416910881566, 0.24108894498928876, 0.24099858083199233, 0.2408881722536381, 0.24067752381005206, 0.24057027913423412, 0.24037386394073174, 0.240617855969734, 0.24033919880728793, 0.24000565247802713, 0.23987379553861615, 0.239218903096413, 0.23873724716572603, 0.23899523282227825, 0.2388861119809918, 0.23887792210190756, 0.23899905902567567, 0.2386004291107192, 0.23885567827294704, 0.2390034570469479, 0.23939234769240103, 0.23952204451626996, 0.23967467538238812, 0.23986786759975315, 0.24024084602586665, 0.2408747813782253, 0.24113853387873926, 0.2411105746952332, 0.24086580030079136, 0.24100213924875474, 0.24079157611996407, 0.24072883854440272, 0.24051850236331423, 0.24040264286982702, 0.24049303750976156, 0.2403577898383278, 0.24025097491488112, 0.23983041888500214, 0.23960225415939626, 0.23965207273130854, 0.23965489485319927, 0.23964562651801435, 0.2396478587615102, 0.23972720552439614, 0.23974814206334114, 0.23984203045802874, 0.23967177659109243, 0.23974796810418225, 0.2396586874319676, 0.2394254493446456, 0.2393501445049809, 0.2393402502303193, 0.2392122909096894, 0.23925712571533075, 0.23895614377220187, 0.23899892773617196, 0.23886902910322025, 0.23896038449890233, 0.23900166462640066, 0.23880459672391022, 0.2386701506521169, 0.23869943919053468, 0.2383630843733772, 0.23854376091914262, 0.23831750371274477, 0.2381044492233711, 0.2379900571364551, 0.2381249079840677, 0.2381598327054574, 0.2384570482230691, 0.23884325257544328, 0.23927817355032086, 0.23938257482074315, 0.2394636677449904, 0.23905018203344847, 0.2388530465474776, 0.23876559120752003, 0.23915865398843486, 0.23899611513254085, 0.23917299233161268, 0.23938689051424544, 0.2395516123592051, 0.2394751854672235, 0.23970897059059806, 0.23962374918006127, 0.23948901692025074, 0.23938760741659595, 0.23922043418881933, 0.23915035797784823, 0.23924825555305054, 0.23916695220873843, 0.2390831270995898, 0.2390911784279222, 0.23908425797533273, 0.23919450159708605, 0.23930783838524924, 0.23922226519039302, 0.23903910015828556, 0.2392500314229016, 0.23946447283867667, 0.23988706563404405, 0.24001926439693003, 0.2401305955545795, 0.24014515768394268, 0.24019484666008276, 0.2401773816489536, 0.24028245442037707, 0.24035839783067772, 0.24023132740488817, 0.24025703280955707, 0.24037846757680761, 0.24006544140198394, 0.2399487286671933, 0.24013571876735063, 0.24031642087752111, 0.24011564890486228, 0.240180904558592, 0.24013301845588572, 0.24042037351382256, 0.24038893829393243, 0.24039700215661544, 0.24043959677944096, 0.2406928881323404, 0.24076965245927118, 0.24068623059362185, 0.2408245226759479, 0.24072482395206873, 0.24072649521180947, 0.24068915325408669, 0.24078015939353326, 0.24073642531567405, 0.2407292224732893, 0.24081591777758443, 0.24093821721128292, 0.24099893639484046, 0.24113668392549237, 0.24114433739898333, 0.24168738769348835, 0.24195834624371002, 0.24196493383644244, 0.2420573434638218, 0.2418581251424162, 0.24170753853406543, 0.24189863613805573, 0.2420171178759667, 0.24247394751602344, 0.24250962247592814, 0.2423181432499709, 0.24226308180393227, 0.2423261454892637, 0.24240308190734225, 0.24263639970364984, 0.242481863766265, 0.24258184138922645, 0.24276667238401228, 0.24284432097712647, 0.2428353932315313, 0.2427410358561659, 0.24284137551078183, 0.24288223287199814, 0.24305385370414867, 0.2430112528713925, 0.2430457615000722, 0.24291294300339705, 0.24270711887078864, 0.24265084057868422, 0.24267324858345435, 0.24284951345701328, 0.24307348244679708, 0.24301229795385706, 0.24321816759057918, 0.24316120158241938, 0.24317632776936712, 0.24313448723696532, 0.24333570603971227, 0.24337317151524562, 0.24313782914346796, 0.24308903401596227, 0.24318573901787588, 0.24319425256680002, 0.24319654224297996, 0.24296023566441846, 0.24297109290435487, 0.24302304180292067, 0.24286546227349118, 0.2427475423595505, 0.24280487561245317, 0.24265903700601557, 0.242655387656909, 0.24251867684300077, 0.2426188588429056, 0.24269541792574148, 0.2427144115214951, 0.24260898431964245, 0.2426442453088511, 0.24258061248887922, 0.2424564742985771, 0.24283643667225968, 0.24289008280291877, 0.24308313418854632, 0.24333591808517693, 0.2431792017050773, 0.24283931501522615, 0.2427096246140286, 0.24275427306087385, 0.2430327271957909, 0.24343469259092362, 0.24348850542220224, 0.2437017287680558, 0.24382041319526954, 0.24359722870655465, 0.24373924330301933, 0.2437491487507064, 0.24412370918115125, 0.24440879013393074, 0.24437533522701907, 0.24441074643614172, 0.2443767648081203, 0.2443470894609674, 0.2445604420585924, 0.24479976245283766, 0.244924834787403, 0.2448870792143491, 0.24490675312667254, 0.2446994197906001, 0.24497371180701294, 0.2450194209121503, 0.24544111663142865, 0.2455445093360765, 0.2455699398403846, 0.2455743104243533, 0.2457016887267671, 0.2457831505925999, 0.24580375805968815, 0.2456006357993517, 0.24568074556270567, 0.24570896960544344, 0.24558107539970928, 0.24559028065292554, 0.24545956302059543, 0.24516361495219596, 0.24506543390334787, 0.2449217138335236, 0.24495586691450122, 0.24480772445127416, 0.24503639730900564, 0.24511552748949916, 0.2448953950076244, 0.24487174505430118, 0.24482448589641237, 0.24501942442130617, 0.2449284791410554, 0.24477520113561851, 0.24472263587769316, 0.24471816687653838, 0.24467053738245706, 0.24464050654857605, 0.24455735541118528, 0.24436715184159985, 0.24445966148071824, 0.2445320476705447, 0.24460484429240195, 0.2445320554758936, 0.2445994653661425, 0.24466295446989256, 0.24472309875187043, 0.24466046280544404, 0.2445777383568835, 0.24411499886289978, 0.24396598759096413, 0.2436530025498034, 0.2434917023307384, 0.24345541578748306, 0.2434743705306056, 0.24335375879733787, 0.24332518450054222, 0.24309402493501364, 0.2431274015339199, 0.24294064485159905, 0.24287047154761074, 0.2427593381974133, 0.24310639445564228, 0.24308098362914882, 0.24298291636985941, 0.24287652286394978, 0.2428610706765423, 0.24274643403342994, 0.2428317372975421, 0.24286440794415065, 0.24273110703908723, 0.24264705995004543, 0.24273711980182044, 0.24270036643875342, 0.24269707481080413, 0.24274896991925962, 0.24288473500912622, 0.2427640537522358, 0.24265974309211352, 0.24254145861539192, 0.2424871301781797, 0.24239992467228497, 0.24246710827668605, 0.2424151877136495, 0.24244308880747834, 0.24236786718970485, 0.24251845736746916, 0.24256209901971862, 0.2424668180704372, 0.24226602736552233, 0.24226695697151318, 0.2423384468779329, 0.24239197772463983, 0.24247607919176206, 0.24247538915890923, 0.24252847023141144, 0.24265582454792456, 0.24273221977452197, 0.2423661393319287, 0.24214150291740416, 0.24217854237984024, 0.24224149432286166, 0.24235033919877852, 0.24242804360866982, 0.24260017884636126, 0.24261002942357604, 0.2429139342051912, 0.2430297118308155, 0.24298422344814094, 0.24282876064754647, 0.24291672879400508, 0.24290286261262983, 0.2430323049006809, 0.24311868334873854, 0.2430770641744827, 0.24298998144714257, 0.24292519724320588, 0.24281679011904617, 0.24285556423052929, 0.2429450485598586, 0.24282747774785954, 0.24268400319317326, 0.24275769743795073, 0.24271115601718768, 0.24252915019868737, 0.2424692956375527, 0.24223719128360752, 0.24225127231901417, 0.24232463424107378, 0.2424105803018634, 0.2425038569950278, 0.24243943025425682, 0.24242728564087895, 0.24237602208792788, 0.2423060940962985, 0.24225924520923747, 0.24203119882767488, 0.24186431354623705, 0.24188033528518504, 0.24207983131728364, 0.2421138051523854, 0.24201049268234764, 0.2420896791393517, 0.2419861305434217, 0.2420896265914378, 0.24227570262820303, 0.24219404948896073, 0.2423402267244809, 0.24227205361298407, 0.24227840885948776, 0.24201065774973732, 0.2420008804131999, 0.2419621910285611, 0.24175199894180932, 0.24167082320641453, 0.2417034532947535, 0.2416898042198495, 0.24159575127769578, 0.2416356209894149, 0.24153055002257376, 0.2415302769378434, 0.2413916979257704, 0.24146865530114084, 0.24158166681249607, 0.2414506198065685, 0.24120920910714874, 0.2412153991299602, 0.2412219965544007, 0.24128810623440022, 0.24128384695104438, 0.2412534970550911, 0.2412847975918146, 0.24117907781291809, 0.2411026413112534, 0.24141123624538344, 0.24169626932554747, 0.24162602390219254, 0.24162549311880593, 0.24166195686039074, 0.2416330703212192, 0.2414145590738578, 0.24117152801239927, 0.24103956535110152, 0.2412101343987938, 0.241292916397225, 0.2413516142609331, 0.24133430101016778, 0.24118242429420547, 0.24114215400103017, 0.2411076997900287, 0.24111430344427617, 0.2410271020256585, 0.24098651851879022, 0.24095480272214748, 0.240755444603005, 0.24066924598728867, 0.24079966102013622, 0.24080774320325155, 0.24067744061313917, 0.24059514629945516, 0.2404831530741201, 0.24045257001108497, 0.24051806758439723, 0.2404941690620035, 0.240533495541631, 0.24060369909245782, 0.24061503558001815, 0.240692961160689, 0.24062591114858792, 0.24053993055766085, 0.24049815789537568, 0.2402863575365551, 0.24033137983586184, 0.24030002620585342, 0.24026522745546183, 0.24027725091849622, 0.2403446172711902, 0.24043936605847832, 0.24046641535334273, 0.24057710779840494, 0.24048661902858234, 0.24036938989703657, 0.24029001179218268, 0.240184772556446, 0.24034813022822474, 0.24039562901978415, 0.24026268359770572, 0.24027154015494878, 0.24016898832104294, 0.2401101612562674, 0.2401264869820628, 0.2401545978488101, 0.24009841198008322, 0.2401463149278533, 0.2401788916236231, 0.24021366320427254, 0.24012221582161916, 0.24024045128134142, 0.2402254248420841, 0.24029083177048005, 0.2402950845103666, 0.2403987607887234, 0.24031390457125804, 0.24033711305623984, 0.2402804299459777, 0.24024847672065688, 0.24016657666445937, 0.24013218355739652, 0.24032635999914637, 0.24031462352074814, 0.2401874789283752, 0.24010653925539496, 0.2400219774266193, 0.24000869484162682, 0.2399448528878827, 0.24000367077184384, 0.24022015274748157, 0.24035592611363749, 0.24026151922940586, 0.2402406517464208, 0.2402484418576479, 0.24021830535202007, 0.24019458549212927, 0.24017385820770096, 0.24004733694054373, 0.2399222554413751, 0.23993842836636953, 0.24009628195712854, 0.24002441568733554, 0.24013461063928054, 0.24001026307378687, 0.24011888859086733, 0.24010933632776726, 0.24014196641712146, 0.2401855361432811, 0.24023830891127604, 0.24033267506014536, 0.24037767291440434, 0.24034533053905052, 0.24033520143414244, 0.2403456604084337, 0.2403426309343174, 0.2403410713904196, 0.2403574955509186, 0.24033816615072462, 0.24024840522386368, 0.24032370222329094, 0.2404919995626317, 0.24049183425691467, 0.24043072752393455, 0.24044017129592574, 0.24038085144526164, 0.24044638761215253, 0.24045498611239674, 0.24045020165118638, 0.24045647521537447, 0.24047576485877195, 0.24051828143178708, 0.24040184521470656, 0.24022023928108935, 0.24023993599638954, 0.24036004711911377, 0.24022063281647665, 0.24022352325809987, 0.24017872790026668, 0.24026126951689875, 0.24014469934997507, 0.2401329481577819, 0.24018716403703808, 0.24028688497175996, 0.24021562612809358, 0.24017461344165517, 0.24020534275160782, 0.24006627284742235, 0.23993773285331974, 0.23980940623573374, 0.23972586745764313, 0.23973660053504176, 0.23973076974340948, 0.2396971730584556, 0.2398018333738413, 0.23972142201764515, 0.23957107730334481, 0.23953589621019428, 0.2394500673721716, 0.2393587785161158, 0.23927462100137256, 0.23929320704943224, 0.23937073958225033, 0.23938172224519894, 0.23931504770478873, 0.23939018339671264, 0.23930382042506293, 0.23920701927018936, 0.2391774528092166, 0.2391896955708236, 0.23922532121258464, 0.23944182504519418, 0.23952591459108744, 0.23944763746072162, 0.2396347454411845, 0.2395936834301872, 0.23962305985873353, 0.2395421216748973, 0.23940491518476031, 0.23928939545263073, 0.23920738398654007, 0.2392478658425493, 0.23922639915858687, 0.23931366002310045, 0.23932728098085296, 0.23938908174502196, 0.23937539459231424, 0.23935193275832883, 0.23921164549325918, 0.2390986732432062, 0.23909707313177994, 0.2389886648342125, 0.2390047221167324, 0.23890584260206912, 0.23887863450609687, 0.23885672430231691, 0.2389675533856623, 0.23896364028987252, 0.23897276774095963, 0.23898191249898254, 0.23881300645047157, 0.23903743902487834, 0.23924858174205407, 0.23931887118113496, 0.23931893929427342, 0.23950447281619952, 0.23953167977962583, 0.23959883621613753, 0.23969238374576124, 0.2398742458428239, 0.23994793356738556, 0.240078756610697, 0.24007633256782795, 0.2401341566326887, 0.24023367303989993, 0.2403699301111157, 0.24028597297682677, 0.24024759918629474, 0.24021178865801018, 0.24032880981406757, 0.24036744290228176, 0.2403913956080114, 0.240384397494769, 0.24047409508501028, 0.24052292981845058, 0.24058162753599066, 0.2405923100725196, 0.24062113419076628, 0.24056413425042644, 0.24062613662023646, 0.24062927906909068, 0.24069013952456456, 0.2408318395632045, 0.24087950145772397, 0.24080051837813535, 0.24066590959524395, 0.24061165375443896, 0.24059424651491226, 0.2405554585189326, 0.24060780872502024, 0.2405734607453352, 0.24060874407162428, 0.2405052180098073, 0.24049986502066956, 0.24048269511311027, 0.2404532058999965, 0.24036065467194223, 0.2403538287466483, 0.2403548493847014, 0.24037873599516144, 0.24050171858903366, 0.2406092932720483, 0.24068585207393625, 0.2408443790257676, 0.24084041211864082, 0.24093125225199438, 0.24081157726607533, 0.2408138385865942, 0.24079354172060502, 0.24075488999304182, 0.24077410970980545, 0.2408311294357045, 0.24071252015878258, 0.24063171687988608, 0.24071079620557298, 0.24089499589933633, 0.24094402638922816, 0.24093933489508174, 0.24079833322338637, 0.2406793926212404, 0.24058065450209798, 0.2404935362179109, 0.240472263073361, 0.24047929385068714, 0.24059808321387605, 0.24054742614129979, 0.240433861768363, 0.24047439254070552, 0.24043946881640252, 0.24033036322582085, 0.2404225998354778, 0.2404911626030247, 0.24060213991382506, 0.24056215067467504, 0.24081192005100005, 0.2408996672255547, 0.24098061755189176, 0.2410596697611317, 0.24111877512730023, 0.24125494339670608, 0.2413598473040826, 0.2414299487425041, 0.2414672488228911, 0.24157585200688367, 0.24149144050785773, 0.2415035319640408, 0.24146322465520895, 0.24141860940352436, 0.24124727307882277, 0.241167724176463, 0.24112337179430313, 0.24107870003316254, 0.24096055680902168, 0.24092813180389092, 0.24098725537793445, 0.24100923695482523, 0.24090831498227522, 0.2408802265982883, 0.2409179343658831, 0.24097779223846902, 0.24095196979646874, 0.2408869353303119, 0.24081169383849474, 0.2408614412341123, 0.2408577460907831, 0.24096664617306432, 0.24097046107220882, 0.24109578154944394, 0.24110981688128622, 0.24120396399992672, 0.24125122088253795, 0.24123156702759985, 0.24118732932729248, 0.24111424028450348, 0.2409875389493799, 0.24097002274175386, 0.24097932926429727, 0.24091071934366987, 0.24093032313037724, 0.24095024920872019, 0.24089484332092756, 0.2409564081581479, 0.24099145442314623, 0.2410473843517213, 0.2410777962100939, 0.24107987934818442, 0.24111230321106616, 0.24114935764857837, 0.24099768968971852, 0.24093713974393993, 0.24088061776115993, 0.24077104770423546, 0.2407017568522148, 0.24070965454565268, 0.2407195632460419, 0.24063151531573296, 0.24060208466772254, 0.24055587434531245, 0.24052910135391897, 0.2406135108817077, 0.2406428567601945, 0.24064761628251913, 0.24071912613259158, 0.24066372992362708, 0.24061696992747958, 0.2405915879533494, 0.24059535526268705, 0.24069004694002094, 0.24065419070701238, 0.2406745888841901, 0.2406295363879268, 0.24057507189778787, 0.24059627728458322, 0.24060160626008253, 0.24063792457979283, 0.24065149211276773, 0.24052359749026658, 0.2404541157620673, 0.24027092017157403, 0.2401841056606944, 0.2402276841942148, 0.24016408378012544, 0.24016921935372734, 0.24020552531862246, 0.2401958001534907, 0.24032074442398924, 0.24017095855220238, 0.2400424480539086, 0.24005945499613968, 0.24002958976054614, 0.24016297299429737, 0.2402692310531889, 0.240210458687804, 0.24015913635837488, 0.2401405436535478, 0.24012015835268466, 0.2402192932666114, 0.24035527642739926, 0.24038289533824253, 0.24039023957201378, 0.24039105981645367, 0.24041544694806738, 0.2403747554056049, 0.2403665615216819, 0.24046321528526649, 0.24054679885615804, 0.24075345302153492, 0.24088707805679807, 0.2408835848705903, 0.24088060744341408, 0.24088021506682983, 0.24083270595914255, 0.24106910692649872, 0.2411683380892225, 0.24122069135061108, 0.2412444017641599, 0.24118012097553573, 0.24107244090169522, 0.2410726818355383, 0.24101377231315643, 0.24105030678719477, 0.24102790393177043, 0.2409262687644634, 0.24090390589825003, 0.24094128278941748, 0.24098438615092937, 0.24096319655790116, 0.2408521451760873, 0.24087544243758946, 0.24093636233612928, 0.2409606272838173, 0.24098725656579428, 0.24100348644526576, 0.2410609439585763, 0.24108268445409914, 0.2411943560428588, 0.24119215972528674, 0.24123681378244116, 0.24117235949823454, 0.2411064413249085, 0.241113411010998, 0.241135757611814, 0.2411015321284267, 0.2410766013183154, 0.24117254382115308, 0.241197886931217, 0.24118579069048812, 0.2411741845409392, 0.2411232385048144, 0.24115490978208898, 0.24100173441393793, 0.2408421175372941, 0.24069526401825442, 0.2407465725564861, 0.24070517043212822, 0.2406645977453182, 0.24065322131564745, 0.24070498364210313, 0.24066549080906594, 0.24057334786308174, 0.24043321935861928, 0.24032468238198443, 0.24032775530634895, 0.24036486632519005, 0.24039084481046197, 0.24031373560902686, 0.24033886645047964, 0.24037592303890168, 0.2403925412024396, 0.2403961953416227, 0.24039903194501008, 0.24030158594925366, 0.24028510455122418, 0.24028594221152105, 0.24041283700470747, 0.24041788135859046, 0.24046424699702496, 0.2405024420927483, 0.24047759345588102, 0.24052287560719873, 0.24052095838471985, 0.24054737030776882, 0.24048127285454962, 0.24040694532256235, 0.24039063742202674, 0.24040712004379225, 0.24047722195915092, 0.2405037206082341, 0.2405092901909108, 0.24051226687314745, 0.2405043496317426, 0.2403976708054461, 0.24034981436992425, 0.24036429713874724, 0.24053163078045384, 0.24053728074845068, 0.2405563863857581, 0.24060067534802806, 0.24062805037381155, 0.24071091270998396, 0.24063680177784225, 0.2406673208414515, 0.2406787949634063, 0.24071648618404057, 0.240823285140194, 0.240684942373638, 0.24063859025168452, 0.24062014575605437, 0.24062924415930967, 0.2406339127120887, 0.2406421179198572, 0.24068207501071015, 0.2407361789215797, 0.24069984234533756, 0.24057385937233822, 0.24046197672454245, 0.2405501274476832, 0.24047264919013064, 0.2403740862369376, 0.2404428861807343, 0.24038443778327115, 0.24023000296719402, 0.24029048547617368, 0.24028305779549444, 0.24022466433556472, 0.2402117157932409, 0.24029857258955686, 0.24035219406899944, 0.24035959296369178, 0.24028945319441727, 0.24030245276805656, 0.24028269594505186, 0.24027852384188245, 0.24018347746254143, 0.24004540609874567, 0.23996491151837593, 0.2398869767055347, 0.23978519441585508, 0.23991175837676615, 0.23993814428845942, 0.23991969859165543, 0.239920199635074, 0.23991458418231504, 0.23981040685864083, 0.2397311023487301, 0.23970554282113837, 0.2396436228712394, 0.23962760585430976, 0.23958078484712395, 0.23961895220337903, 0.23953364969439617, 0.239518010496795, 0.23942384386520643, 0.23939392375091442, 0.23938182444848086, 0.23934294452745403, 0.23929324142492875, 0.23922297392263409, 0.23927603757631483, 0.23925824323486622, 0.239314974070977, 0.23936566738204754, 0.23922814691517189, 0.23915649553088394, 0.2391827695978307, 0.23925510039047482, 0.23928847843449508, 0.2392634447645273, 0.23922695752991271, 0.23920883724237554, 0.2390880673827936, 0.23905689687936899, 0.23906335265985204, 0.23905332990833728, 0.2390326115112547, 0.23902754777398097, 0.23910788410311196, 0.239141749105799, 0.23911849536912014, 0.23914318507621246, 0.23914534947789232, 0.23914298191149946, 0.23913293008613282, 0.23913538267982387, 0.23918949519181493, 0.2391030064183992, 0.23902952188008483, 0.23900056634692976, 0.23896419381014214, 0.23888875776258112, 0.23880032971805107, 0.23886497753387004, 0.2388622574506683, 0.23886850851574096, 0.23883895894153945, 0.23890020743422702, 0.23890679578083576, 0.238881933170551, 0.23876855180610082, 0.23871842597150086, 0.2387878936264446, 0.23870857307550747, 0.23868404691836068, 0.23868673027435428, 0.23872080754369604, 0.23859698724859923, 0.2385622989959602, 0.23864528895560114, 0.23865411232162978, 0.23869064778727972, 0.23867557892518693, 0.23865173768236814, 0.23878839729662407, 0.2388664108786713, 0.2388769053988558, 0.23885627780826366, 0.23884398955565042, 0.23882885006767726, 0.2388540598367611, 0.23890187250026937, 0.23883125442545683, 0.23872048100971707, 0.23874867072407624, 0.23873649799701302, 0.23870517998087615, 0.23873558266739484, 0.23875400065935937, 0.23880012291034927, 0.2387637474286853, 0.23876971422962087, 0.23878566963494588, 0.23879455931346644, 0.23890970304293815, 0.2389467535728068, 0.23892547518044174, 0.2389004137131354, 0.23897359985436195, 0.23906758703000983, 0.23913617700917808, 0.23918001235575725, 0.2391608422963287, 0.23917183099341383, 0.23912296222865304, 0.23913819955479845, 0.23910273454176476, 0.23908574507748187, 0.23919982050502284, 0.23923328291456955, 0.23921794543429964, 0.23918400884987306, 0.23925191646872324, 0.23927516141697974, 0.23938770285245142, 0.23935262032428117, 0.23946877293961488, 0.23951028478563804, 0.23959867148417485, 0.23960857566191912, 0.23958429795242717, 0.23955189444688849, 0.23952261666888633, 0.23947433437480722, 0.23948015625728117, 0.23953009678176262, 0.2397398554471765, 0.23994350251632923, 0.23992756779474533, 0.23981563901375297, 0.23986463666526783, 0.23994225466164484, 0.23987575899755723, 0.23988133776122908, 0.2398009066110586, 0.2397831723398864, 0.2397270168705567, 0.23970841216033562, 0.2396295234694198, 0.23960642634187124, 0.23956342110197926, 0.23953086435953036, 0.23946082804862934, 0.23940460130554894, 0.23944803809933543, 0.23943054840287475, 0.23945024191983486, 0.23951535716005476, 0.239476871065454, 0.23933667330689037, 0.2393384748579696, 0.23934342989315704, 0.23921656832470484, 0.2391880010695443, 0.23920744613285913, 0.23921574278564003, 0.23921736906505456, 0.23919478252667886, 0.23919415992825263, 0.23919293248761486, 0.2391535590321244, 0.23917306917186898, 0.23907539495612154, 0.23913963296232402, 0.2390856362061931, 0.23914866497096196, 0.23912785307438292, 0.23921931615513614, 0.239173483610963, 0.23909516513428547, 0.23917290192306262, 0.2390871784601779, 0.23915989991491698, 0.23917982101739496, 0.23907875685218918, 0.23907010191293745, 0.23902790636991403, 0.2390672258262092, 0.23902439122913782, 0.23895125585377464, 0.23897904591371827, 0.23897991810639477, 0.23897050627563668, 0.2389687474339362, 0.23897304078765394, 0.238963695298864, 0.23897754674516636, 0.23906786389382464, 0.2391803909519907, 0.23918548023177386, 0.23923849752033233, 0.2392875907189746, 0.23923719774317403, 0.23923751922046052, 0.23918626761925074, 0.23921335953778064, 0.23918495068300746, 0.23913479970334636, 0.2390285615377074, 0.2390873518240139, 0.2390955625765498, 0.23916401084697161, 0.23912970723663476, 0.2391023476203692, 0.23907583748359815, 0.23911826402201264, 0.23910731435528615, 0.23919395780435646, 0.23917441474494056, 0.23918904063436178, 0.23913847073888675, 0.23912423052435508, 0.23911177039221157, 0.2391138273358846, 0.23909959787509202, 0.23906443595550825, 0.2389895222574454, 0.2389416706299471, 0.23892307364680726, 0.23892051236899303, 0.23883799106641196, 0.23883207874856263, 0.23885742455960676, 0.2388626350346694, 0.23884990185194524, 0.2388990974341792, 0.23887531095474276, 0.2388375884140258, 0.23892100208385428, 0.23900346538736902, 0.23892896616021342, 0.23891669695704557, 0.23886119619028465, 0.23881443667890515, 0.23881287373936672, 0.23886987643724605, 0.23889715223453928, 0.23890116969133482, 0.23885916272524174, 0.23874412414678683, 0.23874691112100013, 0.23878856747253785, 0.2388100584659235, 0.23880489763381393, 0.23885070201407438, 0.23884808602016874, 0.238838234096206, 0.23880967897190566, 0.23888690763969833, 0.23884341393948066, 0.2389064714646279, 0.23889849898255244, 0.23889212514228506, 0.23889559233128443, 0.23890732812868593, 0.23890316060759298, 0.23893432842772272, 0.23890289824006455, 0.23892900175614623, 0.2389788807942336, 0.23897291953124714, 0.23895841363632928, 0.23895036838212955, 0.23897407989679986, 0.23896306907342074, 0.23893507494771765, 0.2390015040691812, 0.2390116840475716, 0.2390361318664792, 0.23902184975489266, 0.23906795356549165, 0.23906348021115173, 0.2390552098949544, 0.23908294649233788, 0.23911750887972566, 0.23915530959319378, 0.239111843931185, 0.23901301800953154, 0.23906359232009614, 0.2390886092150704, 0.23910991098574405, 0.23914123991373018, 0.2391761589098901, 0.23920494391635247, 0.23921499252811249, 0.23923810779441243, 0.23925739916587277, 0.2392176672794555, 0.23928700890299046, 0.239361427398876, 0.2393344384749315, 0.23935787236373282, 0.2393359503058922, 0.23932203098268542, 0.23923911013410162, 0.23919098175556036, 0.23928475317329234, 0.23938525595599705, 0.2392853446117514, 0.23917391314408296, 0.23925703324478834, 0.2392117213080348, 0.23917996403058664, 0.23911773279753962, 0.23907912327799327, 0.2390343718426039, 0.2390224898470328, 0.23905835663937394, 0.23900546483909293, 0.23900422045699324, 0.23891575874228033, 0.23885588749364065, 0.23878025663739305, 0.23877072723518175, 0.23874507156032265, 0.23874067118993628, 0.23870153199041172, 0.23862349539694944, 0.23868533316693677, 0.23873127882201792, 0.23873730210054844, 0.23875085716502747, 0.2386438740247917, 0.238600387352931, 0.2386318779058259, 0.23862045253923928, 0.23867725369644516, 0.238592192286317, 0.23858847930619922, 0.2385372141748123, 0.23853117620476716, 0.23851148917133, 0.23848230144429766, 0.23845445692719938, 0.23846609929044402, 0.23839448038138605, 0.2383648259382482, 0.23825673198476777, 0.23824757381289494, 0.23820259724463827, 0.2382192475360122, 0.23824282263127244, 0.238293999399885, 0.23834222620265463, 0.23839121349405906, 0.2383427836341169, 0.2383165234055072, 0.23828835285409736, 0.23825548437282218, 0.2381704523091336, 0.238079095207344, 0.23804668428775705, 0.23802014898045074, 0.23804996430736447, 0.2380636278019714, 0.23804073649334656, 0.23801568310160717, 0.23801299697154338, 0.23802735610023387, 0.2379367103537081, 0.23790934621857426, 0.23787597713225864, 0.23787989231150347, 0.23785042679749577, 0.23787916991121397, 0.23788315056391313, 0.23791575506687812, 0.2379669976060672, 0.2379527144629803, 0.23787385009528234, 0.23782955764382274, 0.23785846914538655, 0.2378546934856891, 0.23783623948811863, 0.23790320320176844, 0.23782422857234295, 0.23790530874691448, 0.23796111859474972, 0.23796303591986243, 0.23796160854241863, 0.23793109267466334, 0.23785718675194165, 0.2378558076827589, 0.2379069835508068, 0.23788219764893026, 0.2379037634820482, 0.23784254572349878, 0.2377749760056569, 0.2378019155516703, 0.23782703318115558, 0.2377390134191733, 0.23768398336586805, 0.23762037749189396, 0.23764401936715532, 0.2376450602133664, 0.23764891553623144, 0.23760406288065414, 0.23761677808043158, 0.23764614407544987, 0.23766307650576263, 0.23761083335227298, 0.23762468812176554, 0.23769350796937472, 0.23776989064332918, 0.2376888260328445, 0.23776813961895993, 0.23776299384887417, 0.23773234533370918, 0.23778194307753117, 0.23777501544204854, 0.23776930615694905, 0.23771587852142773, 0.23765171287713194, 0.23754857791628584, 0.23757776530711047, 0.2375437286753187, 0.23755139247011167, 0.23753957306579682, 0.23758074966479778, 0.23757231773658316, 0.2375380289408711, 0.23753815867033856, 0.2376175903263619, 0.23752661986419896, 0.2375147932492353, 0.237386963381787, 0.23735454126611835, 0.2373782379331175, 0.2373969300169905, 0.2374402147732722, 0.23744560962821104, 0.23744074034137205, 0.23745587472369847, 0.23748340040322155, 0.2374952704124306, 0.23750866501629747, 0.2375376800448394, 0.23754439336408922, 0.23758092615124576, 0.2375695461495332, 0.23755323813659496, 0.2375485298161549, 0.23746001986515705, 0.23751018984992395, 0.23750794808140815, 0.23744513150776, 0.23747647867644667, 0.2374604349091785, 0.2374989016364488, 0.23746370205260794, 0.237450861521057, 0.23745750959023507, 0.23747524214865787, 0.23753679135860817, 0.2375276082308333, 0.23749856371959527, 0.23758691447457975, 0.2376058142636028, 0.23759229568721854, 0.2375551653221791, 0.2375851039686168, 0.23757004023665038, 0.2376112482574597, 0.23764824578716032, 0.23767991178733452, 0.23766708299631428, 0.2375732615135353, 0.23748885910232753, 0.23741955900264425, 0.23742396953525477, 0.2373822955794042, 0.23742738380758535, 0.23743784629002726, 0.23748032854736645, 0.23745234372424287, 0.23744055003914805, 0.23741433525453925, 0.2373893761499611, 0.2374976391764488, 0.2374957555693304, 0.23753671701026755, 0.23752299268606342, 0.23746522909347698, 0.2375045775221755, 0.23742956145324823, 0.2373725563142591, 0.23739331585774892, 0.2373723664449866, 0.23742482652220365, 0.23746590350521102, 0.237478688259865, 0.2374299720140597, 0.23747584032458627, 0.237477298414602, 0.23749635661418408, 0.2374908247933159, 0.2374896548510362, 0.23751158504909192, 0.23747416252985123, 0.2374331873463112, 0.23746494608383303, 0.2375655414389883, 0.23759918608863306, 0.237591157579354, 0.23759563933430128, 0.2375949679379661, 0.23762042836355515, 0.23761936254336843, 0.2376000048673258, 0.23761605122594817, 0.23764668181456924, 0.23763055756413115, 0.2376087299876592, 0.2375757878960294, 0.23757292143302589, 0.23762160348123265, 0.2376545163567992, 0.23764784084880647, 0.23757943465386017, 0.2375400723936348, 0.23754529672882158, 0.2375547313037268, 0.2376311703328161, 0.23765667640860794, 0.23769119598481114, 0.23770744852281459, 0.2376950997914134, 0.23766530239912187, 0.2376519252127128, 0.23767280031388133, 0.23763957547607803, 0.2376082808667505, 0.23765785712676857, 0.23764298584264026, 0.23765788243349004, 0.2376153146484354, 0.23768467984212116, 0.2376892716712037, 0.23761523393214243, 0.23753786819023112, 0.2374641467297793, 0.23741124697715377, 0.23739045942100367, 0.23737254487533782, 0.2374255291888682, 0.2374653194927387, 0.23757914484028866, 0.23769026559881334, 0.23764550539797538, 0.23764026372357613, 0.23759054467810212, 0.2376308480936805, 0.2376516150617728, 0.2376502647845267, 0.23764149518922542, 0.23759722099538777, 0.23757013849012942, 0.23750861189289454, 0.23747085190881587, 0.23745160036181442, 0.23748463589630725, 0.23751864264451308, 0.237572316046804, 0.23760766759439012, 0.237625229591615, 0.23761915027382402, 0.2376269691066826, 0.23758875179788325, 0.23747779696101534, 0.23749617112749535, 0.23754050583191072, 0.2375815106444434, 0.23747440451761997, 0.2374446281484888, 0.2374685917414915, 0.23747192629395253, 0.23748131965450794, 0.2374756115144348, 0.23747953523286733, 0.23745376049367667, 0.2375047999486418, 0.23752668747358316, 0.2375452344154267, 0.2375634760872645, 0.23760206660378663, 0.2376474965228452, 0.23759308913754784, 0.23758449665527112, 0.2375879012639767, 0.23755378602000793, 0.237489262916643, 0.23748792388047937, 0.23749188365293106, 0.23754058359379387, 0.23766498036183298, 0.23767708462050152, 0.23769700121693388, 0.2377322256082218, 0.23769911681166403, 0.2376571972765313, 0.23768689375197813, 0.23767678185517624, 0.23768232421157126, 0.23767577191773853, 0.23767055561859596, 0.23772957132946934, 0.23770598699456447, 0.2376816811771657, 0.23771728475948228, 0.23766256562591306, 0.23760730111081177, 0.23755781206656476, 0.23753407147915223, 0.23752874407332417, 0.2374622627396562, 0.23745424378378183, 0.23755048124740924, 0.23759374392395277, 0.23755682247967663, 0.2374534117477637, 0.2374979990508345, 0.2374944564416234, 0.23752963173712727, 0.23755125723900344, 0.23755897864124878, 0.2375924575379093, 0.23752334358452357, 0.23750683497022038, 0.23754579871744985, 0.23751213496378307, 0.2374640713535846, 0.23741231214853578, 0.23743928864590658, 0.2374539124690744, 0.23751750338670563, 0.23750909792228878, 0.23749036244294797, 0.237492985954359, 0.23746144058775448, 0.23742882107206728, 0.2373753697804734, 0.2374210249751603, 0.23746864737593826, 0.23748536050417174, 0.23745787660178105, 0.23744144627911026, 0.23739930960767694, 0.23738114084613718, 0.23733676749135163, 0.23730999470341604, 0.2373211862076936, 0.23730596581772503, 0.2373557433590947, 0.23734921359848987, 0.23734179881924578, 0.23737593544502977, 0.23735075078774573, 0.23726721350234162, 0.2373134001207179, 0.2373771598335689, 0.2373650984091078, 0.23736410873840808, 0.23732844645455814, 0.23728551943107015, 0.2372656147456874, 0.2372580534877332, 0.23731717007989797, 0.23735644032657494, 0.23736441779953035, 0.2373566800671216, 0.2372735623183894, 0.23727383804164837, 0.23716200165279458, 0.23714925227248634, 0.23716672594203148, 0.23718382095853002, 0.23721565294299868, 0.23718776883074336, 0.23723773671175274, 0.2372480970205125, 0.23728139351935523, 0.23730227663827605, 0.23733656574440803, 0.2373422266585355, 0.23736374439770996, 0.23738338967371653, 0.23738451959356752, 0.23738330248745496, 0.2374340258269903, 0.23745896603522346, 0.2375165716638539, 0.23757499629399906, 0.23754510289437536, 0.237547317513394, 0.23754613215535533, 0.23749859355160927, 0.23758123057437544, 0.23761168634788535, 0.2375952156769314, 0.2375523764366566, 0.23756474487978832, 0.23752861127386565, 0.23749677586991355, 0.2375059696218041, 0.2374695145023214, 0.23742351517718474, 0.23737917041599338, 0.23735538832760017, 0.2373456306493737, 0.237333732491964, 0.23732223651537632, 0.2372739797677135, 0.23727789278412337, 0.23733495509096442, 0.23730902917155902, 0.23729032647877132, 0.23725373039575595, 0.2372589637561898, 0.23724014951188746, 0.23729234613741496, 0.2372988189738031, 0.2372915095079202, 0.23729957734433488, 0.23731504067952264, 0.23737367681094315, 0.23744757384226514, 0.23744698603404096, 0.23743934159852864, 0.23745750894660456, 0.2374475784959902, 0.23743503462428328, 0.23746225758560005, 0.23746600017356478, 0.23744567287591792, 0.2373929520096278, 0.23738969477996855, 0.23739418297977313, 0.23738626652127642, 0.23736566092487302, 0.23732336891143055, 0.23740772049188635, 0.23734335412286223, 0.2373512494194884, 0.23737016942926598, 0.23730237057641113, 0.23734647220487223, 0.2373709021674355, 0.23738232194769684, 0.23736770553327888, 0.23731033928537767, 0.23733057463492588, 0.2373343171999797, 0.23736123711868864, 0.23735357768789012, 0.23738084102254603, 0.2373805421763085, 0.2374114791883668, 0.23745361922852093, 0.23742046418720175, 0.23739652124372768, 0.23734954581945839, 0.23733387512920923, 0.23737332884997114, 0.23737437681209253, 0.23739148589056033, 0.2373984967776679, 0.23736948716378226, 0.23739191739064536]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Concatenated 3 datasets"
      ],
      "metadata": {
        "id": "DzPgb5Fnm5zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -R french_xpedbst\n",
        "!mkdir french_xpedbst\n",
        "drive_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_xpedbst/\"\n",
        "!cp -rv $drive_path* french_xpedbst/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1GLBJ4SnQkd",
        "outputId": "6f2e94e2-88db-44b5-9223-b9d88a6d3be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'french_xpedbst': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_xpedbst/data_test.txt' -> 'french_xpedbst/data_test.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_xpedbst/data_train.txt' -> 'french_xpedbst/data_train.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_xpedbst/data_valid.txt' -> 'french_xpedbst/data_valid.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm /content/french_xpedbst/models/blender/blender_400Mdistill/BST400Mdistill_v1.1.tgz"
      ],
      "metadata": {
        "id": "IH0SikVfIaCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/french_xpedbst/\"\n",
        "model_path = \"/content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/\"\n",
        "# !rm -R $model_path\n",
        "!mkdir $model_path\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= data_path + \"data\",\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model= \"transformer/generator\",\n",
        "    model_file= f\"{model_path}model\",\n",
        "\n",
        "    init_model= \"zoo:blender/blender_400Mdistill/model\",\n",
        "    dict_file= f\"zoo:blender/blender_400Mdistill/model.dict\",\n",
        "    datapath= data_path,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    n_heads= 32, n_layers= 2, n_positions= 128, n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    dropout= 0.1, log_every_n_secs= 10,\n",
        "    # multitask_weights= \"1,3,3,3\",\n",
        "    attention_dropout= 0.0,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "\n",
        "    variant= \"prelayernorm\",\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    relu_dropout= 0.0, model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "    # save_after_valid= True,\n",
        "\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 3,\n",
        "    verbose = True,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    batchsize= 8, fp16= True, fp16_impl= \"mem_efficient\",\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\",\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # # customized parameters\n",
        "    # inference = 'topk', \n",
        "    # temperature = 0.7, \n",
        "    # topk=30, \n",
        "    # beam_length_penalty=1.03\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThVmHHxInR4I",
        "outputId": "ec431cf1-bbb0-4276-971f-686ba99d90dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "   .003472      .1042 13.4      .4535         0                27272 991.3 3561 1.837\n",
            "\n",
            "13:55:07 | time:19585s total_exs:498752 total_steps:31172 epochs:2.01 time_left:9684s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.7     1 761.4  2849   .5526      84.49 29.93  304             65536  7.549    .6623 29.56 2.525 6.93e-06 236.5 884.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.49      .4661         0                27291 997.9 3734 1.916\n",
            "\n",
            "13:55:17 | time:19595s total_exs:499048 total_steps:31190 epochs:2.01 time_left:9672s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.1     1 754.8  2758   .5439      81.74 29.24  296             65536  7.429    .6623 30.73 2.609 6.93e-06 245.4 896.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .05405 13.58      .4625         0                27310 1000 3655 1.894\n",
            "\n",
            "13:55:27 | time:19606s total_exs:499352 total_steps:31209 epochs:2.01 time_left:9659s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.4     1 715.8  2625   .5000      75.92 29.34  304             65536  7.526    .6553 30.37  2.52 6.93e-06 242.9   891   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.43      .4646         0                27329 958.7 3516 1.835\n",
            "\n",
            "13:55:38 | time:19616s total_exs:499656 total_steps:31228 epochs:2.01 time_left:9647s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.4     1 774.1  2833   .5428      87.68 29.28  304             65536  7.409    .6321 32.04 2.618 6.93e-06 256.3 938.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.71      .4490         0                27348 1030 3771 1.831\n",
            "\n",
            "13:55:48 | time:19626s total_exs:499960 total_steps:31247 epochs:2.01 time_left:9634s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.3     1 740.2  2751   .5164       75.8 29.73  304             65536  7.333    .6491  29.9 2.505 6.93e-06 239.2 889.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.24      .4664         0                27367 979.4 3640 1.859\n",
            "\n",
            "13:55:58 | time:19637s total_exs:500264 total_steps:31266 epochs:2.01 time_left:9621s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   187.7     1 769.5  2821   .5724      91.54 29.33  304             65536  7.613    .6623 31.02 2.672 6.93e-06 246.9 905.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006579      .1579 14.47      .4382         0                27386 1016 3726 1.834\n",
            "\n",
            "13:56:08 | time:19647s total_exs:500568 total_steps:31285 epochs:2.01 time_left:9608s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   155.5     1 714.8  2717   .4474      66.15 30.41  304             65536  7.633    .6623 28.86 2.504 6.93e-06 230.8 877.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.23      .4699         0                27405 945.7 3595 1.901\n",
            "\n",
            "13:56:19 | time:19657s total_exs:500856 total_steps:31303 epochs:2.02 time_left:9597s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.6     1 736.9  2569   .5104      78.45 27.89  288             65536  7.072    .6623  34.2 2.542 6.93e-06 271.4 946.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "    .01042      .2743 12.7      .4665         0                27423 1008 3515 1.743\n",
            "\n",
            "13:56:29 | time:19667s total_exs:501160 total_steps:31322 epochs:2.02 time_left:9584s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.2     1 747.3  2793   .5362      78.82  29.9  304             65536  7.534    .6623 29.71 2.504 6.93e-06 237.7 888.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.23      .4765         0                27442  985 3682 1.869\n",
            "\n",
            "13:56:39 | time:19677s total_exs:501464 total_steps:31341 epochs:2.02 time_left:9571s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     182     1 762.8  2852   .5362      86.65 29.91  304             65536  7.522    .6321 30.11  2.51 6.93e-06 240.9 900.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 12.3      .4624         0                27461 1004 3753 1.87\n",
            "\n",
            "13:56:49 | time:19688s total_exs:501768 total_steps:31360 epochs:2.02 time_left:9558s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.4     1 733.2  2684   .5000      73.77 29.28  304             65536  7.782    .6623 29.15 2.524 6.93e-06 232.3 850.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003289      .1053 12.48      .4637         0                27480 965.6 3534 1.83\n",
            "\n",
            "13:56:59 | time:19698s total_exs:502056 total_steps:31378 epochs:2.02 time_left:9546s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.3     1 715.2  2570   .4861      69.93 28.75  288             65536  7.335    .6553  32.2 2.541 6.93e-06 257.6 925.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.69      .4587         0                27498 972.8 3496 1.797\n",
            "\n",
            "13:57:10 | time:19708s total_exs:502360 total_steps:31397 epochs:2.02 time_left:9534s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.9     1 774.1  2866   .5592      84.12 29.62  304             65536  7.439    .6434    30  2.46 6.93e-06   240 888.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.71      .4766         0                27517 1014 3755 1.852\n",
            "\n",
            "13:57:20 | time:19718s total_exs:502648 total_steps:31415 epochs:2.02 time_left:9522s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.2     1 739.3  2621   .5069      71.78 28.36  288             65536  7.626    .6623 31.88 2.599 6.93e-06   255   904   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472    .003472 13.45      .4593         0                27535 994.3 3525 1.773\n",
            "\n",
            "13:57:30 | time:19728s total_exs:502936 total_steps:31433 epochs:2.02 time_left:9510s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.5     1 745.2  2671   .5208      75.39 28.67  288             65536  7.357    .6623 32.81 2.531 6.93e-06 262.5 940.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.56      .4634         0                27553 1008 3611 1.793\n",
            "\n",
            "13:57:40 | time:19738s total_exs:503240 total_steps:31452 epochs:2.03 time_left:9497s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.7     1 720.6  2711   .4770      72.59  30.1  304             65536  7.634    .6623 29.76 2.549 6.93e-06 237.9 895.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .02303 12.79      .4648         0                27572 958.5 3606 1.882\n",
            "\n",
            "13:57:50 | time:19749s total_exs:503544 total_steps:31471 epochs:2.03 time_left:9484s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.7     1 763.3  2892   .5395      78.32 30.31  304             65536  7.439    .6377 30.81 2.538 6.93e-06 246.5 933.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.66      .4625         0                27591 1010 3826 1.895\n",
            "\n",
            "13:58:00 | time:19759s total_exs:503848 total_steps:31490 epochs:2.03 time_left:9472s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.1     1 728.6  2701   .5197      78.03 29.66  304             65536  7.428    .6623 31.66 2.547 6.93e-06 251.6 932.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .2039 12.77      .4534         0                27610 980.2 3634 1.854\n",
            "\n",
            "13:58:11 | time:19769s total_exs:504136 total_steps:31508 epochs:2.03 time_left:9460s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.9     1 718.9  2532   .4965      80.01 28.17  288             65536  7.182    .6623 33.25 2.504 6.93e-06   266 936.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472    .006944 12.23      .4708         0                27628 984.9 3468 1.762\n",
            "\n",
            "13:58:21 | time:19779s total_exs:504448 total_steps:31528 epochs:2.03 time_left:9447s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   193.9     1 785.6  3018   .5865      95.68 30.74  312             65536  7.438    .6377 31.66 2.603 6.93e-06 253.3 973.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.51      .4551         0                27647 1039 3992 1.906\n",
            "\n",
            "13:58:31 | time:19789s total_exs:504752 total_steps:31547 epochs:2.03 time_left:9434s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.1     1 754.1  2838   .5263      75.81  30.1  304             65536  7.458    .6434 29.91 2.571 6.93e-06 239.3 900.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 13.09      .4600         0                27666 993.4 3738 1.922\n",
            "\n",
            "13:58:41 | time:19800s total_exs:505048 total_steps:31565 epochs:2.03 time_left:9422s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.5     1 746.8  2708   .5270       78.2 29.01  296             65536  7.574    .6623 31.18  2.49 6.93e-06 247.1 896.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006757      .2939 12.06      .4676         0                27685 993.8 3604 1.875\n",
            "\n",
            "13:58:51 | time:19810s total_exs:505368 total_steps:31585 epochs:2.03 time_left:9408s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.1     1   739  2888   .5062      74.72 31.26  320             65536  7.586    .6321 29.52 2.566 6.93e-06 236.2 922.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 13.01      .4605         0                27705 975.2 3810 1.955\n",
            "\n",
            "13:59:02 | time:19820s total_exs:505656 total_steps:31603 epochs:2.04 time_left:9396s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.1     1   762  2672   .5451      81.82 28.05  288             65536  7.658    .6623 31.89 2.611 6.93e-06   249 872.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "    .01042      .7708 13.61      .4498         0                27723 1011 3544 1.754\n",
            "\n",
            "13:59:12 | time:19830s total_exs:505976 total_steps:31623 epochs:2.04 time_left:9383s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.9     1 743.9  2904   .5281      80.94 31.23  320             65536  7.566    .6434 29.25 2.529 6.93e-06   234 913.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.55      .4634         0                27743 977.9 3818 1.953\n",
            "\n",
            "13:59:22 | time:19840s total_exs:506280 total_steps:31642 epochs:2.04 time_left:9370s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.8     1 726.9  2725   .4967      72.95 29.99  304             65536  7.555    .6623 30.16 2.523 6.93e-06   241 903.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .03618 12.47      .4672         0                27762 967.9 3629 1.875\n",
            "\n",
            "13:59:32 | time:19851s total_exs:506576 total_steps:31661 epochs:2.04 time_left:9358s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.9     1 738.8  2697   .5000      72.54  29.2  296             65536  7.472    .6623 31.32   2.6 6.93e-06 250.4 914.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .01689 13.46      .4513         0                27780 989.3 3611 1.798\n",
            "\n",
            "13:59:43 | time:19861s total_exs:506856 total_steps:31678 epochs:2.04 time_left:9346s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.6     1 725.6  2481   .5000      79.88 27.35  280             65536  7.318    .6553 32.96 2.582 6.93e-06 263.7 901.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 13.23      .4581         0                27798 989.3 3383 1.771\n",
            "\n",
            "13:59:53 | time:19871s total_exs:507152 total_steps:31697 epochs:2.04 time_left:9334s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.9     1 765.5  2830   .5439      85.26 29.58  296             65536  7.193    .6434 32.04  2.53 6.93e-06 256.3 947.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.55      .4597   .003378                27816 1022 3778 1.826\n",
            "\n",
            "14:00:03 | time:19881s total_exs:507424 total_steps:31714 epochs:2.04 time_left:9323s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.3     1   747  2507   .5257       75.9 26.85  272             65536  7.568    .6623  33.4 2.616 6.93e-06   263 882.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "    .01103      .5257 13.68      .4510         0                27833 1010 3390 1.718\n",
            "\n",
            "14:00:13 | time:19891s total_exs:507736 total_steps:31733 epochs:2.04 time_left:9310s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.6     1   729  2755   .5064      73.51 30.24  312             65536  7.508    .6377  29.9 2.476 6.93e-06 239.2 904.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.9      .4684         0                27853 968.2 3659 1.954\n",
            "\n",
            "14:00:23 | time:19902s total_exs:508040 total_steps:31752 epochs:2.04 time_left:9297s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.7     1 743.8  2768   .5066      79.73 29.77  304             65536  7.461    .6623 31.27 2.514 6.93e-06 250.2 930.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.36      .4640         0                27872 993.9 3698 1.861\n",
            "\n",
            "14:00:34 | time:19912s total_exs:508344 total_steps:31771 epochs:2.05 time_left:9285s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.9     1 737.7  2721   .5263      81.69 29.51  304             65536  7.332    .6491 31.73  2.56 6.93e-06 253.9 936.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.94      .4654         0                27891 991.6 3658 1.845\n",
            "\n",
            "14:00:44 | time:19922s total_exs:508632 total_steps:31789 epochs:2.05 time_left:9273s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   183.2     1   779  2758   .5625      85.86 28.32  288             65536  7.349    .6623  33.3 2.568 6.93e-06 264.3 935.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   .006944      .2639 13.04      .4560         0                27909 1043 3693 1.77\n",
            "\n",
            "14:00:54 | time:19932s total_exs:508936 total_steps:31808 epochs:2.05 time_left:9260s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     181     1   759  2883   .5362      86.09 30.39  304             65536  7.745    .6623 29.38 2.528 6.93e-06   234 888.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   .003289      .1316 12.53      .4700         0                27928  993 3772  1.9\n",
            "\n",
            "14:01:04 | time:19943s total_exs:509240 total_steps:31827 epochs:2.05 time_left:9247s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.8     1 732.2  2673   .5197      77.32  29.2  304             65536    7.5    .6377 31.63 2.535 6.93e-06 253.1 923.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.61      .4635         0                27947 985.3 3596 1.826\n",
            "\n",
            "14:01:15 | time:19953s total_exs:509544 total_steps:31846 epochs:2.05 time_left:9235s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.4     1 732.9  2669   .5164      80.78 29.13  304             65536  7.533    .6623 31.41 2.588 6.93e-06 248.8 905.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579      .3191 13.3      .4496         0                27966 981.6 3575 1.821\n",
            "\n",
            "14:01:25 | time:19963s total_exs:509848 total_steps:31865 epochs:2.05 time_left:9222s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.3     1 760.9  2823   .5493      87.23 29.68  304             65536  7.369    .6322 32.28 2.533 6.93e-06 258.2 957.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.59      .4629         0                27985 1019 3781 1.856\n",
            "\n",
            "14:01:35 | time:19974s total_exs:510152 total_steps:31884 epochs:2.05 time_left:9209s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.4     1 735.5  2725   .5099       78.5 29.64  304             65536  7.279    .6377 31.36 2.557 6.93e-06 250.8 929.3   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.9      .4585         0                28004 986.3 3654 1.853\n",
            "\n",
            "14:01:45 | time:19984s total_exs:510464 total_steps:31904 epochs:2.05 time_left:9196s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     166     1 741.5  2873   .5064      73.34 30.99  312             65536  7.751    .6321 28.08  2.51 6.93e-06 224.7 870.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.31      .4745         0                28023 966.2 3743 1.916\n",
            "\n",
            "14:01:55 | time:19994s total_exs:510760 total_steps:31922 epochs:2.06 time_left:9184s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.3     1 727.4  2649   .5034       80.4 29.13  296             65536  7.484    .6434 31.46  2.54 6.93e-06 251.7 916.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.68      .4613         0                28042 979.1 3566 1.888\n",
            "\n",
            "14:02:06 | time:20004s total_exs:511048 total_steps:31940 epochs:2.06 time_left:9172s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.5     1 749.6  2691   .5278      73.83 28.72  288             65536  7.311    .6377 31.97 2.522 6.93e-06 255.8 918.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.45      .4674         0                28060 1005 3610 1.796\n",
            "\n",
            "14:02:16 | time:20014s total_exs:511352 total_steps:31959 epochs:2.06 time_left:9160s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.4     1 740.1  2756   .5066      75.88 29.79  304             65536  7.515    .6623 31.83 2.545 6.93e-06 252.8 941.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579      .2336 12.75      .4615         0                28079 992.9 3697 1.862\n",
            "\n",
            "14:02:26 | time:20024s total_exs:511656 total_steps:31978 epochs:2.06 time_left:9147s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     172     1 735.4  2767   .5263      80.04  30.1  304             65536  7.456    .6492  30.8 2.555 6.93e-06 246.4 927.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.88      .4605         0                28098 981.8 3694 1.882\n",
            "\n",
            "14:02:36 | time:20034s total_exs:511968 total_steps:31998 epochs:2.06 time_left:9134s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.6     1 735.8  2850   .5032      78.58 30.99  312             65536  7.737    .6321 29.55 2.556 6.93e-06 236.4 915.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.89      .4572         0                28117 972.2 3766 1.907\n",
            "\n",
            "14:02:46 | time:20044s total_exs:512264 total_steps:32016 epochs:2.06 time_left:9121s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   187.7     1 764.8  2781   .5574      92.14 29.09  296             65536  7.359    .6377 32.41 2.517 6.93e-06 259.3 942.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 12.39      .4688         0                28136 1024 3724 1.88\n",
            "\n",
            "14:02:56 | time:20055s total_exs:512568 total_steps:32035 epochs:2.06 time_left:9109s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     167     1 720.2  2702   .4934      76.96 30.01  304             65536  7.753    .6623 30.31 2.624 6.93e-06 241.8 907.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .08553 13.79      .4508         0                28155 962.1 3609 1.876\n",
            "\n",
            "14:03:06 | time:20065s total_exs:512872 total_steps:32054 epochs:2.06 time_left:9096s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.3     1 747.4  2815   .5230      70.84 30.13  304             65536  7.806    .6623 28.36 2.568 6.93e-06 226.2 851.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .08553 13.03      .4568         0                28174 973.6 3666 1.884\n",
            "\n",
            "14:03:17 | time:20075s total_exs:513176 total_steps:32073 epochs:2.07 time_left:9083s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   154.6     1 718.1  2637   .4704      64.86 29.37  304             65536  7.364    .6623 30.61   2.5 6.93e-06 244.9 899.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.18      .4703         0                28193  963 3536 1.836\n",
            "\n",
            "14:03:27 | time:20085s total_exs:513480 total_steps:32092 epochs:2.07 time_left:9071s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   187.4     1 778.6  2920   .5954      90.06    30  304             65536   7.24    .6553 31.68 2.487 6.93e-06 253.4 950.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.02      .4703         0                28212 1032 3870 1.876\n",
            "\n",
            "14:03:37 | time:20095s total_exs:513768 total_steps:32110 epochs:2.07 time_left:9059s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.2     1 721.8  2575   .4896      77.95 28.55  288             65536  7.624    .6624 31.56 2.604 6.93e-06 252.4 900.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472    .003472 13.52      .4520         0                28230 974.2 3476 1.784\n",
            "\n",
            "14:03:47 | time:20106s total_exs:514056 total_steps:32128 epochs:2.07 time_left:9047s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.5     1 756.7  2670   .5382      87.94 28.23  288             65536  7.461    .6623 32.38 2.586 6.93e-06   259 913.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472    .006944 13.27      .4590         0                28248 1016 3584 1.765\n",
            "\n",
            "14:03:57 | time:20116s total_exs:514352 total_steps:32147 epochs:2.07 time_left:9035s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.7     1   726  2679   .4932      81.97 29.52  296             65536  7.179    .6434 32.89 2.522 6.93e-06 263.1 970.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.45      .4618         0                28266 989.1 3650 1.822\n",
            "\n",
            "14:04:07 | time:20126s total_exs:514656 total_steps:32166 epochs:2.07 time_left:9022s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.6     1 732.4  2781   .5263      80.09 30.37  304             65536   7.36    .6553 30.43 2.475 6.93e-06 243.4 924.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.88      .4736         0                28285 975.8 3705 1.935\n",
            "\n",
            "14:04:18 | time:20136s total_exs:514968 total_steps:32185 epochs:2.07 time_left:9009s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   153.7     1 715.6  2707   .4647      64.25 30.26  312             65536  7.687    .6321 29.57 2.496 6.93e-06 236.6 894.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.14      .4749         0                28305 952.1 3601 1.953\n",
            "\n",
            "14:04:28 | time:20146s total_exs:515272 total_steps:32204 epochs:2.07 time_left:8996s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   198.7     1 794.9  3017   .6086      99.32 30.36  304             65536  7.665    .6623 31.18 2.615 6.93e-06 249.5 946.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.67      .4578         0                28324 1044 3964 1.898\n",
            "\n",
            "14:04:38 | time:20156s total_exs:515584 total_steps:32224 epochs:2.08 time_left:8983s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   188.7     1 762.5  2932   .5545      93.44 30.77  312             65536   7.53    .6322 30.73 2.578 6.93e-06 245.8 945.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.17      .4580         0                28343 1008 3878 1.904\n",
            "\n",
            "14:04:48 | time:20167s total_exs:515880 total_steps:32242 epochs:2.08 time_left:8971s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.8     1 756.1  2711   .5372      90.25 28.69  296             65536  7.127    .6623 32.94 2.549 6.93e-06 263.4 944.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   .003378     .02027 12.8      .4640         0                28362 1019 3656 1.86\n",
            "\n",
            "14:04:58 | time:20177s total_exs:516176 total_steps:32261 epochs:2.08 time_left:8959s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   155.7     1 705.7  2593   .4662      67.51  29.4  296             65536   7.46    .6553 31.17  2.52 6.93e-06 249.3 916.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.43      .4651         0                28380  955 3510 1.815\n",
            "\n",
            "14:05:08 | time:20187s total_exs:516472 total_steps:32279 epochs:2.08 time_left:8947s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.9     1 753.4  2763   .5372      83.67 29.34  296             65536  7.354    .6492 32.19 2.572 6.93e-06 257.5 944.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.09      .4629         0                28399 1011 3707 1.901\n",
            "\n",
            "14:05:18 | time:20197s total_exs:516784 total_steps:32299 epochs:2.08 time_left:8934s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.9     1 761.6  2950   .5481      72.66 30.99  312             65536  7.491    .6377 29.85 2.502 6.93e-06 238.8 925.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.21      .4708         0                28418 1000 3876 1.907\n",
            "\n",
            "14:05:29 | time:20207s total_exs:517096 total_steps:32318 epochs:2.08 time_left:8921s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.2     1 721.8  2757   .4776      68.98 30.55  312             65536  7.669    .6377 28.96 2.514 6.93e-06 231.7 884.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.35      .4615         0                28438 953.5 3642 1.972\n",
            "\n",
            "14:05:39 | time:20217s total_exs:517392 total_steps:32337 epochs:2.08 time_left:8908s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.9     1 739.5  2736   .4966       78.5  29.6  296             65536  7.419    .6624  30.2 2.439 6.93e-06 241.6 893.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.46      .4745         0                28456 981.1 3630 1.827\n",
            "\n",
            "14:05:49 | time:20227s total_exs:517696 total_steps:32356 epochs:2.08 time_left:8896s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     192     1 778.4  2921   .5822      94.66 30.02  304             65536  7.402    .6322 31.19 2.463 6.93e-06 249.6 936.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.74      .4762         0                28475 1028 3858 1.924\n",
            "\n",
            "14:05:59 | time:20237s total_exs:517992 total_steps:32374 epochs:2.08 time_left:8884s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.1     1 760.5  2796   .5541      86.07 29.41  296             65536  7.311    .6624 31.47 2.493 6.93e-06 251.4 924.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .04054 12.09      .4662         0                28494 1012 3720 1.907\n",
            "\n",
            "14:06:09 | time:20247s total_exs:518272 total_steps:32392 epochs:2.09 time_left:8872s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.6     1 728.6  2531   .5107      84.52 27.79  280             65536  7.181    .6624 34.81 2.596 6.93e-06 277.4 963.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .007143      .1357 13.41      .4552         0                28511 1006 3495 1.714\n",
            "\n",
            "14:06:19 | time:20258s total_exs:518568 total_steps:32410 epochs:2.09 time_left:8860s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     184     1   754  2747   .5372      89.72 29.15  296             65536  7.395    .6624 31.11 2.567 6.93e-06 248.9 906.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 13.02      .4662         0                28530 1003 3654 1.89\n",
            "\n",
            "14:06:29 | time:20268s total_exs:518856 total_steps:32428 epochs:2.09 time_left:8848s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   188.4     1 763.9  2677   .5312      92.95 28.03  288             65536  7.169    .6623  33.5 2.585 6.93e-06 267.8 938.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .03125 13.27      .4512         0                28548 1032 3615 1.753\n",
            "\n",
            "14:06:40 | time:20278s total_exs:519144 total_steps:32446 epochs:2.09 time_left:8837s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.5     1 719.8  2535   .4965      75.55 28.18  288             65536  7.108    .6623  32.2 2.466 6.93e-06   257 905.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472     .06944 11.78      .4741         0                28566 976.8 3441 1.761\n",
            "\n",
            "14:06:50 | time:20288s total_exs:519448 total_steps:32465 epochs:2.09 time_left:8824s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.5     1 750.7  2824   .5296      83.62 30.09  304             65536   7.73    .6434 30.38 2.523 6.93e-06 243.1 914.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.47      .4674         0                28585 993.7 3738 1.882\n",
            "\n",
            "14:07:00 | time:20298s total_exs:519752 total_steps:32484 epochs:2.09 time_left:8811s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     158     1 719.8  2668   .4770      68.06 29.65  304             65536  7.511    .6623 30.45 2.511 6.93e-06 243.6 902.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.32      .4691         0                28604 963.4 3571 1.854\n",
            "\n",
            "14:07:10 | time:20309s total_exs:520056 total_steps:32503 epochs:2.09 time_left:8799s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     165     1   719  2698   .4803      75.15 30.02  304             65536  7.531    .6377 30.76 2.554 6.93e-06 246.1 923.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.86      .4649         0                28623 965.1 3621 1.876\n",
            "\n",
            "14:07:20 | time:20319s total_exs:520376 total_steps:32523 epochs:2.09 time_left:8785s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.7     1 767.6  3023   .5625      86.79 31.51  320             65536  7.589    .6266 28.93 2.585 6.93e-06 231.4 911.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 13.26      .4560         0                28643  999 3934 1.97\n",
            "\n",
            "14:07:30 | time:20329s total_exs:520672 total_steps:32542 epochs:2.10 time_left:8773s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   189.4     1 746.5  2750   .5439      96.09 29.47  296             65536  7.258    .6377 32.36 2.572 6.93e-06 258.9 953.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.09      .4628         0                28661 1005 3704 1.813\n",
            "\n",
            "14:07:41 | time:20339s total_exs:520968 total_steps:32560 epochs:2.10 time_left:8761s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.1     1 740.8  2724   .4932      67.51 29.42  296             65536  7.464    .6624 30.01 2.565 6.93e-06 240.1 883.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0   13      .4604         0                28680 980.9 3607 1.902\n",
            "\n",
            "14:07:51 | time:20349s total_exs:521288 total_steps:32580 epochs:2.10 time_left:8748s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.5     1 730.6  2831   .5094      74.14    31  320             65536  7.519    .6377 29.35 2.577 6.93e-06 234.8 909.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 13.16      .4547         0                28700 965.4 3740 1.938\n",
            "\n",
            "14:08:01 | time:20359s total_exs:521600 total_steps:32600 epochs:2.10 time_left:8734s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   155.9     1   704  2716   .4647      67.87 30.87  312             65536   7.74    .6553 28.44 2.518 6.93e-06 227.5 877.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.41      .4652         0                28719 931.5 3594 1.904\n",
            "\n",
            "14:08:11 | time:20370s total_exs:521896 total_steps:32618 epochs:2.10 time_left:8722s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.9     1 761.3  2766   .5507      85.74 29.06  296             65536  7.609    .6624 30.23 2.524 6.93e-06 241.8 878.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.48      .4626         0                28738 1003 3644 1.881\n",
            "\n",
            "14:08:21 | time:20380s total_exs:522184 total_steps:32636 epochs:2.10 time_left:8711s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     171     1   747  2658   .5278      77.62 28.46  288             65536  7.077    .6492 33.06 2.537 6.93e-06 264.5 940.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 12.64      .4638         0                28756 1012 3599 1.78\n",
            "\n",
            "14:08:32 | time:20390s total_exs:522488 total_steps:32655 epochs:2.10 time_left:8698s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.2     1   749  2746   .5099      67.59 29.33  304             65536  7.539    .6623 31.34 2.535 6.93e-06 250.6 918.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .01316 12.62      .4606         0                28775 999.6 3665 1.834\n",
            "\n",
            "14:08:42 | time:20400s total_exs:522792 total_steps:32674 epochs:2.10 time_left:8686s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.3     1 762.9  2875   .5526      84.98 30.15  304             65536  7.491    .6553    30  2.45 6.93e-06   240 904.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.59      .4809         0                28794 1003 3780 1.885\n",
            "\n",
            "14:08:52 | time:20410s total_exs:523096 total_steps:32693 epochs:2.11 time_left:8673s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     161     1 741.7  2765   .4868      68.28 29.83  304             65536  7.381    .6435 29.91 2.494 6.93e-06 239.3 892.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.1      .4738         0                28813  981 3658 1.865\n",
            "\n",
            "14:09:02 | time:20420s total_exs:523384 total_steps:32711 epochs:2.11 time_left:8661s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     172     1 755.4  2703   .5243      77.55 28.63  288             65536  7.689    .6623 31.34 2.556 6.93e-06 249.6 893.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   .006944      .1458 12.88      .4639         0                28831 1005 3596 1.79\n",
            "\n",
            "14:09:13 | time:20431s total_exs:523704 total_steps:32731 epochs:2.11 time_left:8648s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.6     1 760.6  2885   .5437      74.48 30.35  320             65536  7.755    .6624  29.4  2.56 6.93e-06 235.2 892.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.93      .4546         0                28851 995.8 3777 1.897\n",
            "\n",
            "14:09:23 | time:20441s total_exs:524016 total_steps:32751 epochs:2.11 time_left:8635s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.1     1 767.1  2970   .5385      78.21 30.98  312             65536  7.501    .6435 30.71  2.53 6.93e-06 245.7 951.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.55      .4659         0                28870 1013 3922 1.908\n",
            "\n",
            "14:09:33 | time:20451s total_exs:524312 total_steps:32769 epochs:2.11 time_left:8623s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.2     1 757.5  2742   .5405      79.52 28.95  296             65536  7.407    .6492 30.12 2.501 6.93e-06   241 872.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.2      .4712         0                28889 998.5 3614 1.872\n",
            "\n",
            "14:09:43 | time:20461s total_exs:524600 total_steps:32787 epochs:2.11 time_left:8611s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.4     1 736.9  2644   .5243      83.27 28.71  288             65536   7.27    .6624 32.18  2.45 6.93e-06 256.5 920.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472      .1215 11.59      .4759         0                28907 993.4 3565 1.795\n",
            "\n",
            "14:09:53 | time:20472s total_exs:524904 total_steps:32806 epochs:2.11 time_left:8598s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.8     1 785.4  2932   .5691      86.64 29.87  304             65536  7.516    .6492 30.21   2.5 6.93e-06 241.7 902.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.18      .4667         0                28926 1027 3834 1.867\n",
            "\n",
            "14:10:03 | time:20482s total_exs:525200 total_steps:32825 epochs:2.11 time_left:8586s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.1     1 723.4  2634   .4932      78.64 29.13  296             65536  7.281    .6322 32.43 2.607 6.93e-06 259.4 944.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 13.56      .4452         0                28944 982.8 3579 1.798\n",
            "\n",
            "14:10:14 | time:20492s total_exs:525512 total_steps:32844 epochs:2.12 time_left:8573s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.7     1 749.5  2890   .5321      76.06 30.84  312             65536  7.611    .6322 28.57 2.517 6.93e-06 228.5 881.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.39      .4722         0                28964  978 3771 1.996\n",
            "\n",
            "14:10:24 | time:20502s total_exs:525816 total_steps:32863 epochs:2.12 time_left:8561s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.1     1 760.8  2853   .5263         79    30  304             65536  7.452    .6624 30.38 2.502 6.93e-06   242 907.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003289      .1283 12.2      .4698         0                28983 1003 3761 1.875\n",
            "\n",
            "14:10:34 | time:20512s total_exs:526104 total_steps:32881 epochs:2.12 time_left:8549s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.9     1 737.2  2654   .5139      77.71  28.8  288             65536  7.356    .6624 31.38   2.5 6.93e-06 247.6 891.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "    .01042      .4201 12.18      .4678         0                29001 984.9 3546  1.8\n",
            "\n",
            "14:10:44 | time:20522s total_exs:526408 total_steps:32900 epochs:2.12 time_left:8537s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.1     1 753.4  2803   .5362      79.94 29.76  304             65536  7.457    .6492 29.81 2.533 6.93e-06 238.5 887.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.59      .4665         0                29020 991.9 3690 1.861\n",
            "\n",
            "14:10:54 | time:20532s total_exs:526696 total_steps:32918 epochs:2.12 time_left:8525s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.2     1 753.7  2695   .5278      86.96 28.61  288             65536  7.387    .6492 32.11  2.46 6.93e-06 256.9 918.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.7      .4695         0                29038 1011 3614 1.789\n",
            "\n",
            "14:11:04 | time:20543s total_exs:527000 total_steps:32937 epochs:2.12 time_left:8512s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     157     1 729.2  2700   .4967       65.9 29.62  304             65536   7.47    .6377 30.91 2.488 6.93e-06 247.3 915.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.04      .4701         0                29057 976.5 3615 1.851\n",
            "\n",
            "14:11:14 | time:20553s total_exs:527312 total_steps:32957 epochs:2.12 time_left:8499s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.6     1 750.2  2923   .5481      75.81 31.17  312             65536  7.888    .6322 29.21 2.595 6.93e-06 233.7 910.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 13.4      .4581         0                29076 983.8 3833 1.92\n",
            "\n",
            "14:11:24 | time:20563s total_exs:527616 total_steps:32976 epochs:2.12 time_left:8487s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.9     1 744.6  2793   .5230      81.84 30.01  304             65536  7.356    .6322 31.17 2.466 6.93e-06 249.4 935.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.77      .4698         0                29095  994 3729 1.913\n",
            "\n",
            "14:11:35 | time:20573s total_exs:527912 total_steps:32994 epochs:2.12 time_left:8475s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.5     1 723.8  2677   .4831      69.99 29.59  296             65536   7.83    .6624 29.43 2.602 6.93e-06 234.5 867.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378      .1216 13.49      .4550         0                29114 958.2 3544 1.914\n",
            "\n",
            "14:11:45 | time:20583s total_exs:528216 total_steps:33013 epochs:2.13 time_left:8462s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.9     1 740.9  2793   .5000      74.27 30.16  304             65536  7.325    .6377 31.18 2.527 6.93e-06 249.4 940.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.52      .4653         0                29133 990.4 3733 1.886\n",
            "\n",
            "14:11:55 | time:20593s total_exs:528520 total_steps:33032 epochs:2.13 time_left:8449s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.3     1 751.4  2826   .5099      76.39 30.09  304             65536  7.456    .6266 29.43 2.426 6.93e-06 235.4 885.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.31      .4784         0                29152 986.8 3712 1.882\n",
            "\n",
            "14:12:05 | time:20603s total_exs:528816 total_steps:33051 epochs:2.13 time_left:8437s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.1     1 719.1  2652   .4831      75.18  29.5  296             65536   7.43    .6624 30.95 2.486 6.93e-06 247.3 911.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .04054 12.01      .4731         0                29170 966.3 3564 1.813\n",
            "\n",
            "14:12:15 | time:20613s total_exs:529112 total_steps:33069 epochs:2.13 time_left:8425s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     162     1 721.9  2653   .4899       71.8  29.4  296            110376  7.556    .6624 30.31 2.514 6.93e-06 241.9 889.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003378      .0777 12.35      .4692         0                29189 963.8 3542  1.9\n",
            "\n",
            "14:12:20 | Overflow: setting loss scale to 65536.0\n",
            "14:12:25 | time:20623s total_exs:529424 total_steps:33089 epochs:2.13 time_left:8412s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.7 .9474 764.6  2979   .5353      76.15 31.17  312             96579   7.42    .6435 27.82  2.51 6.93e-06 222.5 867.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.3      .4699         0                29208 987.2 3846 1.919\n",
            "\n",
            "14:12:35 | time:20634s total_exs:529720 total_steps:33107 epochs:2.13 time_left:8400s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.6     1   708  2512   .4764       81.1 28.38  296             65536  7.417    .6624 32.33 2.506 6.93e-06 258.6 917.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.25      .4635         0                29227 966.6 3429 1.834\n",
            "\n",
            "14:12:46 | time:20644s total_exs:529992 total_steps:33124 epochs:2.13 time_left:8389s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.1     1 730.7  2434   .5037      73.75 26.65  272             65536  7.148    .6624 36.35  2.52 6.93e-06 285.6 951.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "    .01103      .6581 12.43      .4652         0                29244 1016 3385 1.666\n",
            "\n",
            "14:12:56 | time:20654s total_exs:530280 total_steps:33142 epochs:2.13 time_left:8378s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     174     1 739.6  2651   .4931      81.58 28.68  288             65536   7.54    .6553 31.19 2.506 6.93e-06 249.5 894.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.26      .4667         0                29262 989.1 3545 1.793\n",
            "\n",
            "14:13:06 | time:20664s total_exs:530600 total_steps:33162 epochs:2.14 time_left:8364s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.2     1 733.5  2896   .4969      75.48 31.58  320             65536  7.645    .6266 29.72 2.472 6.93e-06 237.8 938.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.84      .4745         0                29282 971.3 3834 1.974\n",
            "\n",
            "14:13:16 | time:20674s total_exs:530888 total_steps:33180 epochs:2.14 time_left:8353s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.9     1 738.8  2627   .5069       78.5 28.45  288             65536   7.61    .6624 30.79 2.535 6.93e-06 246.2 875.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .01736 12.62      .4573         0                29300  985 3503 1.779\n",
            "\n",
            "14:13:26 | time:20685s total_exs:531192 total_steps:33199 epochs:2.14 time_left:8340s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.8     1 735.5  2727   .4934      82.84 29.67  304             65536  7.507    .6624 30.58 2.541 6.93e-06 244.6   907   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.69      .4599         0                29319 980.1 3634 1.855\n",
            "\n",
            "14:13:36 | time:20695s total_exs:531512 total_steps:33219 epochs:2.14 time_left:8327s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.1     1 756.8  2948   .5344      69.46 31.16  320             65536  7.846    .6266 28.32 2.462 6.93e-06 226.6 882.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.73      .4728         0                29339 983.3 3831 1.948\n",
            "\n",
            "14:13:47 | time:20705s total_exs:531832 total_steps:33239 epochs:2.14 time_left:8314s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   152.7     1 727.2  2806   .4938       61.8 30.87  320             65536  7.735    .6435 28.66 2.581 6.93e-06 229.2 884.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 13.21      .4566         0                29359 956.4 3691 1.93\n",
            "\n",
            "14:13:57 | time:20715s total_exs:532152 total_steps:33259 epochs:2.14 time_left:8300s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.1     1 767.8  3006   .5531      81.14 31.32  320             65536  7.663    .6624 29.93 2.582 6.93e-06 239.2 936.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003125      .0375 13.22      .4632         0                29379 1007 3943 1.959\n",
            "\n",
            "14:14:07 | time:20726s total_exs:532456 total_steps:33278 epochs:2.14 time_left:8288s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.6     1 733.4  2713   .5099      74.96  29.6  304             65536  7.516    .6554 31.43 2.538 6.93e-06 251.5 930.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.65      .4624         0                29398 984.9 3644 1.851\n",
            "\n",
            "14:14:17 | time:20736s total_exs:532736 total_steps:33296 epochs:2.14 time_left:8277s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.5     1 752.4  2615   .5393      81.46 27.81  280             65536  7.332    .6624 32.72 2.511 6.93e-06 259.7 902.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .007143      .2500 12.32      .4704         0                29415 1012 3518 1.713\n",
            "\n",
            "14:14:28 | time:20746s total_exs:533024 total_steps:33314 epochs:2.15 time_left:8265s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.1     1 744.9  2662   .5243      77.96 28.59  288             65536  7.237    .6624 32.46 2.492 6.93e-06 259.6 927.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .01389 12.09      .4714         0                29433 1004 3590 1.822\n",
            "\n",
            "14:14:38 | time:20756s total_exs:533320 total_steps:33332 epochs:2.15 time_left:8253s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.1     1 726.2  2666   .5034      75.38 29.37  296             65536  7.664    .6435 29.46 2.539 6.93e-06 235.7 865.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.67      .4632   .003378                29452 961.8 3532 1.898\n",
            "\n",
            "14:14:48 | time:20766s total_exs:533616 total_steps:33351 epochs:2.15 time_left:8241s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   154.8     1 710.1  2597   .4662      66.04 29.25  296             65536  7.635    .6378 29.69 2.563 6.93e-06 237.5 868.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.97      .4653         0                29470 947.6 3465 1.807\n",
            "\n",
            "14:14:58 | time:20776s total_exs:533912 total_steps:33369 epochs:2.15 time_left:8229s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.8     1 745.2  2682   .5101      79.61 28.79  296             65536  7.288    .6554 31.04 2.561 6.93e-06 248.4 893.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.94      .4648         0                29489 993.6 3576 1.867\n",
            "\n",
            "14:15:08 | time:20787s total_exs:534216 total_steps:33388 epochs:2.15 time_left:8216s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.3     1   743  2781   .5099      71.47 29.94  304             65536  7.567    .6624 29.58 2.496 6.93e-06 236.1 883.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .05921 12.13      .4738         0                29508 979.1 3665 1.872\n",
            "\n",
            "14:15:19 | time:20797s total_exs:534536 total_steps:33408 epochs:2.15 time_left:8203s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.6     1 763.4  2954   .5531      85.15 30.96  320             65536  7.633    .6322 29.47 2.516 6.93e-06 235.8 912.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.38      .4709         0                29528 999.1 3867 1.936\n",
            "\n",
            "14:15:29 | time:20807s total_exs:534840 total_steps:33427 epochs:2.15 time_left:8190s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.5     1 752.8  2860   .5362      84.37 30.39  304             65536  7.476    .6553 29.28 2.476 6.93e-06 234.2 889.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.89      .4715         0                29547 987.1 3750 1.901\n",
            "\n",
            "14:15:39 | time:20817s total_exs:535144 total_steps:33446 epochs:2.15 time_left:8178s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     188     1   764  2842   .5559       92.5 29.76  304             65536  7.362    .6322    31 2.524 6.93e-06   248 922.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.48      .4676         0                29566 1012 3764 1.861\n",
            "\n",
            "14:15:49 | time:20827s total_exs:535440 total_steps:33465 epochs:2.16 time_left:8166s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.2     1 731.9  2704   .4865      72.76 29.56  296             65536   7.78    .6624 30.35 2.565 6.93e-06 242.1 894.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .08784 13.01      .4582         0                29584 974.1 3599 1.818\n",
            "\n",
            "14:15:59 | time:20837s total_exs:535744 total_steps:33484 epochs:2.16 time_left:8153s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.6     1 769.8  2891   .5526      81.36 30.05  304             65536  7.512    .6435 30.62 2.589 6.93e-06 244.9   920   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.32      .4513         0                29603 1015 3811 1.913\n",
            "\n",
            "14:16:09 | time:20848s total_exs:536040 total_steps:33502 epochs:2.16 time_left:8141s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.6     1 736.5  2662   .5000      76.49 28.91  296             65536  7.411    .6624 30.46 2.529 6.93e-06 243.7 880.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.54      .4621         0                29622 980.2 3542 1.869\n",
            "\n",
            "14:16:19 | time:20858s total_exs:536344 total_steps:33521 epochs:2.16 time_left:8129s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   154.7     1 723.7  2714   .4934       64.2    30  304             65536   7.63    .6624 29.19 2.472 6.93e-06 233.4 875.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .01316 11.84      .4684         0                29641 957.1 3589 1.876\n",
            "\n",
            "14:16:29 | time:20868s total_exs:536632 total_steps:33539 epochs:2.16 time_left:8117s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   154.2     1 698.1  2514   .4583       66.9 28.81  288             65536  7.542    .6624 31.75  2.53 6.93e-06 253.9 914.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .01042 12.56      .4631         0                29659  952 3428 1.801\n",
            "\n",
            "14:16:40 | time:20878s total_exs:536952 total_steps:33559 epochs:2.16 time_left:8104s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.4     1 733.9  2822   .5000       69.7 30.77  320             65536   7.77    .6378 28.23 2.447 6.93e-06 225.8 868.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.56      .4781         0                29679 959.7 3691 1.924\n",
            "\n",
            "14:16:50 | time:20888s total_exs:537256 total_steps:33578 epochs:2.16 time_left:8092s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   183.8     1 765.9  2883   .5493      88.08 30.11  304             65536   7.51    .6378 30.69 2.541 6.93e-06 245.6 924.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.69      .4692   .003289                29698 1011 3807 1.883\n",
            "\n",
            "14:17:00 | time:20899s total_exs:537576 total_steps:33598 epochs:2.16 time_left:8078s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   149.8     1 719.4  2757   .4656      59.88 30.66  320             65536  7.736    .6554 29.48 2.491 6.93e-06 235.8 903.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.07      .4671         0                29718 955.2 3661 1.917\n",
            "\n",
            "14:17:11 | time:20909s total_exs:537872 total_steps:33617 epochs:2.16 time_left:8066s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.8     1 730.5  2676   .4966      70.45 29.31  296             65536  7.351    .6624 31.68 2.526 6.93e-06 253.4 928.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.51      .4652         0                29736 983.9 3605 1.803\n",
            "\n",
            "14:17:21 | time:20919s total_exs:538160 total_steps:33635 epochs:2.17 time_left:8055s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.3     1 741.4  2661   .5104      84.58 28.71  288             65536  7.534    .6624  32.7 2.579 6.93e-06 260.5 934.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472      .1458 13.19      .4592         0                29754 1002 3596 1.835\n",
            "\n",
            "14:17:31 | time:20929s total_exs:538456 total_steps:33653 epochs:2.17 time_left:8043s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   153.8     1 726.2  2619   .4797      63.05 28.85  296             65536  7.282    .6624 31.81 2.488 6.93e-06 253.4 913.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378      .1385 12.04      .4716         0                29773 979.5 3532 1.869\n",
            "\n",
            "14:17:41 | time:20939s total_exs:538776 total_steps:33673 epochs:2.17 time_left:8029s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.5     1 741.6  2895   .5281      76.78 31.23  320             65536  7.894    .6378 28.13 2.588 6.93e-06   225 878.3   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 13.3      .4593         0                29793 966.6 3773 1.952\n",
            "\n",
            "14:17:51 | time:20950s total_exs:539096 total_steps:33693 epochs:2.17 time_left:8016s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.1     1 758.6  2941   .5219      79.28 31.02  320             65536  7.789    .6322 28.18 2.504 6.93e-06 225.4 873.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.23      .4687         0                29813  984 3815 1.939\n",
            "\n",
            "14:18:02 | time:20960s total_exs:539384 total_steps:33711 epochs:2.17 time_left:8005s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.8     1 724.6  2593   .4965      79.19 28.63  288             65536  7.529    .6435  29.8  2.53 6.93e-06 238.4 852.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 12.55      .4594         0                29831 962.9 3446 1.79\n",
            "\n",
            "14:18:12 | time:20970s total_exs:539688 total_steps:33730 epochs:2.17 time_left:7992s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     175     1 757.1  2774   .5164      80.34 29.31  304             65536  7.562    .6552 30.45 2.579 6.93e-06 243.6 892.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.18      .4596         0                29850 1001 3666 1.833\n",
            "\n",
            "14:18:22 | time:20980s total_exs:540000 total_steps:33750 epochs:2.17 time_left:7979s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.5     1 766.8  2960   .5609      88.64 30.88  312             65536  7.583    .6433 29.62 2.482 6.93e-06 236.9 914.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.97      .4748         0                29869 1004 3875 1.909\n",
            "\n",
            "14:18:32 | time:20991s total_exs:540296 total_steps:33768 epochs:2.17 time_left:7967s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     164     1 713.8  2614   .4831      74.82  29.3  296             65536  7.746    .6530 31.88 2.608 6.93e-06 255.1 934.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 13.57      .4546         0                29888 968.8 3548 1.898\n",
            "\n",
            "14:18:42 | time:21001s total_exs:540600 total_steps:33787 epochs:2.18 time_left:7955s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   155.7     1 719.3  2716   .4901      65.76  30.2  304             65536  7.649    .6490 29.75 2.536 6.93e-06   238 898.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.63      .4643         0                29907 957.3 3614 1.888\n",
            "\n",
            "14:18:53 | time:21011s total_exs:540904 total_steps:33806 epochs:2.18 time_left:7942s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.3     1 716.9  2644   .4967      72.72  29.5  304             65536  7.563    .6552 31.02 2.543 6.93e-06 248.2 915.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.72      .4646         0                29926 965.1 3559 1.844\n",
            "\n",
            "14:19:03 | time:21021s total_exs:541208 total_steps:33825 epochs:2.18 time_left:7930s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.6     1   752  2780   .5164      67.62 29.57  304             65536  7.354    .6433 29.79 2.504 6.93e-06 238.3 880.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.23      .4723         0                29945 990.3 3661 1.849\n",
            "\n",
            "14:19:13 | time:21031s total_exs:541512 total_steps:33844 epochs:2.18 time_left:7918s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.3     1 734.3  2731   .4967      68.46 29.75  304             65536  7.367    .6622  30.4 2.477 6.93e-06 242.9 903.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003289     .03947 11.91      .4671         0                29964 977.2 3634 1.86\n",
            "\n",
            "14:19:23 | time:21042s total_exs:541816 total_steps:33863 epochs:2.18 time_left:7905s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.4     1   743  2749   .5362      75.53  29.6  304             65536  7.529    .6622 29.78 2.479 6.93e-06 237.7 879.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579     .06579 11.93      .4704         0                29983 980.7 3628 1.851\n",
            "\n",
            "14:19:34 | time:21052s total_exs:542120 total_steps:33882 epochs:2.18 time_left:7893s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.8     1 718.8  2678   .4737      72.91 29.81  304             65536  7.553    .6622 30.85 2.544 6.93e-06 245.1 913.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579      .2138 12.73      .4677         0                30002 963.9 3592 1.863\n",
            "\n",
            "14:19:44 | time:21062s total_exs:542400 total_steps:33900 epochs:2.18 time_left:7882s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.1     1 760.4  2644   .5429      84.09 27.82  280             65536  7.397    .6622 33.51 2.578 6.93e-06 264.6 920.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "    .01071      .4321 13.17      .4523         0                30019 1025 3564 1.71\n",
            "\n",
            "14:19:54 | time:21072s total_exs:542704 total_steps:33919 epochs:2.18 time_left:7869s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   155.3     1   728  2761   .4671      64.29 30.34  304             65536  7.464    .6320 28.95 2.497 6.93e-06 231.6 878.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.15      .4590         0                30038 959.6 3639 1.932\n",
            "\n",
            "14:20:04 | time:21082s total_exs:543000 total_steps:33937 epochs:2.19 time_left:7857s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   155.4     1 711.7  2611   .4865      66.44 29.34  296             65536   7.81    .6490 30.03 2.578 6.93e-06 240.2 881.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.17      .4557         0                30057  952 3492 1.896\n",
            "\n",
            "14:20:14 | time:21092s total_exs:543304 total_steps:33956 epochs:2.19 time_left:7845s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.3     1 746.8  2814   .5164      78.91 30.15  304             65536  7.492    .6376 31.38  2.49 6.93e-06 251.1 946.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.06      .4715         0                30076 997.9 3761 1.885\n",
            "\n",
            "14:20:24 | time:21102s total_exs:543608 total_steps:33975 epochs:2.19 time_left:7832s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.9     1 743.7  2818   .5428      76.96 30.31  304             65536  7.666    .6376 29.76 2.568 6.93e-06 238.1 902.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 13.05      .4579         0                30095 981.8 3720 1.896\n",
            "\n",
            "14:20:34 | time:21112s total_exs:543912 total_steps:33994 epochs:2.19 time_left:7820s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   185.3     1 778.4  2932   .5493      88.01 30.13  304             65536  7.592    .6552  30.5 2.625 6.93e-06   244 918.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.8      .4511         0                30114 1022 3851 1.884\n",
            "\n",
            "14:20:44 | time:21122s total_exs:544184 total_steps:34011 epochs:2.19 time_left:7809s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.1     1 741.2  2514   .4926      84.47 27.13  272             65536  7.086    .6622 33.59 2.572 6.93e-06 264.9 898.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .007353      .4706 13.09      .4609         0                30131 1006 3412 1.696\n",
            "\n",
            "14:20:54 | time:21133s total_exs:544496 total_steps:34031 epochs:2.19 time_left:7796s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.1     1 756.1  2933   .5481      80.59 31.04  312             65536  7.699    .6622 29.11 2.525 6.93e-06 232.9 903.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.48      .4687         0                30150  989 3837 1.924\n",
            "\n",
            "14:21:04 | time:21143s total_exs:544792 total_steps:34049 epochs:2.19 time_left:7784s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.1     1 734.6  2683   .5034      75.27 29.21  296             65536  7.503    .6490 30.34 2.497 6.93e-06 242.7 886.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.15      .4680         0                30169 977.4 3569 1.897\n",
            "\n",
            "14:21:14 | time:21153s total_exs:545096 total_steps:34068 epochs:2.19 time_left:7772s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.3     1 738.8  2773   .5066      70.99 30.02  304             65536   7.65    .6622 29.51 2.494 6.93e-06 236.1   886   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.11      .4726         0                30188 974.9 3659 1.877\n",
            "\n",
            "14:21:25 | time:21163s total_exs:545384 total_steps:34086 epochs:2.20 time_left:7760s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.8     1 723.9  2580   .4931      74.26 28.51  288             65536  7.441    .6622 32.05  2.54 6.93e-06 255.2 909.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472      .1424 12.68      .4619         0                30206 979.2 3489 1.783\n",
            "\n",
            "14:21:35 | time:21173s total_exs:545696 total_steps:34106 epochs:2.20 time_left:7748s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.9     1 755.8  2927   .5321      73.38 30.99  312             65536  7.554    .6264 30.06 2.482 6.93e-06 240.5 931.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.96      .4704         0                30225 996.3 3859 1.911\n",
            "\n",
            "14:21:45 | time:21183s total_exs:545992 total_steps:34124 epochs:2.20 time_left:7736s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.9     1 736.5  2629   .5034      76.87 28.55  296             65536  7.286    .6433 32.07 2.415 6.93e-06 256.6 915.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.19      .4811         0                30244 993.1 3544 1.847\n",
            "\n",
            "14:21:55 | time:21194s total_exs:546296 total_steps:34143 epochs:2.20 time_left:7723s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.6     1 748.4  2779   .5329      74.01 29.71  304             65536   7.88    .6622 29.91  2.52 6.93e-06 237.2 880.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .2632 12.43      .4669         0                30263 985.6 3660 1.858\n",
            "\n",
            "14:22:05 | time:21204s total_exs:546592 total_steps:34162 epochs:2.20 time_left:7711s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.7     1 729.7  2664   .5068      73.48 29.21  296             65536    7.4    .6622 31.98 2.548 6.93e-06 255.9 934.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.78      .4596         0                30281 985.5 3598 1.806\n",
            "\n",
            "14:22:16 | time:21214s total_exs:546888 total_steps:34180 epochs:2.20 time_left:7699s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   197.4     1 792.2  2831   .5845      98.41 28.58  296             65536  7.145    .6552 32.57 2.499 6.93e-06 260.6   931   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.17      .4755         0                30300 1053 3762 1.854\n",
            "\n",
            "14:22:26 | time:21225s total_exs:547192 total_steps:34199 epochs:2.20 time_left:7687s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.3     1 748.9  2748   .5197      76.73 29.36  304             65536  7.499    .6552  31.4 2.569 6.93e-06 251.2   922   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.05      .4586         0                30319 1000 3670 1.836\n",
            "\n",
            "14:22:36 | time:21235s total_exs:547496 total_steps:34218 epochs:2.20 time_left:7675s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   194.9     1 786.6  2931   .5921      96.55 29.81  304             65536  7.398    .6320 30.39 2.471 6.93e-06 243.2 906.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.83      .4789         0                30338 1030 3838 1.864\n",
            "\n",
            "14:22:47 | time:21245s total_exs:547800 total_steps:34237 epochs:2.20 time_left:7662s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.1     1 742.9  2795   .5428       86.2  30.1  304             65536  7.453    .6320 30.51 2.466 6.93e-06 244.1 918.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.78      .4708         0                30357  987 3713 1.881\n",
            "\n",
            "14:22:57 | time:21255s total_exs:548104 total_steps:34256 epochs:2.21 time_left:7650s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.9     1 771.8  2895   .5526      79.41 30.01  304             65536  7.536    .6320 31.08 2.569 6.93e-06 248.6 932.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.05      .4607         0                30376 1020 3828 1.876\n",
            "\n",
            "14:23:07 | time:21265s total_exs:548416 total_steps:34276 epochs:2.21 time_left:7637s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.2     1 731.9  2839   .5064      75.75 31.03  312             65536  7.536    .6490 30.04 2.488 6.93e-06 240.4 932.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.03      .4727         0                30395 972.3 3771 1.912\n",
            "\n",
            "14:23:17 | time:21275s total_exs:548720 total_steps:34295 epochs:2.21 time_left:7625s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   157.4     1 724.8  2724   .4803       66.8 30.07  304             65536  7.659    .6433 29.83 2.504 6.93e-06 238.6 896.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.23      .4652         0                30414 963.4 3621 1.912\n",
            "\n",
            "14:23:27 | time:21285s total_exs:549016 total_steps:34313 epochs:2.21 time_left:7613s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.5     1 736.5  2705   .5000      88.44 29.38  296             65536  7.584    .6622 30.28  2.52 6.93e-06 242.1 888.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .02365 12.43      .4643         0                30433 978.5 3593 1.898\n",
            "\n",
            "14:23:37 | time:21296s total_exs:549336 total_steps:34333 epochs:2.21 time_left:7600s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.3     1 765.2  2986   .5531      88.63 31.22  320             65536  7.515    .6376 29.56 2.497 6.93e-06 236.4 922.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.15      .4726         0                30453 1002 3909 1.952\n",
            "\n",
            "14:23:47 | time:21306s total_exs:549616 total_steps:34351 epochs:2.21 time_left:7589s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   196.5     1 786.4  2725   .5786       98.2 27.72  280             65536  6.982    .6622 35.04 2.522 6.93e-06 280.1 970.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003571     .03214 12.46      .4697         0                30470 1066 3696 1.706\n",
            "\n",
            "14:23:57 | time:21316s total_exs:549912 total_steps:34369 epochs:2.21 time_left:7577s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.4     1 744.5  2749   .5236      77.37 29.53  296             65536   7.45    .6622 29.64 2.482 6.93e-06 237.1 875.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.96      .4708         0                30489 981.6 3624 1.912\n",
            "\n",
            "14:24:08 | time:21326s total_exs:550200 total_steps:34387 epochs:2.21 time_left:7565s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.2     1 723.6  2570   .4965      70.74 28.41  288             65536  7.572    .6623 30.86 2.515 6.93e-06 246.2 874.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472     .08681 12.36      .4620         0                30507 969.8 3444 1.776\n",
            "\n",
            "14:24:18 | time:21336s total_exs:550504 total_steps:34406 epochs:2.22 time_left:7553s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.2     1 732.9  2690   .5164      79.55 29.36  304             65536  7.454    .6552 30.93 2.464 6.93e-06 247.4   908   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.76      .4789         0                30526 980.4 3598 1.836\n",
            "\n",
            "14:24:28 | time:21346s total_exs:550800 total_steps:34425 epochs:2.22 time_left:7541s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     194     1 778.6  2862   .5574      96.64  29.4  296             65536   7.12    .6491 33.69 2.472 6.93e-06 269.5 990.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.85      .4702         0                30544 1048 3852 1.809\n",
            "\n",
            "14:24:38 | time:21356s total_exs:551088 total_steps:34443 epochs:2.22 time_left:7529s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.3     1 746.5  2667   .5347      80.98 28.58  288             65536   7.68    .6622 31.05  2.52 6.93e-06 247.2 883.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472      .1458 12.42      .4712         0                30562 993.7 3550 1.823\n",
            "\n",
            "14:24:48 | time:21367s total_exs:551392 total_steps:34462 epochs:2.22 time_left:7517s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.6     1   710  2672   .4803      70.88 30.11  304             65536  7.604    .6321 30.52 2.506 6.93e-06 244.1 918.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.25      .4672         0                30581 954.2 3591 1.917\n",
            "\n",
            "14:24:58 | time:21377s total_exs:551688 total_steps:34480 epochs:2.22 time_left:7505s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     174     1 736.1  2722   .5135      81.96 29.59  296             65536  7.332    .6552 31.08 2.439 6.93e-06 248.6 919.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.46      .4795         0                30600 984.7 3642 1.912\n",
            "\n",
            "14:25:08 | time:21387s total_exs:551992 total_steps:34499 epochs:2.22 time_left:7493s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.1     1 726.7  2743   .4868      70.26 30.19  304             65536   7.52    .6433 30.01 2.491 6.93e-06 240.1 906.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.07      .4687         0                30619 966.8 3649 1.888\n",
            "\n",
            "14:25:18 | time:21397s total_exs:552296 total_steps:34518 epochs:2.22 time_left:7480s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.1     1   709  2692   .4671      74.44 30.38  304             65536  7.402    .6622 31.69  2.54 6.93e-06 253.6 962.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.68      .4588         0                30638 962.6 3655 1.899\n",
            "\n",
            "14:25:28 | time:21407s total_exs:552568 total_steps:34535 epochs:2.22 time_left:7469s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     167     1 732.6  2485   .4926      75.44 27.13  272             65536  7.133    .6622 33.93 2.562 6.93e-06 270.7 918.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003676     .09559 12.96      .4572         0                30655 1003 3403 1.697\n",
            "\n",
            "14:25:39 | time:21417s total_exs:552872 total_steps:34554 epochs:2.23 time_left:7457s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.6     1 753.9  2765   .5230      83.35 29.34  304             65536  7.333    .6320 31.44  2.48 6.93e-06 251.5 922.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.95      .4706         0                30674 1005 3687 1.834\n",
            "\n",
            "14:25:49 | time:21428s total_exs:553176 total_steps:34573 epochs:2.23 time_left:7445s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.5     1 715.6  2622   .4901      79.09 29.32  304             65536  7.394    .6552 30.87 2.488 6.93e-06 246.9 904.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.04      .4702         0                30693 962.5 3527 1.833\n",
            "\n",
            "14:25:59 | time:21438s total_exs:553480 total_steps:34592 epochs:2.23 time_left:7433s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.3     1 730.2  2722   .4737      71.04 29.82  304             65536  7.554    .6490 30.98 2.528 6.93e-06 247.8 923.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.52      .4666         0                30712  978 3646 1.864\n",
            "\n",
            "14:26:10 | time:21448s total_exs:553784 total_steps:34611 epochs:2.23 time_left:7420s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.8     1 754.9  2757   .5329      84.47 29.22  304             65536  7.412    .6491 31.14 2.512 6.93e-06 249.1 909.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.33      .4625         0                30731 1004 3667 1.827\n",
            "\n",
            "14:26:20 | time:21458s total_exs:554088 total_steps:34630 epochs:2.23 time_left:7408s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.8     1 761.8  2847   .5461      81.62  29.9  304             65536  7.338    .6376 30.85 2.514 6.93e-06 246.8 922.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 12.36      .4692         0                30750 1009 3770 1.87\n",
            "\n",
            "14:26:30 | time:21469s total_exs:554392 total_steps:34649 epochs:2.23 time_left:7396s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.1     1 756.8  2778   .5395      86.47 29.36  304             65536  7.284    .6491 31.16 2.482 6.93e-06 249.3 915.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.96      .4717         0                30769 1006 3693 1.835\n",
            "\n",
            "14:26:41 | time:21479s total_exs:554664 total_steps:34666 epochs:2.23 time_left:7385s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     188     1   761  2536   .5404      92.83 26.65  272             65536  7.157    .6623 34.87 2.554 6.93e-06 277.2 923.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003676      .2243 12.86      .4605         0                30786 1038 3459 1.667\n",
            "\n",
            "14:26:51 | time:21489s total_exs:554952 total_steps:34684 epochs:2.23 time_left:7374s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.7     1 759.4  2689   .5486      78.74 28.32  288             65536  6.968    .6622 35.05 2.522 6.93e-06 280.2 991.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   .003472     .03125 12.46      .4631         0                30804 1040 3680 1.77\n",
            "\n",
            "14:27:01 | time:21499s total_exs:555256 total_steps:34703 epochs:2.23 time_left:7361s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.7     1 762.1  2843   .5526      75.46 29.84  304             65536  7.625    .6552 30.95 2.549 6.93e-06 247.6 923.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.8      .4621         0                30823 1010 3766 1.865\n",
            "\n",
            "14:27:11 | time:21510s total_exs:555544 total_steps:34721 epochs:2.24 time_left:7350s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     144     1   671  2364   .4410      60.15 28.18  288             65536  7.214    .6552 31.63 2.586 6.93e-06 253.1 891.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 13.28      .4536         0                30841 924.1 3255 1.761\n",
            "\n",
            "14:27:21 | time:21520s total_exs:555848 total_steps:34740 epochs:2.24 time_left:7338s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.1     1 732.8  2713   .5000      81.45 29.61  304             65536  7.385    .6433  31.5 2.545 6.93e-06   252 932.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.74      .4545         0                30860 984.8 3646 1.852\n",
            "\n",
            "14:27:32 | time:21530s total_exs:556160 total_steps:34760 epochs:2.24 time_left:7325s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.2     1 741.9  2869   .5064      69.41 30.94  312             65536  7.552    .6622 30.11  2.48 6.93e-06 240.8 931.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003205    .003205 11.94      .4787   .003205                30879 982.8 3801 1.912\n",
            "\n",
            "14:27:42 | time:21540s total_exs:556456 total_steps:34778 epochs:2.24 time_left:7313s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.7     1 723.1  2597   .4865      70.29 28.74  296             65536  7.516    .6623 31.55 2.559 6.93e-06 252.4 906.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.92      .4582         0                30898 975.5 3504 1.862\n",
            "\n",
            "14:27:52 | time:21550s total_exs:556744 total_steps:34796 epochs:2.24 time_left:7301s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.6     1 740.3  2642   .5139      74.06 28.55  288             65536  7.147    .6491 32.37 2.507 6.93e-06 258.9 924.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.26      .4680         0                30916 999.2 3566 1.785\n",
            "\n",
            "14:28:02 | time:21560s total_exs:557040 total_steps:34815 epochs:2.24 time_left:7290s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.9     1 747.2  2764   .5405      85.53 29.59  296             65536  7.266    .6552 31.33 2.517 6.93e-06 250.7 927.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.39      .4666         0                30934 997.9 3692 1.824\n",
            "\n",
            "14:28:12 | time:21570s total_exs:557336 total_steps:34833 epochs:2.24 time_left:7278s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.2     1 740.7  2730   .5338      81.65 29.49  296             65536  7.537    .6376 30.76 2.544 6.93e-06 246.1   907   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.72      .4590         0                30953 986.8 3637 1.909\n",
            "\n",
            "14:28:22 | time:21581s total_exs:557632 total_steps:34852 epochs:2.24 time_left:7266s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.7     1 785.5  2869   .5676      86.47 29.22  296             65536  7.355    .6622 31.11 2.493 6.93e-06 248.9 908.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378    .006757 12.1      .4660         0                30971 1034 3778 1.803\n",
            "\n",
            "14:28:32 | time:21591s total_exs:557936 total_steps:34871 epochs:2.25 time_left:7253s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.1     1 757.7  2860   .5493      84.38 30.19  304             65536  7.424    .6376 30.69 2.467 6.93e-06 245.6 926.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.79      .4706         0                30990 1003 3786 1.922\n",
            "\n",
            "14:28:43 | time:21601s total_exs:558248 total_steps:34890 epochs:2.25 time_left:7241s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.1     1   771  2913   .5609      84.72 30.23  312             65536  7.315    .6376 31.33 2.482 6.93e-06 250.6 947.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.97      .4734         0                31010 1022 3861 1.949\n",
            "\n",
            "14:28:53 | time:21611s total_exs:558544 total_steps:34909 epochs:2.25 time_left:7229s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.6     1 731.1  2692   .5000      71.24 29.46  296             65536  7.606    .6623 31.44 2.547 6.93e-06 251.1 924.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .05405 12.76      .4599         0                31028 982.1 3617 1.816\n",
            "\n",
            "14:29:03 | time:21621s total_exs:558840 total_steps:34927 epochs:2.25 time_left:7217s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.6     1 775.8  2835   .5541      85.62 29.24  296             65536   7.44    .6623 30.78 2.468 6.93e-06 245.1 895.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378      .1351 11.8      .4690         0                31047 1021 3731 1.892\n",
            "\n",
            "14:29:11 | time:21629s total_exs:559080 total_steps:34942 epochs:2.25 time_left:7207s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   191.6     1 785.6  2937   .5792      93.41 29.91  240             65536  7.628    .6623 30.37 2.511 6.93e-06   243 908.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.32      .4666         0                31062 1029 3846 1.871\n",
            "\n",
            "14:29:11 | running eval: valid\n",
            "14:33:31 | eval completed in 260.27s\n",
            "14:33:31 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   175.4 734.4  8373   .4922      83.52 91.13 23718    .5652 36.19 2.433 6.93e-06 288.9  3294 .001686     .05232 11.39   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4802         0                31062 1023 11667\n",
            "\u001b[0m\n",
            "14:33:31 | \u001b[1;32mnew best ppl: 11.39 (previous best was 11.8)\u001b[0m\n",
            "14:33:31 | saving best valid model: /content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model\n",
            "14:33:58 | time:21916s total_exs:559384 total_steps:34961 epochs:2.25 time_left:7287s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.4     1 734.4  2717   .5000      80.56  29.6  304             65536  7.252    .6624 31.34 2.452 6.93e-06   250 924.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003289     .08882 11.61      .4746         0                31081 984.3 3642 1.85\n",
            "\n",
            "14:34:08 | time:21926s total_exs:559688 total_steps:34980 epochs:2.25 time_left:7274s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   157.4     1 726.2  2711   .4868      66.65 29.87  304             65536  7.465    .6435 31.13   2.5 6.93e-06 249.1 929.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.18      .4627         0                31100 975.3 3641 1.867\n",
            "\n",
            "14:34:18 | time:21937s total_exs:559992 total_steps:34999 epochs:2.25 time_left:7262s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.2     1 752.4  2853   .5395      78.13 30.34  304             65536   7.72    .6624  29.6 2.549 6.93e-06 236.8 897.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.8      .4623         0                31119 989.2 3751 1.897\n",
            "\n",
            "14:34:28 | time:21947s total_exs:560304 total_steps:35019 epochs:2.26 time_left:7249s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.8     1 782.9  3041   .5833       86.9 31.08  312             65536  7.423    .6266 29.64 2.501 6.93e-06 237.1 921.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.19      .4691         0                31138 1020 3963 1.913\n",
            "\n",
            "14:34:38 | time:21957s total_exs:560600 total_steps:35037 epochs:2.26 time_left:7237s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.6     1 720.5  2612   .4831      70.56    29  296             65536  7.449    .6492 29.78 2.502 6.93e-06 238.2 863.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.21      .4680         0                31157 958.8 3476 1.873\n",
            "\n",
            "14:34:49 | time:21967s total_exs:560888 total_steps:35055 epochs:2.26 time_left:7225s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     192     1 789.6  2827   .5833      93.27 28.64  288             65536  7.153    .6624 33.42 2.493 6.93e-06 267.4 957.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.1      .4715         0                31175 1057 3784 1.791\n",
            "\n",
            "14:34:59 | time:21977s total_exs:561192 total_steps:35074 epochs:2.26 time_left:7213s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.7     1 738.3  2729   .5164      77.46 29.57  304             65536  7.198    .6377 32.37 2.493 6.93e-06 258.9 957.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.1      .4671         0                31194 997.2 3686 1.848\n",
            "\n",
            "14:35:09 | time:21987s total_exs:561496 total_steps:35093 epochs:2.26 time_left:7200s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   189.9     1 758.9  2808   .5263         95  29.6  304            117275  7.234    .6378 31.91 2.476 6.93e-06 255.3 944.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 11.9      .4696         0                31213 1014 3752 1.85\n",
            "\n",
            "14:35:16 | Overflow: setting loss scale to 65536.0\n",
            "14:35:19 | time:21998s total_exs:561800 total_steps:35112 epochs:2.26 time_left:7188s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.3 .9474 767.4  2881   .5428      82.38 30.04  304            103478  6.883    .6624 31.59 2.454 6.93e-06 250.9 942.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003289      .2204 11.64      .4784         0                31232 1018 3824 1.878\n",
            "\n",
            "14:35:30 | time:22008s total_exs:562104 total_steps:35131 epochs:2.26 time_left:7176s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.9     1 735.6  2675   .4967      78.91 29.09  304             65536   7.53    .6624 31.42 2.461 6.93e-06 250.6 911.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .09211 11.71      .4729         0                31251 986.2 3587 1.819\n",
            "\n",
            "14:35:40 | time:22018s total_exs:562408 total_steps:35150 epochs:2.26 time_left:7163s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.5     1 731.8  2681   .5099      72.02 29.31  304             65536  7.312    .6624  32.1 2.449 6.93e-06 255.2 935.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .1941 11.58      .4733         0                31270 987.1 3617 1.833\n",
            "\n",
            "14:35:50 | time:22029s total_exs:562704 total_steps:35169 epochs:2.26 time_left:7151s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   188.6     1 759.4  2801   .5338      93.69 29.51  296             65536  7.484    .6624  31.3 2.583 6.93e-06 250.3 923.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .01014 13.24      .4525         0                31288 1010 3724 1.816\n",
            "\n",
            "14:36:00 | time:22039s total_exs:563016 total_steps:35188 epochs:2.27 time_left:7138s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   158.4     1 727.2  2805   .4776      67.48 30.85  312             65536  7.652    .6624 28.95 2.469 6.93e-06 231.3   892   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003205     .03526 11.81      .4779         0                31308 958.5 3697 1.993\n",
            "\n",
            "14:36:10 | time:22049s total_exs:563320 total_steps:35207 epochs:2.27 time_left:7126s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.5     1 711.1  2689   .4737       70.6 30.25  304             65536  7.559    .6624 29.97 2.435 6.93e-06 239.7 906.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289    .003289 11.42      .4783         0                31327 950.8 3595 1.892\n",
            "\n",
            "14:36:20 | time:22059s total_exs:563616 total_steps:35226 epochs:2.27 time_left:7114s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.2     1 742.6  2734   .5372      75.41 29.45  296             65536  7.658    .6553 29.66 2.541 6.93e-06 237.3 873.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.69      .4630         0                31345 979.9 3607 1.818\n",
            "\n",
            "14:36:31 | time:22069s total_exs:563912 total_steps:35244 epochs:2.27 time_left:7102s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.9     1 749.9  2749   .5304      72.11 29.32  296             65536  7.501    .6492 29.82 2.478 6.93e-06 238.5 874.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.92      .4726         0                31364 988.5 3623  1.9\n",
            "\n",
            "14:36:41 | time:22079s total_exs:564216 total_steps:35263 epochs:2.27 time_left:7089s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   194.4     1 779.4  2881   .5888      96.97 29.58  304             65536  7.514    .6322 30.62 2.468 6.93e-06 244.9 905.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.8      .4773         0                31383 1024 3787 1.849\n",
            "\n",
            "14:36:51 | time:22089s total_exs:564520 total_steps:35282 epochs:2.27 time_left:7077s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.4     1 742.3  2743   .5066       67.6 29.57  304             65536  7.293    .6492 30.55 2.432 6.93e-06 244.4 903.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.38      .4798         0                31402 986.7 3647 1.848\n",
            "\n",
            "14:37:01 | time:22100s total_exs:564808 total_steps:35300 epochs:2.27 time_left:7065s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     166     1 730.6  2577   .4896      74.71 28.22  288             65536  7.413    .6624 32.42 2.452 6.93e-06 255.6 901.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006944      .4688 11.62      .4772         0                31420 986.2 3479 1.764\n",
            "\n",
            "14:37:12 | time:22110s total_exs:565096 total_steps:35318 epochs:2.27 time_left:7054s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.5     1 724.7  2502   .4896      75.92 27.62  288             65536  7.332    .6624 33.92 2.538 6.93e-06   271 935.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006944     .03819 12.66      .4645         0                31438 995.7 3438 1.726\n",
            "\n",
            "14:37:22 | time:22121s total_exs:565400 total_steps:35337 epochs:2.28 time_left:7041s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   183.5     1 755.9  2772   .5428      89.06 29.33  304             65536  7.203    .6554 33.41 2.522 6.93e-06 267.3   980   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.45      .4626         0                31457 1023 3752 1.834\n",
            "\n",
            "14:37:32 | time:22131s total_exs:565696 total_steps:35356 epochs:2.28 time_left:7029s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     183     1 751.3  2765   .5372      89.11 29.44  296             65536  7.445    .6553 31.56 2.502 6.93e-06 252.5 929.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.2      .4715         0                31475 1004 3694 1.809\n",
            "\n",
            "14:37:43 | time:22141s total_exs:565992 total_steps:35374 epochs:2.28 time_left:7017s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.8     1 731.7  2641   .5135       80.3 28.87  296             65536  7.527    .6378  31.6   2.5 6.93e-06 252.8 912.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.18      .4641         0                31494 984.5 3553 1.862\n",
            "\n",
            "14:37:53 | time:22151s total_exs:566272 total_steps:35392 epochs:2.28 time_left:7006s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.4     1 728.8  2538   .5179      74.27 27.86  280             65536   7.28    .6624 33.31 2.477 6.93e-06 264.4 920.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .007143      .2571 11.91      .4726         0                31511 993.2 3459 1.713\n",
            "\n",
            "14:38:03 | time:22161s total_exs:566568 total_steps:35410 epochs:2.28 time_left:6994s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.7     1 751.1  2694   .5203      78.82 28.69  296             65536  7.244    .6624 32.97 2.546 6.93e-06 263.6 945.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .01689 12.76      .4681         0                31530 1015 3639 1.856\n",
            "\n",
            "14:38:13 | time:22171s total_exs:566888 total_steps:35430 epochs:2.28 time_left:6981s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.5     1   746  2942   .5062      74.22 31.55  320             65536  7.816    .6266 28.58 2.462 6.93e-06 228.7   902   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.73      .4737         0                31550 974.6 3844 1.973\n",
            "\n",
            "14:38:23 | time:22182s total_exs:567176 total_steps:35448 epochs:2.28 time_left:6969s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.5     1 742.7  2612   .5278      74.61 28.14  288             65536  7.413    .6554 32.23 2.533 6.93e-06 257.8 906.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 12.6      .4559         0                31568 1001 3519 1.76\n",
            "\n",
            "14:38:34 | time:22192s total_exs:567480 total_steps:35467 epochs:2.28 time_left:6957s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   195.6     1   773  2869   .5592      98.99 29.69  304             65536  7.321    .6322 32.85 2.481 6.93e-06 262.8 975.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.95      .4728         0                31587 1036 3844 1.856\n",
            "\n",
            "14:38:44 | time:22202s total_exs:567784 total_steps:35486 epochs:2.29 time_left:6944s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.6     1   729  2690   .5000      70.51 29.52  304             65536    7.4    .6624 31.24 2.448 6.93e-06 249.9   922   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.57      .4757         0                31606 978.9 3612 1.846\n",
            "\n",
            "14:38:54 | time:22212s total_exs:568064 total_steps:35504 epochs:2.29 time_left:6933s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   153.7     1 704.3  2436   .4643      65.66 27.67  280             65536   7.46    .6624 32.76 2.518 6.93e-06 260.6 901.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .007143      .1929 12.4      .4627         0                31623 964.9 3337 1.701\n",
            "\n",
            "14:39:04 | time:22223s total_exs:568360 total_steps:35522 epochs:2.29 time_left:6921s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.3     1 746.8  2695   .5236      81.94 28.87  296             65536  7.513    .6435 30.82 2.458 6.93e-06 246.6 889.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.69      .4769         0                31642 993.4 3584 1.868\n",
            "\n",
            "14:39:15 | time:22233s total_exs:568664 total_steps:35541 epochs:2.29 time_left:6909s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.5     1 744.3  2743   .4901       77.5 29.49  304             65536  7.493    .6624 31.17 2.492 6.93e-06 248.4 915.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .1250 12.08      .4653         0                31661 992.7 3659 1.844\n",
            "\n",
            "14:39:25 | time:22243s total_exs:568968 total_steps:35560 epochs:2.29 time_left:6896s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.6     1 757.3  2821   .5132      87.95  29.8  304             65536  7.245    .6378 31.33 2.436 6.93e-06 250.6 933.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.43      .4810         0                31680 1008 3754 1.864\n",
            "\n",
            "14:39:35 | time:22253s total_exs:569272 total_steps:35579 epochs:2.29 time_left:6884s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.6     1   737  2769   .5164      76.46 30.06  304             65536  7.765    .6624 29.84  2.54 6.93e-06 237.3 891.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003289      .1743 12.68      .4628         0                31699 974.4 3661 1.88\n",
            "\n",
            "14:39:45 | time:22264s total_exs:569576 total_steps:35598 epochs:2.29 time_left:6872s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.9     1 742.4  2784   .5099      81.15    30  304             65536   7.31    .6624 30.12 2.451 6.93e-06 239.9 899.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .1316 11.6      .4800         0                31718 982.2 3683 1.875\n",
            "\n",
            "14:39:55 | time:22274s total_exs:569880 total_steps:35617 epochs:2.29 time_left:6859s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     184     1 773.7  2851   .5658      87.27 29.48  304             65536  7.136    .6492 31.76 2.442 6.93e-06 254.1 936.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.5      .4793         0                31737 1028 3787 1.843\n",
            "\n",
            "14:40:06 | time:22284s total_exs:570200 total_steps:35637 epochs:2.29 time_left:6846s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.8     1 725.1  2812   .4844      70.18 31.02  320             65536  7.813    .6378 28.88 2.533 6.93e-06   231 895.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.59      .4652         0                31757 956.1 3707 1.939\n",
            "\n",
            "14:40:16 | time:22294s total_exs:570504 total_steps:35656 epochs:2.30 time_left:6834s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.9     1   752  2814   .5132      74.89 29.93  304             65536  7.645    .6553 29.71 2.509 6.93e-06 237.7 889.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.29      .4700         0                31776 989.7 3703 1.871\n",
            "\n",
            "14:40:26 | time:22304s total_exs:570800 total_steps:35675 epochs:2.30 time_left:6822s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   158.3     1 715.9  2648   .4764      68.76 29.59  296             65536  7.424    .6378 30.24 2.493 6.93e-06 241.9 894.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.1      .4663         0                31794 957.8 3543 1.822\n",
            "\n",
            "14:40:36 | time:22314s total_exs:571104 total_steps:35694 epochs:2.30 time_left:6809s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   192.1     1 762.7  2890   .5362      96.75 30.31  304             65536  7.289    .6266 31.91 2.469 6.93e-06 255.3 967.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.81      .4809         0                31813 1018 3857 1.927\n",
            "\n",
            "14:40:46 | time:22324s total_exs:571400 total_steps:35712 epochs:2.30 time_left:6797s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   157.6     1 730.5  2698   .4899      66.32 29.54  296             65536  7.887    .6435 29.28 2.534 6.93e-06 234.2   865   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.61      .4649         0                31832 964.7 3563 1.907\n",
            "\n",
            "14:40:56 | time:22334s total_exs:571688 total_steps:35730 epochs:2.30 time_left:6785s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.7     1 735.9  2649   .5069      76.75  28.8  288             65536  7.578    .6624 31.03 2.552 6.93e-06   247 889.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472      .1528 12.83      .4576         0                31850 982.9 3538 1.801\n",
            "\n",
            "14:41:06 | time:22345s total_exs:571984 total_steps:35749 epochs:2.30 time_left:6773s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   187.3     1 770.5  2846   .5642      90.96 29.55  296             65536  7.597    .6624 30.53 2.549 6.93e-06 243.6 899.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .08108 12.8      .4651         0                31868 1014 3746 1.818\n",
            "\n",
            "14:41:16 | time:22355s total_exs:572280 total_steps:35767 epochs:2.30 time_left:6762s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.3     1 725.3  2621   .4899      75.67 28.91  296             65536  7.498    .6554 30.46  2.48 6.93e-06 243.7 880.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 11.95      .4671         0                31887  969 3502 1.87\n",
            "\n",
            "14:41:26 | time:22365s total_exs:572584 total_steps:35786 epochs:2.30 time_left:6749s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.8     1 755.1  2843   .5362      85.39 30.12  304             65536  7.324    .6624 32.61 2.483 6.93e-06   260 978.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003289      .1151 11.98      .4709         0                31906 1015 3822 1.883\n",
            "\n",
            "14:41:37 | time:22375s total_exs:572896 total_steps:35806 epochs:2.31 time_left:6736s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     175     1 761.2  2959   .5256      79.82  31.1  312             65536   7.63    .6322 28.88  2.47 6.93e-06 231.1 898.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.82      .4733         0                31925 992.3 3858 1.916\n",
            "\n",
            "14:41:47 | time:22385s total_exs:573192 total_steps:35824 epochs:2.31 time_left:6724s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     184     1 780.7  2876   .5709       86.4 29.47  296             65536  7.457    .6266 30.29 2.489 6.93e-06 242.4 892.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.05      .4726         0                31944 1023 3769 1.906\n",
            "\n",
            "14:41:57 | time:22395s total_exs:573504 total_steps:35844 epochs:2.31 time_left:6711s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     169     1 749.2  2890   .5096      75.37 30.85  312             65536  7.731    .6554 29.59  2.54 6.93e-06 236.7 913.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.68      .4705         0                31963  986 3803 1.915\n",
            "\n",
            "14:42:07 | time:22405s total_exs:573800 total_steps:35862 epochs:2.31 time_left:6699s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.4     1 723.3  2671   .4831      71.02 29.54  296             65536   7.55    .6624 30.86  2.45 6.93e-06 246.9 911.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.58      .4774         0                31982 970.2 3583 1.919\n",
            "\n",
            "14:42:17 | time:22415s total_exs:574088 total_steps:35880 epochs:2.31 time_left:6688s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.4     1   724  2584   .4896      76.87 28.55  288             65536  7.153    .6554 33.72 2.467 6.93e-06 269.7 962.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.79      .4725         0                32000 993.8 3547 1.786\n",
            "\n",
            "14:42:27 | time:22425s total_exs:574376 total_steps:35898 epochs:2.31 time_left:6676s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.4     1 751.5  2658   .5278      76.43 28.29  288             65536  7.414    .6624 32.02 2.467 6.93e-06 254.5   900   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "    .01042      .2118 11.79      .4692         0                32018 1006 3558 1.769\n",
            "\n",
            "14:42:37 | time:22436s total_exs:574680 total_steps:35917 epochs:2.31 time_left:6664s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     168     1 752.1  2783   .5230      74.01 29.61  304             65536  7.722    .6378 29.86 2.556 6.93e-06 238.8   884   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.89      .4642         0                32037 990.9 3667 1.851\n",
            "\n",
            "14:42:47 | time:22446s total_exs:574976 total_steps:35936 epochs:2.31 time_left:6652s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.2     1 744.6  2725   .5135      85.14 29.28  296             65536  7.377    .6624 31.66 2.523 6.93e-06 253.3   927   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.47      .4592         0                32055 997.9 3652 1.801\n",
            "\n",
            "14:42:58 | time:22456s total_exs:575288 total_steps:35955 epochs:2.32 time_left:6639s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     183     1 779.5  2971   .5705      85.54 30.49  312             65536  7.304    .6322 30.15 2.394 6.93e-06 241.2 919.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.95      .4906         0                32075 1021 3891 1.969\n",
            "\n",
            "14:43:08 | time:22466s total_exs:575584 total_steps:35974 epochs:2.32 time_left:6627s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   188.5     1 762.5  2809   .5473      93.19 29.47  296             65536  7.627    .6378 30.49 2.503 6.93e-06 243.9 898.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.22      .4630         0                32093 1006 3708 1.811\n",
            "\n",
            "14:43:18 | time:22476s total_exs:575880 total_steps:35992 epochs:2.32 time_left:6615s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   189.5     1 776.6  2833   .5777       92.4 29.18  296             65536  7.222    .6435  33.2 2.482 6.93e-06 265.6 968.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.97      .4718         0                32112 1042 3801 1.886\n",
            "\n",
            "14:43:28 | time:22486s total_exs:576176 total_steps:36011 epochs:2.32 time_left:6603s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.2     1 748.7  2751   .5338      76.57 29.39  296             65536  7.385    .6624 32.09 2.479 6.93e-06 254.2 933.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378      .3243 11.93      .4693         0                32130 1003 3684 1.808\n",
            "\n",
            "14:43:38 | time:22497s total_exs:576488 total_steps:36030 epochs:2.32 time_left:6591s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.8     1 739.9  2833   .5064      74.35 30.63  312             65536  7.801    .6624 27.52 2.526 6.93e-06 219.3 839.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003205      .1058 12.5      .4670   .003205                32150 959.2 3672 1.978\n",
            "\n",
            "14:43:48 | time:22507s total_exs:576776 total_steps:36048 epochs:2.32 time_left:6579s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   194.2     1 763.1  2699   .5521      98.83  28.3  288             65536  7.243    .6624 33.65 2.453 6.93e-06 268.6 950.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .06944 11.62      .4794         0                32168 1032 3649 1.769\n",
            "\n",
            "14:43:58 | time:22517s total_exs:577088 total_steps:36068 epochs:2.32 time_left:6566s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   197.1     1 781.5  3038   .5673      99.38  31.1  312             65536  7.738    .6624 30.32 2.529 6.93e-06 241.6 939.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003205      .1218 12.54      .4684         0                32187 1023 3977 1.914\n",
            "\n",
            "14:44:09 | time:22527s total_exs:577384 total_steps:36086 epochs:2.32 time_left:6554s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.1     1 752.3  2773   .5270      90.07 29.49  296             65536  7.392    .6492  31.3 2.483 6.93e-06 250.4   923   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.98      .4767         0                32206 1003 3696 1.905\n",
            "\n",
            "14:44:19 | time:22537s total_exs:577688 total_steps:36105 epochs:2.33 time_left:6542s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     172     1 739.4  2742   .5164      79.55 29.67  304             65536  7.365    .6492 30.91 2.448 6.93e-06 247.3   917   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.56      .4765         0                32225 986.7 3659 1.855\n",
            "\n",
            "14:44:29 | time:22547s total_exs:577976 total_steps:36123 epochs:2.33 time_left:6530s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   188.5     1 770.6  2756   .5660      92.17 28.61  288             65536  7.238    .6435  32.1 2.439 6.93e-06 256.8 918.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.47      .4796         0                32243 1027 3674 1.789\n",
            "\n",
            "14:44:39 | time:22558s total_exs:578280 total_steps:36142 epochs:2.33 time_left:6518s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     180     1   751  2787   .5329      86.15 29.68  304             65536  7.586    .6492 30.75 2.507 6.93e-06   246 912.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.27      .4755         0                32262  997 3699 1.855\n",
            "\n",
            "14:44:49 | time:22568s total_exs:578584 total_steps:36161 epochs:2.33 time_left:6506s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.8     1 743.7  2793   .5230      77.88 30.04  304             65536  7.624    .6624 31.02 2.456 6.93e-06 248.1 931.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289    .006579 11.66      .4733         0                32281 991.9 3724 1.878\n",
            "\n",
            "14:45:00 | time:22578s total_exs:578888 total_steps:36180 epochs:2.33 time_left:6493s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.4     1 758.5  2798   .5197      76.55 29.51  304             65536  7.595    .6624 30.78 2.518 6.93e-06 243.7 898.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006579      .3191 12.4      .4614         0                32300 1002 3696 1.845\n",
            "\n",
            "14:45:10 | time:22588s total_exs:579168 total_steps:36198 epochs:2.33 time_left:6482s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   185.9     1 767.5  2648   .5571      89.96  27.6  280             65536   7.27    .6624 34.11 2.527 6.93e-06 271.9 938.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .007143      .1250 12.52      .4619         0                32317 1039 3586 1.698\n",
            "\n",
            "14:45:20 | time:22598s total_exs:579480 total_steps:36217 epochs:2.33 time_left:6469s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.5     1 744.7  2834   .5192      79.37 30.44  312             65536  7.548    .6214 30.99 2.547 6.93e-06 247.9 943.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.77      .4645         0                32337 992.7 3777 1.967\n",
            "\n",
            "14:45:30 | time:22609s total_exs:579768 total_steps:36235 epochs:2.33 time_left:6458s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   192.3     1 786.5  2764   .5729      94.02 28.12  288             65536  7.208    .6624 32.49 2.471 6.93e-06 259.7 912.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .03125 11.84      .4736         0                32355 1046 3677 1.758\n",
            "\n",
            "14:45:40 | time:22619s total_exs:580056 total_steps:36253 epochs:2.33 time_left:6446s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   185.9     1 755.5  2717   .5312      91.47 28.77  288             65536  7.216    .6624 34.08 2.463 6.93e-06 271.9 977.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .09375 11.74      .4686         0                32373 1027 3695 1.799\n",
            "\n",
            "14:45:51 | time:22629s total_exs:580360 total_steps:36272 epochs:2.34 time_left:6434s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.5     1 751.2  2799   .5329      71.63 29.81  304             65536   7.46    .6624  29.4 2.436 6.93e-06 235.2 876.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.43      .4742         0                32392 986.4 3676 1.864\n",
            "\n",
            "14:46:01 | time:22639s total_exs:580664 total_steps:36291 epochs:2.34 time_left:6422s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.8     1 761.3  2826   .5526      85.66  29.7  304             65536  7.671    .6378    30 2.505 6.93e-06   240   891   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.24      .4711         0                32411 1001 3717 1.857\n",
            "\n",
            "14:46:11 | time:22650s total_exs:580984 total_steps:36311 epochs:2.34 time_left:6409s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.6     1 750.2  2903   .5188      79.79 30.95  320             65536  8.038    .6492 28.35 2.489 6.93e-06 226.8 877.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.05      .4667         0                32431  977 3780 1.936\n",
            "\n",
            "14:46:21 | time:22660s total_exs:581272 total_steps:36329 epochs:2.34 time_left:6397s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.7     1   734  2601   .4896      81.98 28.34  288             65536  7.504    .6624 32.75 2.532 6.93e-06   256   907   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006944      .7535 12.58      .4658         0                32449  990 3508 1.772\n",
            "\n",
            "14:46:32 | time:22670s total_exs:581592 total_steps:36349 epochs:2.34 time_left:6384s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.6     1 756.1  2932   .5219      79.11 31.02  320             65536  7.605    .6378 29.85 2.465 6.93e-06 238.8   926   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.77      .4696         0                32469  995 3858 1.939\n",
            "\n",
            "14:46:42 | time:22680s total_exs:581896 total_steps:36368 epochs:2.34 time_left:6372s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.6     1 753.3  2801   .5132      76.42 29.75  304             65536  7.477    .6492 30.07 2.471 6.93e-06 240.6 894.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.84      .4738         0                32488 993.8 3696 1.86\n",
            "\n",
            "14:46:52 | time:22691s total_exs:582184 total_steps:36386 epochs:2.34 time_left:6360s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   188.6     1 768.9  2688   .5556       92.5 27.97  288             65536  7.178    .6624 34.13 2.544 6.93e-06 269.7 942.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "    .01042      .4132 12.73      .4544   .003472                32506 1039 3631 1.749\n",
            "\n",
            "14:47:03 | time:22701s total_exs:582504 total_steps:36406 epochs:2.34 time_left:6347s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.5     1 706.5  2740   .4625      77.16 31.03  320             65536  7.443    .6322 29.82 2.446 6.93e-06 238.6 925.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.55      .4718         0                32526 945.1 3666 1.94\n",
            "\n",
            "14:47:13 | time:22711s total_exs:582808 total_steps:36425 epochs:2.35 time_left:6335s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     170     1 728.1  2765   .5132      78.94 30.38  304             65536  7.597    .6624 29.85 2.448 6.93e-06 238.7 906.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .01316 11.57      .4809         0                32545 966.7 3671 1.899\n",
            "\n",
            "14:47:23 | time:22721s total_exs:583112 total_steps:36444 epochs:2.35 time_left:6323s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.9     1 738.5  2706   .4967      75.54 29.31  304             65536  7.639    .6624 31.52 2.477 6.93e-06 249.8 915.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579      .2928 11.91      .4742         0                32564 988.3 3621 1.832\n",
            "\n",
            "14:47:33 | time:22732s total_exs:583416 total_steps:36463 epochs:2.35 time_left:6310s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.1     1   735  2723   .4868      80.23 29.64  304             65536  7.285    .6322 31.62 2.401 6.93e-06   253 937.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.04      .4854         0                32583  988 3661 1.853\n",
            "\n",
            "14:47:43 | time:22742s total_exs:583720 total_steps:36482 epochs:2.35 time_left:6298s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.3     1 734.1  2743   .5033       75.5 29.89  304             65536  7.794    .6378 29.34 2.491 6.93e-06 234.7   877   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.07      .4762         0                32602 968.8 3620 1.869\n",
            "\n",
            "14:47:54 | time:22752s total_exs:584008 total_steps:36500 epochs:2.35 time_left:6287s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.9     1 749.1  2652   .5417      84.22 28.32  288             65536  7.349    .6492 32.32 2.544 6.93e-06 258.5 915.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.73      .4632         0                32620 1008 3567 1.771\n",
            "\n",
            "14:48:04 | time:22762s total_exs:584320 total_steps:36520 epochs:2.35 time_left:6274s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.6     1   755  2928   .5449       82.2 31.03  312             65536  7.345    .6378 30.44  2.48 6.93e-06 243.5 944.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.94      .4700         0                32639 998.5 3873 1.914\n",
            "\n",
            "14:48:14 | time:22772s total_exs:584624 total_steps:36539 epochs:2.35 time_left:6261s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.2     1 745.5  2830   .5164      77.06 30.37  304             65536  7.268    .6378  31.1 2.455 6.93e-06 248.8 944.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.65      .4713   .003289                32658 994.3 3774 1.938\n",
            "\n",
            "14:48:24 | time:22782s total_exs:584920 total_steps:36557 epochs:2.35 time_left:6250s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.6     1 743.2  2686   .5169      74.71 28.91  296             65536   7.23    .6624 31.76 2.428 6.93e-06 253.1 914.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006757      .1216 11.34      .4775         0                32677 996.3 3601 1.871\n",
            "\n",
            "14:48:34 | time:22793s total_exs:585224 total_steps:36576 epochs:2.36 time_left:6237s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.8     1 731.3  2705   .5033      73.37 29.59  304             65536  7.519    .6554 30.34 2.423 6.93e-06 242.7 897.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.28      .4823         0                32696 974.1 3603 1.851\n",
            "\n",
            "14:48:45 | time:22803s total_exs:585528 total_steps:36595 epochs:2.36 time_left:6225s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.7     1 727.4  2692   .5000      69.78 29.61  304             65536  7.711    .6378  30.6 2.516 6.93e-06 244.8 906.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.37      .4673         0                32715 972.2 3598 1.852\n",
            "\n",
            "14:48:55 | time:22813s total_exs:585824 total_steps:36614 epochs:2.36 time_left:6213s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.8     1 750.1  2754   .5304         88 29.37  296             65536  7.725    .6625 29.39  2.55 6.93e-06 234.5 861.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006757     .07432 12.8      .4599         0                32733 984.7 3615 1.803\n",
            "\n",
            "14:49:05 | time:22823s total_exs:586120 total_steps:36632 epochs:2.36 time_left:6201s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.7     1 742.6  2619   .5203       82.9 28.21  296             65536  7.368    .6624 31.71 2.569 6.93e-06 253.6 894.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .01351 13.06      .4622         0                32752 996.2 3513 1.821\n",
            "\n",
            "14:49:15 | time:22834s total_exs:586432 total_steps:36652 epochs:2.36 time_left:6189s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   191.1     1 769.7  2989   .5577      94.88 31.07  312             65536  7.648    .6624 29.42 2.454 6.93e-06 235.1 912.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003205     .03846 11.63      .4789         0                32771 1005 3902 1.913\n",
            "\n",
            "14:49:25 | time:22844s total_exs:586728 total_steps:36670 epochs:2.36 time_left:6177s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.9     1 753.4  2756   .5473       82.7 29.26  296             65536  7.357    .6378 31.67  2.47 6.93e-06 253.3 926.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.82      .4812         0                32790 1007 3682 1.892\n",
            "\n",
            "14:49:36 | time:22854s total_exs:587032 total_steps:36689 epochs:2.36 time_left:6164s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.6     1 759.2  2785   .5099      73.65 29.35  304             65536  7.302    .6492 31.08 2.461 6.93e-06 248.7 912.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.72      .4821         0                32809 1008 3698 1.835\n",
            "\n",
            "14:49:46 | time:22864s total_exs:587336 total_steps:36708 epochs:2.36 time_left:6152s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     167     1 737.4  2788   .4934      74.78 30.24  304             65536  7.258    .6624 31.39 2.472 6.93e-06 251.1 949.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289    .009868 11.84      .4778         0                32828 988.5 3737 1.891\n",
            "\n",
            "14:49:56 | time:22874s total_exs:587640 total_steps:36727 epochs:2.37 time_left:6140s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   186.1     1 765.7  2869   .5493      90.42 29.98  304             65536  7.273    .6493 32.17 2.443 6.93e-06 257.3 964.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.5      .4822         0                32847 1023 3834 1.874\n",
            "\n",
            "14:50:06 | time:22884s total_exs:587960 total_steps:36747 epochs:2.37 time_left:6127s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.1     1 752.1  3002   .5250      71.04 31.94  320             65536  7.798    .6214 26.75 2.438 6.93e-06   214 854.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.45      .4742         0                32867 966.1 3857 1.997\n",
            "\n",
            "14:50:16 | time:22895s total_exs:588264 total_steps:36766 epochs:2.37 time_left:6115s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   183.8     1 766.5  2847   .5526      88.01 29.72  304             65536  7.698    .6323 31.38 2.494 6.93e-06   251 932.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.11      .4739         0                32886 1017 3780 1.858\n",
            "\n",
            "14:50:26 | time:22905s total_exs:588560 total_steps:36785 epochs:2.37 time_left:6103s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.7     1 744.4  2728   .5135      78.62 29.31  296             65536  7.649    .6624 30.62 2.512 6.93e-06   245 897.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.32      .4640         0                32904 989.4 3625 1.801\n",
            "\n",
            "14:50:36 | time:22915s total_exs:588864 total_steps:36804 epochs:2.37 time_left:6090s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   151.6     1 690.5  2609   .4441      65.26 30.22  304             65536  7.752    .6492 29.11 2.498 6.93e-06 232.9 879.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.16      .4692         0                32923 923.4 3488 1.921\n",
            "\n",
            "14:50:47 | time:22925s total_exs:589176 total_steps:36823 epochs:2.37 time_left:6078s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.1     1 732.9  2818   .4808      73.48 30.76  312             65536  7.814    .6554 28.56 2.457 6.93e-06 228.5 878.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.68      .4787         0                32943 961.4 3696 1.984\n",
            "\n",
            "14:50:57 | time:22935s total_exs:589464 total_steps:36841 epochs:2.37 time_left:6066s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   188.5     1 761.6  2735   .5694      93.27 28.73  288             65536  7.456    .6624  31.3  2.49 6.93e-06 249.8   897   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .08333 12.06      .4706         0                32961 1011 3632 1.797\n",
            "\n",
            "14:51:06 | Overflow: setting loss scale to 32768.0\n",
            "14:51:07 | time:22945s total_exs:589768 total_steps:36860 epochs:2.37 time_left:6054s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   187.4 .9474 769.9  2820   .5592      91.19  29.3  304             62087  6.883    .6323 32.65 2.521 6.93e-06 261.2 956.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.44      .4679         0                32980 1031 3777 1.832\n",
            "\n",
            "14:51:17 | time:22955s total_exs:590088 total_steps:36880 epochs:2.38 time_left:6041s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.7     1 738.6  2946   .5250       74.4 31.91  320             32768  7.935    .6435 27.71 2.481 6.93e-06 221.7 884.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.96      .4722         0                33000 960.3 3831 1.995\n",
            "\n",
            "14:51:27 | time:22966s total_exs:590400 total_steps:36900 epochs:2.38 time_left:6028s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.5     1 754.4  2906   .5513      84.24 30.81  312             32768  7.516    .6624  30.2 2.495 6.93e-06 241.2 928.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003205     .05449 12.12      .4708         0                33019 995.6 3835  1.9\n",
            "\n",
            "14:51:37 | time:22976s total_exs:590680 total_steps:36917 epochs:2.38 time_left:6017s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.9     1 727.5  2547   .5000      72.96 28.01  280             32768  7.479    .6625 31.98 2.496 6.93e-06   254 889.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003571      .2321 12.14      .4633         0                33037 981.5 3436 1.814\n",
            "\n",
            "14:51:47 | time:22986s total_exs:590984 total_steps:36936 epochs:2.38 time_left:6005s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.5     1 757.3  2860   .5362      80.88 30.22  304             32768  7.841    .6435  29.3 2.484 6.93e-06 234.4 885.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.99      .4773         0                33056 991.7 3746 1.89\n",
            "\n",
            "14:51:57 | time:22996s total_exs:591280 total_steps:36955 epochs:2.38 time_left:5993s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.7     1 724.3  2646   .5034      83.13 29.23  296             32768  7.373    .6435 32.86 2.502 6.93e-06 262.9 960.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.2      .4583         0                33074 987.1 3607 1.798\n",
            "\n",
            "14:52:08 | time:23006s total_exs:591592 total_steps:36974 epochs:2.38 time_left:5980s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.4     1 728.3  2751   .5064      72.31 30.21  312             32768  7.495    .6378 29.36 2.456 6.93e-06 234.9   887   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.65      .4762         0                33094 963.2 3638 1.951\n",
            "\n",
            "14:52:18 | time:23016s total_exs:591880 total_steps:36992 epochs:2.38 time_left:5969s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     179     1 766.5  2758   .5660      83.16 28.78  288             32768  7.566    .6624  30.7 2.523 6.93e-06 245.3 882.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   .003472     .04514 12.47      .4642         0                33112 1012 3640  1.8\n",
            "\n",
            "14:52:28 | time:23026s total_exs:592184 total_steps:37011 epochs:2.38 time_left:5957s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.5     1 734.9  2751   .4901       69.6 29.95  304             32768  7.507    .6624 30.24 2.478 6.93e-06 240.8 901.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .1382 11.92      .4693         0                33131 975.7 3653 1.873\n",
            "\n",
            "14:52:38 | time:23036s total_exs:592488 total_steps:37030 epochs:2.38 time_left:5944s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.7     1 768.5  2886   .5658      86.62 30.05  304             32768   7.48    .6493 30.95  2.48 6.93e-06 247.6 930.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.94      .4698         0                33150 1016 3817 1.879\n",
            "\n",
            "14:52:48 | time:23047s total_exs:592808 total_steps:37050 epochs:2.39 time_left:5931s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.7     1 760.4  2948   .5500      77.67 31.01  320             32768  7.516    .6323 29.62 2.459 6.93e-06 236.9 918.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.69      .4776         0                33170 997.4 3866 1.939\n",
            "\n",
            "14:52:59 | time:23057s total_exs:593096 total_steps:37068 epochs:2.39 time_left:5920s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.9     1 712.2  2547   .4514      71.91 28.61  288             32768  7.265    .6554  31.7  2.52 6.93e-06 253.6   907   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.43      .4658         0                33188 965.8 3454 1.789\n",
            "\n",
            "14:53:09 | time:23067s total_exs:593400 total_steps:37087 epochs:2.39 time_left:5908s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.2     1 722.7  2642   .4967      81.85 29.25  304             32768  7.372    .6322 31.11 2.539 6.93e-06 248.9 909.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.67      .4630         0                33207 971.6 3552 1.828\n",
            "\n",
            "14:53:19 | time:23078s total_exs:593704 total_steps:37106 epochs:2.39 time_left:5895s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.9     1 747.6  2773   .5526       88.4 29.68  304             32768  7.497    .6624 30.94 2.551 6.93e-06 245.6 911.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579      .2368 12.81      .4578         0                33226 993.2 3684 1.855\n",
            "\n",
            "14:53:30 | time:23088s total_exs:594008 total_steps:37125 epochs:2.39 time_left:5883s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.9     1 745.3  2732   .5197      78.76 29.33  304             32768  7.544    .6554    30 2.477 6.93e-06   240 879.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.91      .4759         0                33245 985.3 3612 1.834\n",
            "\n",
            "14:53:40 | time:23098s total_exs:594320 total_steps:37145 epochs:2.39 time_left:5871s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     163     1 743.1  2893   .5032      70.12 31.15  312             32768  7.592    .6624 29.77  2.44 6.93e-06 236.9 922.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003205      .1506 11.47      .4727         0                33264 980.1 3816 1.919\n",
            "\n",
            "14:53:50 | time:23108s total_exs:594616 total_steps:37163 epochs:2.39 time_left:5859s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     166     1 736.8  2722   .4966      73.92 29.56  296             32768  7.399    .6378 30.72 2.466 6.93e-06 245.8 908.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.78      .4731         0                33283 982.5 3631 1.911\n",
            "\n",
            "14:54:00 | time:23118s total_exs:594920 total_steps:37182 epochs:2.39 time_left:5847s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.7     1 758.1  2841   .5362       74.9 29.97  304             32768   7.49    .6624 30.03 2.459 6.93e-06 240.1 899.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .02303 11.7      .4813         0                33302 998.2 3740 1.874\n",
            "\n",
            "14:54:10 | time:23128s total_exs:595224 total_steps:37201 epochs:2.40 time_left:5834s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   152.9     1 710.9  2696   .4803         64 30.34  304             32768  7.746    .6624 29.38 2.505 6.93e-06 233.4 885.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .2138 12.24      .4711         0                33321 944.3 3581 1.896\n",
            "\n",
            "14:54:20 | time:23138s total_exs:595512 total_steps:37219 epochs:2.40 time_left:5823s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.1     1 754.5  2691   .5625      81.81 28.54  288             32768  7.312    .6624 33.11 2.526 6.93e-06 264.9 944.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472    .006944 12.51      .4650         0                33339 1019 3636 1.785\n",
            "\n",
            "14:54:30 | time:23149s total_exs:595816 total_steps:37238 epochs:2.40 time_left:5811s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.6     1 728.9  2718   .5132      74.46 29.83  304             32768  7.428    .6625 32.22 2.503 6.93e-06 257.8 961.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.22      .4725         0                33358 986.7 3679 1.865\n",
            "\n",
            "14:54:40 | time:23159s total_exs:596120 total_steps:37257 epochs:2.40 time_left:5798s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   187.1     1 770.7  2860   .5658      90.79 29.69  304             32768  7.365    .6378 31.34 2.463 6.93e-06 250.7 930.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.74      .4772         0                33377 1021 3791 1.856\n",
            "\n",
            "14:54:50 | time:23169s total_exs:596416 total_steps:37276 epochs:2.40 time_left:5787s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   157.9     1 724.2  2668   .4831      67.36 29.48  296             32768  7.597    .6625  30.4 2.499 6.93e-06 243.2   896   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.17      .4711         0                33395 967.4 3565 1.819\n",
            "\n",
            "14:55:01 | time:23179s total_exs:596712 total_steps:37294 epochs:2.40 time_left:5775s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.6     1 760.2  2749   .5405      89.55 28.93  296             32768  7.361    .6625 29.87 2.458 6.93e-06 238.6   863   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .04054 11.68      .4774         0                33414 998.8 3612 1.874\n",
            "\n",
            "14:55:11 | time:23189s total_exs:597016 total_steps:37313 epochs:2.40 time_left:5762s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.1     1 766.5  2894   .5526      81.25  30.2  304             32768  7.543    .6625  29.2 2.427 6.93e-06 233.4 881.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .01974 11.32      .4891         0                33433 999.9 3775 1.889\n",
            "\n",
            "14:55:21 | time:23199s total_exs:597320 total_steps:37332 epochs:2.40 time_left:5750s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     170     1 733.7  2707   .5000      78.32 29.51  304             32768  7.432    .6378 31.99 2.452 6.93e-06 255.9 944.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.61      .4758         0                33452 989.7 3651 1.845\n",
            "\n",
            "14:55:31 | time:23210s total_exs:597624 total_steps:37351 epochs:2.41 time_left:5738s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.4     1   744  2755   .5263      77.36 29.62  304             32768  7.609    .6323 30.17 2.464 6.93e-06 241.3 893.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.75      .4693         0                33471 985.3 3648 1.852\n",
            "\n",
            "14:55:42 | time:23220s total_exs:597944 total_steps:37371 epochs:2.41 time_left:5725s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.8     1 740.4  2866   .4938       71.2 30.97  320             32768  7.846    .6493 28.69 2.432 6.93e-06 229.6 888.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.38      .4796         0                33491  970 3755 1.936\n",
            "\n",
            "14:55:52 | time:23230s total_exs:598248 total_steps:37390 epochs:2.41 time_left:5713s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     166     1 747.4  2752   .5230      72.57 29.46  304             32768  7.675    .6625    29 2.431 6.93e-06 231.3 851.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579     .09539 11.37      .4843   .003289                33510 978.7 3604 1.842\n",
            "\n",
            "14:56:02 | time:23241s total_exs:598552 total_steps:37409 epochs:2.41 time_left:5701s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.5     1   744  2800   .5066      78.54 30.11  304             32768  7.396    .6493 30.08 2.393 6.93e-06 240.7 905.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.94      .4934         0                33529 984.6 3705 1.882\n",
            "\n",
            "14:56:12 | time:23251s total_exs:598856 total_steps:37428 epochs:2.41 time_left:5689s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.6     1 728.3  2713   .4836       81.6  29.8  304             32768   7.42    .6323 31.06 2.497 6.93e-06 248.4 925.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.14      .4716         0                33548 976.8 3638 1.864\n",
            "\n",
            "14:56:23 | time:23261s total_exs:599152 total_steps:37447 epochs:2.41 time_left:5677s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.5     1 733.5  2686   .5068      82.77 29.29  296             32768  7.262    .6625 32.91 2.496 6.93e-06 261.9   959   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .006757      .1723 12.13      .4747         0                33566 995.4 3645  1.8\n",
            "\n",
            "14:56:33 | time:23271s total_exs:599464 total_steps:37466 epochs:2.41 time_left:5664s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.1     1 767.8  2961   .5577      77.16 30.85  312             32768  7.734    .6323 29.32 2.497 6.93e-06 234.6 904.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.14      .4697         0                33586 1002 3866 1.991\n",
            "\n",
            "14:56:43 | time:23282s total_exs:599768 total_steps:37485 epochs:2.41 time_left:5652s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   187.7     1 775.5  2812   .5526      90.75 29.01  304             32768  7.536    .6625 32.12  2.48 6.93e-06 256.9 931.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.94      .4736         0                33605 1032 3744 1.814\n",
            "\n",
            "14:56:54 | time:23292s total_exs:600072 total_steps:37504 epochs:2.42 time_left:5640s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   188.9     1 744.9  2725   .5461      95.82 29.26  304             32768  7.216    .6493 32.19 2.473 6.93e-06 257.5   942   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.86      .4740         0                33624 1002 3667 1.829\n",
            "\n",
            "14:57:04 | time:23302s total_exs:600360 total_steps:37522 epochs:2.42 time_left:5628s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     166     1 731.9  2634   .5000      74.56 28.79  288             32768   7.22    .6436 31.99 2.404 6.93e-06 255.9 921.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.07      .4817         0                33642 987.8 3555 1.801\n",
            "\n",
            "14:57:14 | time:23312s total_exs:600656 total_steps:37541 epochs:2.42 time_left:5617s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   190.4     1 758.4  2791   .5642      95.55 29.44  296             32768  7.343    .6624 32.92 2.506 6.93e-06 260.2 957.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378      .3919 12.26      .4683         0                33660 1019 3748 1.811\n",
            "\n",
            "14:57:24 | time:23322s total_exs:600968 total_steps:37560 epochs:2.42 time_left:5604s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.4     1 747.1  2832   .5128      81.01 30.32  312             32768  7.589    .6625 29.37 2.465 6.93e-06 232.8 882.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003205      .2628 11.77      .4701         0                33680 979.9 3714 1.958\n",
            "\n",
            "14:57:34 | time:23332s total_exs:601272 total_steps:37579 epochs:2.42 time_left:5592s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.9     1 744.7  2788   .5263      73.82 29.95  304             32768  7.686    .6493  29.1 2.532 6.93e-06 232.8 871.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.58      .4691         0                33699 977.5 3660 1.873\n",
            "\n",
            "14:57:44 | time:23343s total_exs:601576 total_steps:37598 epochs:2.42 time_left:5580s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.5     1 772.7  2890   .5428      82.93 29.92  304             32768  7.446    .6625 30.62 2.478 6.93e-06 244.3 913.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003289     .07895 11.92      .4714         0                33718 1017 3804 1.871\n",
            "\n",
            "14:57:54 | time:23353s total_exs:601872 total_steps:37617 epochs:2.42 time_left:5568s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.9     1 743.5  2725   .5101      80.93 29.32  296             32768   7.44    .6625 31.18 2.513 6.93e-06   249 912.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .05068 12.34      .4672         0                33736 992.5 3637 1.802\n",
            "\n",
            "14:58:05 | time:23363s total_exs:602168 total_steps:37635 epochs:2.42 time_left:5556s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.5     1 749.5  2736   .5304      77.77 29.21  296             32768  7.495    .6554 30.93 2.565 6.93e-06 247.4 903.3   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0   13      .4641         0                33755  997 3640 1.887\n",
            "\n",
            "14:58:15 | time:23373s total_exs:602472 total_steps:37654 epochs:2.42 time_left:5544s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.3     1 746.9  2766   .5000      73.92 29.63  304             32768  7.222    .6323 30.68 2.465 6.93e-06 245.5 909.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.76      .4773         0                33774 992.4 3675 1.853\n",
            "\n",
            "14:58:25 | time:23384s total_exs:602776 total_steps:37673 epochs:2.43 time_left:5532s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     170     1 736.6  2675   .5230      77.92 29.05  304             32768  7.489    .6625 31.54 2.511 6.93e-06 248.2 901.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579      .5132 12.31      .4646         0                33793 984.9 3576 1.816\n",
            "\n",
            "14:58:36 | time:23394s total_exs:603064 total_steps:37691 epochs:2.43 time_left:5520s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.6     1 732.5  2560   .4931      78.03 27.96  288             32768  7.336    .6625  32.3 2.513 6.93e-06 258.4 902.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.35      .4661         0                33811 990.9 3463 1.748\n",
            "\n",
            "14:58:46 | time:23404s total_exs:603368 total_steps:37710 epochs:2.43 time_left:5508s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.9     1 750.5  2745   .5296      81.04 29.26  304             32768  7.306    .6625 30.55 2.429 6.93e-06 244.4 894.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.34      .4816         0                33830 994.9 3639 1.829\n",
            "\n",
            "14:58:56 | time:23415s total_exs:603672 total_steps:37729 epochs:2.43 time_left:5496s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.8     1 729.9  2749   .5033      76.56 30.14  304             32768  7.486    .6493 30.97 2.552 6.93e-06 247.7 933.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.83      .4645         0                33849 977.6 3683 1.884\n",
            "\n",
            "14:59:06 | time:23425s total_exs:603968 total_steps:37748 epochs:2.43 time_left:5484s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.3     1 781.1  2875   .5743      86.66 29.45  296             32768  7.266    .6554 32.26 2.495 6.93e-06 258.1 950.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.12      .4620         0                33867 1039 3826 1.814\n",
            "\n",
            "14:59:16 | time:23435s total_exs:604264 total_steps:37766 epochs:2.43 time_left:5473s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.9     1 748.3  2703   .5034      79.36  28.9  296             32768  7.284    .6625 32.31 2.458 6.93e-06   258   932   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   .003378     .06081 11.68      .4706         0                33886 1006 3635 1.87\n",
            "\n",
            "14:59:27 | time:23445s total_exs:604584 total_steps:37786 epochs:2.43 time_left:5460s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     184     1   773  3062   .5687      87.34 31.69  320             32768  7.533    .6323 28.91   2.4 6.93e-06 231.2   916   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.03      .4802         0                33906 1004 3978 1.981\n",
            "\n",
            "14:59:37 | time:23455s total_exs:604872 total_steps:37804 epochs:2.43 time_left:5448s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   194.9     1 784.6  2743   .5868      96.84 27.97  288             32768  7.245    .6625  33.6 2.507 6.93e-06 268.6 939.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .03125 12.27      .4726         0                33924 1053 3683 1.749\n",
            "\n",
            "14:59:47 | time:23465s total_exs:605168 total_steps:37823 epochs:2.44 time_left:5436s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     178     1 744.5  2730   .5270       84.9 29.33  296             32768  7.436    .6554 32.64 2.574 6.93e-06 261.1 957.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.11      .4506         0                33942 1006 3687 1.816\n",
            "\n",
            "14:59:57 | time:23475s total_exs:605472 total_steps:37842 epochs:2.44 time_left:5424s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   156.6     1 702.9  2670   .4737      68.77 30.39  304             32768  7.547    .6436 29.15 2.452 6.93e-06 233.2 885.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.62      .4799         0                33961 936.1 3556 1.943\n",
            "\n",
            "15:00:07 | time:23485s total_exs:605768 total_steps:37860 epochs:2.44 time_left:5412s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.8     1 769.5  2830   .5405      78.64 29.42  296             32768  7.706    .6436 29.28 2.483 6.93e-06 234.2 861.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.98      .4684         0                33980 1004 3692 1.903\n",
            "\n",
            "15:00:17 | time:23496s total_exs:606080 total_steps:37880 epochs:2.44 time_left:5400s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.2     1 742.3  2888   .5192      69.38 31.12  312             32768  7.539    .6625 29.82 2.485 6.93e-06   236 918.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003205      .3237   12      .4670         0                33999 978.3 3806 1.918\n",
            "\n",
            "15:00:27 | time:23506s total_exs:606368 total_steps:37898 epochs:2.44 time_left:5389s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.8     1 736.6  2640   .5035       67.7 28.67  288             32768  7.208    .6625 32.55 2.438 6.93e-06 256.1 917.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006944      .5312 11.45      .4760   .003472                34017 992.7 3558 1.824\n",
            "\n",
            "15:00:37 | time:23516s total_exs:606664 total_steps:37916 epochs:2.44 time_left:5377s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.3     1 739.8  2723   .5068      80.82 29.45  296             32768  7.706    .6554 29.41 2.461 6.93e-06 235.3   866   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.71      .4749         0                34036 975.1 3589 1.903\n",
            "\n",
            "15:00:47 | time:23526s total_exs:606952 total_steps:37934 epochs:2.44 time_left:5365s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   156.4     1 700.8  2482   .4444      68.81 28.33  288             32768  7.404    .6625 33.04 2.504 6.93e-06 262.9 931.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472      .1701 12.24      .4633         0                34054 963.8 3413 1.772\n",
            "\n",
            "15:00:57 | time:23536s total_exs:607248 total_steps:37953 epochs:2.44 time_left:5354s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.1     1 734.3  2717   .5068      81.29  29.6  296             32768  7.372    .6625 32.79 2.508 6.93e-06 262.2 970.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .01351 12.28      .4629         0                34072 996.5 3688 1.824\n",
            "\n",
            "15:01:07 | time:23546s total_exs:607552 total_steps:37972 epochs:2.45 time_left:5341s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   187.2     1 778.6  2957   .5724      89.91 30.38  304             32768   7.45    .6436 30.62 2.446 6.93e-06   245 930.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.54      .4712         0                34091 1024 3887 1.951\n",
            "\n",
            "15:01:18 | time:23556s total_exs:607848 total_steps:37990 epochs:2.45 time_left:5330s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   158.8     1 736.9  2680   .5203      66.73 29.09  296             32768  7.738    .6625 29.03 2.534 6.93e-06   232 843.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003378     .02703 12.61      .4624         0                34110 968.9 3524 1.89\n",
            "\n",
            "15:01:28 | time:23566s total_exs:608136 total_steps:38008 epochs:2.45 time_left:5318s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.4     1 733.2  2636   .5174      84.75 28.76  288             32768  7.349    .6625 31.55 2.514 6.93e-06 250.9 901.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472      .1875 12.36      .4673         0                34128 984.1 3538 1.799\n",
            "\n",
            "15:01:38 | time:23576s total_exs:608424 total_steps:38026 epochs:2.45 time_left:5307s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.1     1 751.1  2666   .5174      70.22  28.4  288             32768  7.426    .6493  31.3 2.576 6.93e-06 250.4   889   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.15      .4567         0                34146 1001 3555 1.776\n",
            "\n",
            "15:01:48 | time:23586s total_exs:608720 total_steps:38045 epochs:2.45 time_left:5295s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   156.2     1 722.9  2662   .4831      65.82 29.46  296             32768  7.511    .6379 29.73 2.501 6.93e-06 237.9 875.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 12.2      .4624         0                34164 960.8 3538 1.81\n",
            "\n",
            "15:01:58 | time:23596s total_exs:609016 total_steps:38063 epochs:2.45 time_left:5283s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.5     1 771.8  2842   .5676         84 29.45  296             32768  7.285    .6379 30.75 2.446 6.93e-06   246 905.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.55      .4830         0                34183 1018 3748 1.903\n",
            "\n",
            "15:02:08 | time:23606s total_exs:609320 total_steps:38082 epochs:2.45 time_left:5271s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.5     1 733.2  2783   .5099       71.8 30.36  304             32768  7.642    .6625 29.85 2.494 6.93e-06 237.4 901.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .1711 12.11      .4663         0                34202 970.7 3684 1.898\n",
            "\n",
            "15:02:18 | time:23617s total_exs:609608 total_steps:38100 epochs:2.45 time_left:5260s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.4     1 712.8  2539   .4722      71.31  28.5  288             32768  7.323    .6625 31.69 2.489 6.93e-06 252.7 900.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472      .1042 12.04      .4688         0                34220 965.5 3439 1.782\n",
            "\n",
            "15:02:28 | time:23627s total_exs:609904 total_steps:38119 epochs:2.45 time_left:5248s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.1     1 759.6  2771   .5372      72.14 29.18  296             32768  7.366    .6625 31.96 2.515 6.93e-06 255.1 930.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .07432 12.37      .4667         0                34238 1015 3701 1.802\n",
            "\n",
            "15:02:39 | time:23637s total_exs:610200 total_steps:38137 epochs:2.46 time_left:5236s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.2     1 746.6  2698   .5101      74.89 28.91  296             32768  7.374    .6555 30.96 2.435 6.93e-06 247.7 895.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.41      .4778         0                34257 994.3 3593 1.874\n",
            "\n",
            "15:02:49 | time:23647s total_exs:610504 total_steps:38156 epochs:2.46 time_left:5224s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     168     1 743.3  2725   .5230      75.06 29.32  304             32768  7.579    .6536 30.93 2.476 6.93e-06 246.8 904.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .08224 11.9      .4756         0                34276 990.1 3629 1.833\n",
            "\n",
            "15:02:59 | time:23657s total_exs:610800 total_steps:38175 epochs:2.46 time_left:5212s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.1     1 750.6  2750   .5101      85.32 29.31  296             32768  7.082    .6436 32.31 2.451 6.93e-06 258.5   947   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.6      .4786         0                34294 1009 3697 1.805\n",
            "\n",
            "15:03:09 | time:23668s total_exs:611112 total_steps:38194 epochs:2.46 time_left:5200s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   188.5     1 766.5  2893   .5449      92.73 30.19  312             32768   7.44    .6323  30.6 2.448 6.93e-06 244.8   924   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.56      .4884         0                34314 1011 3817 1.951\n",
            "\n",
            "15:03:20 | time:23678s total_exs:611400 total_steps:38212 epochs:2.46 time_left:5188s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.6     1 748.9  2624   .5312      87.03 28.03  288             32768  7.236    .6625 33.48 2.506 6.93e-06 266.1 932.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472      .2257 12.26      .4673         0                34332 1015 3556 1.752\n",
            "\n",
            "15:03:30 | time:23689s total_exs:611704 total_steps:38231 epochs:2.46 time_left:5176s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.5     1   731  2652   .4967      68.08 29.02  304             32768  7.498    .6625 30.29 2.472 6.93e-06 241.6 876.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "    .01316     .09211 11.85      .4678         0                34351 972.6 3529 1.815\n",
            "\n",
            "15:03:40 | time:23699s total_exs:611992 total_steps:38249 epochs:2.46 time_left:5165s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     170     1   751  2670   .5035      76.17 28.44  288             32768  7.638    .6625 30.42 2.492 6.93e-06 242.3 861.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472      .1250 12.08      .4688         0                34369 993.4 3531 1.778\n",
            "\n",
            "15:03:50 | time:23709s total_exs:612288 total_steps:38268 epochs:2.46 time_left:5153s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.9     1 733.4  2715   .4899      75.24 29.61  296             32768  7.363    .6625 30.59 2.424 6.93e-06 244.6 905.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .02027 11.29      .4765         0                34387 977.9 3620 1.822\n",
            "\n",
            "15:04:00 | time:23719s total_exs:612592 total_steps:38287 epochs:2.47 time_left:5141s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.3     1 738.3  2805   .5099      74.02 30.39  304             32768  7.927    .6625 28.09 2.484 6.93e-06 222.1 843.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579      .3289 11.98      .4781         0                34406 960.4 3648 1.936\n",
            "\n",
            "15:04:11 | time:23729s total_exs:612904 total_steps:38306 epochs:2.47 time_left:5129s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.2     1 740.6  2779   .5256      71.64 30.02  312             32768   7.46    .6625  29.7 2.474 6.93e-06 237.4 890.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003205     .03205 11.87      .4761         0                34426  978 3670 1.938\n",
            "\n",
            "15:04:21 | time:23740s total_exs:613208 total_steps:38325 epochs:2.47 time_left:5117s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.6     1 749.8  2740   .5329      76.88 29.23  304             32768  7.331    .6379 31.16 2.503 6.93e-06 249.3 910.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.22      .4670         0                34445 999.1 3651 1.828\n",
            "\n",
            "15:04:32 | time:23750s total_exs:613512 total_steps:38344 epochs:2.47 time_left:5105s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.1     1 769.8  2820   .5559      81.83 29.31  304             32768  7.548    .6625 29.81 2.442 6.93e-06   238 871.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003289     .05921 11.5      .4791         0                34464 1008 3692 1.833\n",
            "\n",
            "15:04:42 | time:23760s total_exs:613800 total_steps:38362 epochs:2.47 time_left:5093s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   192.9     1 752.4  2660   .5451      98.81 28.28  288             32768  7.275    .6625 32.52 2.454 6.93e-06 259.9 918.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .02778 11.64      .4746         0                34482 1012 3578 1.768\n",
            "\n",
            "15:04:52 | time:23770s total_exs:614112 total_steps:38382 epochs:2.47 time_left:5081s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.3     1 777.3  2990   .5673      85.15 30.78  312             32768  7.328    .6379 30.32 2.438 6.93e-06 242.5 933.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 11.45      .4783   .003205                34501 1020 3924  1.9\n",
            "\n",
            "15:05:02 | time:23781s total_exs:614424 total_steps:38401 epochs:2.47 time_left:5068s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.7     1 751.5  2825   .5353      82.72 30.07  312             32768  7.662    .6379 29.31 2.483 6.93e-06 234.5 881.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.97      .4703         0                34521 985.9 3706 1.944\n",
            "\n",
            "15:05:12 | time:23791s total_exs:614704 total_steps:38419 epochs:2.47 time_left:5057s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     174     1 728.1  2537   .5000      82.99 27.88  280             32768  7.105    .6625 33.65 2.478 6.93e-06 268.9 937.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003571     .03571 11.92      .4698         0                34538  997 3474 1.714\n",
            "\n",
            "15:05:23 | time:23801s total_exs:615008 total_steps:38438 epochs:2.48 time_left:5045s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.6     1   739  2784   .5132      75.27 30.14  304             32768  7.557    .6436 28.72 2.378 6.93e-06 229.8 865.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.78      .4895         0                34557 968.7 3650 1.921\n",
            "\n",
            "15:05:33 | time:23811s total_exs:615304 total_steps:38456 epochs:2.48 time_left:5033s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   187.2     1 761.2  2750   .5507      92.06  28.9  296             32768  7.343    .6554 33.15 2.473 6.93e-06 265.2 958.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.86      .4728   .003378                34576 1026 3708 1.869\n",
            "\n",
            "15:05:43 | time:23821s total_exs:615608 total_steps:38475 epochs:2.48 time_left:5021s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.1     1 734.2  2768   .5066      77.34 30.16  304             32768  7.467    .6625 30.33 2.404 6.93e-06 241.5 910.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .1447 11.07      .4825         0                34595 975.7 3679 1.886\n",
            "\n",
            "15:05:53 | time:23831s total_exs:615920 total_steps:38495 epochs:2.48 time_left:5009s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   185.4     1 770.3  2968   .5609      89.09 30.83  312             32768  7.624    .6267 29.88 2.537 6.93e-06 239.1 921.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.64      .4684         0                34614 1009 3889 1.901\n",
            "\n",
            "15:06:03 | time:23842s total_exs:616216 total_steps:38513 epochs:2.48 time_left:4997s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.8     1 743.2  2702   .5034      80.94 29.08  296             32768  7.421    .6493 32.44 2.523 6.93e-06 259.5 943.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 12.46      .4668         0                34633 1003 3645 1.88\n",
            "\n",
            "15:06:13 | time:23852s total_exs:616504 total_steps:38531 epochs:2.48 time_left:4986s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.6     1   745  2669   .5174      78.46 28.66  288             32768  7.694    .6625 29.92 2.517 6.93e-06 239.3 857.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.39      .4620         0                34651 984.3 3526 1.792\n",
            "\n",
            "15:06:24 | time:23862s total_exs:616808 total_steps:38550 epochs:2.48 time_left:4974s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.1     1   742  2730   .5066       77.3 29.43  304             32768  7.312    .6379 31.67 2.415 6.93e-06 253.4 932.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.19      .4855         0                34670 995.4 3662 1.84\n",
            "\n",
            "15:06:34 | time:23872s total_exs:617104 total_steps:38569 epochs:2.48 time_left:4962s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.9     1 741.6  2735   .5169      78.24 29.51  296             32768  7.469    .6555 30.32 2.461 6.93e-06 242.6 894.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.72      .4782         0                34688 984.2 3630 1.825\n",
            "\n",
            "15:06:44 | time:23882s total_exs:617400 total_steps:38587 epochs:2.48 time_left:4950s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.9     1 766.1  2829   .5608      86.17 29.54  296             32768  7.447    .6625 30.53 2.468 6.93e-06 243.8 900.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .05068 11.8      .4806         0                34707 1010 3729 1.917\n",
            "\n",
            "15:06:54 | time:23892s total_exs:617696 total_steps:38606 epochs:2.49 time_left:4939s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.8     1 738.4  2730   .5169      79.53 29.58  296             32768  7.429    .6493 31.35 2.519 6.93e-06 250.8 927.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.41      .4697         0                34725 989.2 3657 1.835\n",
            "\n",
            "15:07:04 | time:23902s total_exs:618008 total_steps:38625 epochs:2.49 time_left:4926s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.1     1 736.5  2816   .5032      72.02 30.58  312             32768  7.522    .6555 28.95 2.445 6.93e-06 231.6 885.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.53      .4820         0                34745 968.2 3701 1.985\n",
            "\n",
            "15:07:14 | time:23913s total_exs:618312 total_steps:38644 epochs:2.49 time_left:4914s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.7     1   722  2683   .4934      69.41 29.73  304             32768  7.629    .6625 30.51 2.455 6.93e-06 244.1 907.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.64      .4746         0                34764 966.1 3590 1.859\n",
            "\n",
            "15:07:24 | time:23923s total_exs:618584 total_steps:38661 epochs:2.49 time_left:4903s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.5     1 725.1  2423   .4853      69.87 26.73  272             32768  7.306    .6625 33.78   2.5 6.93e-06 266.2 889.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .007353      .5000 12.19      .4658   .003676                34781 991.4 3312 1.671\n",
            "\n",
            "15:07:35 | time:23933s total_exs:618904 total_steps:38681 epochs:2.49 time_left:4891s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.1     1 732.9  2823   .4938      75.45 30.82  320             32768  7.616    .6323 28.62 2.465 6.93e-06 228.9 881.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.76      .4804         0                34801 961.9 3705 1.927\n",
            "\n",
            "15:07:45 | time:23943s total_exs:619200 total_steps:38700 epochs:2.49 time_left:4879s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.1     1 752.7  2766   .5270      86.97  29.4  296             32768  7.247    .6436  31.7 2.429 6.93e-06 253.6   932   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.35      .4779         0                34819 1006 3699 1.812\n",
            "\n",
            "15:07:55 | time:23953s total_exs:619496 total_steps:38718 epochs:2.49 time_left:4867s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.7     1 730.9  2667   .5034      78.29 29.19  296             32768  7.219    .6436 32.09 2.427 6.93e-06 256.8 936.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.32      .4846         0                34838 987.7 3603 1.889\n",
            "\n",
            "15:08:05 | time:23964s total_exs:619800 total_steps:38737 epochs:2.49 time_left:4855s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.1     1 757.8  2798   .5362      85.34 29.54  304             32768  7.317    .6625 30.86 2.413 6.93e-06 246.8 911.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.17      .4877         0                34857 1005 3710 1.847\n",
            "\n",
            "15:08:15 | time:23974s total_exs:620112 total_steps:38757 epochs:2.50 time_left:4843s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   154.7     1 715.3  2762   .4808      65.25 30.89  312             32768  7.515    .6379 28.55 2.432 6.93e-06 228.4 881.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.38      .4878         0                34876 943.7 3643 1.909\n",
            "\n",
            "15:08:26 | time:23984s total_exs:620416 total_steps:38776 epochs:2.50 time_left:4831s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.4     1 766.9  2883   .5461      76.52 30.08  304             32768  7.404    .6625 30.28 2.401 6.93e-06 241.5 907.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003289     .08882 11.04      .4826         0                34895 1008 3791 1.921\n",
            "\n",
            "15:08:36 | time:23994s total_exs:620728 total_steps:38795 epochs:2.50 time_left:4818s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.3     1 740.9  2825   .5353      85.71  30.5  312             32768  7.353    .6267 31.35 2.443 6.93e-06 250.8   956   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.51      .4826         0                34915 991.7 3781 1.97\n",
            "\n",
            "15:08:46 | time:24004s total_exs:621024 total_steps:38814 epochs:2.50 time_left:4806s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.4     1 719.1  2653   .4865       76.5 29.51  296             32768  7.164    .6379 32.81 2.462 6.93e-06 262.5 968.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.73      .4714         0                34933 981.5 3621 1.822\n",
            "\n",
            "15:08:52 | time:24010s total_exs:621200 total_steps:38825 epochs:2.50 time_left:4800s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   187.1     1 772.9  2772   .5682      90.47 28.69  176             32768   7.26    .6323 32.88 2.489 6.93e-06   263 943.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.05      .4744         0                34944 1036 3715 1.854\n",
            "\n",
            "15:08:52 | running eval: valid\n",
            "15:13:14 | eval completed in 261.94s\n",
            "15:13:14 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   175.4 734.4  8319   .4922      83.52 90.55 23718    .5654 36.19 2.401 6.93e-06 288.9  3273 .001686     .05232 11.03   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4850         0                34944 1023 11592\n",
            "\u001b[0m\n",
            "15:13:14 | \u001b[1;32mnew best ppl: 11.03 (previous best was 11.39)\u001b[0m\n",
            "15:13:14 | saving best valid model: /content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model\n",
            "15:13:39 | time:24297s total_exs:621512 total_steps:38844 epochs:2.50 time_left:4842s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.6     1 734.7  2781   .5032      72.78 30.29  312             32768  7.647    .6626 29.06  2.44 6.93e-06 232.5 880.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.47      .4837         0                34964 967.2 3662 1.942\n",
            "\n",
            "15:13:49 | time:24308s total_exs:621824 total_steps:38864 epochs:2.50 time_left:4830s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.7     1 735.6  2856   .5064      75.73 31.06  312             41391  7.637    .6626 28.46  2.44 6.93e-06 226.8 880.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003205      .1026 11.47      .4813   .003205                34983 962.4 3737 1.92\n",
            "\n",
            "15:13:59 | time:24318s total_exs:622120 total_steps:38882 epochs:2.50 time_left:4818s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     180     1 756.6  2759   .5372      85.41 29.17  296             65536  7.226    .6626  32.4 2.456 6.93e-06 257.2 937.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   .006757      .2466 11.66      .4794         0                35002 1014 3697 1.89\n",
            "\n",
            "15:14:10 | time:24328s total_exs:622424 total_steps:38901 epochs:2.51 time_left:4806s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.8     1 746.4  2754   .5164      75.47 29.52  304             65536  7.284    .6626 30.57 2.398 6.93e-06 244.6 902.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0   11      .4875         0                35021 990.9 3656 1.846\n",
            "\n",
            "15:14:20 | time:24338s total_exs:622720 total_steps:38920 epochs:2.51 time_left:4794s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   191.8     1 767.8  2825   .5608      95.81 29.44  296             65536  7.417    .6437  31.7 2.529 6.93e-06 253.6 933.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.54      .4698         0                35039 1021 3759 1.814\n",
            "\n",
            "15:14:30 | time:24348s total_exs:623024 total_steps:38939 epochs:2.51 time_left:4782s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.3     1   748  2828   .5197      70.78 30.24  304             65536  7.735    .6626  30.7 2.454 6.93e-06 243.9 922.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003289      .2039 11.63      .4694         0                35058 991.9 3750 1.93\n",
            "\n",
            "15:14:40 | time:24358s total_exs:623328 total_steps:38958 epochs:2.51 time_left:4769s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     189     1 779.1  2954   .5789      91.66 30.34  304             65536  7.477    .6437 30.35 2.459 6.93e-06 242.8 920.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.69      .4717         0                35077 1022 3875 1.937\n",
            "\n",
            "15:14:50 | time:24368s total_exs:623608 total_steps:38975 epochs:2.51 time_left:4758s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     183     1 762.2  2658   .5393      87.72 27.89  280             65536  7.372    .6626 32.61 2.572 6.93e-06 260.9 909.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 13.09      .4553         0                35095 1023 3567 1.808\n",
            "\n",
            "15:15:00 | time:24379s total_exs:623912 total_steps:38994 epochs:2.51 time_left:4746s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.8     1 732.8  2677   .5000      81.16 29.23  304             65536   7.33    .6626 32.16 2.457 6.93e-06 255.6 933.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579      .2039 11.67      .4735         0                35114 988.4 3611 1.828\n",
            "\n",
            "15:15:10 | time:24389s total_exs:624208 total_steps:39013 epochs:2.51 time_left:4734s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.4     1 726.6  2665   .5034      80.56 29.34  296             65536  7.459    .6380 31.06 2.453 6.93e-06 248.5 911.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.63      .4791         0                35132 975.1 3576 1.803\n",
            "\n",
            "15:15:20 | time:24399s total_exs:624504 total_steps:39031 epochs:2.51 time_left:4722s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.2     1 735.3  2701   .4966      77.33 29.38  296             65536  7.266    .6626 31.96 2.499 6.93e-06 255.2 937.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .05743 12.18      .4630         0                35151 990.6 3638 1.899\n",
            "\n",
            "15:15:31 | time:24409s total_exs:624808 total_steps:39050 epochs:2.51 time_left:4710s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.7     1 748.7  2838   .5362      80.09 30.32  304             65536   7.38    .6380 30.99 2.508 6.93e-06 247.9 939.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.28      .4708         0                35170 996.6 3777 1.895\n",
            "\n",
            "15:15:41 | time:24419s total_exs:625120 total_steps:39070 epochs:2.52 time_left:4697s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.3     1 741.6  2837   .5000      77.61  30.6  312             65536  7.526    .6626 30.35 2.486 6.93e-06 242.7 928.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003205    .003205 12.01      .4731         0                35189 984.4 3765 1.904\n",
            "\n",
            "15:15:51 | time:24429s total_exs:625416 total_steps:39088 epochs:2.52 time_left:4686s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.6     1   736  2671   .5169      80.59 29.04  296             65536  7.289    .6324 30.54 2.477 6.93e-06 244.3 886.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.9      .4775         0                35208 980.3 3558 1.89\n",
            "\n",
            "15:16:01 | time:24439s total_exs:625712 total_steps:39107 epochs:2.52 time_left:4674s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.1     1 751.6  2753   .5236      84.14  29.3  296             65536  7.408    .6494 30.04 2.386 6.93e-06 240.3 880.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.87      .4886         0                35226 991.9 3633 1.801\n",
            "\n",
            "15:16:11 | time:24450s total_exs:625992 total_steps:39124 epochs:2.52 time_left:4663s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     164     1 729.1  2522   .4964       72.9 27.67  280             65536  7.488    .6626 32.68  2.54 6.93e-06 260.5   901   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003571      .1179 12.67      .4661         0                35244 989.5 3423 1.79\n",
            "\n",
            "15:16:21 | time:24460s total_exs:626304 total_steps:39144 epochs:2.52 time_left:4650s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.8     1 776.4  3000   .5545      82.72 30.91  312             65536  7.487    .6437 29.46 2.418 6.93e-06 235.7 910.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.23      .4841         0                35263 1012 3910 1.902\n",
            "\n",
            "15:16:32 | time:24470s total_exs:626616 total_steps:39163 epochs:2.52 time_left:4638s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.2     1 735.5  2808   .5224      77.21 30.54  312             65536  7.639    .6626 28.47 2.414 6.93e-06 227.7 869.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003205     .00641 11.18      .4829         0                35283 963.3 3678 1.97\n",
            "\n",
            "15:16:42 | time:24480s total_exs:626928 total_steps:39183 epochs:2.52 time_left:4625s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   146.5     1 681.4  2638   .4423      61.29 30.97  312             65536  7.734    .6325 28.63 2.478 6.93e-06 229.1 886.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.92      .4795         0                35302 910.5 3524 1.907\n",
            "\n",
            "15:16:52 | time:24490s total_exs:627224 total_steps:39201 epochs:2.52 time_left:4613s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.7     1 733.5  2619   .5034      81.99 28.57  296             65536  7.611    .6626 30.21  2.46 6.93e-06 241.5 862.3   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .02703 11.7      .4806         0                35321  975 3482 1.847\n",
            "\n",
            "15:17:02 | time:24501s total_exs:627512 total_steps:39219 epochs:2.53 time_left:4602s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.8     1 774.6  2714   .5625      86.01 28.03  288             65536  7.556    .6626 31.78 2.527 6.93e-06 250.5 877.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006944      .4722 12.52      .4621         0                35339 1025 3591 1.752\n",
            "\n",
            "15:17:13 | time:24511s total_exs:627816 total_steps:39238 epochs:2.53 time_left:4590s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.4     1 742.9  2778   .5132      79.54 29.92  304             65536  7.817    .6325 28.92 2.498 6.93e-06 231.3 865.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.16      .4699         0                35358 974.2 3643 1.871\n",
            "\n",
            "15:17:23 | time:24521s total_exs:628120 total_steps:39257 epochs:2.53 time_left:4577s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   189.5     1 763.5  2811   .5559      94.11 29.45  304             65536  7.307    .6495 31.84  2.53 6.93e-06 254.7 937.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.55      .4623         0                35377 1018 3748 1.841\n",
            "\n",
            "15:17:33 | time:24531s total_exs:628424 total_steps:39276 epochs:2.53 time_left:4565s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.2     1 737.2  2786   .4967      68.06 30.24  304             65536  7.506    .6494 29.86 2.458 6.93e-06 238.9 902.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.68      .4746         0                35396 976.1 3689 1.89\n",
            "\n",
            "15:17:43 | time:24541s total_exs:628712 total_steps:39294 epochs:2.53 time_left:4554s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.2     1 741.7  2659   .5069      76.53 28.68  288             65536  7.245    .6627  33.1  2.48 6.93e-06 264.2 947.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .07986 11.94      .4685         0                35414 1006 3606 1.793\n",
            "\n",
            "15:17:53 | time:24552s total_exs:629016 total_steps:39313 epochs:2.53 time_left:4542s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.5     1 774.3  2900   .5526      76.71 29.97  304             65536  7.468    .6325 30.63 2.486 6.93e-06 245.1   918   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.01      .4707         0                35433 1019 3818 1.874\n",
            "\n",
            "15:18:04 | time:24562s total_exs:629336 total_steps:39333 epochs:2.53 time_left:4529s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.2     1 754.5  2895   .5281      80.93  30.7  320             65536  7.703    .6626 29.46 2.456 6.93e-06 235.3   903   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003125     .04375 11.65      .4765         0                35453 989.9 3798 1.919\n",
            "\n",
            "15:18:14 | time:24572s total_exs:629624 total_steps:39351 epochs:2.53 time_left:4517s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   195.1     1 764.4  2750   .5660      99.54 28.78  288             65536  7.148    .6626 33.73 2.507 6.93e-06 268.7 966.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006944      .1458 12.27      .4659         0                35471 1033 3716 1.799\n",
            "\n",
            "15:18:24 | time:24582s total_exs:629928 total_steps:39370 epochs:2.54 time_left:4505s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.4     1   741  2744   .5033      69.79 29.62  304             65536  7.451    .6626 30.91 2.491 6.93e-06 245.8 910.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579      .1875 12.07      .4710         0                35490 986.8 3654 1.852\n",
            "\n",
            "15:18:34 | time:24592s total_exs:630232 total_steps:39389 epochs:2.54 time_left:4493s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.8     1 751.3  2833   .5099      79.91 30.17  304             65536  7.412    .6495 30.44 2.425 6.93e-06 243.5 918.3   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.3      .4794         0                35509 994.8 3752 1.886\n",
            "\n",
            "15:18:44 | time:24603s total_exs:630536 total_steps:39408 epochs:2.54 time_left:4481s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.8     1 765.1  2820   .5362      77.19 29.48  304             65536   7.63    .6380 30.98 2.517 6.93e-06 247.8 913.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.39      .4629         0                35528 1013 3733 1.844\n",
            "\n",
            "15:18:54 | time:24613s total_exs:630832 total_steps:39427 epochs:2.54 time_left:4469s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.7     1   730  2686   .5000      78.49 29.43  296             65536  7.522    .6626 31.31 2.487 6.93e-06 250.5 921.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.03      .4677         0                35546 980.4 3607 1.818\n",
            "\n",
            "15:19:05 | time:24623s total_exs:631144 total_steps:39446 epochs:2.54 time_left:4456s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.7     1 730.5  2780   .5000      77.43 30.44  312             65536  7.331    .6268 30.62 2.395 6.93e-06 244.9 932.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.97      .4929         0                35566 975.4 3712 1.969\n",
            "\n",
            "15:19:15 | time:24633s total_exs:631448 total_steps:39465 epochs:2.54 time_left:4444s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.3     1 732.6  2733   .5000       71.7 29.84  304             65536  7.375    .6556 30.32 2.499 6.93e-06 242.5 904.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.17      .4664         0                35585 975.2 3637 1.866\n",
            "\n",
            "15:19:25 | time:24644s total_exs:631752 total_steps:39484 epochs:2.54 time_left:4432s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.7     1 730.3  2678   .4967      75.42 29.34  304             65536  7.495    .6627 30.91 2.473 6.93e-06 246.9 905.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .03947 11.86      .4787         0                35604 977.2 3584 1.835\n",
            "\n",
            "15:19:35 | time:24654s total_exs:632040 total_steps:39502 epochs:2.54 time_left:4421s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   203.4     1 780.1  2786   .5729      105.9 28.58  288             65536   7.26    .6626 32.98 2.519 6.93e-06 263.5 941.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .04167 12.42      .4658         0                35622 1044 3727 1.786\n",
            "\n",
            "15:19:46 | time:24664s total_exs:632344 total_steps:39521 epochs:2.55 time_left:4409s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.5     1 732.1  2708   .5033      83.01 29.59  304             65536   7.42    .6626 31.57 2.484 6.93e-06   252 932.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   .003289     .07237 11.99      .4733         0                35641  984 3640 1.85\n",
            "\n",
            "15:19:56 | time:24674s total_exs:632632 total_steps:39539 epochs:2.55 time_left:4397s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.2     1 735.9  2626   .5208       80.2 28.54  288             65536  7.127    .6495 32.44 2.446 6.93e-06 259.5 925.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.54      .4751         0                35659 995.4 3551 1.784\n",
            "\n",
            "15:20:06 | time:24684s total_exs:632936 total_steps:39558 epochs:2.55 time_left:4385s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     173     1 766.9  2845   .5428      77.18 29.68  304             65536  7.591    .6627 30.84 2.509 6.93e-06 246.7 915.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.29      .4663         0                35678 1014 3760 1.855\n",
            "\n",
            "15:20:16 | time:24695s total_exs:633240 total_steps:39577 epochs:2.55 time_left:4373s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     179     1 753.6  2825   .5461      84.75 29.99  304             65536  7.153    .6626 31.79 2.509 6.93e-06 254.3 953.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.29      .4669         0                35697 1008 3778 1.874\n",
            "\n",
            "15:20:26 | time:24705s total_exs:633544 total_steps:39596 epochs:2.55 time_left:4361s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.6     1 740.4  2770   .5230      80.01 29.94  304             65536  7.286    .6325 30.21 2.411 6.93e-06 241.7 904.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.14      .4796         0                35716  982 3675 1.872\n",
            "\n",
            "15:20:37 | time:24715s total_exs:633832 total_steps:39614 epochs:2.55 time_left:4349s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.1     1 767.9  2696   .5382      78.14 28.08  288             65536  7.608    .6627 31.14 2.502 6.93e-06 247.4 868.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006944      .2118 12.21      .4688         0                35734 1015 3564 1.756\n",
            "\n",
            "15:20:47 | time:24725s total_exs:634152 total_steps:39634 epochs:2.55 time_left:4336s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.4     1 775.4  3045   .5656      82.47 31.41  320             65536  7.657    .6269 28.56 2.385 6.93e-06 228.4 897.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.86      .4930         0                35754 1004 3942 1.964\n",
            "\n",
            "15:20:57 | time:24735s total_exs:634456 total_steps:39653 epochs:2.55 time_left:4324s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.1     1 763.3  2892   .5395       78.7 30.31  304             65536  7.402    .6626 30.48 2.444 6.93e-06 242.8   920   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003289      .1250 11.52      .4862         0                35773 1006 3812 1.895\n",
            "\n",
            "15:21:07 | time:24745s total_exs:634744 total_steps:39671 epochs:2.55 time_left:4313s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   189.3     1 752.4  2704   .5382      95.22 28.75  288             65536  7.233    .6627  33.4 2.476 6.93e-06 266.9 959.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .04514 11.89      .4737         0                35791 1019 3663 1.798\n",
            "\n",
            "15:21:17 | time:24756s total_exs:635064 total_steps:39691 epochs:2.56 time_left:4300s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.7     1 742.8  2877   .5219      74.88 30.99  320             65536  7.481    .6495 29.06 2.428 6.93e-06 232.5 900.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.34      .4828         0                35811 975.2 3777 1.938\n",
            "\n",
            "15:21:27 | time:24766s total_exs:635368 total_steps:39710 epochs:2.56 time_left:4288s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.2     1 715.1  2680   .4934      72.79 29.99  304             65536  7.391    .6627 30.61 2.434 6.93e-06 244.8 917.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.4      .4772         0                35830 959.9 3598 1.875\n",
            "\n",
            "15:21:37 | time:24776s total_exs:635672 total_steps:39729 epochs:2.56 time_left:4276s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.2     1 749.8  2843   .5099      72.47 30.33  304             65536  7.656    .6437  28.9 2.378 6.93e-06 231.2 876.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.78      .4867         0                35849  981 3720 1.897\n",
            "\n",
            "15:21:48 | time:24786s total_exs:635976 total_steps:39748 epochs:2.56 time_left:4264s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.8     1 746.3  2745   .5132      81.52 29.42  304             65536  7.523    .6380 30.73 2.457 6.93e-06 245.9 904.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.66      .4816         0                35868 992.2 3649 1.84\n",
            "\n",
            "15:21:58 | time:24796s total_exs:636272 total_steps:39767 epochs:2.56 time_left:4252s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.5     1 749.3  2763   .5372      80.84  29.5  296             65536  7.507    .6627 31.23 2.514 6.93e-06   249 918.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378      .1014 12.36      .4671         0                35886 998.3 3682 1.819\n",
            "\n",
            "15:22:08 | time:24806s total_exs:636584 total_steps:39786 epochs:2.56 time_left:4239s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.8     1 780.7  3019   .5513       84.2 30.93  312             65536   7.52    .6627 28.46 2.429 6.93e-06 227.6 879.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   .003205     .01603 11.35      .4847         0                35906 1008 3899    2\n",
            "\n",
            "15:22:18 | time:24816s total_exs:636880 total_steps:39805 epochs:2.56 time_left:4228s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.5     1 752.3  2770   .5203      80.49 29.45  296             65536  7.335    .6556 31.66 2.479 6.93e-06 253.3 932.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.92      .4691         0                35924 1006 3702 1.814\n",
            "\n",
            "15:22:28 | time:24827s total_exs:637176 total_steps:39823 epochs:2.56 time_left:4216s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.6     1 734.5  2657   .5101      71.77 28.94  296             65536  7.275    .6627  31.1 2.391 6.93e-06 247.9 896.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378      .1149 10.92      .4904         0                35943 982.4 3554 1.873\n",
            "\n",
            "15:22:39 | time:24837s total_exs:637480 total_steps:39842 epochs:2.57 time_left:4204s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.2     1 753.1  2784   .5296      79.06 29.57  304             65536  7.553    .6627 30.98 2.449 6.93e-06 247.6 915.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003289     .03947 11.58      .4702         0                35962 1001 3699 1.849\n",
            "\n",
            "15:22:49 | time:24847s total_exs:637784 total_steps:39861 epochs:2.57 time_left:4192s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     162     1 727.5  2681   .5000       71.1 29.48  304             65536  7.605    .6380 29.72 2.491 6.93e-06 237.7 876.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.07      .4709         0                35981 965.3 3558 1.844\n",
            "\n",
            "15:22:59 | time:24858s total_exs:638104 total_steps:39881 epochs:2.57 time_left:4179s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.5     1 753.4  2850   .5375      79.31 30.27  320             65536  7.752    .6627 28.75 2.467 6.93e-06 229.3 867.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003125     .08125 11.79      .4705         0                36001 982.7 3718 1.893\n",
            "\n",
            "15:23:09 | time:24868s total_exs:638392 total_steps:39899 epochs:2.57 time_left:4167s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.4     1 751.6  2705   .5347      82.42  28.8  288             65536  7.393    .6627 31.47 2.467 6.93e-06 250.5 901.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006944      .1632 11.79      .4713         0                36019 1002 3607 1.801\n",
            "\n",
            "15:23:20 | time:24878s total_exs:638696 total_steps:39918 epochs:2.57 time_left:4155s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.2     1 748.2  2774   .5197      72.73 29.66  304             65536  7.593    .6627 29.41 2.448 6.93e-06   235 871.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .03947 11.57      .4748         0                36038 983.2 3645 1.855\n",
            "\n",
            "15:23:30 | time:24888s total_exs:638992 total_steps:39937 epochs:2.57 time_left:4143s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.6     1 758.6  2792   .5608      86.76 29.45  296             65536  7.486    .6627 32.07  2.48 6.93e-06 256.2 943.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .03716 11.94      .4702         0                36056 1015 3735 1.817\n",
            "\n",
            "15:23:40 | time:24898s total_exs:639296 total_steps:39956 epochs:2.57 time_left:4131s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.3     1 733.8  2781   .5033      67.56 30.32  304             65536  7.586    .6437  30.1 2.442 6.93e-06 240.8 912.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.49      .4740         0                36075 974.6 3693 1.943\n",
            "\n",
            "15:23:50 | time:24908s total_exs:639592 total_steps:39974 epochs:2.57 time_left:4120s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.5     1 722.9  2618   .4696      70.19 28.98  296             65536  7.593    .6627 31.05 2.457 6.93e-06 248.4 899.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.66      .4735         0                36094 971.3 3518 1.879\n",
            "\n",
            "15:24:00 | time:24919s total_exs:639896 total_steps:39993 epochs:2.58 time_left:4107s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.6     1 727.9  2714   .5132      78.58 29.82  304             65536  7.383    .6269 31.37 2.484 6.93e-06   251 935.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.99      .4692         0                36113 978.9 3649 1.865\n",
            "\n",
            "15:24:10 | time:24929s total_exs:640184 total_steps:40011 epochs:2.58 time_left:4096s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.5     1 791.3  2835   .5764      85.62 28.66  288             65536  7.254    .6627 32.26 2.465 6.93e-06   258 924.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472    .006944 11.76      .4772         0                36131 1049 3759 1.792\n",
            "\n",
            "15:24:21 | time:24939s total_exs:640472 total_steps:40029 epochs:2.58 time_left:4085s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   185.7     1 750.5  2599   .5278      91.85  27.7  288             65536   7.38    .6556 32.99 2.499 6.93e-06 263.9 913.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.17      .4655         0                36149 1014 3512 1.732\n",
            "\n",
            "15:24:31 | time:24949s total_exs:640760 total_steps:40047 epochs:2.58 time_left:4073s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.6     1 727.2  2595   .4896      73.69 28.54  288             65536  7.282    .6627 32.07 2.482 6.93e-06 256.2 914.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472     .04167 11.96      .4728   .003472                36167 983.5 3509 1.785\n",
            "\n",
            "15:24:41 | time:24959s total_exs:641064 total_steps:40066 epochs:2.58 time_left:4061s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   154.2     1 720.8  2715   .4836      64.11 30.13  304             65536   7.37    .6626 29.64 2.417 6.93e-06 237.1 893.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.21      .4826         0                36186 957.9 3608 1.884\n",
            "\n",
            "15:24:51 | time:24970s total_exs:641352 total_steps:40084 epochs:2.58 time_left:4050s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   185.4     1 759.1  2662   .5417      90.48 28.06  288             65536  7.413    .6627 32.77 2.482 6.93e-06 260.4 913.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "    .01042      .2222 11.97      .4676         0                36204 1019 3576 1.755\n",
            "\n",
            "15:25:01 | time:24980s total_exs:641656 total_steps:40103 epochs:2.58 time_left:4038s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     166     1 738.3  2760   .5230       73.7 29.91  304             65536  7.565    .6627  29.4 2.459 6.93e-06 234.9 878.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .006579     .03289 11.69      .4764         0                36223 973.2 3638 1.87\n",
            "\n",
            "15:25:12 | time:24990s total_exs:641960 total_steps:40122 epochs:2.58 time_left:4026s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.1     1 739.9  2725   .5197      83.61 29.46  304             65536  7.421    .6438 31.04  2.47 6.93e-06 248.3 914.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.83      .4737         0                36242 988.2 3640 1.843\n",
            "\n",
            "15:25:22 | time:25000s total_exs:642256 total_steps:40141 epochs:2.58 time_left:4014s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   154.1     1 702.6  2570   .4595      66.25 29.26  296             65536   7.56    .6627 30.69 2.502 6.93e-06   244 892.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "    .01014      .1892 12.21      .4660         0                36260 946.6 3463 1.804\n",
            "\n",
            "15:25:32 | time:25010s total_exs:642560 total_steps:40160 epochs:2.59 time_left:4002s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.4     1   737  2769   .5099      74.24 30.06  304             65536  7.481    .6438 31.24 2.467 6.93e-06 249.9 939.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.79      .4680         0                36279 986.9 3708 1.93\n",
            "\n",
            "15:25:42 | time:25021s total_exs:642856 total_steps:40178 epochs:2.59 time_left:3990s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   183.6     1 755.9  2727   .5405      89.11 28.86  296             65536  7.243    .6437 31.44 2.418 6.93e-06 251.5 907.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.22      .4892         0                36298 1007 3634 1.874\n",
            "\n",
            "15:25:52 | time:25031s total_exs:643144 total_steps:40196 epochs:2.59 time_left:3979s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.6     1 749.5  2667   .5139      86.95 28.46  288             65536   7.31    .6556 31.48 2.374 6.93e-06 251.8   896   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 10.74      .4955         0                36316 1001 3563 1.78\n",
            "\n",
            "15:26:03 | time:25041s total_exs:643464 total_steps:40216 epochs:2.59 time_left:3966s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.3     1   757  2920   .5250      83.69 30.86  320             65536  7.774    .6269 29.41 2.436 6.93e-06 235.3 907.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.43      .4791         0                36336 992.4 3828 1.93\n",
            "\n",
            "15:26:13 | time:25051s total_exs:643768 total_steps:40235 epochs:2.59 time_left:3954s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.2     1 760.9  2871   .5428      77.06 30.18  304             65536  7.398    .6556 29.74 2.365 6.93e-06 237.9 897.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.65      .4965         0                36355 998.8 3768 1.887\n",
            "\n",
            "15:26:23 | time:25062s total_exs:644072 total_steps:40254 epochs:2.59 time_left:3942s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   183.1     1 740.1  2711   .5263      90.61  29.3  304             65536  7.418    .6380 31.07 2.417 6.93e-06 248.5 910.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.21      .4779         0                36374 988.6 3621 1.832\n",
            "\n",
            "15:26:34 | time:25072s total_exs:644376 total_steps:40273 epochs:2.59 time_left:3930s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.8     1 758.4  2835   .5329      80.04 29.91  304             65536  7.504    .6325 29.41   2.4 6.93e-06 235.3 879.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.02      .4902         0                36393 993.7 3715 1.87\n",
            "\n",
            "15:26:44 | time:25082s total_exs:644680 total_steps:40292 epochs:2.59 time_left:3918s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.6     1 741.9  2757   .5164       83.9 29.73  304             65536   7.37    .6381 32.48  2.48 6.93e-06 259.9 965.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.94      .4742         0                36412 1002 3723 1.859\n",
            "\n",
            "15:26:54 | time:25093s total_exs:645000 total_steps:40312 epochs:2.60 time_left:3905s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.7     1 749.5  2885   .5219      82.02 30.79  320             65536  7.691    .6380  29.1 2.436 6.93e-06 232.8 895.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.43      .4828         0                36432 982.3 3780 1.925\n",
            "\n",
            "15:27:05 | time:25103s total_exs:645304 total_steps:40331 epochs:2.60 time_left:3893s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.6     1 749.5  2728   .5296      76.95 29.11  304             65536  7.396    .6556 31.22 2.407 6.93e-06 249.8   909   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.1      .4832         0                36451 999.3 3637 1.82\n",
            "\n",
            "15:27:15 | time:25113s total_exs:645608 total_steps:40350 epochs:2.60 time_left:3881s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.9     1 765.4  2831   .5296      82.26 29.59  304             65536  7.625    .6557 29.89 2.469 6.93e-06 239.1 884.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 11.81      .4728         0                36470 1005 3716 1.85\n",
            "\n",
            "15:27:25 | time:25123s total_exs:645912 total_steps:40369 epochs:2.60 time_left:3869s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   195.4     1 783.9  2979   .5855      97.38  30.4  304             65536  7.363    .6325 30.73 2.443 6.93e-06 245.9 934.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.51      .4782         0                36489 1030 3914 1.901\n",
            "\n",
            "15:27:35 | time:25133s total_exs:646216 total_steps:40388 epochs:2.60 time_left:3857s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     179     1 759.1  2864   .5559      84.09 30.18  304             65536  7.463    .6438 30.58 2.447 6.93e-06 244.6   923   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.55      .4848         0                36508 1004 3787 1.887\n",
            "\n",
            "15:27:45 | time:25144s total_exs:646528 total_steps:40408 epochs:2.60 time_left:3844s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   197.7     1 794.7  3065   .5897      98.31 30.85  312             65536  7.478    .6380 30.85 2.464 6.93e-06 246.8 951.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.75      .4743         0                36527 1042 4016 1.899\n",
            "\n",
            "15:27:55 | time:25154s total_exs:646824 total_steps:40426 epochs:2.60 time_left:3832s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     173     1 727.9  2683   .5101      82.02 29.49  296             65536  7.555    .6325 32.09 2.446 6.93e-06 256.8 946.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.54      .4778         0                36546 984.6 3630 1.906\n",
            "\n",
            "15:28:05 | time:25164s total_exs:647136 total_steps:40446 epochs:2.60 time_left:3820s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.9     1 724.6  2815   .4744      71.29 31.08  312             65536  7.724    .6325 29.76 2.503 6.93e-06 238.1 925.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.22      .4673         0                36565 962.7 3741 1.925\n",
            "\n",
            "15:28:15 | time:25174s total_exs:647440 total_steps:40465 epochs:2.61 time_left:3808s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   195.7     1 794.2  2983   .6020      96.45 30.04  304             65536   7.48    .6269 29.64 2.411 6.93e-06 237.1 890.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.15      .4852         0                36584 1031 3873 1.921\n",
            "\n",
            "15:28:25 | time:25184s total_exs:647744 total_steps:40484 epochs:2.61 time_left:3796s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     162     1 735.7  2769   .5066      70.01 30.11  304             65536  7.786    .6381 28.44 2.436 6.93e-06 227.6 856.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.42      .4801         0                36603 963.3 3625 1.922\n",
            "\n",
            "15:28:36 | time:25194s total_exs:648056 total_steps:40503 epochs:2.61 time_left:3783s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.7     1 753.4  2882   .5224      72.56  30.6  312             65536  7.913    .6325 27.75 2.438 6.93e-06   222 849.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.45      .4837         0                36623 975.4 3731 1.978\n",
            "\n",
            "15:28:46 | Overflow: setting loss scale to 32768.0\n",
            "15:28:46 | time:25204s total_exs:648376 total_steps:40523 epochs:2.61 time_left:3771s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.9 .9500 757.7  2945   .5406       70.2 31.09  320             62259  7.431    .6438 27.27 2.455 6.93e-06 218.2 847.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.65      .4792         0                36643 975.9 3793 1.944\n",
            "\n",
            "15:28:56 | time:25215s total_exs:648680 total_steps:40542 epochs:2.61 time_left:3759s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.9     1   735  2708   .5066      74.07 29.47  304             32768  7.476    .6557 30.37 2.446 6.93e-06 242.9   895   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.54      .4734         0                36662 977.9 3603 1.843\n",
            "\n",
            "15:29:06 | time:25225s total_exs:648984 total_steps:40561 epochs:2.61 time_left:3747s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.7     1 730.4  2746   .5197      80.38 30.08  304             32768  7.573    .6325 29.83 2.394 6.93e-06 238.7 897.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.95      .4872         0                36681 969.1 3643 1.881\n",
            "\n",
            "15:29:17 | time:25235s total_exs:649288 total_steps:40580 epochs:2.61 time_left:3734s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     175     1 739.2  2779   .5230       82.6 30.07  304             32768  7.634    .6495 30.03 2.464 6.93e-06 240.3 903.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.75      .4774         0                36700 979.5 3682 1.881\n",
            "\n",
            "15:29:27 | time:25245s total_exs:649576 total_steps:40598 epochs:2.61 time_left:3723s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   186.8     1 764.6  2744   .5451      91.25 28.71  288             32768  7.356    .6557 31.61 2.453 6.93e-06 252.9 907.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.62      .4763         0                36718 1017 3651 1.795\n",
            "\n",
            "15:29:37 | time:25255s total_exs:649880 total_steps:40617 epochs:2.62 time_left:3711s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.9     1 747.7  2732   .5197      77.39 29.23  304             32768  7.515    .6627 29.93 2.494 6.93e-06 237.6 868.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579      .2270 12.11      .4806         0                36737 985.3 3600 1.828\n",
            "\n",
            "15:29:47 | time:25266s total_exs:650184 total_steps:40636 epochs:2.62 time_left:3699s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   183.3     1 769.6  2841   .5559      87.15 29.54  304             32768  7.421    .6557 31.51 2.515 6.93e-06 252.1 930.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.36      .4700         0                36756 1022 3772 1.846\n",
            "\n",
            "15:29:58 | time:25276s total_exs:650488 total_steps:40655 epochs:2.62 time_left:3687s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.2     1 774.3  2890   .5658      81.44 29.86  304             32768  7.598    .6557 30.87 2.506 6.93e-06 246.9 921.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.25      .4690         0                36775 1021 3812 1.867\n",
            "\n",
            "15:30:08 | time:25286s total_exs:650792 total_steps:40674 epochs:2.62 time_left:3675s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   192.5     1 768.1  2903   .5461      96.49 30.23  304             32768   7.32    .6325 31.19 2.387 6.93e-06 249.5 942.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 10.88      .4903         0                36794 1018 3846 1.89\n",
            "\n",
            "15:30:18 | time:25296s total_exs:651096 total_steps:40693 epochs:2.62 time_left:3663s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   158.3     1 711.4  2639   .4572      69.42 29.68  304             32768  7.675    .6627  30.1 2.488 6.93e-06 239.4   888   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .1809 12.04      .4704         0                36813 950.8 3527 1.856\n",
            "\n",
            "15:30:28 | time:25307s total_exs:651400 total_steps:40712 epochs:2.62 time_left:3651s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.8     1 743.6  2735   .5230      81.87 29.43  304             32768  7.457    .6627 31.74 2.536 6.93e-06 253.3 931.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003289     .06908 12.63      .4622         0                36832 996.9 3667 1.84\n",
            "\n",
            "15:30:38 | time:25317s total_exs:651704 total_steps:40731 epochs:2.62 time_left:3639s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.6     1 746.3  2781   .5132      73.33  29.8  304             32768  7.335    .6627 30.73 2.443 6.93e-06 244.6 911.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006579      .1480 11.51      .4798         0                36851  991 3692 1.863\n",
            "\n",
            "15:30:49 | time:25327s total_exs:652008 total_steps:40750 epochs:2.62 time_left:3627s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     169     1 741.1  2699   .5066      76.32 29.13  304             32768  7.589    .6627 30.87 2.552 6.93e-06 246.9 899.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.83      .4655         0                36870  988 3598 1.821\n",
            "\n",
            "15:30:59 | time:25337s total_exs:652304 total_steps:40769 epochs:2.63 time_left:3615s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     190     1 769.5  2814   .5405      93.84 29.26  296             32768   7.38    .6495 32.04 2.433 6.93e-06 256.4 937.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.4      .4752         0                36888 1026 3752 1.806\n",
            "\n",
            "15:31:09 | time:25347s total_exs:652592 total_steps:40787 epochs:2.63 time_left:3604s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.6     1 736.2  2639   .5069      77.55 28.68  288             32768  7.122    .6381  32.5   2.4 6.93e-06   260 932.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.02      .4823         0                36906 996.2 3572 1.832\n",
            "\n",
            "15:31:19 | time:25358s total_exs:652888 total_steps:40805 epochs:2.63 time_left:3592s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     169     1 748.5  2746   .5270      75.43 29.35  296             32768  7.366    .6381 30.57 2.455 6.93e-06 244.5 897.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.64      .4787         0                36925  993 3643 1.899\n",
            "\n",
            "15:31:29 | time:25368s total_exs:653200 total_steps:40825 epochs:2.63 time_left:3580s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.7     1 723.9  2816   .4840       71.2 31.12  312             32768  7.485    .6325 29.37 2.347 6.93e-06 234.9 913.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.46      .4941         0                36944 958.9 3730 1.917\n",
            "\n",
            "15:31:39 | time:25378s total_exs:653496 total_steps:40843 epochs:2.63 time_left:3568s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.8     1 737.9  2672   .5135      72.51 28.97  296             32768  7.524    .6627 31.55 2.387 6.93e-06 251.4 910.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378      .1182 10.88      .4856         0                36963 989.4 3582 1.873\n",
            "\n",
            "15:31:50 | time:25388s total_exs:653808 total_steps:40863 epochs:2.63 time_left:3556s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.3     1 756.4  2934   .5481      78.78 31.03  312             32768  7.559    .6495 29.66 2.397 6.93e-06 237.3 920.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0   11      .4884   .003205                36982 993.7 3855 1.912\n",
            "\n",
            "15:32:00 | time:25398s total_exs:654104 total_steps:40881 epochs:2.63 time_left:3544s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.2     1 748.1  2657   .5372      76.71 28.42  296             32768  7.434    .6627    31 2.485 6.93e-06 247.9 880.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .01689   12      .4694         0                37001 995.9 3538 1.837\n",
            "\n",
            "15:32:10 | time:25409s total_exs:654408 total_steps:40900 epochs:2.63 time_left:3532s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   151.7     1 720.4  2670   .4836      61.62 29.66  304             32768  7.792    .6438 29.09 2.425 6.93e-06 232.7 862.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.31      .4817         0                37020 953.1 3533 1.854\n",
            "\n",
            "15:32:21 | time:25419s total_exs:654712 total_steps:40919 epochs:2.64 time_left:3520s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.5     1 771.6  2844   .5461       82.1 29.49  304             32768    7.5    .6381 30.29 2.442 6.93e-06 242.3 893.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.5      .4731         0                37039 1014 3737 1.844\n",
            "\n",
            "15:32:31 | time:25429s total_exs:655016 total_steps:40938 epochs:2.64 time_left:3508s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     169     1 738.9  2792   .5033      76.66 30.23  304             32768  7.343    .6627  30.6 2.404 6.93e-06 244.7 924.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003289    .003289 11.06      .4889         0                37058 983.6 3717 1.89\n",
            "\n",
            "15:32:41 | time:25439s total_exs:655336 total_steps:40958 epochs:2.64 time_left:3495s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.2     1 763.5  2955   .5500      81.78 30.96  320             32768  7.486    .6325 29.85 2.416 6.93e-06 238.8 924.3   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.2      .4837         0                37078 1002 3880 1.936\n",
            "\n",
            "15:32:51 | time:25450s total_exs:655624 total_steps:40976 epochs:2.64 time_left:3484s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.1     1 760.1  2686   .5417      87.05 28.27  288             32768   7.37    .6627 32.48 2.461 6.93e-06 259.2 916.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .07639 11.71      .4725         0                37096 1019 3603 1.768\n",
            "\n",
            "15:33:01 | time:25460s total_exs:655936 total_steps:40996 epochs:2.64 time_left:3471s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.8     1   730  2813   .5032      74.51 30.83  312             32768  7.704    .6438 29.77 2.437 6.93e-06 238.2 917.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.44      .4794         0                37115 968.2 3731 1.907\n",
            "\n",
            "15:33:12 | time:25470s total_exs:656240 total_steps:41015 epochs:2.64 time_left:3459s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   191.5     1   785  2939   .5822      93.38 29.96  304             32768  7.523    .6627 30.57 2.454 6.93e-06 244.3 914.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   .003289     .02961 11.64      .4755         0                37134 1029 3854 1.92\n",
            "\n",
            "15:33:22 | time:25480s total_exs:656552 total_steps:41034 epochs:2.64 time_left:3447s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   156.3     1   728  2708   .4872      65.27 29.75  312             32768  7.699    .6495 28.15 2.386 6.93e-06 225.2 837.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.87      .4874         0                37154 953.3 3545 1.926\n",
            "\n",
            "15:33:32 | time:25491s total_exs:656840 total_steps:41052 epochs:2.64 time_left:3436s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.9     1 738.3  2604   .5069      76.64 28.22  288             32768  7.364    .6627 31.68 2.441 6.93e-06 252.9 892.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472     .06597 11.48      .4723         0                37172 991.2 3496 1.764\n",
            "\n",
            "15:33:42 | time:25501s total_exs:657144 total_steps:41071 epochs:2.64 time_left:3424s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.4     1 726.8  2714   .4934      68.52 29.87  304             32768   7.48    .6381 29.88 2.342 6.93e-06   239 892.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.4      .4981         0                37191 965.8 3607 1.867\n",
            "\n",
            "15:33:53 | time:25511s total_exs:657448 total_steps:41090 epochs:2.65 time_left:3412s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.2     1 758.8  2867   .5461       85.3 30.22  304             32768  7.575    .6325 30.23 2.398 6.93e-06 241.8 913.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0   11      .4857         0                37210 1001 3780 1.889\n",
            "\n",
            "15:34:03 | time:25521s total_exs:657752 total_steps:41109 epochs:2.65 time_left:3400s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.8     1 768.1  2901   .5691      86.83 30.22  304             32768  7.652    .6627 29.55 2.463 6.93e-06 235.1 887.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003289      .1678 11.74      .4790         0                37229 1003 3789 1.889\n",
            "\n",
            "15:34:13 | time:25531s total_exs:658056 total_steps:41128 epochs:2.65 time_left:3388s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     175     1 754.7  2809   .5329      80.63 29.78  304             32768  7.405    .6381 31.53 2.443 6.93e-06 252.3 939.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.51      .4784         0                37248 1007 3749 1.862\n",
            "\n",
            "15:34:23 | time:25542s total_exs:658360 total_steps:41147 epochs:2.65 time_left:3376s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.8     1 733.5  2710   .5263      83.13 29.56  304             32768  7.485    .6325 30.57 2.428 6.93e-06 244.6 903.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.34      .4829   .003289                37267 978.1 3614 1.849\n",
            "\n",
            "15:34:34 | time:25552s total_exs:658664 total_steps:41166 epochs:2.65 time_left:3364s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   192.1     1 766.2  2815   .5493      96.29 29.39  304             32768  7.497    .6438 31.39 2.454 6.93e-06 251.2 922.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.63      .4727         0                37286 1017 3738 1.838\n",
            "\n",
            "15:34:44 | time:25562s total_exs:658976 total_steps:41186 epochs:2.65 time_left:3351s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.9     1 736.3  2855   .4968      72.88 31.02  312             32768  7.623    .6269 29.23  2.45 6.93e-06 233.8 906.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.59      .4798         0                37305 970.1 3761 1.918\n",
            "\n",
            "15:34:54 | time:25572s total_exs:659272 total_steps:41204 epochs:2.65 time_left:3340s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.2     1 728.8  2619   .4865      70.12 28.75  296             32768  7.559    .6627 31.42 2.532 6.93e-06 250.8 901.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .07095 12.58      .4701         0                37324 979.5 3520 1.862\n",
            "\n",
            "15:35:04 | time:25582s total_exs:659584 total_steps:41224 epochs:2.65 time_left:3327s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   192.8     1   782  3005   .5705      95.07 30.74  312             32768  7.382    .6438 31.51 2.468 6.93e-06 252.1 968.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 11.8      .4739         0                37343 1034 3973  1.9\n",
            "\n",
            "15:35:14 | time:25593s total_exs:659872 total_steps:41242 epochs:2.66 time_left:3316s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.6     1   725  2576   .4965         79 28.43  288             32768  7.146    .6627  33.5 2.453 6.93e-06 267.9 952.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472     .01389 11.62      .4767         0                37361 992.9 3529 1.823\n",
            "\n",
            "15:35:24 | time:25603s total_exs:660168 total_steps:41260 epochs:2.66 time_left:3304s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.1     1 738.6  2713   .5169      76.75 29.38  296             32768  7.251    .6381 31.39 2.445 6.93e-06 251.1 922.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.53      .4771         0                37380 989.8 3635 1.906\n",
            "\n",
            "15:35:34 | time:25613s total_exs:660456 total_steps:41278 epochs:2.66 time_left:3293s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.7     1 753.9  2715   .5382      73.42  28.8  288             32768  7.402    .6496 30.61 2.432 6.93e-06 244.9 881.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.38      .4770         0                37398 998.8 3596 1.801\n",
            "\n",
            "15:35:45 | time:25623s total_exs:660760 total_steps:41297 epochs:2.66 time_left:3281s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.7     1 751.3  2793   .5329      74.73 29.74  304             32768  7.684    .6438    30 2.437 6.93e-06   240 892.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.44      .4806         0                37417 991.3 3686 1.859\n",
            "\n",
            "15:35:55 | time:25633s total_exs:661080 total_steps:41317 epochs:2.66 time_left:3268s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   150.7     1 708.7  2738   .4656       62.1 30.91  320             32768  7.571    .6495 27.88 2.343 6.93e-06   223 861.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.41      .4956         0                37437 931.7 3600 1.933\n",
            "\n",
            "15:36:05 | time:25644s total_exs:661368 total_steps:41335 epochs:2.66 time_left:3257s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.8     1 755.9  2655   .5174      84.26  28.1  288             32768  7.243    .6627 33.79 2.484 6.93e-06 269.3   946   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472      .1250 11.99      .4750         0                37455 1025 3601 1.757\n",
            "\n",
            "15:36:15 | time:25654s total_exs:661640 total_steps:41352 epochs:2.66 time_left:3247s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.7     1 730.1  2436   .4853      87.42  26.7  272             32768  7.107    .6627 35.29 2.504 6.93e-06 280.3 935.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .007353      .2463 12.23      .4608         0                37472 1010 3372 1.669\n",
            "\n",
            "15:36:26 | time:25664s total_exs:661936 total_steps:41371 epochs:2.66 time_left:3235s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.8     1 752.9  2754   .5270      77.72 29.27  296             32768  7.481    .6627 30.62 2.414 6.93e-06 244.3 893.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006757     .08784 11.18      .4755   .003378                37490 997.2 3648 1.819\n",
            "\n",
            "15:36:36 | time:25674s total_exs:662224 total_steps:41389 epochs:2.67 time_left:3224s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.7     1 714.3  2551   .4722      70.38 28.57  288             32768  7.607    .6627  31.8 2.549 6.93e-06 254.4 908.3   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.8      .4575         0                37508 968.7 3459 1.831\n",
            "\n",
            "15:36:46 | time:25684s total_exs:662520 total_steps:41407 epochs:2.67 time_left:3212s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.5     1 747.8  2750   .4966      71.04 29.42  296             32768  7.396    .6627 30.05 2.385 6.93e-06 240.3 883.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .01014 10.86      .4869         0                37527 988.1 3633 1.902\n",
            "\n",
            "15:36:56 | time:25694s total_exs:662824 total_steps:41426 epochs:2.67 time_left:3200s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   187.4     1 784.8  2899   .5625      89.33 29.55  304             32768  7.388    .6269 31.71 2.462 6.93e-06 253.7 937.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.73      .4757         0                37546 1039 3836 1.848\n",
            "\n",
            "15:37:06 | time:25705s total_exs:663128 total_steps:41445 epochs:2.67 time_left:3188s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.3     1 733.6  2699   .5033      74.64 29.43  304             32768   7.49    .6438 30.48 2.468 6.93e-06 243.9 897.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.79      .4776         0                37565 977.5 3596 1.84\n",
            "\n",
            "15:37:17 | time:25715s total_exs:663416 total_steps:41463 epochs:2.67 time_left:3177s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.3     1 757.4  2670   .5382      86.67  28.2  288             32768   7.34    .6627 31.62 2.437 6.93e-06 252.8 891.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .02083 11.43      .4774         0                37583 1010 3561 1.764\n",
            "\n",
            "15:37:27 | time:25725s total_exs:663736 total_steps:41483 epochs:2.67 time_left:3164s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.9     1 780.7  3029   .5656      78.34 31.04  320             32768  7.779    .6325 29.48 2.483 6.93e-06 235.8   915   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 11.97      .4686         0                37603 1017 3944 1.94\n",
            "\n",
            "15:37:37 | time:25735s total_exs:664040 total_steps:41502 epochs:2.67 time_left:3152s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.2     1 737.4  2763   .5164      75.01 29.98  304             32768  7.444    .6557 29.15 2.357 6.93e-06 233.2   874   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.55      .4919         0                37622 970.7 3637 1.875\n",
            "\n",
            "15:37:47 | time:25746s total_exs:664344 total_steps:41521 epochs:2.67 time_left:3140s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.3     1 733.4  2760   .5033      77.61 30.11  304             32768  7.682    .6557 30.02 2.441 6.93e-06 240.2   904   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.48      .4815         0                37641 973.6 3665 1.882\n",
            "\n",
            "15:37:58 | time:25756s total_exs:664648 total_steps:41540 epochs:2.68 time_left:3128s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   185.7     1 764.5  2776   .5362      90.13 29.04  304             32768  7.699    .6496 30.88 2.495 6.93e-06   247 896.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.12      .4688         0                37660 1012 3672 1.816\n",
            "\n",
            "15:38:08 | time:25766s total_exs:664936 total_steps:41558 epochs:2.68 time_left:3117s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   191.8     1 761.8  2667   .5417      96.61 28.01  288             32768  6.961    .6627 34.18 2.472 6.93e-06 270.5 947.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006944      .3646 11.85      .4775         0                37678 1032 3614 1.751\n",
            "\n",
            "15:38:18 | time:25777s total_exs:665240 total_steps:41577 epochs:2.68 time_left:3105s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.9     1 745.9  2766   .5230       72.7 29.66  304             32768  7.593    .6495 30.07 2.412 6.93e-06 240.6   892   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.15      .4783         0                37697 986.4 3658 1.855\n",
            "\n",
            "15:38:29 | time:25787s total_exs:665544 total_steps:41596 epochs:2.68 time_left:3093s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.2     1 727.5  2693   .5033      68.29 29.62  304             32768  7.917    .6627  29.3 2.435 6.93e-06 231.4 856.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .3717 11.42      .4774         0                37716 958.9 3550 1.852\n",
            "\n",
            "15:38:39 | time:25797s total_exs:665872 total_steps:41617 epochs:2.68 time_left:3080s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.6     1 759.8  3099   .5518      70.67 32.63  328             32768  7.822    .6381 27.21 2.408 6.93e-06 217.7 887.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.11      .4821         0                37736 977.5 3987 2.02\n",
            "\n",
            "15:38:49 | time:25807s total_exs:666168 total_steps:41635 epochs:2.68 time_left:3068s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   155.3     1 718.4  2555   .4831      65.54 28.45  296             32768  7.565    .6557 30.94 2.454 6.93e-06 247.5 880.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.63      .4710         0                37755 965.9 3435 1.843\n",
            "\n",
            "15:38:59 | time:25818s total_exs:666488 total_steps:41655 epochs:2.68 time_left:3056s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.3     1 719.4  2782   .4813      71.34 30.94  320             32768  7.697    .6325 29.12 2.489 6.93e-06 232.9 900.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.05      .4695         0                37775 952.4 3683 1.934\n",
            "\n",
            "15:39:10 | time:25828s total_exs:666792 total_steps:41674 epochs:2.68 time_left:3044s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.9     1 747.1  2786   .5099       74.5 29.83  304             32768  7.322    .6438 30.45 2.451 6.93e-06 243.6 908.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.6      .4786         0                37794 990.7 3694 1.865\n",
            "\n",
            "15:39:20 | time:25838s total_exs:667080 total_steps:41692 epochs:2.68 time_left:3033s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.4     1 739.1  2593   .5174      83.04 28.07  288             32768  6.821    .6496 35.11 2.415 6.93e-06 280.9 985.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.19      .4843         0                37812 1020 3579 1.755\n",
            "\n",
            "15:39:30 | time:25848s total_exs:667384 total_steps:41711 epochs:2.69 time_left:3021s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.9     1 739.6  2764   .5066       70.5  29.9  304             32768  7.432    .6381 30.98  2.43 6.93e-06 247.8 926.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.36      .4801         0                37831 987.4 3691 1.869\n",
            "\n",
            "15:39:40 | time:25859s total_exs:667688 total_steps:41730 epochs:2.69 time_left:3009s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.8     1 704.1  2577   .4671      71.78 29.28  304             32768  7.434    .6438 32.25 2.526 6.93e-06   258 944.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.5      .4666         0                37850  962 3521 1.831\n",
            "\n",
            "15:39:51 | time:25869s total_exs:668000 total_steps:41750 epochs:2.69 time_left:2996s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   183.1     1 766.4  2958   .5513      87.32 30.87  312             32768  7.467    .6325  30.8 2.472 6.93e-06 246.4 951.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.85      .4774         0                37869 1013 3909 1.909\n",
            "\n",
            "15:40:01 | time:25879s total_exs:668304 total_steps:41769 epochs:2.69 time_left:2984s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.1     1 720.5  2716   .4901      70.03 30.15  304             32768  7.546    .6627 30.44 2.487 6.93e-06 243.1 916.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .04934 12.02      .4701         0                37888 963.6 3632 1.922\n",
            "\n",
            "15:40:11 | time:25890s total_exs:668616 total_steps:41788 epochs:2.69 time_left:2972s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.6     1 738.5  2751   .4936      75.27  29.8  312             32768  7.549    .6627 30.05 2.512 6.93e-06 240.4 895.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.33      .4635   .003205                37908 978.9 3646 1.922\n",
            "\n",
            "15:40:21 | time:25900s total_exs:668904 total_steps:41806 epochs:2.69 time_left:2961s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     196     1 779.7  2737   .5764      98.57 28.08  288             32768  6.995    .6627 33.62 2.501 6.93e-06   269 944.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.19      .4671         0                37926 1049 3681 1.756\n",
            "\n",
            "15:40:31 | time:25910s total_exs:669192 total_steps:41824 epochs:2.69 time_left:2950s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.9     1 752.4  2698   .5312      86.85 28.69  288             32768   7.09    .6628 33.27 2.467 6.93e-06 266.1 954.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.79      .4765         0                37944 1018 3653 1.794\n",
            "\n",
            "15:40:42 | time:25920s total_exs:669496 total_steps:41843 epochs:2.69 time_left:2938s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.5     1 763.8  2838   .5461      89.04 29.72  304             32768  7.337    .6496 30.77 2.413 6.93e-06 246.2 914.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.17      .4885         0                37963 1010 3752 1.858\n",
            "\n",
            "15:40:52 | time:25930s total_exs:669792 total_steps:41862 epochs:2.70 time_left:2926s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.1     1 767.9  2811   .5473      88.13 29.28  296             32768  7.386    .6627 31.34 2.478 6.93e-06 248.9 911.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006757      .2264 11.92      .4755         0                37981 1017 3722 1.808\n",
            "\n",
            "15:41:02 | time:25940s total_exs:670088 total_steps:41880 epochs:2.70 time_left:2914s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   147.4     1 687.9  2498   .4324      61.43 29.05  296             32768  7.596    .6326 30.16 2.429 6.93e-06 241.3 876.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.35      .4790         0                38000 929.1 3374 1.882\n",
            "\n",
            "15:41:12 | time:25951s total_exs:670384 total_steps:41899 epochs:2.70 time_left:2903s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.2     1 748.4  2747   .5372      77.66 29.37  296             32768  7.436    .6627 32.01 2.399 6.93e-06 255.3 937.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378      .1014 11.01      .4835         0                38018 1004 3685 1.812\n",
            "\n",
            "15:41:22 | time:25961s total_exs:670688 total_steps:41918 epochs:2.70 time_left:2891s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   143.2     1 688.4  2594   .4145      57.17 30.14  304             32768  7.472    .6438    30 2.458 6.93e-06   240 904.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.68      .4702         0                38037 928.5 3498 1.92\n",
            "\n",
            "15:41:32 | time:25971s total_exs:671000 total_steps:41937 epochs:2.70 time_left:2879s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     168     1 748.5  2886   .5385      74.48 30.84  312             32768  7.556    .6326 28.54 2.405 6.93e-06 228.3 880.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.08      .4832         0                38057 976.9 3766 1.99\n",
            "\n",
            "15:41:43 | time:25981s total_exs:671304 total_steps:41956 epochs:2.70 time_left:2867s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     165     1 727.5  2734   .5099      74.07 30.07  304             32768  7.364    .6381 30.38  2.41 6.93e-06   243 913.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.13      .4870         0                38076 970.5 3647 1.88\n",
            "\n",
            "15:41:53 | time:25991s total_exs:671608 total_steps:41975 epochs:2.70 time_left:2855s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.2     1 762.6  2832   .5395      78.92 29.71  304             32768  7.501    .6627 30.69 2.434 6.93e-06 245.6 911.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.41      .4808   .003289                38095 1008 3743 1.857\n",
            "\n",
            "15:42:03 | time:26001s total_exs:671912 total_steps:41994 epochs:2.70 time_left:2843s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.4     1 729.9  2763   .4967      72.17 30.29  304             32768  7.431    .6627 30.37 2.352 6.93e-06 242.7 918.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .02961 10.5      .4859         0                38114 972.6 3682 1.894\n",
            "\n",
            "15:42:13 | time:26011s total_exs:672216 total_steps:42013 epochs:2.71 time_left:2831s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.4     1 738.5  2774   .4901       68.1 30.05  304             32768  7.397    .6326 30.35 2.406 6.93e-06 242.8 912.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.09      .4791         0                38133 981.3 3686 1.879\n",
            "\n",
            "15:42:23 | time:26021s total_exs:672504 total_steps:42031 epochs:2.71 time_left:2820s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.9     1 740.3  2644   .5243      85.32 28.57  288             32768   7.45    .6438 31.77 2.441 6.93e-06 254.2 907.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.49      .4754         0                38151 994.4 3552 1.787\n",
            "\n",
            "15:42:33 | time:26032s total_exs:672808 total_steps:42050 epochs:2.71 time_left:2808s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.6     1   774  2922   .5559      85.84  30.2  304             32768  7.532    .6438 29.78 2.488 6.93e-06 238.2 899.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.04      .4781         0                38170 1012 3822 1.889\n",
            "\n",
            "15:42:43 | time:26042s total_exs:673112 total_steps:42069 epochs:2.71 time_left:2796s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.3     1 724.3  2698   .4868       73.8  29.8  304             32768  7.333    .6438 29.76 2.343 6.93e-06 238.1 886.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.41      .4905         0                38189 962.4 3584 1.863\n",
            "\n",
            "15:42:54 | time:26052s total_exs:673416 total_steps:42088 epochs:2.71 time_left:2784s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.7     1 756.7  2812   .5164      69.15 29.73  304             32768  7.653    .6627 29.54 2.451 6.93e-06   236   877   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .03947 11.6      .4735         0                38208 992.7 3689 1.859\n",
            "\n",
            "15:43:04 | time:26062s total_exs:673720 total_steps:42107 epochs:2.71 time_left:2772s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   152.9     1 702.4  2584   .4638      65.09 29.43  304             32768  7.435    .6628  30.8 2.422 6.93e-06 246.3 906.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .003289    .006579 11.27      .4804         0                38227 948.7 3490 1.84\n",
            "\n",
            "15:43:14 | time:26072s total_exs:674032 total_steps:42127 epochs:2.71 time_left:2760s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.8     1 744.5  2875   .5064      82.75  30.9  312             32768  7.823    .6557 29.32  2.45 6.93e-06 234.6 905.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.58      .4739         0                38246  979 3781 1.901\n",
            "\n",
            "15:43:24 | time:26083s total_exs:674328 total_steps:42145 epochs:2.71 time_left:2748s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.3     1 715.2  2620   .4696      74.89 29.31  296             32768  7.447    .6381 29.99 2.382 6.93e-06 239.9 879.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.83      .4822         0                38265 955.2 3499 1.894\n",
            "\n",
            "15:43:34 | time:26093s total_exs:674616 total_steps:42163 epochs:2.72 time_left:2737s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.6     1 715.1  2570   .4826      72.26 28.75  288             32768  7.578    .6628 30.95 2.529 6.93e-06 246.8 886.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472      .1111 12.54      .4584         0                38283 961.8 3457 1.798\n",
            "\n",
            "15:43:44 | time:26103s total_exs:674904 total_steps:42181 epochs:2.72 time_left:2726s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     168     1 740.6  2655   .5208      75.42 28.68  288             32768  7.286    .6628 32.47 2.501 6.93e-06 257.3 922.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006944      .3056 12.2      .4714         0                38301 997.9 3577 1.793\n",
            "\n",
            "15:43:54 | time:26113s total_exs:675200 total_steps:42200 epochs:2.72 time_left:2714s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     168     1 735.8  2723   .5034      76.06 29.61  296             32768  7.422    .6628 30.96 2.436 6.93e-06 246.1 910.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006757      .2061 11.43      .4784         0                38319 981.8 3634 1.825\n",
            "\n",
            "15:44:04 | time:26123s total_exs:675504 total_steps:42219 epochs:2.72 time_left:2702s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.3     1 754.7  2839   .5362      79.95 30.09  304             32768  7.614    .6325 30.56 2.504 6.93e-06 244.5 919.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.24      .4705         0                38338 999.2 3758 1.918\n",
            "\n",
            "15:44:15 | time:26133s total_exs:675792 total_steps:42237 epochs:2.72 time_left:2691s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   189.7     1 769.1  2747   .5556      93.59 28.58  288             32768  7.188    .6496 33.57 2.434 6.93e-06 268.6 959.3   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.4      .4818         0                38356 1038 3707 1.829\n",
            "\n",
            "15:44:25 | time:26143s total_exs:676104 total_steps:42256 epochs:2.72 time_left:2678s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   197.3     1 794.6  2996   .5737      98.02 30.16  312             32768   7.49    .6628 31.21 2.477 6.93e-06 249.5 940.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003205     .02564 11.9      .4774         0                38376 1044 3936 1.953\n",
            "\n",
            "15:44:35 | time:26153s total_exs:676416 total_steps:42276 epochs:2.72 time_left:2666s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   158.9     1   728  2804   .5000      67.87 30.81  312             32768  7.623    .6381 29.98 2.425 6.93e-06 239.8 923.8   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.3      .4851         0                38395 967.8 3728 1.898\n",
            "\n",
            "15:44:45 | time:26164s total_exs:676728 total_steps:42295 epochs:2.72 time_left:2654s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.4     1 740.2  2815   .5160      75.93 30.43  312             32768  7.645    .6381    28 2.426 6.93e-06   224 851.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.31      .4865         0                38415 964.1 3667 1.964\n",
            "\n",
            "15:44:56 | time:26174s total_exs:677032 total_steps:42314 epochs:2.72 time_left:2642s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.9     1 738.7  2754   .4967      70.61 29.83  304             32768  7.554    .6627 30.94 2.481 6.93e-06 247.5 922.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.96      .4708         0                38434 986.2 3677 1.865\n",
            "\n",
            "15:45:06 | time:26184s total_exs:677344 total_steps:42334 epochs:2.73 time_left:2630s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     170     1 736.9  2838   .4968      77.88  30.8  312             32768  7.661    .6628 30.38 2.429 6.93e-06 241.7 930.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003205      .1699 11.35      .4814         0                38453 978.6 3768 1.904\n",
            "\n",
            "15:45:16 | time:26194s total_exs:677640 total_steps:42352 epochs:2.73 time_left:2618s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   159.4     1 704.3  2566   .4730      71.36 29.15  296             32768  7.451    .6628 30.86 2.374 6.93e-06 246.7 898.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006757     .03041 10.74      .4873         0                38472  951 3465 1.889\n",
            "\n",
            "15:45:26 | time:26204s total_exs:677936 total_steps:42371 epochs:2.73 time_left:2607s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.2     1 748.3  2760   .5169      77.63 29.51  296             32768   7.63    .6628 30.39 2.428 6.93e-06 242.5 894.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .07432 11.34      .4790         0                38490 990.8 3654 1.815\n",
            "\n",
            "15:45:36 | time:26215s total_exs:678232 total_steps:42389 epochs:2.73 time_left:2595s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   190.3     1 772.6  2795   .5811      93.75 28.94  296             32768  7.355    .6628 31.74 2.509 6.93e-06 253.9 918.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.29      .4674         0                38509 1027 3713 1.871\n",
            "\n",
            "15:45:46 | time:26225s total_exs:678520 total_steps:42407 epochs:2.73 time_left:2584s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   152.9     1 707.2  2525   .4618      64.53 28.56  288             32768  7.589    .6627 29.81 2.412 6.93e-06   238 849.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472     .05208 11.16      .4822         0                38527 945.2 3375 1.786\n",
            "\n",
            "15:45:56 | time:26235s total_exs:678824 total_steps:42426 epochs:2.73 time_left:2572s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   144.7     1 712.6  2648   .4605      55.64 29.73  304             32768  7.815    .6557 29.24 2.436 6.93e-06 233.9 869.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.43      .4848         0                38546 946.6 3518 1.859\n",
            "\n",
            "15:46:07 | time:26245s total_exs:679136 total_steps:42446 epochs:2.73 time_left:2560s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   188.6     1 757.9  2935   .5545      93.81 30.97  312             32768  7.492    .6628 29.85 2.451 6.93e-06 238.8 924.6   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.6      .4795         0                38565 996.7 3859 1.91\n",
            "\n",
            "15:46:17 | time:26255s total_exs:679440 total_steps:42465 epochs:2.73 time_left:2548s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   183.1     1 774.9  2901   .5461      86.25 29.95  304             32768  7.632    .6381 30.61 2.494 6.93e-06 244.9 916.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.11      .4705         0                38584 1020 3818 1.914\n",
            "\n",
            "15:46:27 | time:26265s total_exs:679736 total_steps:42483 epochs:2.74 time_left:2536s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.7     1 710.8  2585   .4932      75.83  29.1  296             32768    7.5    .6628 30.08 2.357 6.93e-06 240.5 874.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .02365 10.56      .4919         0                38603 951.3 3460 1.885\n",
            "\n",
            "15:46:37 | time:26275s total_exs:680040 total_steps:42502 epochs:2.74 time_left:2524s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.6     1 746.6  2838   .5099      72.24 30.41  304             32768  7.572    .6496 29.77  2.43 6.93e-06 238.2 905.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.36      .4798         0                38622 984.8 3743 1.902\n",
            "\n",
            "15:46:47 | time:26285s total_exs:680344 total_steps:42521 epochs:2.74 time_left:2512s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.9     1 733.9  2760   .5164      76.21 30.09  304             32768  7.788    .6627 28.65 2.432 6.93e-06 229.2 862.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.39      .4848         0                38641 963.2 3622 1.881\n",
            "\n",
            "15:46:57 | time:26296s total_exs:680664 total_steps:42541 epochs:2.74 time_left:2500s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.5     1 743.6  2901   .5062      75.58 31.21  320             65536  7.338    .6326 29.12 2.344 6.93e-06 232.9 908.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.42      .4968   .003125                38661 976.6 3810 1.951\n",
            "\n",
            "15:47:08 | time:26306s total_exs:680984 total_steps:42561 epochs:2.74 time_left:2487s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.9     1 748.6  2924   .5219      73.28 31.25  320             65536  7.417    .6218 29.03  2.43 6.93e-06 232.3 907.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.36      .4869         0                38681 980.9 3832 1.954\n",
            "\n",
            "15:47:18 | time:26316s total_exs:681272 total_steps:42579 epochs:2.74 time_left:2476s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.1     1 758.6  2704   .5347       83.3 28.52  288             65536  7.251    .6628 33.87 2.487 6.93e-06 268.9 958.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006944      .2535 12.02      .4692         0                38699 1027 3663 1.783\n",
            "\n",
            "15:47:28 | time:26326s total_exs:681576 total_steps:42598 epochs:2.74 time_left:2464s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.7     1 767.8  2859   .5592      86.73 29.79  304             65536  7.264    .6326 30.59 2.403 6.93e-06 244.7 911.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.06      .4831         0                38718 1013 3770 1.862\n",
            "\n",
            "15:47:38 | time:26337s total_exs:681880 total_steps:42617 epochs:2.74 time_left:2452s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   186.8     1 771.4  2809   .5559      90.34 29.13  304             65536  7.113    .6557 32.38 2.413 6.93e-06 259.1 943.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.17      .4836         0                38737 1030 3752 1.822\n",
            "\n",
            "15:47:49 | time:26347s total_exs:682184 total_steps:42636 epochs:2.75 time_left:2440s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.5     1   737  2731   .5197      80.39 29.65  304             65536  7.191    .6381 31.78 2.421 6.93e-06 254.3 942.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.26      .4830         0                38756 991.2 3674 1.854\n",
            "\n",
            "15:47:59 | time:26357s total_exs:682488 total_steps:42655 epochs:2.75 time_left:2429s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   153.8     1 712.8  2676   .4737      64.67 30.03  304             65536  7.509    .6496 29.54 2.449 6.93e-06 236.3 887.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.58      .4803         0                38775 949.1 3563 1.877\n",
            "\n",
            "15:48:09 | time:26367s total_exs:682792 total_steps:42674 epochs:2.75 time_left:2417s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.4     1 724.2  2707   .4901      72.83  29.9  304             65536  7.211    .6326 31.35 2.407 6.93e-06 250.8 937.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.1      .4874         0                38794 975.1 3645 1.869\n",
            "\n",
            "15:48:19 | time:26378s total_exs:683096 total_steps:42693 epochs:2.75 time_left:2405s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.1     1 769.9  2858   .5395       83.9  29.7  304             65536  7.447    .6381 30.76 2.429 6.93e-06 246.1 913.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.34      .4806         0                38813 1016 3772 1.857\n",
            "\n",
            "15:48:27 | time:26385s total_exs:683320 total_steps:42707 epochs:2.75 time_left:2396s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.4     1 743.4  2758   .5089      80.43 29.68  224             65536  7.416    .6628 30.88 2.449 6.93e-06 247.1 916.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.58      .4775         0                38827 990.5 3674 1.855\n",
            "\n",
            "15:48:27 | running eval: valid\n",
            "15:52:48 | eval completed in 261.06s\n",
            "15:52:48 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   175.4 734.4  8347   .4922      83.52 90.86 23718    .5657 36.19 2.373 6.93e-06 288.9  3284 .001686     .05232 10.73   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4898         0                38827 1023 11631\n",
            "\u001b[0m\n",
            "15:52:48 | \u001b[1;32mnew best ppl: 10.73 (previous best was 11.03)\u001b[0m\n",
            "15:52:48 | saving best valid model: /content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model\n",
            "15:53:15 | time:26673s total_exs:683624 total_steps:42726 epochs:2.75 time_left:2409s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   157.5     1 721.4  2712   .4836      67.37 30.08  304             65536  7.679    .6629 29.12 2.384 6.93e-06 232.9 875.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 10.85      .4871         0                38846 954.3 3588 1.88\n",
            "\n",
            "15:53:25 | time:26683s total_exs:683936 total_steps:42746 epochs:2.75 time_left:2397s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   150.7     1 686.1  2655   .4327      64.93 30.95  312             65536   7.56    .6505 29.48 2.401 6.93e-06 235.8 912.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.04      .4835         0                38865 921.9 3567 1.905\n",
            "\n",
            "15:53:35 | time:26693s total_exs:684232 total_steps:42764 epochs:2.75 time_left:2385s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.9     1 765.3  2818   .5507      80.27 29.46  296             65536  7.546    .6629 29.68 2.382 6.93e-06 235.7   868   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378      .2162 10.82      .4875         0                38884 1001 3686 1.904\n",
            "\n",
            "15:53:45 | time:26704s total_exs:684536 total_steps:42783 epochs:2.76 time_left:2373s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   198.7     1   748  2736   .5132      105.2 29.27  304             65536  7.546    .6497 32.16 2.425 6.93e-06 257.3 941.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 11.31      .4813         0                38903 1005 3678 1.83\n",
            "\n",
            "15:53:56 | time:26714s total_exs:684840 total_steps:42802 epochs:2.76 time_left:2361s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.6     1 715.6  2637   .4868      73.16 29.48  304             65536  7.649    .6559 31.22 2.418 6.93e-06 249.7 920.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.23      .4821         0                38922 965.4 3558 1.844\n",
            "\n",
            "15:54:06 | time:26724s total_exs:685144 total_steps:42821 epochs:2.76 time_left:2349s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   153.2     1 700.3  2561   .4474      65.65 29.25  304             65536   7.51    .6629  30.5  2.45 6.93e-06 243.4 890.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .06908 11.59      .4780         0                38941 943.8 3451 1.829\n",
            "\n",
            "15:54:16 | time:26735s total_exs:685464 total_steps:42841 epochs:2.76 time_left:2337s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.4     1 767.3  2974   .5312      76.51    31  320             65536  7.535    .6558 29.49 2.437 6.93e-06 235.9 914.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.44      .4838         0                38961 1003 3888 1.939\n",
            "\n",
            "15:54:26 | time:26745s total_exs:685776 total_steps:42861 epochs:2.76 time_left:2324s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     165     1   735  2860   .5032      73.11 31.13  312             65536  7.519    .6440 29.22 2.475 6.93e-06 233.8 909.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.88      .4782         0                38980 968.8 3770 1.92\n",
            "\n",
            "15:54:36 | time:26755s total_exs:686064 total_steps:42879 epochs:2.76 time_left:2313s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.6     1 733.9  2615   .5035      77.87 28.51  288             65536  7.066    .6497 32.18 2.468 6.93e-06 257.4 917.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.79      .4716         0                38998 991.4 3533 1.816\n",
            "\n",
            "15:54:47 | time:26765s total_exs:686376 total_steps:42898 epochs:2.76 time_left:2301s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     171     1 741.5  2785   .5160      78.31 30.05  312             65536  7.487    .6558 29.86 2.391 6.93e-06 238.9 897.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 10.92      .4851         0                39018 980.4 3682 1.94\n",
            "\n",
            "15:54:57 | time:26775s total_exs:686672 total_steps:42917 epochs:2.76 time_left:2289s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.7     1 747.9  2729   .5270       82.2 29.19  296             65536    7.2    .6440 32.77 2.393 6.93e-06 262.2 956.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.95      .4761         0                39036 1010 3685 1.801\n",
            "\n",
            "15:55:07 | time:26785s total_exs:686968 total_steps:42935 epochs:2.76 time_left:2277s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.2     1 770.8  2821   .5709      82.83 29.28  296             65536  7.357    .6559 30.43 2.471 6.93e-06 243.4   891   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.83      .4773         0                39055 1014 3712 1.896\n",
            "\n",
            "15:55:17 | time:26796s total_exs:687264 total_steps:42954 epochs:2.77 time_left:2266s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   185.5     1 773.5  2844   .5372      88.84 29.42  296             65536  7.285    .6629 31.31 2.382 6.93e-06 249.8 918.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006757     .09122 10.83      .4877         0                39073 1023 3762 1.813\n",
            "\n",
            "15:55:27 | time:26806s total_exs:687568 total_steps:42973 epochs:2.77 time_left:2254s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   154.8     1 725.1  2726   .5033      64.19 30.08  304             65536  7.564    .6629 29.46 2.457 6.93e-06 235.7 886.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.66      .4816         0                39092 960.8 3613 1.917\n",
            "\n",
            "15:55:37 | time:26816s total_exs:687872 total_steps:42992 epochs:2.77 time_left:2242s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.8     1 772.5  2932   .5592      79.22 30.36  304             65536   7.77    .6497 29.47 2.447 6.93e-06 235.8 894.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.56      .4814         0                39111 1008 3827 1.931\n",
            "\n",
            "15:55:47 | time:26826s total_exs:688176 total_steps:43011 epochs:2.77 time_left:2230s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.9     1 742.6  2824   .5296      81.09 30.42  304             65536  7.766    .6559 29.84 2.484 6.93e-06 238.7 907.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.99      .4702         0                39130 981.3 3731 1.937\n",
            "\n",
            "15:55:57 | time:26836s total_exs:688472 total_steps:43029 epochs:2.77 time_left:2218s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.8     1 767.8  2833   .5574      79.81 29.52  296             65536  7.599    .6440 28.45 2.329 6.93e-06 227.6 839.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.27      .4977         0                39149 995.4 3673 1.909\n",
            "\n",
            "15:56:08 | time:26846s total_exs:688760 total_steps:43047 epochs:2.77 time_left:2207s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     171     1 741.1  2569   .5312      78.39 27.73  288             65536  7.062    .6559 34.02 2.454 6.93e-06 272.2 943.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.63      .4731         0                39167 1013 3512 1.734\n",
            "\n",
            "15:56:18 | time:26857s total_exs:689064 total_steps:43066 epochs:2.77 time_left:2195s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.6     1 738.8  2707   .4967      77.21 29.31  304             65536  7.341    .6629 31.27 2.436 6.93e-06 248.5 910.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .2072 11.42      .4735         0                39186 987.3 3617 1.833\n",
            "\n",
            "15:56:28 | time:26867s total_exs:689368 total_steps:43085 epochs:2.77 time_left:2183s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.1     1 748.3  2833   .5362       78.6 30.29  304             65536  7.661    .6497 29.99 2.415 6.93e-06 239.9 908.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.19      .4828         0                39205 988.2 3742 1.894\n",
            "\n",
            "15:56:38 | time:26877s total_exs:689672 total_steps:43104 epochs:2.78 time_left:2171s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   186.8     1 760.6  2885   .5461      91.67 30.35  304             65536  7.634    .6383 29.83   2.4 6.93e-06 238.7 905.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.03      .4810         0                39224 999.3 3791 1.897\n",
            "\n",
            "15:56:49 | time:26887s total_exs:689960 total_steps:43122 epochs:2.78 time_left:2159s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   190.6     1 776.8  2730   .5694      93.46 28.11  288             65536  7.239    .6629 33.74 2.464 6.93e-06 269.9 948.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.75      .4756         0                39242 1047 3678 1.758\n",
            "\n",
            "15:56:59 | time:26897s total_exs:690280 total_steps:43142 epochs:2.78 time_left:2147s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.5     1 757.9  2930   .5094      70.78 30.93  320             65536  7.652    .6327 28.54 2.406 6.93e-06 228.3 882.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.08      .4909         0                39262 986.2 3813 1.934\n",
            "\n",
            "15:57:09 | time:26907s total_exs:690568 total_steps:43160 epochs:2.78 time_left:2135s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.2     1   740  2648   .5104      91.73 28.63  288             65536    7.2    .6383 32.85  2.46 6.93e-06 262.8 940.4   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 11.7      .4729         0                39280 1003 3588 1.79\n",
            "\n",
            "15:57:19 | time:26918s total_exs:690872 total_steps:43179 epochs:2.78 time_left:2123s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.2     1 746.2  2782   .5197       75.9 29.82  304             65536  7.557    .6629 31.09 2.441 6.93e-06 248.7 927.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.48      .4751         0                39299 994.9 3709 1.865\n",
            "\n",
            "15:57:29 | time:26928s total_exs:691168 total_steps:43198 epochs:2.78 time_left:2112s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.6     1 734.2  2703   .5101      77.85 29.45  296             65536  7.184    .6629 31.98 2.419 6.93e-06 254.8 937.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378      .1318 11.23      .4812   .003378                39317 988.9 3640 1.812\n",
            "\n",
            "15:57:40 | time:26938s total_exs:691480 total_steps:43217 epochs:2.78 time_left:2100s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   188.7     1 795.5  3022   .5897      89.25 30.39  312             65536  7.633    .6629 29.46 2.486 6.93e-06 234.4 890.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003205      .1635 12.01      .4755         0                39337 1030 3912 1.962\n",
            "\n",
            "15:57:50 | time:26948s total_exs:691784 total_steps:43236 epochs:2.78 time_left:2088s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.1     1 741.1  2739   .5164      81.51 29.57  304             65536  7.273    .6629 30.95 2.373 6.93e-06 247.6 915.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.73      .4936         0                39356 988.7 3655 1.849\n",
            "\n",
            "15:58:00 | time:26958s total_exs:692080 total_steps:43255 epochs:2.79 time_left:2076s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.1     1 735.7  2681   .5000       71.1 29.15  296             65536  7.385    .6497 32.75 2.479 6.93e-06   262 954.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.93      .4719         0                39374 997.7 3635 1.804\n",
            "\n",
            "15:58:10 | time:26969s total_exs:692376 total_steps:43273 epochs:2.79 time_left:2064s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.6     1 727.9  2663   .5034      76.65 29.26  296             65536  7.552    .6629 29.59 2.438 6.93e-06 236.1 863.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "   .003378     .07432 11.45      .4836         0                39393  964 3526  1.9\n",
            "\n",
            "15:58:20 | time:26979s total_exs:692696 total_steps:43293 epochs:2.79 time_left:2052s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   149.9     1 711.6  2786   .4594      60.91 31.32  320             65536  7.913    .6559 27.74 2.426 6.93e-06 221.9 869.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.32      .4788         0                39413 933.6 3656 1.959\n",
            "\n",
            "15:58:31 | time:26989s total_exs:693000 total_steps:43312 epochs:2.79 time_left:2040s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   154.7     1 719.6  2715   .4671      64.72 30.19  304             65536  7.819    .6383 28.92 2.457 6.93e-06 231.4 873.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.67      .4784         0                39432  951 3588 1.887\n",
            "\n",
            "15:58:41 | time:26999s total_exs:693288 total_steps:43330 epochs:2.79 time_left:2028s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.7     1   725  2541   .4861      73.11 28.04  288             65536  7.012    .6629 34.67 2.485 6.93e-06 274.2 960.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472      .4028   12      .4725         0                39450 999.2 3502 1.753\n",
            "\n",
            "15:58:51 | time:27009s total_exs:693592 total_steps:43349 epochs:2.79 time_left:2016s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.3     1 731.6  2714   .5263      73.86 29.68  304             65536  7.224    .6559 30.89 2.493 6.93e-06 247.1 916.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.1      .4729         0                39469 978.7 3631 1.856\n",
            "\n",
            "15:59:01 | time:27020s total_exs:693904 total_steps:43369 epochs:2.79 time_left:2004s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.1     1 751.5  2905   .5160      79.17 30.92  312             65536  7.484    .6559 30.18 2.445 6.93e-06 241.4 933.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.53      .4780         0                39488 992.9 3838 1.915\n",
            "\n",
            "15:59:11 | time:27030s total_exs:694208 total_steps:43388 epochs:2.79 time_left:1992s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.2     1 739.4  2791   .5099      72.77  30.2  304             65536  7.449    .6629 30.23  2.43 6.93e-06 241.8 912.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.35      .4806         0                39507 981.2 3704 1.93\n",
            "\n",
            "15:59:21 | time:27040s total_exs:694504 total_steps:43406 epochs:2.80 time_left:1981s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.1     1 764.4  2777   .5405       85.6 29.06  296             65536  7.336    .6559 31.81 2.465 6.93e-06 254.5 924.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.76      .4699         0                39526 1019 3701 1.879\n",
            "\n",
            "15:59:32 | time:27050s total_exs:694792 total_steps:43424 epochs:2.80 time_left:1969s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     174     1 736.6  2642   .5278       81.9 28.69  288             65536   7.42    .6497 31.17 2.449 6.93e-06 249.4 894.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.58      .4757         0                39544 985.9 3536 1.794\n",
            "\n",
            "15:59:42 | time:27060s total_exs:695080 total_steps:43442 epochs:2.80 time_left:1958s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     166     1 742.2  2659   .5069      73.18 28.66  288             65536  7.341    .6440 32.53 2.394 6.93e-06 260.2 932.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.96      .4851         0                39562 1002 3591 1.792\n",
            "\n",
            "15:59:52 | time:27070s total_exs:695384 total_steps:43461 epochs:2.80 time_left:1946s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.8     1 745.5  2808   .5033      73.62 30.13  304             65536  7.767    .6271 30.35 2.509 6.93e-06 242.8 914.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.29      .4769         0                39581 988.3 3722 1.884\n",
            "\n",
            "16:00:02 | time:27080s total_exs:695688 total_steps:43480 epochs:2.80 time_left:1934s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   158.4     1 710.5  2631   .4967      69.57 29.63  304             65536  7.517    .6629 29.48 2.445 6.93e-06 235.8 873.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.53      .4737         0                39600 946.3 3505 1.853\n",
            "\n",
            "16:00:12 | time:27090s total_exs:695976 total_steps:43498 epochs:2.80 time_left:1923s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.8     1 719.3  2583   .4792      72.88 28.73  288             65536  7.308    .6629 33.32  2.49 6.93e-06 266.6 957.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.07      .4678         0                39618 985.8 3541 1.797\n",
            "\n",
            "16:00:22 | time:27100s total_exs:696280 total_steps:43517 epochs:2.80 time_left:1911s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.1     1 724.9  2723   .4901      70.47 30.05  304             65536   7.51    .6559 29.46 2.377 6.93e-06 235.7 885.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.77      .4876         0                39637 960.6 3608 1.879\n",
            "\n",
            "16:00:32 | time:27111s total_exs:696600 total_steps:43537 epochs:2.80 time_left:1898s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.5     1 733.4  2885   .4813      75.87 31.47  320             65536  7.799    .6440 29.14  2.46 6.93e-06 233.1 917.1   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.7      .4797   .003125                39657 966.5 3802 1.968\n",
            "\n",
            "16:00:42 | time:27121s total_exs:696904 total_steps:43556 epochs:2.80 time_left:1886s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.2     1 736.9  2775   .5033      73.05 30.13  304             65536  7.373    .6383 30.95 2.382 6.93e-06 247.6 932.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.83      .4882         0                39676 984.5 3708 1.884\n",
            "\n",
            "16:00:53 | time:27131s total_exs:697208 total_steps:43575 epochs:2.81 time_left:1874s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.9     1 756.1  2808   .5493      87.39  29.7  304             65536  7.451    .6497 31.33 2.481 6.93e-06 250.6 930.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.96      .4743         0                39695 1007 3738 1.857\n",
            "\n",
            "16:01:03 | time:27141s total_exs:697512 total_steps:43594 epochs:2.81 time_left:1862s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.2     1 721.4  2719   .4934      76.04 30.15  304             65536  7.323    .6383 31.56 2.489 6.93e-06 252.4 951.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.05      .4713         0                39714 973.9 3671 1.885\n",
            "\n",
            "16:01:13 | time:27152s total_exs:697800 total_steps:43612 epochs:2.81 time_left:1851s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.9     1 743.6  2576   .5174      78.91 27.71  288             65536  7.214    .6629 31.72 2.406 6.93e-06 253.8 879.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.09      .4825         0                39732 997.4 3455 1.733\n",
            "\n",
            "16:01:23 | time:27162s total_exs:698104 total_steps:43631 epochs:2.81 time_left:1839s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.3     1 769.7  2924   .5362      80.11 30.39  304             65536  7.398    .6497 30.46 2.384 6.93e-06 243.7 925.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 10.85      .4924         0                39751 1013 3850  1.9\n",
            "\n",
            "16:01:34 | time:27172s total_exs:698408 total_steps:43650 epochs:2.81 time_left:1827s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.7     1 723.8  2646   .5099      72.22 29.24  304             65536  7.586    .6559 30.29 2.428 6.93e-06 242.3 885.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.34      .4789         0                39770 966.1 3531 1.829\n",
            "\n",
            "16:01:44 | time:27182s total_exs:698696 total_steps:43668 epochs:2.81 time_left:1816s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.9     1 761.4  2674   .5451      79.76  28.1  288             65536  7.247    .6629 31.99 2.414 6.93e-06 253.9 891.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006944      .2535 11.18      .4788         0                39788 1015 3566 1.757\n",
            "\n",
            "16:01:54 | time:27192s total_exs:699016 total_steps:43688 epochs:2.81 time_left:1803s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.9     1 746.8  2938   .5062      76.53 31.47  320             65536  7.494    .6327 27.57 2.363 6.93e-06 220.6 867.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.62      .4934         0                39808 967.4 3806 1.968\n",
            "\n",
            "16:02:04 | time:27203s total_exs:699320 total_steps:43707 epochs:2.81 time_left:1791s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.9     1 743.9  2771   .4901      75.88  29.8  304             65536  7.453    .6629 29.77 2.352 6.93e-06 237.3 883.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .1053 10.5      .4869         0                39827 981.2 3655 1.863\n",
            "\n",
            "16:02:15 | time:27213s total_exs:699640 total_steps:43727 epochs:2.82 time_left:1779s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.1     1 732.1  2860   .5125      79.57 31.25  320             65536  7.389    .6629 28.97 2.342 6.93e-06 231.7 904.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003125     .00625 10.4      .5092         0                39847 963.8 3765 1.953\n",
            "\n",
            "16:02:25 | time:27223s total_exs:699928 total_steps:43745 epochs:2.82 time_left:1768s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.3     1 737.1  2639   .4965      75.19 28.64  288             65536  7.507    .6629 31.33 2.455 6.93e-06 249.4 892.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472      .1562 11.65      .4779         0                39865 986.5 3532 1.791\n",
            "\n",
            "16:02:35 | time:27233s total_exs:700232 total_steps:43764 epochs:2.82 time_left:1756s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.3     1 753.4  2859   .5197      79.14 30.36  304             65536  7.377    .6440 31.48 2.432 6.93e-06 251.9 955.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.38      .4840         0                39884 1005 3815 1.898\n",
            "\n",
            "16:02:45 | time:27243s total_exs:700544 total_steps:43784 epochs:2.82 time_left:1743s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.4     1   757  2918   .5256      77.78 30.84  312             65536  7.252    .6383 30.53 2.452 6.93e-06 244.2 941.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 11.61      .4786         0                39903 1001 3860  1.9\n",
            "\n",
            "16:02:55 | time:27253s total_exs:700840 total_steps:43802 epochs:2.82 time_left:1732s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   162.9     1 733.3  2677   .5101      71.23 29.21  296             65536  7.379    .6629 30.75 2.366 6.93e-06 245.7   897   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .03716 10.66      .4954         0                39922  979 3574 1.888\n",
            "\n",
            "16:03:05 | time:27263s total_exs:701136 total_steps:43821 epochs:2.82 time_left:1720s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.8     1 757.6  2789   .5439      84.07 29.45  296             65536  7.259    .6629 31.45 2.447 6.93e-06 251.4 925.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .02365 11.55      .4749         0                39940 1009 3714 1.819\n",
            "\n",
            "16:03:15 | time:27274s total_exs:701448 total_steps:43840 epochs:2.82 time_left:1708s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.2     1 759.3  2860   .5192      78.29 30.13  312             65536  7.561    .6383  30.1 2.368 6.93e-06 240.8 906.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 10.67      .4903         0                39960 1000 3767 1.95\n",
            "\n",
            "16:03:26 | time:27284s total_exs:701752 total_steps:43859 epochs:2.82 time_left:1696s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   157.5     1 735.8  2688   .5066      65.57 29.23  304             65536  7.433    .6629 30.34 2.419 6.93e-06   241 880.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579      .2204 11.23      .4836         0                39979 976.8 3569 1.828\n",
            "\n",
            "16:03:36 | time:27294s total_exs:702056 total_steps:43878 epochs:2.83 time_left:1684s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.6     1 762.3  2892   .5559      82.35 30.35  304             65536  7.588    .6327 30.02 2.446 6.93e-06 240.2 911.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.54      .4761         0                39998 1002 3803 1.898\n",
            "\n",
            "16:03:46 | time:27304s total_exs:702368 total_steps:43898 epochs:2.83 time_left:1672s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   152.2     1 700.5  2705   .4423      64.64  30.9  312             65536  7.616    .6327 28.95 2.433 6.93e-06 231.6 894.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.39      .4815         0                40017 932.1 3600 1.91\n",
            "\n",
            "16:03:54 | Overflow: setting loss scale to 32768.0\n",
            "16:03:56 | time:27314s total_exs:702656 total_steps:43916 epochs:2.83 time_left:1661s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.6 .9444 737.6  2655   .5243      79.38  28.8  288             56434  6.612    .6629 33.22 2.427 6.93e-06 265.2 954.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .06597 11.33      .4780         0                40035 1003 3610 1.841\n",
            "\n",
            "16:04:06 | time:27324s total_exs:702960 total_steps:43935 epochs:2.83 time_left:1649s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.6     1 731.8  2744   .5329      76.16 29.99  304             32768  7.357    .6440 31.25 2.482 6.93e-06   250 937.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.96      .4707         0                40054 981.8 3681 1.92\n",
            "\n",
            "16:04:16 | time:27335s total_exs:703272 total_steps:43954 epochs:2.83 time_left:1636s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.6     1 738.7  2803   .4936      77.21 30.36  312             32768  7.309    .6383 30.24 2.365 6.93e-06 241.9 918.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.64      .4919         0                40074 980.6 3721 1.966\n",
            "\n",
            "16:04:27 | time:27345s total_exs:703560 total_steps:43972 epochs:2.83 time_left:1625s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     174     1 719.3  2516   .4896      84.12 27.98  288             32768  7.469    .6629 33.45 2.474 6.93e-06 260.2 910.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .006944      .9167 11.87      .4714         0                40092 979.5 3426 1.75\n",
            "\n",
            "16:04:37 | time:27355s total_exs:703880 total_steps:43992 epochs:2.83 time_left:1613s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.5     1 763.4  2972   .5375      77.09 31.14  320             32768  7.346    .6629 30.39 2.409 6.93e-06 242.6 944.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003125     .06875 11.12      .4798         0                40112 1006 3916 1.947\n",
            "\n",
            "16:04:47 | time:27366s total_exs:704200 total_steps:44012 epochs:2.83 time_left:1600s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.9     1 753.9  2913   .5344      69.63  30.9  320             32768  7.788    .6629 28.34 2.436 6.93e-06 226.4 874.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003125     .04063 11.42      .4743         0                40132 980.3 3787 1.932\n",
            "\n",
            "16:04:57 | time:27376s total_exs:704488 total_steps:44030 epochs:2.84 time_left:1589s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.5     1 735.9  2643   .5035       82.5 28.74  288             32768  7.176    .6629 32.12 2.455 6.93e-06 256.4 920.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472     .06944 11.65      .4762         0                40150 992.3 3564 1.797\n",
            "\n",
            "16:05:08 | time:27386s total_exs:704784 total_steps:44049 epochs:2.84 time_left:1577s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.7     1 726.6  2660   .4932      77.85 29.29  296             32768  7.401    .6629 31.73 2.427 6.93e-06 253.2 927.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .08108 11.32      .4833         0                40168 979.8 3587 1.805\n",
            "\n",
            "16:05:18 | time:27396s total_exs:705080 total_steps:44067 epochs:2.84 time_left:1566s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.8     1 779.2  2825   .5642      80.36 29.01  296             32768  7.375    .6440 31.94 2.435 6.93e-06 255.5 926.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.41      .4814         0                40187 1035 3752 1.877\n",
            "\n",
            "16:05:28 | time:27406s total_exs:705368 total_steps:44085 epochs:2.84 time_left:1554s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.6     1 720.5  2532   .4722      75.51 28.11  288             32768  7.256    .6629  33.3 2.495 6.93e-06 262.6 922.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472      .4688 12.12      .4719         0                40205 983.1 3455 1.758\n",
            "\n",
            "16:05:38 | time:27417s total_exs:705664 total_steps:44104 epochs:2.84 time_left:1543s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.2     1   745  2721   .5000       81.1 29.22  296             32768  7.418    .6629 32.43 2.399 6.93e-06 258.1 942.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378      .1723 11.01      .4824         0                40223 1003 3664 1.805\n",
            "\n",
            "16:05:48 | time:27427s total_exs:705960 total_steps:44122 epochs:2.84 time_left:1531s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.3     1 735.4  2654   .5203      79.41 28.88  296             32768  7.365    .6629 31.05 2.415 6.93e-06 248.4 896.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.19      .4844         0                40242 983.8 3551 1.872\n",
            "\n",
            "16:05:59 | time:27437s total_exs:706248 total_steps:44140 epochs:2.84 time_left:1520s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   152.5     1 682.6  2418   .4306      67.13 28.34  288             32768  7.618    .6629 30.55 2.432 6.93e-06 243.7 863.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006944     .09028 11.38      .4794         0                40260 926.2 3281 1.772\n",
            "\n",
            "16:06:09 | time:27447s total_exs:706552 total_steps:44159 epochs:2.84 time_left:1508s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.9     1 735.4  2724   .5132      77.97 29.63  304             32768  7.384    .6629 30.98 2.417 6.93e-06 247.7 917.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .02303 11.21      .4834         0                40279 983.1 3642 1.853\n",
            "\n",
            "16:06:19 | time:27457s total_exs:706856 total_steps:44178 epochs:2.84 time_left:1496s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     171     1 738.1  2757   .5230      78.79 29.88  304             32768   7.51    .6629 30.07 2.428 6.93e-06 240.4 897.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .02303 11.34      .4770         0                40298 978.5 3655 1.868\n",
            "\n",
            "16:06:29 | time:27468s total_exs:707160 total_steps:44197 epochs:2.85 time_left:1484s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.4     1 758.1  2863   .5263      76.67 30.21  304             32768  7.426    .6497 29.84 2.403 6.93e-06 238.7 901.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.06      .4827         0                40317 996.8 3764 1.888\n",
            "\n",
            "16:06:39 | time:27478s total_exs:707464 total_steps:44216 epochs:2.85 time_left:1472s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.9     1   767  2856   .5526      85.99 29.79  304             32768  7.282    .6328  31.6 2.438 6.93e-06 252.8 941.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.45      .4738         0                40336 1020 3798 1.863\n",
            "\n",
            "16:06:50 | time:27488s total_exs:707784 total_steps:44236 epochs:2.85 time_left:1460s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.7     1 763.1  2954   .5437      78.35 30.96  320             32768  7.554    .6327 28.67 2.488 6.93e-06 229.4 887.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.04      .4713         0                40356 992.5 3841 1.936\n",
            "\n",
            "16:07:00 | time:27498s total_exs:708088 total_steps:44255 epochs:2.85 time_left:1448s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     169     1 730.4  2677   .4901      77.75 29.32  304             32768  7.471    .6629 31.66 2.439 6.93e-06 253.3 928.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.46      .4812         0                40375 983.7 3606 1.834\n",
            "\n",
            "16:07:10 | time:27509s total_exs:708392 total_steps:44274 epochs:2.85 time_left:1436s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     164     1 712.3  2675   .4605         75 30.04  304             32768  7.562    .6630 30.18 2.421 6.93e-06 240.5   903   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .1184 11.26      .4756         0                40394 952.7 3578 1.879\n",
            "\n",
            "16:07:20 | time:27519s total_exs:708696 total_steps:44293 epochs:2.85 time_left:1424s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.8     1 749.3  2807   .5230      78.12 29.97  304             32768  7.266    .6630 30.44 2.352 6.93e-06 243.3 911.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .02961 10.51      .5027         0                40413 992.6 3719 1.874\n",
            "\n",
            "16:07:31 | time:27529s total_exs:708984 total_steps:44311 epochs:2.85 time_left:1413s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.3     1 744.3  2611   .5069      78.26 28.06  288             32768   7.22    .6629 32.32 2.432 6.93e-06 258.4 906.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006944     .02083 11.38      .4834         0                40431 1003 3517 1.755\n",
            "\n",
            "16:07:41 | time:27539s total_exs:709280 total_steps:44330 epochs:2.85 time_left:1401s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.6     1 737.4  2697   .5000      77.41 29.26  296             32768  7.273    .6629 31.58 2.376 6.93e-06 252.2 922.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378     .05068 10.76      .4830         0                40449 989.6 3620 1.798\n",
            "\n",
            "16:07:51 | time:27549s total_exs:709584 total_steps:44349 epochs:2.86 time_left:1390s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     167     1 735.3  2775   .5197      75.13 30.19  304             32768  7.406    .6630 30.54 2.406 6.93e-06 242.4 914.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006579      .2401 11.09      .4852         0                40468 977.7 3689 1.927\n",
            "\n",
            "16:08:01 | time:27559s total_exs:709888 total_steps:44368 epochs:2.86 time_left:1378s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.7     1 751.8  2828   .5263      78.77  30.1  304             32768  7.278    .6559 31.03 2.406 6.93e-06 248.3   934   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "         0          0 11.09      .4838         0                40487 1000 3762 1.92\n",
            "\n",
            "16:08:11 | time:27569s total_exs:710184 total_steps:44386 epochs:2.86 time_left:1366s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.6     1 767.2  2826   .5507      77.73 29.47  296             32768  7.684    .6630  29.8 2.442 6.93e-06 238.4 877.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378    .006757 11.49      .4765         0                40506 1006 3704 1.906\n",
            "\n",
            "16:08:21 | time:27580s total_exs:710480 total_steps:44405 epochs:2.86 time_left:1355s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.2     1 761.9  2794   .5304      86.98 29.34  296             32768  7.252    .6630 31.78 2.439 6.93e-06 254.2 932.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.47      .4785         0                40524 1016 3726 1.803\n",
            "\n",
            "16:08:31 | time:27590s total_exs:710792 total_steps:44424 epochs:2.86 time_left:1342s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.5     1 759.1  2882   .5321       85.6 30.37  312             32768  7.629    .6383 29.71  2.45 6.93e-06 237.7 902.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.58      .4812         0                40544 996.8 3784 1.96\n",
            "\n",
            "16:08:42 | time:27600s total_exs:711104 total_steps:44444 epochs:2.86 time_left:1330s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.2     1 757.4  2922   .5481      74.51 30.86  312             32768  7.591    .6559 29.09 2.369 6.93e-06 232.7 897.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.69      .4913         0                40563 990.1 3819 1.901\n",
            "\n",
            "16:08:52 | time:27610s total_exs:711408 total_steps:44463 epochs:2.86 time_left:1318s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.8     1 755.9  2852   .5296      83.33 30.18  304             32768  7.495    .6559 29.45 2.395 6.93e-06 235.6   889   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.96      .4873         0                40582 991.6 3741 1.922\n",
            "\n",
            "16:09:02 | time:27620s total_exs:711720 total_steps:44482 epochs:2.86 time_left:1306s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   191.4     1 783.8  2980   .5897      93.47 30.41  312             32768  7.585    .6383 29.57 2.443 6.93e-06 236.5 899.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.5      .4851         0                40602 1020 3879 1.964\n",
            "\n",
            "16:09:12 | time:27630s total_exs:712024 total_steps:44501 epochs:2.87 time_left:1294s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.3     1 745.8  2795   .5296      78.06 29.98  304             32768  7.741    .6559 28.72  2.43 6.93e-06 229.7   861   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.36      .4821         0                40621 975.5 3656 1.874\n",
            "\n",
            "16:09:22 | time:27641s total_exs:712336 total_steps:44521 epochs:2.87 time_left:1282s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.9     1 724.6  2787   .5000      74.28 30.77  312             32768  7.162    .6383  31.5 2.427 6.93e-06   252 969.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.32      .4840         0                40640 976.6 3756 1.902\n",
            "\n",
            "16:09:32 | time:27651s total_exs:712632 total_steps:44539 epochs:2.87 time_left:1270s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.1     1 759.5  2783   .5270      84.11 29.31  296             32768  7.161    .6559 31.64 2.385 6.93e-06 253.1 927.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.86      .4894         0                40659 1013 3710 1.899\n",
            "\n",
            "16:09:42 | time:27661s total_exs:712912 total_steps:44557 epochs:2.87 time_left:1260s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.3     1 735.3  2575   .5036      89.34 28.02  280             32768  7.181    .6629 33.66 2.441 6.93e-06 265.1 928.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003571      .5250 11.49      .4795         0                40676 1000 3504 1.728\n",
            "\n",
            "16:09:53 | time:27671s total_exs:713208 total_steps:44575 epochs:2.87 time_left:1248s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.1     1 760.5  2738   .5338      80.08  28.8  296             32768  7.329    .6630 32.31 2.371 6.93e-06 257.5 926.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378      .1284 10.7      .4929         0                40695 1018 3665 1.867\n",
            "\n",
            "16:10:03 | time:27681s total_exs:713512 total_steps:44594 epochs:2.87 time_left:1236s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   156.3     1 716.4  2659   .4901      66.77  29.7  304             32768  7.478    .6630 30.68 2.436 6.93e-06 245.2 910.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .03289 11.43      .4766         0                40714 961.7 3570 1.857\n",
            "\n",
            "16:10:13 | time:27691s total_exs:713816 total_steps:44613 epochs:2.87 time_left:1224s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.4     1 751.1  2819   .5395      84.55 30.03  304             32768  7.426    .6328 30.16 2.442 6.93e-06 241.3 905.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.5      .4805   .003289                40733 992.4 3725 1.878\n",
            "\n",
            "16:10:23 | time:27702s total_exs:714112 total_steps:44632 epochs:2.87 time_left:1213s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.9     1 743.7  2726   .5304      89.93 29.32  296             32768  7.454    .6630  31.2 2.464 6.93e-06 248.4 910.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .006757      .1486 11.75      .4795         0                40751 992.1 3636 1.81\n",
            "\n",
            "16:10:33 | time:27712s total_exs:714408 total_steps:44650 epochs:2.88 time_left:1201s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.2     1 740.9  2698   .5101      81.55 29.13  296             32768  7.358    .6441 31.32  2.44 6.93e-06 250.5 912.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.48      .4748         0                40770 991.4 3610 1.888\n",
            "\n",
            "16:10:44 | time:27722s total_exs:714712 total_steps:44669 epochs:2.88 time_left:1189s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.7     1 733.7  2745   .4868         75 29.93  304             32768  7.395    .6328 30.86 2.379 6.93e-06 246.9 923.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.79      .4877         0                40789 980.5 3668 1.872\n",
            "\n",
            "16:10:54 | time:27732s total_exs:715016 total_steps:44688 epochs:2.88 time_left:1177s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     165     1 741.8  2817   .5197      72.27 30.38  304             32768  7.355    .6559 29.71 2.359 6.93e-06 237.7 902.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 10.58      .4947         0                40808 979.4 3720  1.9\n",
            "\n",
            "16:11:04 | time:27742s total_exs:715320 total_steps:44707 epochs:2.88 time_left:1166s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.1     1 740.9  2737   .5066      77.54 29.55  304             32768  7.473    .6498 30.84 2.408 6.93e-06 246.7 911.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.11      .4818         0                40827 987.6 3648 1.848\n",
            "\n",
            "16:11:14 | time:27752s total_exs:715624 total_steps:44726 epochs:2.88 time_left:1154s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.8     1 741.7  2771   .5263      82.07 29.89  304             32768  7.238    .6327 31.59 2.466 6.93e-06 252.7 944.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.78      .4758         0                40846 994.4 3716 1.869\n",
            "\n",
            "16:11:24 | time:27763s total_exs:715928 total_steps:44745 epochs:2.88 time_left:1142s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.1     1 747.4  2828   .5230      76.69 30.27  304             32768  7.396    .6630 30.04 2.435 6.93e-06 238.9 903.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .1809 11.42      .4822         0                40865 986.3 3732 1.893\n",
            "\n",
            "16:11:34 | time:27773s total_exs:716224 total_steps:44764 epochs:2.88 time_left:1130s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     162     1 741.6  2734   .5068      69.35 29.49  296             32768  7.326    .6630 31.63 2.424 6.93e-06 251.9 928.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378      .1419 11.29      .4852         0                40883 993.5 3662 1.813\n",
            "\n",
            "16:11:44 | time:27783s total_exs:716528 total_steps:44783 epochs:2.88 time_left:1118s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.2     1 747.9  2834   .5197      80.73 30.31  304             32768  7.614    .6384 29.29 2.448 6.93e-06 234.3 887.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.57      .4788   .003289                40902 982.2 3722 1.933\n",
            "\n",
            "16:11:54 | time:27793s total_exs:716816 total_steps:44801 epochs:2.89 time_left:1107s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.2     1 732.1  2637   .4965      76.67 28.81  288             32768    7.3    .6559  30.9 2.409 6.93e-06 247.2 890.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.12      .4854         0                40920 979.3 3527 1.839\n",
            "\n",
            "16:12:04 | time:27803s total_exs:717128 total_steps:44820 epochs:2.89 time_left:1095s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   195.2     1 773.3  2971   .5545      98.56 30.74  312             32768  7.606    .6383 29.99 2.454 6.93e-06 239.9 921.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.63      .4871         0                40940 1013 3893 1.986\n",
            "\n",
            "16:12:15 | time:27813s total_exs:717424 total_steps:44839 epochs:2.89 time_left:1084s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.3     1 730.5  2677   .5068      85.03 29.31  296             32768  7.403    .6383 33.62 2.533 6.93e-06   269 985.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.59      .4648         0                40958 999.5 3663 1.807\n",
            "\n",
            "16:12:25 | time:27823s total_exs:717704 total_steps:44856 epochs:2.89 time_left:1073s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.8     1 736.4  2528   .5179      74.79 27.47  280             32768  7.106    .6630 33.52 2.431 6.93e-06 266.5 915.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .007143      .2107 11.37      .4770         0                40976 1003 3443 1.781\n",
            "\n",
            "16:12:35 | time:27833s total_exs:717992 total_steps:44874 epochs:2.89 time_left:1061s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.8     1 743.4  2675   .4896      85.88 28.79  288             32768  7.313    .6559 31.69 2.435 6.93e-06 253.5 912.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.42      .4778         0                40994  997 3587 1.799\n",
            "\n",
            "16:12:45 | time:27843s total_exs:718296 total_steps:44893 epochs:2.89 time_left:1050s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     171     1 743.2  2805   .5263      78.08 30.19  304             32768  7.625    .6559 29.43 2.392 6.93e-06 235.5 888.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.93      .4892         0                41013 978.7 3693 1.887\n",
            "\n",
            "16:12:55 | time:27853s total_exs:718584 total_steps:44911 epochs:2.89 time_left:1038s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.7     1 739.4  2621   .5000      86.32 28.36  288             32768  7.303    .6440 32.93 2.461 6.93e-06 263.4 933.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.72      .4759         0                41031 1003 3555 1.773\n",
            "\n",
            "16:13:05 | time:27864s total_exs:718896 total_steps:44931 epochs:2.89 time_left:1026s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   161.4     1 715.3  2746   .4776      72.02 30.71  312             32768  7.284    .6630 29.85 2.426 6.93e-06 238.7 916.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003205    .009615 11.31      .4839         0                41050  954 3662 1.899\n",
            "\n",
            "16:13:15 | time:27874s total_exs:719192 total_steps:44949 epochs:2.89 time_left:1015s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.6     1 727.5  2624   .4899      75.62 28.85  296             32768  7.644    .6630 29.88 2.433 6.93e-06   239   862   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.39      .4802         0                41069 966.5 3486 1.869\n",
            "\n",
            "16:13:26 | time:27884s total_exs:719488 total_steps:44968 epochs:2.90 time_left:1003s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   154.1     1 719.1  2640   .4696      64.24 29.37  296             32768  7.399    .6630 31.86 2.426 6.93e-06 253.1 929.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006757      .2230 11.31      .4814         0                41087 972.2 3569 1.826\n",
            "\n",
            "16:13:36 | time:27894s total_exs:719784 total_steps:44986 epochs:2.90 time_left:992s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   165.6     1 715.7  2623   .4865      76.14 29.32  296             32768  7.378    .6272 30.09 2.425 6.93e-06 240.7 882.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.3      .4782         0                41106 956.4 3505 1.908\n",
            "\n",
            "16:13:46 | time:27904s total_exs:720088 total_steps:45005 epochs:2.90 time_left:980s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.1     1 758.3  2842   .5395      74.36 29.98  304             32768  7.511    .6630 29.04 2.359 6.93e-06 232.3 870.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.58      .4845         0                41125 990.6 3713 1.875\n",
            "\n",
            "16:13:56 | time:27914s total_exs:720368 total_steps:45023 epochs:2.90 time_left:969s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   190.7     1 773.3  2699   .5714      94.06 27.92  280             32768  7.187    .6630 33.31 2.428 6.93e-06 266.4 929.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003571     .01786 11.33      .4832         0                41142 1040 3629 1.723\n",
            "\n",
            "16:14:06 | time:27924s total_exs:720656 total_steps:45041 epochs:2.90 time_left:958s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   154.7     1   707  2540   .4549      66.33 28.74  288             32768  7.826    .6630 29.16 2.486 6.93e-06 233.3   838   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.01      .4752         0                41160 940.3 3378 1.839\n",
            "\n",
            "16:14:16 | time:27934s total_exs:720952 total_steps:45059 epochs:2.90 time_left:946s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   190.1     1 753.2  2774   .5304       95.9 29.47  296             32768  7.256    .6498 31.61 2.381 6.93e-06 252.9 931.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.81      .4869         0                41179 1006 3706 1.907\n",
            "\n",
            "16:14:26 | time:27944s total_exs:721264 total_steps:45079 epochs:2.90 time_left:934s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.6     1 767.3  2983   .5385      78.66  31.1  312             32768  7.613    .6272 29.53  2.44 6.93e-06 236.2 918.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.47      .4830         0                41198 1004 3901 1.916\n",
            "\n",
            "16:14:36 | time:27955s total_exs:721560 total_steps:45097 epochs:2.90 time_left:923s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   193.2     1 766.6  2791   .5608      97.38 29.13  296             32768  7.702    .6630 30.28 2.434 6.93e-06 241.9 880.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .03378 11.4      .4830         0                41217 1009 3672 1.883\n",
            "\n",
            "16:14:46 | time:27965s total_exs:721856 total_steps:45116 epochs:2.91 time_left:911s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.3     1 776.7  2860   .5845       85.2 29.46  296             32768   7.59    .6441 30.05 2.511 6.93e-06 240.4 885.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.31      .4688         0                41235 1017 3745 1.812\n",
            "\n",
            "16:14:56 | time:27975s total_exs:722144 total_steps:45134 epochs:2.91 time_left:900s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   172.8     1   758  2705   .5347      78.03 28.55  288             32768  7.466    .6630 30.77 2.365 6.93e-06   246 877.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .02431 10.65      .4886         0                41253 1004 3583 1.821\n",
            "\n",
            "16:15:07 | time:27985s total_exs:722440 total_steps:45152 epochs:2.91 time_left:888s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   155.3     1 701.9  2555   .4561      67.58 29.13  296             32768  7.596    .6630 31.04 2.423 6.93e-06 248.2 903.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378    .006757 11.27      .4815         0                41272 950.1 3459 1.885\n",
            "\n",
            "16:15:17 | time:27995s total_exs:722744 total_steps:45171 epochs:2.91 time_left:877s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   153.5     1 705.5  2617   .4572      65.34 29.67  304             32768  7.542    .6328 30.64 2.464 6.93e-06 245.2 909.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.75      .4783         0                41291 950.6 3526 1.855\n",
            "\n",
            "16:15:27 | time:28005s total_exs:723032 total_steps:45189 epochs:2.91 time_left:865s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.2     1   750  2682   .5139      77.43 28.61  288             32768  7.229    .6630 31.24 2.422 6.93e-06 249.9 893.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.27      .4828         0                41309 999.9 3576 1.789\n",
            "\n",
            "16:15:37 | time:28015s total_exs:723320 total_steps:45207 epochs:2.91 time_left:854s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     170     1 735.4  2619   .5139      78.05 28.49  288             32768  7.312    .6630 32.16 2.495 6.93e-06 256.9 914.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006944     .04167 12.12      .4676         0                41327 992.3 3534 1.781\n",
            "\n",
            "16:15:47 | time:28026s total_exs:723624 total_steps:45226 epochs:2.91 time_left:842s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     174     1 742.7  2771   .5230      81.13 29.84  304             32768  7.359    .6560 30.87  2.43 6.93e-06   247 921.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.36      .4797         0                41346 989.7 3692 1.866\n",
            "\n",
            "16:15:57 | time:28036s total_exs:723936 total_steps:45246 epochs:2.91 time_left:830s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   194.2     1 798.4  3081   .5897       94.4 30.87  312             32768  7.359    .6272 30.31 2.408 6.93e-06 242.5 935.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.11      .4911         0                41365 1041 4017 1.906\n",
            "\n",
            "16:16:07 | time:28046s total_exs:724224 total_steps:45264 epochs:2.91 time_left:819s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.5     1 736.5  2646   .5174      87.45 28.74  288             32768  7.205    .6560 32.91 2.501 6.93e-06 263.3   946   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.2      .4678         0                41383 999.8 3592 1.839\n",
            "\n",
            "16:16:18 | time:28056s total_exs:724512 total_steps:45282 epochs:2.92 time_left:808s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.6     1 729.5  2595   .5139      82.43 28.46  288             32768      7    .6441 33.28 2.382 6.93e-06 266.2 947.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.83      .4813         0                41401 995.8 3542 1.821\n",
            "\n",
            "16:16:28 | time:28066s total_exs:724808 total_steps:45300 epochs:2.92 time_left:796s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.8     1 750.8  2753   .5338      76.96 29.33  296             32768  7.348    .6630 30.97 2.455 6.93e-06 246.4 903.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003378      .1723 11.64      .4784         0                41420 997.2 3656 1.899\n",
            "\n",
            "16:16:38 | time:28076s total_exs:725104 total_steps:45319 epochs:2.92 time_left:785s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     187     1 749.5  2750   .5169      93.31 29.35  296             32768  7.212    .6560 32.44  2.43 6.93e-06 259.5 952.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.36      .4729         0                41438 1009 3702 1.811\n",
            "\n",
            "16:16:48 | time:28086s total_exs:725400 total_steps:45337 epochs:2.92 time_left:773s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     165     1 732.1  2624   .5068      73.52 28.67  296             32768   7.16    .6498 33.57 2.463 6.93e-06 268.6 962.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.74      .4792         0                41457 1001 3587 1.857\n",
            "\n",
            "16:16:58 | time:28097s total_exs:725704 total_steps:45356 epochs:2.92 time_left:762s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   167.4     1 745.3  2825   .5099      74.21 30.33  304             32768  7.613    .6630 29.79 2.427 6.93e-06 236.2 895.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .2697 11.32      .4773         0                41476 981.4 3721 1.896\n",
            "\n",
            "16:17:09 | time:28107s total_exs:726024 total_steps:45376 epochs:2.92 time_left:749s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.2     1 773.6  2992   .5687       84.5 30.94  320             32768  7.609    .6630 28.73 2.393 6.93e-06 228.2 882.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003125      .2000 10.95      .4846         0                41496 1002 3875 1.934\n",
            "\n",
            "16:17:19 | time:28117s total_exs:726328 total_steps:45395 epochs:2.92 time_left:737s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.8     1 749.7  2803   .5197       76.1 29.91  304             32768  7.873    .6559 29.14 2.455 6.93e-06 233.1 871.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 11.64      .4799         0                41515 982.8 3674 1.87\n",
            "\n",
            "16:17:29 | time:28127s total_exs:726616 total_steps:45413 epochs:2.92 time_left:726s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.3     1 747.6  2661   .5104      79.86 28.47  288             32768  7.222    .6630 31.99 2.359 6.93e-06 252.1 897.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "   .006944      .4722 10.58      .4938         0                41533 999.8 3558 1.78\n",
            "\n",
            "16:17:39 | time:28137s total_exs:726936 total_steps:45433 epochs:2.93 time_left:714s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.8     1   745  2906   .5094      75.63  31.2  320             32768  7.656    .6328 28.86 2.412 6.93e-06 230.9 900.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.16      .4892         0                41553 975.9 3806 1.951\n",
            "\n",
            "16:17:49 | time:28148s total_exs:727232 total_steps:45452 epochs:2.93 time_left:702s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.3     1 754.5  2783   .5405      79.97 29.51  296             32768  7.316    .6328 32.12 2.384 6.93e-06 256.9 947.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.85      .4918         0                41571 1011 3731 1.825\n",
            "\n",
            "16:17:59 | time:28158s total_exs:727528 total_steps:45470 epochs:2.93 time_left:691s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.7     1   733  2681   .5135      77.07 29.27  296             32768  7.375    .6328  30.9 2.425 6.93e-06 247.2 904.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.31      .4800         0                41590 980.2 3586 1.898\n",
            "\n",
            "16:18:10 | time:28168s total_exs:727848 total_steps:45490 epochs:2.93 time_left:678s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.4     1 728.2  2822   .4938      72.35    31  320             32768  7.735    .6272 29.11 2.458 6.93e-06 232.9 902.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.68      .4768         0                41610 961.1 3725 1.939\n",
            "\n",
            "16:18:20 | time:28178s total_exs:728160 total_steps:45510 epochs:2.93 time_left:666s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.7     1 768.5  2986   .5545      83.64 31.08  312             32768  7.598    .6328 29.82 2.402 6.93e-06 238.5 926.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.04      .4815         0                41629 1007 3913 1.925\n",
            "\n",
            "16:18:30 | time:28188s total_exs:728440 total_steps:45527 epochs:2.93 time_left:655s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   157.4     1 718.1  2481   .4786      67.59 27.64  280             32768  7.148    .6630  31.4 2.395 6.93e-06 249.6 862.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003571      .1964 10.96      .4855         0                41647 967.7 3343 1.795\n",
            "\n",
            "16:18:40 | time:28198s total_exs:728744 total_steps:45546 epochs:2.93 time_left:643s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     184     1 780.1  2890   .5757      86.51 29.64  304             32768  7.136    .6441 30.87 2.342 6.93e-06 246.9 914.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.4      .4937         0                41666 1027 3805 1.853\n",
            "\n",
            "16:18:50 | time:28209s total_exs:729048 total_steps:45565 epochs:2.93 time_left:632s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.8     1 751.3  2804   .5263      90.93 29.86  304             32768  7.597    .6630 29.41 2.411 6.93e-06   235 877.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289     .03618 11.15      .4835         0                41685 986.3 3681 1.867\n",
            "\n",
            "16:19:01 | time:28219s total_exs:729368 total_steps:45585 epochs:2.94 time_left:619s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.4     1 730.4  2810   .5125      84.09 30.78  320             32768  7.485    .6328 30.06 2.424 6.93e-06 240.4 925.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.29      .4831         0                41705 970.8 3736 1.925\n",
            "\n",
            "16:19:11 | time:28229s total_exs:729672 total_steps:45604 epochs:2.94 time_left:607s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.8     1 752.5  2810   .5164      74.69 29.87  304             32768  7.439    .6560 30.28 2.419 6.93e-06 242.2 904.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.24      .4862         0                41724 994.7 3715 1.868\n",
            "\n",
            "16:19:21 | time:28239s total_exs:729968 total_steps:45623 epochs:2.94 time_left:596s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.8     1 760.2  2787   .5270      82.73 29.33  296             32768  7.302    .6630  31.7 2.385 6.93e-06 253.5 929.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378    .006757 10.86      .4875         0                41742 1014 3716 1.823\n",
            "\n",
            "16:19:31 | time:28250s total_exs:730256 total_steps:45641 epochs:2.94 time_left:585s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     164     1 736.8  2621   .4861      71.92 28.46  288             32768  7.437    .6630 30.92 2.398 6.93e-06 247.4 879.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0   11      .4814         0                41760 984.1 3501 1.83\n",
            "\n",
            "16:19:41 | time:28260s total_exs:730536 total_steps:45658 epochs:2.94 time_left:574s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   166.5     1 724.4  2523   .5036      75.96 27.87  280             32768  7.163    .6441 32.36  2.47 6.93e-06 258.9 901.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.83      .4753         0                41778 983.3 3425 1.808\n",
            "\n",
            "16:19:51 | time:28270s total_exs:730840 total_steps:45677 epochs:2.94 time_left:562s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.5     1 742.2  2804   .5230      70.73 30.22  304             32768  7.424    .6384 29.01 2.341 6.93e-06 232.1 876.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "         0          0 10.39      .4973         0                41797 974.3 3681 1.89\n",
            "\n",
            "16:20:01 | time:28280s total_exs:731136 total_steps:45696 epochs:2.94 time_left:551s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   188.4     1 779.8  2875   .5642      90.89  29.5  296             32768   7.33    .6328 31.75 2.404 6.93e-06   254 936.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.07      .4862         0                41815 1034 3812 1.815\n",
            "\n",
            "16:20:12 | time:28290s total_exs:731432 total_steps:45714 epochs:2.94 time_left:539s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.9     1 755.5  2698   .5135      81.47 28.57  296             32768   7.42    .6630 30.69 2.419 6.93e-06 245.5 876.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.24      .4846         0                41834 1001 3575 1.847\n",
            "\n",
            "16:20:22 | time:28300s total_exs:731736 total_steps:45733 epochs:2.95 time_left:527s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   156.5     1 725.8  2739   .4901      65.77 30.19  304             32768  7.591    .6441 28.83 2.356 6.93e-06 230.6 870.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.55      .4901         0                41853 956.4 3609 1.888\n",
            "\n",
            "16:20:32 | time:28310s total_exs:732048 total_steps:45753 epochs:2.95 time_left:515s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     175     1 763.4  2965   .5449      79.57 31.07  312             32768  7.521    .6384 30.65 2.444 6.93e-06 245.2 952.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.52      .4727         0                41872 1009 3917 1.914\n",
            "\n",
            "16:20:42 | time:28320s total_exs:732344 total_steps:45771 epochs:2.95 time_left:504s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.1     1 736.1  2681   .5135       78.1 29.14  296             32768  7.678    .6560 30.36 2.443 6.93e-06 242.9 884.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.51      .4838         0                41891  979 3566 1.883\n",
            "\n",
            "16:20:52 | time:28331s total_exs:732648 total_steps:45790 epochs:2.95 time_left:492s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.9     1 765.4  2851   .5461      84.25  29.8  304             32768  7.352    .6383    30 2.397 6.93e-06   240 893.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.99      .4847         0                41910 1005 3745 1.863\n",
            "\n",
            "16:21:03 | time:28341s total_exs:732936 total_steps:45808 epochs:2.95 time_left:481s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.4     1 741.4  2585   .5104      76.76 27.89  288             32768  7.239    .6630 33.11 2.469 6.93e-06 260.3 907.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "    .01736      .5764 11.81      .4661         0                41928 1002 3493 1.744\n",
            "\n",
            "16:21:13 | time:28351s total_exs:733240 total_steps:45827 epochs:2.95 time_left:469s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   151.7     1 688.3  2524   .4474      65.65 29.33  304             32768  7.519    .6630 30.26 2.474 6.93e-06 242.1 887.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.87      .4716         0                41947 930.4 3411 1.834\n",
            "\n",
            "16:21:23 | time:28362s total_exs:733544 total_steps:45846 epochs:2.95 time_left:457s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.5     1 733.8  2746   .4934      71.78 29.94  304             32768  7.565    .6560 30.31 2.513 6.93e-06 242.4 907.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.35      .4728         0                41966 976.2 3654 1.872\n",
            "\n",
            "16:21:33 | time:28372s total_exs:733832 total_steps:45864 epochs:2.95 time_left:446s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     182     1 776.1  2764   .5590      84.94 28.49  288             32768  7.109    .6630 32.39 2.435 6.93e-06 259.1 922.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.42      .4821         0                41984 1035 3687 1.782\n",
            "\n",
            "16:21:44 | time:28382s total_exs:734136 total_steps:45883 epochs:2.95 time_left:434s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   176.5     1 745.3  2763   .5395       83.3 29.66  304             32768  7.508    .6384 30.05 2.437 6.93e-06 240.4 891.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.44      .4795         0                42003 985.7 3654 1.855\n",
            "\n",
            "16:21:54 | time:28392s total_exs:734440 total_steps:45902 epochs:2.96 time_left:423s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   171.7     1 725.6  2673   .5099      81.02 29.47  304             32768  7.454    .6630  30.6 2.416 6.93e-06 242.4 892.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006579      .3026 11.2      .4788         0                42022  968 3566 1.843\n",
            "\n",
            "16:22:04 | time:28402s total_exs:734744 total_steps:45921 epochs:2.96 time_left:411s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   169.5     1 744.7  2788   .5362      76.44 29.95  304             51739  7.482    .6384 30.27 2.385 6.93e-06 242.2 906.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.86      .4867         0                42041 986.9 3695 1.873\n",
            "\n",
            "16:22:14 | time:28413s total_exs:735048 total_steps:45940 epochs:2.96 time_left:399s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.5     1 762.6  2811   .5296      82.18 29.49  304             65536  7.447    .6328 31.68 2.482 6.93e-06 253.4 934.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.97      .4771   .003289                42060 1016 3746 1.844\n",
            "\n",
            "16:22:25 | time:28423s total_exs:735352 total_steps:45959 epochs:2.96 time_left:387s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     182     1 772.5  2864   .5625      85.48 29.66  304             65536  7.443    .6384 30.78 2.443 6.93e-06 246.3 912.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.51      .4810         0                42079 1019 3777 1.854\n",
            "\n",
            "16:22:35 | time:28433s total_exs:735672 total_steps:45979 epochs:2.96 time_left:375s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.5     1 757.9  2913   .5469      86.79 30.75  320             65536  7.171    .6384 30.67 2.367 6.93e-06 245.4 943.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.66      .4944         0                42099 1003 3856 1.923\n",
            "\n",
            "16:22:45 | time:28444s total_exs:735976 total_steps:45998 epochs:2.96 time_left:363s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   158.3     1 739.8  2776   .5099      65.83 30.01  304             65536  7.541    .6498 29.85 2.413 6.93e-06 238.8 895.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.17      .4818         0                42118 978.6 3672 1.877\n",
            "\n",
            "16:22:55 | time:28454s total_exs:736264 total_steps:46016 epochs:2.96 time_left:352s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   174.7     1 744.4  2652   .5278      81.63  28.5  288             65536  7.148    .6630 32.08 2.422 6.93e-06 256.1 912.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003472     .07292 11.26      .4793         0                42136 1000 3565 1.782\n",
            "\n",
            "16:23:05 | time:28464s total_exs:736584 total_steps:46036 epochs:2.96 time_left:340s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   164.8     1 751.1  2991   .5188      70.94 31.85  320             65536  7.642    .6220 27.74   2.4 6.93e-06 221.9 883.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.02      .4872         0                42156  973 3874 1.991\n",
            "\n",
            "16:23:16 | time:28474s total_exs:736872 total_steps:46054 epochs:2.97 time_left:329s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   163.9     1 733.5  2580   .4861      72.23 28.14  288             65536  7.365    .6630 32.55 2.502 6.93e-06 260.2 915.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003472     .03125 12.21      .4697         0                42174 993.7 3495 1.759\n",
            "\n",
            "16:23:26 | time:28484s total_exs:737176 total_steps:46073 epochs:2.97 time_left:317s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.2     1 749.6  2796   .5230      74.46 29.84  304             65536  7.326    .6441 30.48 2.394 6.93e-06 243.8 909.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.96      .4845         0                42193 993.4 3706 1.865\n",
            "\n",
            "16:23:36 | time:28494s total_exs:737480 total_steps:46092 epochs:2.97 time_left:305s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   180.2     1 758.3  2811   .5263      85.44 29.66  304             65536  7.257    .6630 32.12  2.36 6.93e-06   257 952.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.59      .4910         0                42212 1015 3764 1.854\n",
            "\n",
            "16:23:46 | time:28505s total_exs:737784 total_steps:46111 epochs:2.97 time_left:293s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.8     1 747.8  2747   .5296       86.3 29.38  304             65536  7.411    .6630 30.41 2.439 6.93e-06 243.1   893   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003289     .02303 11.46      .4820         0                42231  991 3640 1.837\n",
            "\n",
            "16:23:57 | time:28515s total_exs:738096 total_steps:46131 epochs:2.97 time_left:281s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.8     1 759.4  2946   .5385      75.88 31.04  312             65536  7.499    .6630 28.16 2.384 6.93e-06 225.2 873.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "    .00641    .009615 10.85      .4845         0                42250 984.6 3820 1.925\n",
            "\n",
            "16:24:07 | time:28525s total_exs:738408 total_steps:46150 epochs:2.97 time_left:269s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   160.5     1   719  2699   .5064      70.62 30.04  312             65536  7.415    .6384 30.46 2.442 6.93e-06 243.7 915.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.49      .4774         0                42270 962.7 3615 1.945\n",
            "\n",
            "16:24:17 | time:28535s total_exs:738720 total_steps:46170 epochs:2.97 time_left:257s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.5     1 765.5  2977   .5513      85.81 31.11  312             65536   7.36    .6328 31.02 2.365 6.93e-06 248.2 965.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 10.64      .4900         0                42289 1014 3942 1.917\n",
            "\n",
            "16:24:27 | time:28545s total_exs:739024 total_steps:46189 epochs:2.97 time_left:245s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   177.1     1 747.7  2842   .5362      83.64 30.41  304             65536  7.528    .6630 29.72 2.443 6.93e-06 235.5 895.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .003289      .2829 11.51      .4790         0                42308 983.2 3737 1.947\n",
            "\n",
            "16:24:37 | time:28556s total_exs:739320 total_steps:46207 epochs:2.98 time_left:234s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   182.3     1 767.5  2760   .5507      86.38 28.77  296             65536  7.109    .6630 31.64 2.381 6.93e-06 252.7 908.7   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .05068 10.81      .4875         0                42327 1020 3668 1.866\n",
            "\n",
            "16:24:47 | time:28566s total_exs:739632 total_steps:46227 epochs:2.98 time_left:222s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   156.3     1 710.4  2732   .4615      67.54 30.76  312             65536  7.423    .6384  30.2 2.382 6.93e-06 241.6   929   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.83      .4902         0                42346 951.9 3661 1.902\n",
            "\n",
            "16:24:58 | time:28576s total_exs:739928 total_steps:46245 epochs:2.98 time_left:210s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   185.8     1 781.7  2856   .5473      88.06 29.23  296             65536  7.346    .6384 31.72 2.428 6.93e-06 253.7 927.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.33      .4815         0                42365 1035 3784 1.894\n",
            "\n",
            "16:25:08 | time:28586s total_exs:740240 total_steps:46265 epochs:2.98 time_left:198s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.6     1 762.1  2938   .5288      78.36 30.84  312             65536  7.448    .6441 29.67 2.412 6.93e-06 237.3 915.1   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.16      .4889         0                42384 999.4 3853 1.907\n",
            "\n",
            "16:25:18 | time:28596s total_exs:740536 total_steps:46283 epochs:2.98 time_left:187s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   158.6     1 725.1  2601   .4966      67.96 28.69  296             65536  7.694    .6630 30.28 2.495 6.93e-06 242.2 868.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 12.12      .4715         0                42403 967.4 3469 1.858\n",
            "\n",
            "16:25:28 | time:28607s total_exs:740824 total_steps:46301 epochs:2.98 time_left:176s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   147.2     1 691.2  2466   .4306      60.76 28.54  288             65536  7.403    .6498 30.65 2.437 6.93e-06 245.2 874.9   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.44      .4811         0                42421 936.4 3341 1.785\n",
            "\n",
            "16:25:38 | time:28617s total_exs:741128 total_steps:46320 epochs:2.98 time_left:164s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     153     1 720.7  2663   .4704      62.97 29.56  304             65536  7.662    .6560 29.82 2.405 6.93e-06 238.6 881.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.08      .4882         0                42440 959.2 3544 1.848\n",
            "\n",
            "16:25:49 | time:28627s total_exs:741448 total_steps:46340 epochs:2.98 time_left:152s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   170.1     1   742  2888   .5188       77.3 31.13  320             65536  7.713    .6328 29.01 2.423 6.93e-06 232.1 903.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.27      .4789         0                42460 974.1 3791 1.947\n",
            "\n",
            "16:25:59 | time:28637s total_exs:741768 total_steps:46360 epochs:2.99 time_left:139s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.8     1 762.7  2995   .5281       78.5 31.41  320             65536  7.488    .6441 29.01 2.415 6.93e-06 232.1 911.4   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.18      .4892         0                42480 994.8 3906 1.964\n",
            "\n",
            "16:26:09 | time:28647s total_exs:742064 total_steps:46379 epochs:2.99 time_left:128s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.7     1 765.3  2816   .5439      84.07 29.44  296             65536  7.431    .6630  31.9  2.44 6.93e-06   255 938.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .006757     .02703 11.47      .4796         0                42498 1020 3755 1.814\n",
            "\n",
            "16:26:19 | time:28658s total_exs:742376 total_steps:46398 epochs:2.99 time_left:116s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.8     1 748.2  2824   .5353      85.24  30.2  312             65536  7.521    .6272 29.98  2.42 6.93e-06 239.9 905.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.24      .4791         0                42518 988.1 3730 1.952\n",
            "\n",
            "16:26:29 | time:28668s total_exs:742672 total_steps:46417 epochs:2.99 time_left:104s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.6     1 743.1  2732   .5169      82.75 29.42  296             65536  7.257    .6328 31.94 2.397 6.93e-06 255.5 939.5   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.99      .4832         0                42536 998.6 3672 1.816\n",
            "\n",
            "16:26:40 | time:28678s total_exs:742960 total_steps:46435 epochs:2.99 time_left:93s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   175.1     1 745.9  2669   .5208      81.85 28.63  288             65536  7.128    .6384 32.55 2.409 6.93e-06 260.4 931.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.12      .4838         0                42554 1006 3601 1.828\n",
            "\n",
            "16:26:50 | time:28688s total_exs:743264 total_steps:46454 epochs:2.99 time_left:81s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   184.5     1 747.4  2836   .5033      91.04 30.35  304             65536  7.261    .6441 30.66  2.31 6.93e-06 245.3 930.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 10.07      .5031         0                42573 992.7 3767 1.942\n",
            "\n",
            "16:27:00 | time:28698s total_exs:743552 total_steps:46472 epochs:2.99 time_left:70s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   178.4     1 750.2  2681   .5312      84.61 28.59  288             65536  7.635    .6630 31.09 2.434 6.93e-06 247.3 883.7   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006944      .1806 11.4      .4763         0                42591 997.4 3565 1.844\n",
            "\n",
            "16:27:10 | time:28708s total_exs:743848 total_steps:46490 epochs:2.99 time_left:59s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   173.7     1 746.5  2730   .5169      80.34 29.26  296             65536  7.607    .6384 29.96 2.446 6.93e-06 239.7 876.6   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.54      .4820         0                42610 986.1 3607 1.905\n",
            "\n",
            "16:27:20 | time:28718s total_exs:744144 total_steps:46509 epochs:3.00 time_left:47s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   150.2     1 710.9  2628   .4628      61.34 29.58  296             65536    7.4    .6498 31.42  2.41 6.93e-06 251.4 929.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.14      .4863         0                42628 962.3 3558 1.827\n",
            "\n",
            "16:27:30 | time:28728s total_exs:744448 total_steps:46528 epochs:3.00 time_left:36s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   168.7     1 727.4  2747   .5000      77.75 30.21  304             65536  7.242    .6384 31.37 2.421 6.93e-06   251 947.8   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "         0          0 11.25      .4810         0                42647 978.4 3695 1.926\n",
            "\n",
            "16:27:40 | time:28738s total_exs:744736 total_steps:46546 epochs:3.00 time_left:25s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     156     1 705.3  2510   .4618      67.81 28.47  288             65536  7.819    .6630 31.84 2.494 6.93e-06 251.8 896.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   .006944      .3681 12.11      .4679         0                42665 957.1 3406 1.815\n",
            "\n",
            "16:27:50 | time:28749s total_exs:745024 total_steps:46564 epochs:3.00 time_left:14s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.5     1 749.8  2674   .5312      85.75 28.53  288             65536  7.154    .6630 33.69 2.438 6.93e-06 269.5 961.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 11.45      .4825         0                42683 1019 3635 1.818\n",
            "\n",
            "16:28:00 | time:28759s total_exs:745320 total_steps:46582 epochs:3.00 time_left:2s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   179.5     1 757.6  2731   .5405      84.84 28.84  296             65536  7.578    .6630 30.96  2.41 6.93e-06 247.3 891.3   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   .003378     .05405 11.14      .4843         0                42702 1005 3622 1.865\n",
            "\n",
            "16:28:02 | time:28761s total_exs:745376 total_steps:46586 epochs:3.00 time_left:0s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   181.8     1 760.6  2978   .5893      86.73 31.32   56             65536  7.617    .6272 31.55 2.523 6.93e-06 252.4 988.2   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "         0          0 12.47      .4624         0                42705 1013 3966 1.831\n",
            "\n",
            "16:28:02 | num_epochs completed:3.0 time elapsed:28760.643780708313s\n",
            "16:28:02 | \u001b[33mOverriding opt[\"dict_file\"] to /content/french_xpedbst/models/blender/blender_400Mdistill/model.dict (previously: /content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model.dict)\u001b[0m\n",
            "16:28:02 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,world_logs: ,save_format: conversations,log_keep_fields: all,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: /content/french_xpedbst/data,fromfile_datatype_extension: True,checkpoint_activations: False,datapath: /content/french_xpedbst/,verbose: True,download_path: None,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "16:28:02 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--init-opt /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt --allow-missing-init-opts True --task blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --multitask-weights 1.0,3.0,3.0,3.0 --dynamic-batching full --eval-batchsize 8 --num-epochs -1 --validation-every-n-secs 900.0 --save-after-valid True --validation-every-n-epochs -1.0 --validation-patience 20 --distributed-world-size 8 --port 61337 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --chosen-topic-delimiter \n",
            " --num-topics 5 --train-experiencer-only False --remove-political-convos False --teacher-model /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model --task-loss-coeff 1.0 --encoder-loss-coeff 24.0 --hidden-loss-coeff 5.0 --pred-loss-coeff 8.0 --embedding-loss-coeff 0.35 --self-attn-loss-coeff 0.6 --enc-dec-attn-loss-coeff 3.0 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --inference beam --force-fp16-tokens False --learningrate 0.0004 --gpu 0 --bpe-vocab /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json --bpe-merge /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt --bpe-add-prefix-space True --max-lr-steps -1 --parlai-home /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI --show-advanced-args False --numthreads 1 --rank 0\u001b[0m\n",
            "16:28:02 | Using CUDA\n",
            "16:28:02 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model.dict\n",
            "16:28:02 | num words = 8008\n",
            "16:28:08 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "16:28:08 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model\n",
            "16:28:30 | creating task(s): fromfile:parlaiformat\n",
            "16:28:30 | Loading ParlAI text data: /content/french_xpedbst/data_valid.txt\n",
            "16:28:30 | running eval: valid\n",
            "16:32:52 | eval completed in 261.92s\n",
            "16:32:52 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   175.4 734.4  8320   .4922      83.52 90.56 23718    .5230 36.19 2.373 6.93e-06 288.9  3273 .001686     .05232 10.73   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4898         0                38827 1023 11594\n",
            "\u001b[0m\n",
            "16:32:52 | creating task(s): fromfile:parlaiformat\n",
            "16:32:52 | Loading ParlAI text data: /content/french_xpedbst/data_test.txt\n",
            "16:32:52 | running eval: test\n",
            "16:37:05 | eval completed in 252.71s\n",
            "16:37:05 | \u001b[1mtest:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   182.3   744  8294   .5079       89.1 89.03 22497    .5221 37.38 2.399 6.93e-06   298  3322 .002178     .06436 11.02   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4855         0                38827 1042 11617\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'clen': AverageMetric(175.4),\n",
              "  'ctpb': GlobalAverageMetric(734.4),\n",
              "  'ctps': GlobalTimerMetric(8320),\n",
              "  'ctrunc': AverageMetric(0.4922),\n",
              "  'ctrunclen': AverageMetric(83.52),\n",
              "  'exps': GlobalTimerMetric(90.56),\n",
              "  'exs': SumMetric(2.372e+04),\n",
              "  'gpu_mem': GlobalAverageMetric(0.523),\n",
              "  'llen': AverageMetric(36.19),\n",
              "  'loss': AverageMetric(2.373),\n",
              "  'lr': GlobalAverageMetric(6.93e-06),\n",
              "  'ltpb': GlobalAverageMetric(288.9),\n",
              "  'ltps': GlobalTimerMetric(3273),\n",
              "  'ltrunc': AverageMetric(0.001686),\n",
              "  'ltrunclen': AverageMetric(0.05232),\n",
              "  'ppl': PPLMetric(10.73),\n",
              "  'token_acc': AverageMetric(0.4898),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(3.883e+04),\n",
              "  'tpb': GlobalAverageMetric(1023),\n",
              "  'tps': GlobalTimerMetric(1.159e+04)},\n",
              " {'clen': AverageMetric(182.3),\n",
              "  'ctpb': GlobalAverageMetric(744),\n",
              "  'ctps': GlobalTimerMetric(8294),\n",
              "  'ctrunc': AverageMetric(0.5079),\n",
              "  'ctrunclen': AverageMetric(89.1),\n",
              "  'exps': GlobalTimerMetric(89.03),\n",
              "  'exs': SumMetric(2.25e+04),\n",
              "  'gpu_mem': GlobalAverageMetric(0.5221),\n",
              "  'llen': AverageMetric(37.38),\n",
              "  'loss': AverageMetric(2.399),\n",
              "  'lr': GlobalAverageMetric(6.93e-06),\n",
              "  'ltpb': GlobalAverageMetric(298),\n",
              "  'ltps': GlobalTimerMetric(3322),\n",
              "  'ltrunc': AverageMetric(0.002178),\n",
              "  'ltrunclen': AverageMetric(0.06436),\n",
              "  'ppl': PPLMetric(11.02),\n",
              "  'token_acc': AverageMetric(0.4855),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(3.883e+04),\n",
              "  'tpb': GlobalAverageMetric(1042),\n",
              "  'tps': GlobalTimerMetric(1.162e+04)})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/french_xpedbst/\"\n",
        "model_path = \"/content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/\"\n",
        "print(f'{model_path}/model')\n",
        "\n",
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= data_path + \"data\",\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    # model= \"transformer/generator\",\n",
        "    model_file= f\"{model_path}model\",\n",
        "    # init_model= \"zoo:blender/blender_400Mdistill/model\",\n",
        "    # dict_file= f\"zoo:blender/blender_400Mdistill/model.dict\",\n",
        "\n",
        "    datatype= \"test\",\n",
        "    # fromfile_datatype_extension= True,\n",
        "\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    # inference = 'topk', # Generation algorithm. Choices: beam, topk, greedy, delayedbeam, nucleus\n",
        "    # temperature = 0.7, # Temperature to add during decoding. Default 1.0\n",
        "    # topk=30, # K used in Top K sampling\n",
        "    # beam_length_penalty=1.03 # Applies a length penalty. Set to 0 for no penalty. Default: 0.65.\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOz5CzorB1IP",
        "outputId": "1f81b6cf-74a9-446d-fb1c-91d654858471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M//model\n",
            "16:41:37 | \u001b[33mOverriding opt[\"datatype\"] to test (previously: train)\u001b[0m\n",
            "16:41:37 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "16:41:37 | Using CUDA\n",
            "16:41:37 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model.dict\n",
            "16:41:37 | num words = 8008\n",
            "16:41:42 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "16:41:42 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model\n",
            "16:41:48 | creating task(s): fromfile:parlaiformat\n",
            "16:41:48 | Loading ParlAI text data: /content/french_xpedbst/data_test.txt\n",
            "16:41:48 | Opt:\n",
            "16:41:48 |     activation: gelu\n",
            "16:41:48 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "16:41:48 |     adam_eps: 1e-08\n",
            "16:41:48 |     add_p1_after_newln: False\n",
            "16:41:48 |     aggregate_micro: False\n",
            "16:41:48 |     allow_missing_init_opts: False\n",
            "16:41:48 |     attention_dropout: 0.0\n",
            "16:41:48 |     batchsize: 8\n",
            "16:41:48 |     beam_block_full_context: True\n",
            "16:41:48 |     beam_block_list_filename: None\n",
            "16:41:48 |     beam_block_ngram: -1\n",
            "16:41:48 |     beam_context_block_ngram: -1\n",
            "16:41:48 |     beam_delay: 30\n",
            "16:41:48 |     beam_length_penalty: 0.65\n",
            "16:41:48 |     beam_min_length: 1\n",
            "16:41:48 |     beam_size: 1\n",
            "16:41:48 |     betas: '[0.9, 0.999]'\n",
            "16:41:48 |     bpe_add_prefix_space: None\n",
            "16:41:48 |     bpe_debug: False\n",
            "16:41:48 |     bpe_dropout: None\n",
            "16:41:48 |     bpe_merge: /content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model.dict-merges.txt\n",
            "16:41:48 |     bpe_vocab: /content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model.dict-vocab.json\n",
            "16:41:48 |     checkpoint_activations: False\n",
            "16:41:48 |     compute_tokenized_bleu: False\n",
            "16:41:48 |     datapath: /content/french_xpedbst/\n",
            "16:41:48 |     datatype: test\n",
            "16:41:48 |     delimiter: '  '\n",
            "16:41:48 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "16:41:48 |     dict_endtoken: __end__\n",
            "16:41:48 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model.dict\n",
            "16:41:48 |     dict_include_test: False\n",
            "16:41:48 |     dict_include_valid: False\n",
            "16:41:48 |     dict_initpath: None\n",
            "16:41:48 |     dict_language: english\n",
            "16:41:48 |     dict_loaded: True\n",
            "16:41:48 |     dict_lower: False\n",
            "16:41:48 |     dict_max_ngram_size: -1\n",
            "16:41:48 |     dict_maxexs: -1\n",
            "16:41:48 |     dict_maxtokens: -1\n",
            "16:41:48 |     dict_minfreq: 0\n",
            "16:41:48 |     dict_nulltoken: __null__\n",
            "16:41:48 |     dict_starttoken: __start__\n",
            "16:41:48 |     dict_textfields: text,labels\n",
            "16:41:48 |     dict_tokenizer: bytelevelbpe\n",
            "16:41:48 |     dict_unktoken: __unk__\n",
            "16:41:48 |     display_add_fields: \n",
            "16:41:48 |     display_examples: False\n",
            "16:41:48 |     download_path: None\n",
            "16:41:48 |     dropout: 0.1\n",
            "16:41:48 |     dynamic_batching: None\n",
            "16:41:48 |     embedding_projection: random\n",
            "16:41:48 |     embedding_size: 1280\n",
            "16:41:48 |     embedding_type: random\n",
            "16:41:48 |     embeddings_scale: True\n",
            "16:41:48 |     eval_batchsize: None\n",
            "16:41:48 |     eval_dynamic_batching: None\n",
            "16:41:48 |     evaltask: None\n",
            "16:41:48 |     ffn_size: 5120\n",
            "16:41:48 |     final_extra_opt: \n",
            "16:41:48 |     force_fp16_tokens: True\n",
            "16:41:48 |     fp16: True\n",
            "16:41:48 |     fp16_impl: mem_efficient\n",
            "16:41:48 |     fromfile_datapath: /content/french_xpedbst/data\n",
            "16:41:48 |     fromfile_datatype_extension: True\n",
            "16:41:48 |     gpu: -1\n",
            "16:41:48 |     gradient_clip: 0.1\n",
            "16:41:48 |     hide_labels: False\n",
            "16:41:48 |     history_add_global_end_token: end\n",
            "16:41:48 |     history_reversed: False\n",
            "16:41:48 |     history_size: -1\n",
            "16:41:48 |     image_cropsize: 224\n",
            "16:41:48 |     image_mode: raw\n",
            "16:41:48 |     image_size: 256\n",
            "16:41:48 |     inference: greedy\n",
            "16:41:48 |     init_model: zoo:blender/blender_400Mdistill/model\n",
            "16:41:48 |     init_opt: None\n",
            "16:41:48 |     interactive_mode: False\n",
            "16:41:48 |     invsqrt_lr_decay_gamma: -1\n",
            "16:41:48 |     is_debug: False\n",
            "16:41:48 |     label_truncate: 128\n",
            "16:41:48 |     learn_positional_embeddings: False\n",
            "16:41:48 |     learningrate: 7e-06\n",
            "16:41:48 |     log_every_n_secs: 10.0\n",
            "16:41:48 |     log_every_n_steps: 50\n",
            "16:41:48 |     log_keep_fields: all\n",
            "16:41:48 |     loglevel: info\n",
            "16:41:48 |     lr_scheduler: reduceonplateau\n",
            "16:41:48 |     lr_scheduler_decay: 0.5\n",
            "16:41:48 |     lr_scheduler_patience: 3\n",
            "16:41:48 |     max_train_steps: -1\n",
            "16:41:48 |     max_train_time: -1\n",
            "16:41:48 |     metrics: default\n",
            "16:41:48 |     model: transformer/generator\n",
            "16:41:48 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model\n",
            "16:41:48 |     model_parallel: False\n",
            "16:41:48 |     momentum: 0\n",
            "16:41:48 |     multitask_weights: [1]\n",
            "16:41:48 |     mutators: None\n",
            "16:41:48 |     n_decoder_layers: 12\n",
            "16:41:48 |     n_encoder_layers: 2\n",
            "16:41:48 |     n_heads: 32\n",
            "16:41:48 |     n_layers: 2\n",
            "16:41:48 |     n_positions: 128\n",
            "16:41:48 |     n_segments: 0\n",
            "16:41:48 |     nesterov: True\n",
            "16:41:48 |     no_cuda: False\n",
            "16:41:48 |     num_epochs: 3.0\n",
            "16:41:48 |     num_examples: 20\n",
            "16:41:48 |     num_workers: 0\n",
            "16:41:48 |     nus: [0.7]\n",
            "16:41:48 |     optimizer: mem_eff_adam\n",
            "16:41:48 |     output_scaling: 1.0\n",
            "16:41:48 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '/content/french_xpedbst/data', 'fromfile_datatype_extension': True, 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-xpedbst-400M/model', 'datatype': 'test', 'num_examples': '20', 'skip_generation': False}\"\n",
            "16:41:48 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "16:41:48 |     person_tokens: False\n",
            "16:41:48 |     rank_candidates: False\n",
            "16:41:48 |     relu_dropout: 0.0\n",
            "16:41:48 |     save_after_valid: False\n",
            "16:41:48 |     save_every_n_secs: -1\n",
            "16:41:48 |     save_format: conversations\n",
            "16:41:48 |     share_word_embeddings: True\n",
            "16:41:48 |     short_final_eval: False\n",
            "16:41:48 |     skip_generation: False\n",
            "16:41:48 |     special_tok_lst: None\n",
            "16:41:48 |     split_lines: False\n",
            "16:41:48 |     starttime: May09_07-57\n",
            "16:41:48 |     task: fromfile:parlaiformat\n",
            "16:41:48 |     temperature: 1.0\n",
            "16:41:48 |     tensorboard_log: False\n",
            "16:41:48 |     tensorboard_logdir: None\n",
            "16:41:48 |     text_truncate: 128\n",
            "16:41:48 |     topk: 10\n",
            "16:41:48 |     topp: 0.9\n",
            "16:41:48 |     truncate: 128\n",
            "16:41:48 |     update_freq: 2\n",
            "16:41:48 |     use_reply: label\n",
            "16:41:48 |     validation_cutoff: 1.0\n",
            "16:41:48 |     validation_every_n_epochs: 0.25\n",
            "16:41:48 |     validation_every_n_secs: -1\n",
            "16:41:48 |     validation_every_n_steps: -1\n",
            "16:41:48 |     validation_max_exs: -1\n",
            "16:41:48 |     validation_metric: ppl\n",
            "16:41:48 |     validation_metric_mode: min\n",
            "16:41:48 |     validation_patience: 10\n",
            "16:41:48 |     validation_share_agent: False\n",
            "16:41:48 |     variant: prelayernorm\n",
            "16:41:48 |     verbose: False\n",
            "16:41:48 |     wandb_entity: None\n",
            "16:41:48 |     wandb_log: False\n",
            "16:41:48 |     wandb_name: None\n",
            "16:41:48 |     wandb_project: None\n",
            "16:41:48 |     warmup_rate: 0.0001\n",
            "16:41:48 |     warmup_updates: 100\n",
            "16:41:48 |     weight_decay: None\n",
            "16:41:48 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: french_xpedbst/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mwow, je ne suis jamais timide. vous êtes anxieux ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: oui. je finis par transpirer et rougir et j'ai l'impression que je vais vomir.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis sûr que je suis sûr que je suis sûr que je ne peux pas vraiment.\u001b[0;0m\n",
            "\u001b[0met pourquoi ça ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: je pense que c'est parce que dans ma tête, je pense que tout le monde me juge. je commence à transpirer et j'ai mal au ventre.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d'entendre ça.\u001b[0;0m\n",
            "\u001b[0mintéressant mais je sais ce que tu ressens, surtout le fait que les gens te disent que c'est dans ta tête. \u001b[0;0m\n",
            "\u001b[1;94m    labels: je n'ai pas vraiment de gens qui me le disent dans ma tête, plutôt dans mon dos.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d'entendre ça. je suis une bonne idée.\u001b[0;0m\n",
            "\u001b[0mdang, c'est vrai. mais je comprends aussi ça. j'ai des gens qui parlent dans mon dos à cause de certaines choses auxquelles je crois. \u001b[0;0m\n",
            "\u001b[1;94m    labels: moi aussi ! en quoi crois-tu ? je crois aux dragons... je viens de finir de regarder game of thrones. mec, ces choses sont géniales\u001b[0;0m\n",
            "\u001b[0;95m     model: c'est génial !  je suis désolé d'entendre ça.  je suis désolé d'entendre ça.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_xpedbst/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0moui. je finis par transpirer et rougir et j'ai l'impression que je vais vomir.\u001b[0;0m\n",
            "\u001b[1;94m    labels: et pourquoi ça ?\u001b[0;0m\n",
            "\u001b[0;95m     model: c'est génial !  je suis désolé d'entendre ça.\u001b[0;0m\n",
            "\u001b[0mje pense que c'est parce que dans ma tête, je pense que tout le monde me juge. je commence à transpirer et j'ai mal au ventre.\u001b[0;0m\n",
            "\u001b[1;94m    labels: intéressant mais je sais ce que tu ressens, surtout le fait que les gens te disent que c'est dans ta tête. \u001b[0;0m\n",
            "\u001b[0;95m     model: c'est génial !  je suis désolé d'entendre ça.\u001b[0;0m\n",
            "\u001b[0mje n'ai pas vraiment de gens qui me le disent dans ma tête, plutôt dans mon dos.\u001b[0;0m\n",
            "\u001b[1;94m    labels: dang, c'est vrai. mais je comprends aussi ça. j'ai des gens qui parlent dans mon dos à cause de certaines choses auxquelles je crois. \u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d'entendre ça. \u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_xpedbst/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mma tortue s'est enfuie aujourd'hui.\u001b[0;0m\n",
            "\u001b[1;94m    labels: oh mon dieu. vous vous êtes disputés ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d'entendre ça.\u001b[0;0m\n",
            "\u001b[0mc'est drôle. non. je le laisse errer dans la maison.\u001b[0;0m\n",
            "\u001b[1;94m    labels: que mange ta tortue ?  est-il difficile de s'occuper d'une tortue ?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh non, je suis désolé d'entendre ça.\u001b[0;0m\n",
            "\u001b[0mil mange des insectes, des feuilles et des graines de tournesol. c'est facile. il n'a pas besoin de marcher et le nettoyage est simple. avez-vous des animaux domestiques ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: non, pas pour le moment.  j'ai 3 filles et elles ont assez d'ennuis ! lol\u001b[0;0m\n",
            "\u001b[0;95m     model: oui, je suis une fois.  je suis une fois.  je suis une fois.\u001b[0;0m\n",
            "\u001b[0mfélicitations. venez-vous d'une grande famille ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: je n'en ai pas, juste 2 frères et sœurs.  mais ma femme voulait beaucoup d'enfants. donc, voilà.\u001b[0;0m\n",
            "\u001b[0;95m     model: non, je ne suis pas d'une grande famille.  je suis un grande famille.  je suis une grande famille.\u001b[0;0m\n",
            "\u001b[0mwow. cool. combien de fois utilisez-vous des ordinateurs ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: j'utilise un mac pour mon travail - je suis professeur de musique.  je le trouve très utile.  et vous ?  utilisez-vous beaucoup l'ordinateur ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis une fois.  je suis une grande famille.  je suis une grande famille.\u001b[0;0m\n",
            "\u001b[0msouvent. je les utilise au travail pour le montage. je suis à la recherche d'un moyen de diffuser des vidéos ou des films depuis la bibliothèque locale.\u001b[0;0m\n",
            "\u001b[1;94m    labels: stream de la bibliothèque - maintenant, cela semble intéressant. ma femme et moi faisons de la plongée sous-marine et nous voulons commencer à filmer sous l'eau. vous avez déjà essayé ?\u001b[0;0m\n",
            "\u001b[0;95m     model: c'est génial !  j'ai une bonne idée de la recherche de vidéos locale.  je suis une bonne chance.  je suis une bonne idée.\u001b[0;0m\n",
            "\u001b[0mnon, mais j'ai fait du snorkeling.\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est cool aussi. c'est comme ça qu'on a commencé.  j'ai vu beaucoup de tortues !  lol\u001b[0;0m\n",
            "\u001b[0;95m     model: oh, c'est génial. je suis désolé d'entendre ça.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_xpedbst/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0moh mon dieu. vous vous êtes disputés ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est drôle. non. je le laisse errer dans la maison.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d'entendre ça. je suis une fois.\u001b[0;0m\n",
            "\u001b[0mque mange ta tortue ?  est-il difficile de s'occuper d'une tortue ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: il mange des insectes, des feuilles et des graines de tournesol. c'est facile. il n'a pas besoin de marcher et le nettoyage est simple. avez-vous des animaux domestiques ?\u001b[0;0m\n",
            "\u001b[0;95m     model: oui, je suis une fois.\u001b[0;0m\n",
            "\u001b[0mnon, pas pour le moment.  j'ai 3 filles et elles ont assez d'ennuis ! lol\u001b[0;0m\n",
            "\u001b[1;94m    labels: félicitations. venez-vous d'une grande famille ?\u001b[0;0m\n",
            "\u001b[0;95m     model: haha, c'est une bonne chose. je suis désolé d'entendre ça.\u001b[0;0m\n",
            "\u001b[0mje n'en ai pas, juste 2 frères et sœurs.  mais ma femme voulait beaucoup d'enfants. donc, voilà.\u001b[0;0m\n",
            "\u001b[1;94m    labels: wow. cool. combien de fois utilisez-vous des ordinateurs ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d'entendre ça.  je suis une grande famille.\u001b[0;0m\n",
            "\u001b[0mj'utilise un mac pour mon travail - je suis professeur de musique.  je le trouve très utile.  et vous ?  utilisez-vous beaucoup l'ordinateur ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: souvent. je les utilise au travail pour le montage. je suis à la recherche d'un moyen de diffuser des vidéos ou des films depuis la bibliothèque locale.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis une fois.  je suis une fois que je suis une fois.  je suis une fois.\u001b[0;0m\n",
            "\u001b[0mstream de la bibliothèque - maintenant, cela semble intéressant. ma femme et moi faisons de la plongée sous-marine et nous voulons commencer à filmer sous l'eau. vous avez déjà essayé ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: non, mais j'ai fait du snorkeling.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis désolé d'entendre ça. je suis désolé d'entendre ça.\u001b[0;0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otw09UVD41gw"
      },
      "source": [
        "### Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wenmDNs41gw",
        "outputId": "c1901cff-6a70-4fd1-d474-61029eaf75e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "124122\n",
            "248244\n"
          ]
        }
      ],
      "source": [
        "double_lines = []\n",
        "with open('fr_finetuned_train.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    double_lines.extend(lines)\n",
        "    double_lines.extend(lines)\n",
        "\n",
        "print(len(lines))\n",
        "print(len(double_lines))\n",
        "with open('fr_finetuned_train.txt', 'w') as ff:\n",
        "    ff.writelines(double_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uES45OSB41gw",
        "outputId": "d6664f48-774f-4657-ebc7-078de9dcb5e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"text:salut comment vas-tu ? Je me prépare à faire du guépard pour rester en forme.\\tlabels:vous devez être très rapide. la chasse est l'un de mes passe-temps préférés.\\n\",\n",
              " \"text:je suis ! pour mon hobby j'aime faire la mise en conserve ou un peu tailler.\\tlabels:je remodèle aussi des maisons quand je ne suis pas à la chasse à l'arc.\\n\",\n",
              " \"text:c'est bien. quand j'étais au lycée, je me suis placé 6ème au 100m dash!\\tlabels:c'est génial . avez-vous une saison ou une période préférée de l'année?\\n\",\n",
              " \"text:Non . mais j'ai une viande préférée car c'est tout ce que je mange exclusivement.\\tlabels:quelle est votre viande préférée à manger?\\n\",\n",
              " \"text:je devrais dire sa côte de bœuf. avez-vous des aliments préférés?\\tlabels:j'aime le poulet ou les macaronis et le fromage.\\n\",\n",
              " \"text:avez-vous prévu quelque chose pour aujourd'hui? je pense que je vais faire de la mise en conserve.\\tlabels:je vais regarder le football. que conservez-vous?\\n\",\n",
              " \"text:je pense que je vais pouvoir un peu de confiture. jouez-vous aussi pour le plaisir?\\tlabels:si j'ai le temps en dehors des maisons de chasse et de rénovation. ce qui n'est pas grand chose!\\tepisode_done:True\\n\",\n",
              " \"text:Bonjour comment allez-vous aujourd'hui ?\\tlabels:je passe du temps avec mes 4 soeurs que faites-vous\\n\",\n",
              " \"text:wow, quatre sœurs. je regarde juste le jeu des trônes.\\tlabels:c'est un bon spectacle je regarde ça en buvant du thé glacé\\n\",\n",
              " \"text:je suis d'accord . Comment gagnez-vous votre vie ?\\tlabels:je suis un chercheur je recherche le fait que les sirènes sont réelles\\n\"]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiQG2p5E41gx"
      },
      "outputs": [],
      "source": [
        "finetuned_model_path = f'{mydrive_path}finetuned-400m-topk-double'\n",
        "init_model = 'zoo:blender/blender_400Mdistill/model'\n",
        "dict_file  = 'zoo:blender/blender_400Mdistill/model.dict'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bVzG2FW41gx"
      },
      "outputs": [],
      "source": [
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    # task= \"blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues\",\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model= \"transformer/generator\",\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "\n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file= dict_file,\n",
        "\n",
        "    # arguments we get from the pretrained model.\n",
        "    n_heads= 32, n_layers= 2, n_positions= 128, n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    dropout= 0.1, log_every_n_secs= 10,\n",
        "    multitask_weights= \"1,3,3,3\",\n",
        "    attention_dropout= 0.0,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "\n",
        "    variant= \"prelayernorm\",\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    relu_dropout= 0.0, model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "    # save_after_valid= True,\n",
        "\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 5,\n",
        "    verbose = True,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    batchsize= 8, fp16= True, fp16_impl= \"mem_efficient\",\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\",\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    inference = 'topk', \n",
        "    temperature = 0.7, \n",
        "    topk=30, \n",
        "    beam_length_penalty=1.03\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReiaPwRi41gx",
        "outputId": "e3c6c778-9e94-43c8-ef52-a75f5718fdb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model\n",
            "09:48:09 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "09:48:09 | Using CUDA\n",
            "09:48:09 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model.dict\n",
            "09:48:09 | num words = 8008\n",
            "09:48:15 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "09:48:15 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model\n",
            "09:48:35 | creating task(s): fromfile:parlaiformat\n",
            "09:48:35 | Loading ParlAI text data: fr_finetuned_valid.txt\n",
            "09:48:35 | Opt:\n",
            "09:48:35 |     activation: gelu\n",
            "09:48:35 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "09:48:35 |     adam_eps: 1e-08\n",
            "09:48:35 |     add_p1_after_newln: False\n",
            "09:48:35 |     aggregate_micro: False\n",
            "09:48:35 |     allow_missing_init_opts: False\n",
            "09:48:35 |     attention_dropout: 0.0\n",
            "09:48:35 |     batchsize: 8\n",
            "09:48:35 |     beam_block_full_context: True\n",
            "09:48:35 |     beam_block_list_filename: None\n",
            "09:48:35 |     beam_block_ngram: -1\n",
            "09:48:35 |     beam_context_block_ngram: -1\n",
            "09:48:35 |     beam_delay: 30\n",
            "09:48:35 |     beam_length_penalty: 1.03\n",
            "09:48:35 |     beam_min_length: 1\n",
            "09:48:35 |     beam_size: 1\n",
            "09:48:35 |     betas: '[0.9, 0.999]'\n",
            "09:48:35 |     bpe_add_prefix_space: None\n",
            "09:48:35 |     bpe_debug: False\n",
            "09:48:35 |     bpe_dropout: None\n",
            "09:48:35 |     bpe_merge: /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model.dict-merges.txt\n",
            "09:48:35 |     bpe_vocab: /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model.dict-vocab.json\n",
            "09:48:35 |     checkpoint_activations: False\n",
            "09:48:35 |     compute_tokenized_bleu: False\n",
            "09:48:35 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "09:48:35 |     datatype: train\n",
            "09:48:35 |     delimiter: '  '\n",
            "09:48:35 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "09:48:35 |     dict_endtoken: __end__\n",
            "09:48:35 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model.dict\n",
            "09:48:35 |     dict_include_test: False\n",
            "09:48:35 |     dict_include_valid: False\n",
            "09:48:35 |     dict_initpath: None\n",
            "09:48:35 |     dict_language: english\n",
            "09:48:35 |     dict_loaded: True\n",
            "09:48:35 |     dict_lower: False\n",
            "09:48:35 |     dict_max_ngram_size: -1\n",
            "09:48:35 |     dict_maxexs: -1\n",
            "09:48:35 |     dict_maxtokens: -1\n",
            "09:48:35 |     dict_minfreq: 0\n",
            "09:48:35 |     dict_nulltoken: __null__\n",
            "09:48:35 |     dict_starttoken: __start__\n",
            "09:48:35 |     dict_textfields: text,labels\n",
            "09:48:35 |     dict_tokenizer: bytelevelbpe\n",
            "09:48:35 |     dict_unktoken: __unk__\n",
            "09:48:35 |     display_add_fields: \n",
            "09:48:35 |     display_examples: False\n",
            "09:48:35 |     download_path: None\n",
            "09:48:35 |     dropout: 0.1\n",
            "09:48:35 |     dynamic_batching: None\n",
            "09:48:35 |     embedding_projection: random\n",
            "09:48:35 |     embedding_size: 1280\n",
            "09:48:35 |     embedding_type: random\n",
            "09:48:35 |     embeddings_scale: True\n",
            "09:48:35 |     eval_batchsize: None\n",
            "09:48:35 |     eval_dynamic_batching: None\n",
            "09:48:35 |     evaltask: None\n",
            "09:48:35 |     ffn_size: 5120\n",
            "09:48:35 |     final_extra_opt: \n",
            "09:48:35 |     force_fp16_tokens: True\n",
            "09:48:35 |     fp16: True\n",
            "09:48:35 |     fp16_impl: mem_efficient\n",
            "09:48:35 |     fromfile_datapath: fr_finetuned\n",
            "09:48:35 |     fromfile_datatype_extension: True\n",
            "09:48:35 |     gpu: -1\n",
            "09:48:35 |     gradient_clip: 0.1\n",
            "09:48:35 |     hide_labels: False\n",
            "09:48:35 |     history_add_global_end_token: end\n",
            "09:48:35 |     history_reversed: False\n",
            "09:48:35 |     history_size: -1\n",
            "09:48:35 |     image_cropsize: 224\n",
            "09:48:35 |     image_mode: raw\n",
            "09:48:35 |     image_size: 256\n",
            "09:48:35 |     inference: topk\n",
            "09:48:35 |     init_model: zoo:blender/blender_400Mdistill/model\n",
            "09:48:35 |     init_opt: None\n",
            "09:48:35 |     interactive_mode: False\n",
            "09:48:35 |     invsqrt_lr_decay_gamma: -1\n",
            "09:48:35 |     is_debug: False\n",
            "09:48:35 |     label_truncate: 128\n",
            "09:48:35 |     learn_positional_embeddings: False\n",
            "09:48:35 |     learningrate: 7e-06\n",
            "09:48:35 |     log_every_n_secs: 10.0\n",
            "09:48:35 |     log_every_n_steps: 50\n",
            "09:48:35 |     log_keep_fields: all\n",
            "09:48:35 |     loglevel: info\n",
            "09:48:35 |     lr_scheduler: reduceonplateau\n",
            "09:48:35 |     lr_scheduler_decay: 0.5\n",
            "09:48:35 |     lr_scheduler_patience: 3\n",
            "09:48:35 |     max_train_steps: -1\n",
            "09:48:35 |     max_train_time: -1\n",
            "09:48:35 |     metrics: default\n",
            "09:48:35 |     model: transformer/generator\n",
            "09:48:35 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model\n",
            "09:48:35 |     model_parallel: False\n",
            "09:48:35 |     momentum: 0\n",
            "09:48:35 |     multitask_weights: '[1.0, 3.0, 3.0, 3.0]'\n",
            "09:48:35 |     mutators: None\n",
            "09:48:35 |     n_decoder_layers: 12\n",
            "09:48:35 |     n_encoder_layers: 2\n",
            "09:48:35 |     n_heads: 32\n",
            "09:48:35 |     n_layers: 2\n",
            "09:48:35 |     n_positions: 128\n",
            "09:48:35 |     n_segments: 0\n",
            "09:48:35 |     nesterov: True\n",
            "09:48:35 |     no_cuda: False\n",
            "09:48:35 |     num_epochs: 5.0\n",
            "09:48:35 |     num_examples: 20\n",
            "09:48:35 |     num_workers: 0\n",
            "09:48:35 |     nus: [0.7]\n",
            "09:48:35 |     optimizer: mem_eff_adam\n",
            "09:48:35 |     output_scaling: 1.0\n",
            "09:48:35 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'fr_finetuned', 'fromfile_datatype_extension': True, 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-400m-topk/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict', 'num_examples': '20', 'skip_generation': False, 'inference': 'topk', 'temperature': 0.7, 'topk': 30, 'beam_length_penalty': 1.03}\"\n",
            "09:48:35 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "09:48:35 |     person_tokens: False\n",
            "09:48:35 |     rank_candidates: False\n",
            "09:48:35 |     relu_dropout: 0.0\n",
            "09:48:35 |     save_after_valid: False\n",
            "09:48:35 |     save_every_n_secs: -1\n",
            "09:48:35 |     save_format: conversations\n",
            "09:48:35 |     share_word_embeddings: True\n",
            "09:48:35 |     short_final_eval: False\n",
            "09:48:35 |     skip_generation: False\n",
            "09:48:35 |     special_tok_lst: None\n",
            "09:48:35 |     split_lines: False\n",
            "09:48:35 |     starttime: Mar31_11-31\n",
            "09:48:35 |     task: fromfile:parlaiformat\n",
            "09:48:35 |     temperature: 0.7\n",
            "09:48:35 |     tensorboard_log: False\n",
            "09:48:35 |     tensorboard_logdir: None\n",
            "09:48:35 |     text_truncate: 128\n",
            "09:48:35 |     topk: 30\n",
            "09:48:35 |     topp: 0.9\n",
            "09:48:35 |     truncate: 128\n",
            "09:48:35 |     update_freq: 2\n",
            "09:48:35 |     use_reply: label\n",
            "09:48:35 |     validation_cutoff: 1.0\n",
            "09:48:35 |     validation_every_n_epochs: 0.25\n",
            "09:48:35 |     validation_every_n_secs: -1\n",
            "09:48:35 |     validation_every_n_steps: -1\n",
            "09:48:35 |     validation_max_exs: -1\n",
            "09:48:35 |     validation_metric: ppl\n",
            "09:48:35 |     validation_metric_mode: min\n",
            "09:48:35 |     validation_patience: 10\n",
            "09:48:35 |     validation_share_agent: False\n",
            "09:48:35 |     variant: prelayernorm\n",
            "09:48:35 |     verbose: False\n",
            "09:48:35 |     wandb_entity: None\n",
            "09:48:35 |     wandb_log: False\n",
            "09:48:35 |     wandb_name: None\n",
            "09:48:35 |     wandb_project: None\n",
            "09:48:35 |     warmup_rate: 0.0001\n",
            "09:48:35 |     warmup_updates: 100\n",
            "09:48:35 |     weight_decay: None\n",
            "09:48:35 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finetuned_valid.txt- - -\u001b[0;0m\n",
            "\u001b[0msalut ! comment vas-tu ce soir ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: je le fais bien . juste se détendre avec mes deux chiens.\u001b[0;0m\n",
            "\u001b[0;95m     model: Je vais bien. toi ?\u001b[0;0m\n",
            "\u001b[0mgénial . dans mon temps libre je fais du bénévolat.\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est bien. quel genre de travail bénévole faites-vous?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis une bonneur que je pourrais vous?\u001b[0;0m\n",
            "\u001b[0mje travaille dans un refuge pour sans-abri dans ma ville.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Bien pour vous . vous aimez les voitures anciennes? j'ai deux anciennes mustangs.\u001b[0;0m\n",
            "\u001b[0;95m     model: je n'ai pas la nager. j'aime pas vraiment une fiction.\u001b[0;0m\n",
            "\u001b[0mcool . pas vraiment dans les voitures. mon travail de jour est dur.\u001b[0;0m\n",
            "\u001b[1;94m    labels: les voitures sont mon truc. voitures anciennes . j'adore travailler sur cela. est-ce que tu aimes ça ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je n'ai pas de terminer le temps.\u001b[0;0m\n",
            "\u001b[0moui, j'aime la foule, faire connaissance avec les gens.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je n'ai pas pensé à l'aspect foule de la lutte. je n'aime pas les foules.\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai une bonne chose. pas de ma maman est à la maison.\u001b[0;0m\n",
            "\u001b[0mcompris . quand j'ai besoin d'être seul, je m'entraîne plusieurs fois par semaine.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je suis d'accord . travailler est un excellent moyen de faire ses doléances. Aimez-vous la musique country ?\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai un film film de la musique. elle est mon père sur le faire.\u001b[0;0m\n",
            "\u001b[0mun petit peu . je peux admirer taylor swift.\u001b[0;0m\n",
            "\u001b[1;94m    labels: lol. elle est plus pop maintenant. ses vieux trucs étaient country. j'aime le country classique.\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai commencé à un récemment.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finetuned_valid.txt- - -\u001b[0;0m\n",
            "\u001b[0msalut comment allez-vous ? je vais bien et vous?\u001b[0;0m\n",
            "\u001b[1;94m    labels: je vais bien. sirotant juste du thé. que faites-vous pour le travail?\u001b[0;0m\n",
            "\u001b[0;95m     model: je vais bien . j'ai une grande de mes deux chiens.\u001b[0;0m\n",
            "\u001b[0mje faisais de l'aide à domicile mais maintenant je suis handicapé.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je suis désolé de l'entendre . Qu'est-il arrivé\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un fabneur et c'est une bonne journée.\u001b[0;0m\n",
            "\u001b[0mj'ai obtenu mon diplôme et obtenu mon permis de travail un certain temps et est devenu je vais.\u001b[0;0m\n",
            "\u001b[1;94m    labels: alors que faites-vous maintenant pour vous amuser? J'aime lire .\u001b[0;0m\n",
            "\u001b[0;95m     model: pas de terminer des frères.\u001b[0;0m\n",
            "\u001b[0mj'ai des chiens et je les promène. et un chat.\u001b[0;0m\n",
            "\u001b[1;94m    labels: quel genre de chiens avez-vous?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un peu de l'argent pour la plupart.\u001b[0;0m\n",
            "\u001b[0mj'aime lire sur michael jackson et la psychologie aujourd'hui.\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'aime lire . j'ai une grande bibliothèque chez moi.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui, ça a l'air génial .\u001b[0;0m\n",
            "\u001b[0mc'est très agréable j'aime lire et lire des revues.\u001b[0;0m\n",
            "\u001b[1;94m    labels: j'ai du mal à entendre donc la lecture est mon endroit heureux.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis dans les gens. je ne vais pas dépétarien aussi.\u001b[0;0m\n",
            "\u001b[0mcool ça va. ma maman était très dure avec moi.\u001b[0;0m\n",
            "\u001b[1;94m    labels: je travaille comme guide d'excursion de musée de malentendants.\u001b[0;0m\n",
            "\u001b[0;95m     model: c'est cool . je suis allé à la musique. je suis uneur.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finetuned_valid.txt- - -\u001b[0;0m\n",
            "\u001b[0msalut ! vous aimez les tortues?\u001b[0;0m\n",
            "\u001b[1;94m    labels: en fait, personnellement, je préfère le chat.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne suis pas aller à la maison. je suis allez-vous ?\u001b[0;0m\n",
            "\u001b[0mj'ai une tortue son nom est rapide. les chatons sont sympas aussi, tho!\u001b[0;0m\n",
            "\u001b[1;94m    labels: c'est un nom adorable pour une tortue. j'ai 2 chats\u001b[0;0m\n",
            "\u001b[0;95m     model: j'ai une petit ami.\u001b[0;0m\n",
            "\u001b[0mquels sont vos noms de chatons?\u001b[0;0m\n",
            "\u001b[1;94m    labels: neige et hiver, du nom de ma saison préférée\u001b[0;0m\n",
            "\u001b[0;95m     model: je parie que je suis dormir de la guités de l'automne.\u001b[0;0m\n",
            "\u001b[0mJ'aime ça ! je vais à l'école maternelle.\u001b[0;0m\n",
            "\u001b[1;94m    labels: oh tu es si jeune!\u001b[0;0m\n",
            "\u001b[0;95m     model: il y a une fois. vous avez les choses.\u001b[0;0m\n",
            "\u001b[0mquel âge avez-vous ? j'ai fêté mes quatre ans au jour de mon anniversaire!\u001b[0;0m\n",
            "\u001b[1;94m    labels: je suis une vieille femme. j'ai gagné une médaille d'or en 1993\u001b[0;0m\n",
            "\u001b[0;95m     model: c'est bien, je suis désolé .\u001b[0;0m\n",
            "\u001b[0mHou la la ! ! tu as gagné l'or! êtes-vous riche ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: non . je dépense tout mon argent sur des chapeaux! j'en ai plus d'un millier\u001b[0;0m\n",
            "\u001b[0;95m     model: je n'ai pas, mais j'aime faire des animaux deux.\u001b[0;0m\n"
          ]
        }
      ],
      "source": [
        "print(f'{finetuned_model_path}/model')\n",
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat',\n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    dict_file= dict_file,\n",
        "\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    inference = 'topk', \n",
        "    temperature = 0.7, \n",
        "    topk=30, \n",
        "    beam_length_penalty=1.03\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVWS7OYh41gx",
        "outputId": "58755077-ea3b-4984-83b5-87ee33c24c9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting twython\n",
            "  Downloading twython-3.9.1-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython) (2.27.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.2.0)\n",
            "Installing collected packages: twython\n",
            "Successfully installed twython-3.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install twython "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ5Bnd4v41gy"
      },
      "outputs": [],
      "source": [
        "test_set_length = sum(len(d) for d in dialogs_test)\n",
        "model_path = f'{finetuned_model_path}/model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiuFwqwR41gy"
      },
      "outputs": [],
      "source": [
        "# another_model_path = f'{mydrive_path}finetuned-beam_prmts_infrnc'\n",
        "\n",
        "!(parlai display_model \\\n",
        "    --task 'fromfile:parlaiformat' \\\n",
        "    --fromfile-datapath 'fr_finetuned' \\\n",
        "    --datatype test \\\n",
        "    --fromfile-datatype-extension True \\\n",
        "    --model-file $model_path \\\n",
        "    --dict-file $dict_file \\\n",
        "    --num-examples $test_set_length \\\n",
        "    --skip-generation False \\\n",
        "    --inference 'topk' \\\n",
        "    --temperature 0.7 \\\n",
        "    --topk 30 \\\n",
        "    --beam-length-penalty 1.03) \\\n",
        "    > \"09-finetuned-400m-topk-5epochs-testset-output.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOiQc4Yk41gy"
      },
      "outputs": [],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat',\n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    datatype= \"test\",\n",
        "    num_examples=test_set_length,\n",
        "    skip_generation=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06.1.Training"
      ],
      "metadata": {
        "id": "nxMkxGXAMqDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !ls -lh /content/fr_red*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bYaOj9d6BuL",
        "outputId": "1a7cab27-a28c-40f3-e425-1123e341cf5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 438M May  3 08:35 /content/fr_reddit.txt\n",
            "-rw-r--r-- 1 root root 160M May  3 11:01 /content/fr_reddit.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://kkb-production.jupyter-proxy.kaggle.net/k/94631344/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2IiwidHlwIjoiSldUIn0..bNqg6wTYqaf4XVJcYZ9lNw.U1JvAYEzhkK9IZdAVy93UP9pTRp-2DxS5Dt0J257NAVqrsOG_T244zJ_Ld1pDP4arBq5AdmzVRJVot8TOir7iZh6rvf5GyMQw91C9ppVrzJMXytdnZzrusHfXsK_xOfaCZHQBVXc2pkjqJ95ZGFoufXKm8mtnVgehJAnOiQc_Rg7Z2TVRF0ISoNMIzE26tenz2kvP-Jxhy1gbSmf4IrEMg.KnxjDOwKGyGY00XXQj8CpA/proxy/files/fr_reddit.zip"
      ],
      "metadata": {
        "id": "NHKs2kv5MyUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/colabs/aliae-workspace/datasets/fr_reddit.zip /content/\n",
        "!unzip -q fr_reddit.zip"
      ],
      "metadata": {
        "id": "v6hKNRMflH2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/\"\n",
        "# !rm -R $model_path\n",
        "# !mkdir $model_path\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    task= 'fromfile:parlaiformat', \n",
        "    fromfile_datapath='fr_reddit',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model= \"transformer/generator\",\n",
        "    model_file= f\"{model_path}model\",\n",
        "    \n",
        "    # depend on your gpu\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 3,\n",
        "    log_every_n_secs= 60,\n",
        "    verbose = True,\n",
        "    attention_dropout= 0.0, \n",
        "    batchsize= 32, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    \n",
        "    # arguments we get from the pretrained model. \"from recipes page for 2.7B model\" \n",
        "    embedding_size= 2560, ffn_size= 10240,\n",
        "    variant= \"prelayernorm\",\n",
        "    n_heads= 32, n_positions= 128, \n",
        "    n_encoder_layers= 2, n_decoder_layers= 24,\n",
        "    \n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "    dropout= 0.1,\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    optimizer= \"adam\",\n",
        "    relu_dropout= 0.0, \n",
        "    model_parallel= True,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "    # save_after_valid= True,\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    # inference = 'topk', \n",
        "    # temperature = 0.7, \n",
        "    # topk=30, \n",
        "    # beam_length_penalty=1.03\n",
        ")\n"
      ],
      "metadata": {
        "id": "55ScZU3TsuJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06.2 Coherency Metric"
      ],
      "metadata": {
        "id": "YE6aLi9CneA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/Sahajtomar/french_semantic\n",
        "!git clone https://huggingface.co/inokufu/flaubert-base-uncased-xnli-sts\n",
        "!git clone https://huggingface.co/dangvantuan/sentence-camembert-large\n",
        "!git clone https://huggingface.co/dangvantuan/sentence-camembert-base\n",
        "\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "_52TwTZ3nlB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_lines = []\n",
        "# with open(\"/kaggle/input/modelsresults/09-finetuned-400m-topk-5epochs-testset-output.txt\") as f:\n",
        "# with open(\"/kaggle/input/dialogs/french_blended_skill_talk_test_set.txt\") as f:\n",
        "with open(\"/content/french_bst_test_set.txt\") as f:\n",
        "    output_lines = f.readlines()\n",
        "    \n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "# model =  SentenceTransformer(\"inokufu/flaubert-base-uncased-xnli-sts\")\n",
        "# model = SentenceTransformer('Sahajtomar/french_semantic')\n",
        "model =  SentenceTransformer(\"dangvantuan/sentence-camembert-base\")\n",
        "# model =  SentenceTransformer(\"dangvantuan/sentence-camembert-large\")"
      ],
      "metadata": {
        "id": "vBBkAc8MnuSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for pure datasets\n",
        "for i,line in enumerate(output_lines):\n",
        "    if (line.startswith(\"[id]: dataset_french\")): break\n",
        "        \n",
        "label = \"\"\n",
        "generated = \"\"\n",
        "text = \"\"\n",
        "context = \"\"\n",
        "list_of_similarity_scores_for_turns_in_a_dialog = []\n",
        "list_of_similarity_scores_for_all_dialogs = []\n",
        "\n",
        "for i,line in enumerate(output_lines[i+1:]):\n",
        "    if (line.startswith(\"[labels]: \")):\n",
        "        label = line[len(\"[labels]: \")+1:].replace(\"\\n\",\"\")\n",
        "#         print(\"context:\", context)\n",
        "#         print(\"label:\", label)\n",
        "        \n",
        "        # print(label, context)\n",
        "        embeddings_label = model.encode(label, convert_to_tensor=True)\n",
        "        embeddings_context = model.encode(context[:300], convert_to_tensor=True)\n",
        "        cosine_similarity = util.pytorch_cos_sim(embeddings_label, embeddings_context)\n",
        "#         print(\"similarity:\", cosine_similarity.item(),\"\\n\\n\")\n",
        "        list_of_similarity_scores_for_turns_in_a_dialog.append(cosine_similarity.item())\n",
        "        context += label\n",
        "        \n",
        "#     elif (line.startswith(\"     model:\")):\n",
        "#         generated = line[len(\"     model:\")+1:].replace(\"\\n\",\"\")\n",
        "        \n",
        "    elif (line.startswith(\"[text]: \")):\n",
        "        text = line[len(\"[text]: \")+1:].replace(\"\\n\",\"\")\n",
        "        context += text\n",
        "        \n",
        "    elif (line.startswith(\"- - - - - - - END OF EPISODE - - - - - - - - - -\")):\n",
        "        average_similarity_score = sum(list_of_similarity_scores_for_turns_in_a_dialog) / len(list_of_similarity_scores_for_turns_in_a_dialog)\n",
        "        list_of_similarity_scores_for_all_dialogs.append(average_similarity_score)\n",
        "        context = \"\"\n",
        "        label = \"\"\n",
        "        print(f\"{i}: Similarity Score for a dialog:\",average_similarity_score)\n",
        "    \n",
        "#     if len(list_of_similarity_scores_for_all_dialogs) > 5: break\n",
        "\n",
        "\n",
        "print(\"Count: \", len(list_of_similarity_scores_for_all_dialogs))\n",
        "print(\"Average: \", sum(list_of_similarity_scores_for_all_dialogs)/ len(list_of_similarity_scores_for_all_dialogs))\n",
        "print(list_of_similarity_scores_for_all_dialogs)"
      ],
      "metadata": {
        "id": "gF2ggG3Rnuin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7Rsg9Ooho6v"
      },
      "source": [
        "# 07.Fine tuning (with python command line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzINEYAzc2_7"
      },
      "outputs": [],
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3l5AXEHkOXZ"
      },
      "source": [
        "parlai setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfqz8yVih2hm",
        "outputId": "1b145840-b0c4-4169-ba23-b08fffaf8baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ParlAI'...\n",
            "remote: Enumerating objects: 43583, done.\u001b[K\n",
            "remote: Counting objects: 100% (3188/3188), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1453/1453), done.\u001b[K\n",
            "remote: Total 43583 (delta 2049), reused 2678 (delta 1688), pack-reused 40395\u001b[K\n",
            "Receiving objects: 100% (43583/43583), 89.98 MiB | 15.51 MiB/s, done.\n",
            "Resolving deltas: 100% (30926/30926), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/ParlAI.git ParlAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qojEvGW4kS0r"
      },
      "outputs": [],
      "source": [
        "%cd ParlAI\n",
        "!python setup.py develop\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk-Tmkd-jFBP"
      },
      "outputs": [],
      "source": [
        "# %cd ParlAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kxu1N3mjH6e"
      },
      "outputs": [],
      "source": [
        "# !git log -n 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySnXGpsTj7Y_"
      },
      "outputs": [],
      "source": [
        "# !git checkout 1fa8f7a8034a394149b1c9f7ead2a021bf08518a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xa4F3WYkHqZ"
      },
      "outputs": [],
      "source": [
        "# %cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8ez71jTkqH1"
      },
      "source": [
        "display data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlUXmAiotwEm",
        "outputId": "1ef24816-66d5-4b0c-c2c1-e4d676b3bb81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: iopath in /usr/local/lib/python3.7/dist-packages/iopath-0.1.9-py3.7.egg (0.1.9)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from iopath) (4.62.3)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install iopath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwZdc12_hqwl",
        "outputId": "039ab894-e468-48c8-9a57-2d2743de04c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11:36:09 INFO | Opt:\n",
            "11:36:09 INFO |     allow_missing_init_opts: False\n",
            "11:36:09 INFO |     batchsize: 1\n",
            "11:36:09 INFO |     datapath: /content/ParlAI/data\n",
            "11:36:09 INFO |     datatype: train:ordered\n",
            "11:36:09 INFO |     dict_class: None\n",
            "11:36:09 INFO |     display_add_fields: \n",
            "11:36:09 INFO |     download_path: None\n",
            "11:36:09 INFO |     dynamic_batching: None\n",
            "11:36:09 INFO |     fromfile_datapath: fr_finet\n",
            "11:36:09 INFO |     fromfile_datatype_extension: True\n",
            "11:36:09 INFO |     hide_labels: False\n",
            "11:36:09 INFO |     ignore_agent_reply: True\n",
            "11:36:09 INFO |     image_cropsize: 224\n",
            "11:36:09 INFO |     image_mode: raw\n",
            "11:36:09 INFO |     image_size: 256\n",
            "11:36:09 INFO |     init_model: None\n",
            "11:36:09 INFO |     init_opt: None\n",
            "11:36:09 INFO |     is_debug: False\n",
            "11:36:09 INFO |     loglevel: info\n",
            "11:36:09 INFO |     max_display_len: 1000\n",
            "11:36:09 INFO |     model: None\n",
            "11:36:09 INFO |     model_file: None\n",
            "11:36:09 INFO |     multitask_weights: [1]\n",
            "11:36:09 INFO |     mutators: None\n",
            "11:36:09 INFO |     num_examples: 10\n",
            "11:36:09 INFO |     override: \"{'task': 'fromfile', 'fromfile_datapath': 'fr_finet', 'fromfile_datatype_extension': True}\"\n",
            "11:36:09 INFO |     parlai_home: /content/ParlAI\n",
            "11:36:09 INFO |     starttime: Dec08_11-36\n",
            "11:36:09 INFO |     task: fromfile\n",
            "11:36:09 INFO |     verbose: False\n",
            "11:36:09 INFO | creating task(s): fromfile\n",
            "11:36:09 INFO | Loading ParlAI text data: fr_finet_train.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finet_train.txt - - -\u001b[0;0m\n",
            "\u001b[0msalut comment vas-tu ? Je me prépare à faire du guépard pour rester en forme.\u001b[0;0m\n",
            "   \u001b[1;94mvous devez être très rapide. la chasse est l'un de mes passe-temps préférés.\u001b[0;0m\n",
            "\u001b[0mje suis ! pour mon hobby j'aime faire la mise en conserve ou un peu tailler.\u001b[0;0m\n",
            "   \u001b[1;94mje remodèle aussi des maisons quand je ne suis pas à la chasse à l'arc.\u001b[0;0m\n",
            "\u001b[0mc'est bien. quand j'étais au lycée, je me suis placé 6ème au 100m dash!\u001b[0;0m\n",
            "   \u001b[1;94mc'est génial . avez-vous une saison ou une période préférée de l'année?\u001b[0;0m\n",
            "\u001b[0mNon . mais j'ai une viande préférée car c'est tout ce que je mange exclusivement.\u001b[0;0m\n",
            "   \u001b[1;94mquelle est votre viande préférée à manger?\u001b[0;0m\n",
            "\u001b[0mje devrais dire sa côte de bœuf. avez-vous des aliments préférés?\u001b[0;0m\n",
            "   \u001b[1;94mj'aime le poulet ou les macaronis et le fromage.\u001b[0;0m\n",
            "\u001b[0mavez-vous prévu quelque chose pour aujourd'hui? je pense que je vais faire de la mise en conserve.\u001b[0;0m\n",
            "   \u001b[1;94mje vais regarder le football. que conservez-vous?\u001b[0;0m\n",
            "\u001b[0mje pense que je vais pouvoir un peu de confiture. jouez-vous aussi pour le plaisir?\u001b[0;0m\n",
            "   \u001b[1;94msi j'ai le temps en dehors des maisons de chasse et de rénovation. ce qui n'est pas grand chose!\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_finet_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mBonjour comment allez-vous aujourd'hui ?\u001b[0;0m\n",
            "   \u001b[1;94mje passe du temps avec mes 4 soeurs que faites-vous\u001b[0;0m\n",
            "\u001b[0mwow, quatre sœurs. je regarde juste le jeu des trônes.\u001b[0;0m\n",
            "   \u001b[1;94mc'est un bon spectacle je regarde ça en buvant du thé glacé\u001b[0;0m\n",
            "\u001b[0mje suis d'accord . Comment gagnez-vous votre vie ?\u001b[0;0m\n",
            "   \u001b[1;94mje suis un chercheur je recherche le fait que les sirènes sont réelles\u001b[0;0m\n",
            "11:36:11 INFO | loaded 16878 episodes with a total of 124122 examples\n"
          ]
        }
      ],
      "source": [
        "!python ParlAI/parlai/scripts/display_data.py --task fromfile --fromfile_datapath fr_finet  --fromfile-datatype-extension True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAzvDiRZlQYE"
      },
      "source": [
        "fine tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCmTsMaQwx1n"
      },
      "outputs": [],
      "source": [
        "# !pip install urllib3==1.26.7\n",
        "# !pip install Sphinx==2.2.0\n",
        "# !pip install attrs==20.2.0\n",
        "# !pip install regex==2021.8.3\n",
        "# !pip install importlib-metadata==4.0.1\n",
        "# !pip install docutils==0.14\n",
        "# !pip3 uninstall awsebcli && pip3 install --upgrade awsebcli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8_dp0k-lRhS",
        "outputId": "61800703-38d6-47de-cc52-0f88a4634da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13:19:44 | building data: /content/ParlAI/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n",
            "13:19:44 | Downloading http://parl.ai/downloads/_models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz to /content/ParlAI/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n",
            "Downloading tutorial_transformer_generator_v1.tar.gz: 100% 1.12G/1.12G [01:32<00:00, 12.1MB/s]\n",
            "13:21:41 | building dictionary first...\n",
            "13:21:41 | No model with opt yet at: model/test_train_90M(.opt)\n",
            "13:21:41 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /content/ParlAI/data,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: fr_finet,fromfile_datatype_extension: True,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,parlai_home: /content/ParlAI\u001b[0m\n",
            "13:21:41 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --multitask-weights 1 --batchsize 48 --num-epochs 5.0 --validation-every-n-secs 1800.0 --save-every-n-secs -1 --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "13:21:41 | Using CUDA\n",
            "13:21:41 | loading dictionary from /content/ParlAI/data/models/tutorial_transformer_generator/model.dict\n",
            "13:21:41 | num words = 54944\n",
            "13:21:42 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
            "13:21:45 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "13:21:45 | Loading existing model params from /content/ParlAI/data/models/tutorial_transformer_generator/model\n",
            "13:21:46 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
            "13:21:46 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
            "13:21:47 | Opt:\n",
            "13:21:47 |     activation: gelu\n",
            "13:21:47 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "13:21:47 |     adam_eps: 1e-08\n",
            "13:21:47 |     add_p1_after_newln: False\n",
            "13:21:47 |     aggregate_micro: False\n",
            "13:21:47 |     allow_missing_init_opts: False\n",
            "13:21:47 |     attention_dropout: 0.0\n",
            "13:21:47 |     batchsize: 16\n",
            "13:21:47 |     beam_block_full_context: True\n",
            "13:21:47 |     beam_block_list_filename: None\n",
            "13:21:47 |     beam_block_ngram: -1\n",
            "13:21:47 |     beam_context_block_ngram: -1\n",
            "13:21:47 |     beam_delay: 30\n",
            "13:21:47 |     beam_length_penalty: 0.65\n",
            "13:21:47 |     beam_min_length: 1\n",
            "13:21:47 |     beam_size: 1\n",
            "13:21:47 |     betas: '(0.9, 0.999)'\n",
            "13:21:47 |     bpe_add_prefix_space: None\n",
            "13:21:47 |     bpe_debug: False\n",
            "13:21:47 |     bpe_dropout: None\n",
            "13:21:47 |     bpe_merge: None\n",
            "13:21:47 |     bpe_vocab: None\n",
            "13:21:47 |     checkpoint_activations: False\n",
            "13:21:47 |     compute_tokenized_bleu: False\n",
            "13:21:47 |     datapath: /content/ParlAI/data\n",
            "13:21:47 |     datatype: train\n",
            "13:21:47 |     delimiter: '\\n'\n",
            "13:21:47 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "13:21:47 |     dict_endtoken: __end__\n",
            "13:21:47 |     dict_file: /content/ParlAI/data/models/tutorial_transformer_generator/model.dict\n",
            "13:21:47 |     dict_include_test: False\n",
            "13:21:47 |     dict_include_valid: False\n",
            "13:21:47 |     dict_initpath: None\n",
            "13:21:47 |     dict_language: english\n",
            "13:21:47 |     dict_loaded: True\n",
            "13:21:47 |     dict_lower: True\n",
            "13:21:47 |     dict_max_ngram_size: -1\n",
            "13:21:47 |     dict_maxexs: -1\n",
            "13:21:47 |     dict_maxtokens: -1\n",
            "13:21:47 |     dict_minfreq: 0\n",
            "13:21:47 |     dict_nulltoken: __null__\n",
            "13:21:47 |     dict_starttoken: __start__\n",
            "13:21:47 |     dict_textfields: text,labels\n",
            "13:21:47 |     dict_tokenizer: bpe\n",
            "13:21:47 |     dict_unktoken: __unk__\n",
            "13:21:47 |     display_examples: False\n",
            "13:21:47 |     download_path: None\n",
            "13:21:47 |     dropout: 0.1\n",
            "13:21:47 |     dynamic_batching: None\n",
            "13:21:47 |     embedding_projection: random\n",
            "13:21:47 |     embedding_size: 512\n",
            "13:21:47 |     embedding_type: random\n",
            "13:21:47 |     embeddings_scale: True\n",
            "13:21:47 |     eval_batchsize: None\n",
            "13:21:47 |     eval_dynamic_batching: None\n",
            "13:21:47 |     evaltask: None\n",
            "13:21:47 |     ffn_size: 2048\n",
            "13:21:47 |     final_extra_opt: \n",
            "13:21:47 |     force_fp16_tokens: False\n",
            "13:21:47 |     fp16: True\n",
            "13:21:47 |     fp16_impl: safe\n",
            "13:21:47 |     fromfile_datapath: fr_finet\n",
            "13:21:47 |     fromfile_datatype_extension: True\n",
            "13:21:47 |     gpu: -1\n",
            "13:21:47 |     gradient_clip: 0.1\n",
            "13:21:47 |     hide_labels: False\n",
            "13:21:47 |     history_add_global_end_token: None\n",
            "13:21:47 |     history_reversed: False\n",
            "13:21:47 |     history_size: -1\n",
            "13:21:47 |     image_cropsize: 224\n",
            "13:21:47 |     image_mode: raw\n",
            "13:21:47 |     image_size: 256\n",
            "13:21:47 |     inference: greedy\n",
            "13:21:47 |     init_model: /content/ParlAI/data/models/tutorial_transformer_generator/model\n",
            "13:21:47 |     init_opt: None\n",
            "13:21:47 |     interactive_mode: False\n",
            "13:21:47 |     invsqrt_lr_decay_gamma: -1\n",
            "13:21:47 |     is_debug: False\n",
            "13:21:47 |     label_truncate: 128\n",
            "13:21:47 |     learn_positional_embeddings: True\n",
            "13:21:47 |     learningrate: 0.0001\n",
            "13:21:47 |     load_from_checkpoint: True\n",
            "13:21:47 |     log_every_n_secs: -1\n",
            "13:21:47 |     log_every_n_steps: 50\n",
            "13:21:47 |     loglevel: info\n",
            "13:21:47 |     lr_scheduler: reduceonplateau\n",
            "13:21:47 |     lr_scheduler_decay: 0.5\n",
            "13:21:47 |     lr_scheduler_patience: 3\n",
            "13:21:47 |     max_train_steps: -1\n",
            "13:21:47 |     max_train_time: -1\n",
            "13:21:47 |     metrics: default\n",
            "13:21:47 |     model: transformer/generator\n",
            "13:21:47 |     model_file: model/test_train_90M\n",
            "13:21:47 |     model_parallel: False\n",
            "13:21:47 |     momentum: 0\n",
            "13:21:47 |     multitask_weights: '(1.0, 3.0, 3.0, 3.0)'\n",
            "13:21:47 |     mutators: None\n",
            "13:21:47 |     n_decoder_layers: -1\n",
            "13:21:47 |     n_encoder_layers: -1\n",
            "13:21:47 |     n_heads: 16\n",
            "13:21:47 |     n_layers: 8\n",
            "13:21:47 |     n_positions: 512\n",
            "13:21:47 |     n_segments: 0\n",
            "13:21:47 |     nesterov: True\n",
            "13:21:47 |     no_cuda: False\n",
            "13:21:47 |     num_epochs: 2.0\n",
            "13:21:47 |     num_workers: 0\n",
            "13:21:47 |     nus: (0.7,)\n",
            "13:21:47 |     optimizer: adamax\n",
            "13:21:47 |     output_scaling: 1.0\n",
            "13:21:47 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'fr_finet', 'fromfile_datatype_extension': True, 'model': 'transformer/generator', 'multitask_weights': (1.0, 3.0, 3.0, 3.0), 'init_model': 'zoo:tutorial_transformer_generator/model', 'dict_file': '/content/ParlAI/data/models/tutorial_transformer_generator/model.dict', 'embedding_size': 512, 'n_layers': 8, 'ffn_size': 2048, 'dropout': 0.1, 'n_heads': 16, 'learn_positional_embeddings': True, 'n_positions': 512, 'variant': 'xlm', 'activation': 'gelu', 'skip_generation': True, 'fp16': True, 'text_truncate': 512, 'label_truncate': 128, 'dict_tokenizer': 'bpe', 'dict_lower': True, 'learningrate': 0.0001, 'optimizer': 'adamax', 'lr_scheduler': 'reduceonplateau', 'gradient_clip': 0.1, 'validation_every_n_epochs': 0.25, 'betas': (0.9, 0.999), 'update_freq': 1, 'attention_dropout': 0.0, 'relu_dropout': 0.0, 'validation_patience': 15, 'save_every_n_secs': 60.0, 'validation_max_exs': 20000, 'batchsize': 16, 'validation_metric': 'ppl', 'validation_metric_mode': 'min', 'save_after_valid': True, 'num_epochs': 2.0, 'model_file': 'model/test_train_90M'}\"\n",
            "13:21:47 |     parlai_home: /content/ParlAI\n",
            "13:21:47 |     person_tokens: False\n",
            "13:21:47 |     rank_candidates: False\n",
            "13:21:47 |     relu_dropout: 0.0\n",
            "13:21:47 |     save_after_valid: True\n",
            "13:21:47 |     save_every_n_secs: 60.0\n",
            "13:21:47 |     share_word_embeddings: True\n",
            "13:21:47 |     short_final_eval: False\n",
            "13:21:47 |     skip_generation: True\n",
            "13:21:47 |     special_tok_lst: None\n",
            "13:21:47 |     split_lines: False\n",
            "13:21:47 |     starttime: Dec08_13-21\n",
            "13:21:47 |     task: fromfile:parlaiformat\n",
            "13:21:47 |     temperature: 1.0\n",
            "13:21:47 |     tensorboard_log: False\n",
            "13:21:47 |     tensorboard_logdir: None\n",
            "13:21:47 |     text_truncate: 512\n",
            "13:21:47 |     topk: 10\n",
            "13:21:47 |     topp: 0.9\n",
            "13:21:47 |     truncate: -1\n",
            "13:21:47 |     update_freq: 1\n",
            "13:21:47 |     use_reply: label\n",
            "13:21:47 |     validation_cutoff: 1.0\n",
            "13:21:47 |     validation_every_n_epochs: 0.25\n",
            "13:21:47 |     validation_every_n_secs: -1\n",
            "13:21:47 |     validation_every_n_steps: -1\n",
            "13:21:47 |     validation_max_exs: 20000\n",
            "13:21:47 |     validation_metric: ppl\n",
            "13:21:47 |     validation_metric_mode: min\n",
            "13:21:47 |     validation_patience: 15\n",
            "13:21:47 |     validation_share_agent: False\n",
            "13:21:47 |     variant: xlm\n",
            "13:21:47 |     verbose: False\n",
            "13:21:47 |     wandb_entity: None\n",
            "13:21:47 |     wandb_log: False\n",
            "13:21:47 |     wandb_name: None\n",
            "13:21:47 |     wandb_project: None\n",
            "13:21:47 |     warmup_rate: 0.0001\n",
            "13:21:47 |     warmup_updates: -1\n",
            "13:21:47 |     weight_decay: None\n",
            "13:21:47 | Current ParlAI commit: d9548b567d8c5a271b157e9a8bbbf9a24714a2a1\n",
            "13:21:47 | creating task(s): fromfile:parlaiformat\n",
            "13:21:47 | Loading ParlAI text data: fr_finet_train.txt\n",
            "13:21:48 | training...\n",
            "13:22:51 | saving model checkpoint: model/test_train_90M.checkpoint\n",
            "13:22:51 | Saving dictionary to model/test_train_90M.checkpoint.dict\n",
            "Traceback (most recent call last):\n",
            "  File \"ParlAI/parlai/scripts/train_model.py\", line 1002, in <module>\n",
            "    TrainModel.main()\n",
            "  File \"/content/ParlAI/parlai/core/script.py\", line 129, in main\n",
            "    return cls._run_args(None)\n",
            "  File \"/content/ParlAI/parlai/core/script.py\", line 101, in _run_args\n",
            "    return cls._run_from_parser_and_opt(opt, parser)\n",
            "  File \"/content/ParlAI/parlai/core/script.py\", line 108, in _run_from_parser_and_opt\n",
            "    return script.run()\n",
            "  File \"ParlAI/parlai/scripts/train_model.py\", line 998, in run\n",
            "    return self.train_loop.train()\n",
            "  File \"ParlAI/parlai/scripts/train_model.py\", line 950, in train\n",
            "    for _train_log in self.train_steps():\n",
            "  File \"ParlAI/parlai/scripts/train_model.py\", line 857, in train_steps\n",
            "    world.parley()\n",
            "  File \"/content/ParlAI/parlai/core/worlds.py\", line 873, in parley\n",
            "    batch_act = self.batch_act(agent_idx, batch_observations[agent_idx])\n",
            "  File \"/content/ParlAI/parlai/core/worlds.py\", line 841, in batch_act\n",
            "    batch_actions = a.batch_act(batch_observation)\n",
            "  File \"/content/ParlAI/parlai/core/torch_agent.py\", line 2234, in batch_act\n",
            "    output = self.train_step(batch)\n",
            "  File \"/content/ParlAI/parlai/core/torch_generator_agent.py\", line 735, in train_step\n",
            "    self.backward(loss)\n",
            "  File \"/content/ParlAI/parlai/core/torch_agent.py\", line 2320, in backward\n",
            "    self.optimizer.backward(loss, update_main_grads=False)\n",
            "  File \"/content/ParlAI/parlai/utils/fp16.py\", line 194, in backward\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 307, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 156, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "#  -t fromfile:parlaiformat \\\n",
        "#   --fromfile_datapath Fr_persona_split_valid_parlai.txt \n",
        "\n",
        "!python ParlAI/parlai/scripts/train_model.py \\\n",
        "  --task fromfile:parlaiformat --fromfile_datapath fr_finet --fromfile-datatype-extension True \\\n",
        "  -m transformer/generator \\\n",
        "   --multitask-weights 1,3,3,3 --init-model zoo:tutorial_transformer_generator/model \\\n",
        "    --dict-file zoo:tutorial_transformer_generator/model.dict \\\n",
        "     --embedding-size 512 \\\n",
        "      --n-layers 8 \\\n",
        "       --ffn-size 2048 \\\n",
        "        --dropout 0.1 \\\n",
        "         --n-heads 16 \\\n",
        "          --learn-positional-embeddings True \\\n",
        "           --n-positions 512 \\\n",
        "            --variant xlm \\\n",
        "             --activation gelu \\\n",
        "              --skip-generation True \\\n",
        "               --fp16 True \\\n",
        "                --text-truncate 512 \\\n",
        "                 --label-truncate 128 \\\n",
        "                  --dict-tokenizer bpe \\\n",
        "                   --dict-lower True -lr 1e-04 \\\n",
        "                    --optimizer adamax \\\n",
        "                     --lr-scheduler reduceonplateau \\\n",
        "                      --gradient-clip 0.1 -veps 0.25 \\\n",
        "                       --betas 0.9,0.999 \\\n",
        "                        --update-freq 1 \\\n",
        "                         --attention-dropout 0.0 \\\n",
        "                          --relu-dropout 0.0 \\\n",
        "                           --skip-generation True -vp 15 -stim 60 -vme 20000 -bs 16 -vmt ppl -vmm min \\\n",
        "                            --save-after-valid True \\\n",
        "                            --num_epochs 2 \\\n",
        "                             --model-file model/test_train_90M -bs 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXYc5iCRnfPW"
      },
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXLaBsP3ngSB",
        "outputId": "b54e7b3e-5f22-4689-a8d2-bb53685b6894"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16:29:02 WARN | Overriding opt[\"task\"] to blended_skill_talk (previously: fromfile:parlaiformat)\n",
            "16:29:02 INFO | Using CUDA\n",
            "16:29:02 INFO | loading dictionary from model/test_train_90M.dict\n",
            "16:29:02 INFO | num words = 54944\n",
            "16:29:02 INFO | TransformerGenerator: full interactive mode on.\n",
            "16:29:03 WARN | DEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\n",
            "16:29:06 INFO | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "16:29:06 INFO | Loading existing model params from model/test_train_90M\n",
            "16:29:10 INFO | Opt:\n",
            "16:29:10 INFO |     activation: gelu\n",
            "16:29:10 INFO |     adafactor_eps: '[1e-30, 0.001]'\n",
            "16:29:10 INFO |     adam_eps: 1e-08\n",
            "16:29:10 INFO |     add_p1_after_newln: False\n",
            "16:29:10 INFO |     aggregate_micro: False\n",
            "16:29:10 INFO |     allow_missing_init_opts: False\n",
            "16:29:10 INFO |     attention_dropout: 0.0\n",
            "16:29:10 INFO |     batchsize: 16\n",
            "16:29:10 INFO |     beam_block_full_context: True\n",
            "16:29:10 INFO |     beam_block_list_filename: None\n",
            "16:29:10 INFO |     beam_block_ngram: -1\n",
            "16:29:10 INFO |     beam_context_block_ngram: -1\n",
            "16:29:10 INFO |     beam_delay: 30\n",
            "16:29:10 INFO |     beam_length_penalty: 0.65\n",
            "16:29:10 INFO |     beam_min_length: 1\n",
            "16:29:10 INFO |     beam_size: 1\n",
            "16:29:10 INFO |     betas: '[0.9, 0.999]'\n",
            "16:29:10 INFO |     bpe_add_prefix_space: None\n",
            "16:29:10 INFO |     bpe_debug: False\n",
            "16:29:10 INFO |     bpe_dropout: None\n",
            "16:29:10 INFO |     bpe_merge: None\n",
            "16:29:10 INFO |     bpe_vocab: None\n",
            "16:29:10 INFO |     checkpoint_activations: False\n",
            "16:29:10 INFO |     compute_tokenized_bleu: False\n",
            "16:29:10 INFO |     datapath: /content/ParlAI/data\n",
            "16:29:10 INFO |     datatype: train\n",
            "16:29:10 INFO |     delimiter: '\\n'\n",
            "16:29:10 INFO |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "16:29:10 INFO |     dict_endtoken: __end__\n",
            "16:29:10 INFO |     dict_file: model/test_train_90M.dict\n",
            "16:29:10 INFO |     dict_include_test: False\n",
            "16:29:10 INFO |     dict_include_valid: False\n",
            "16:29:10 INFO |     dict_initpath: None\n",
            "16:29:10 INFO |     dict_language: english\n",
            "16:29:10 INFO |     dict_loaded: True\n",
            "16:29:10 INFO |     dict_lower: True\n",
            "16:29:10 INFO |     dict_max_ngram_size: -1\n",
            "16:29:10 INFO |     dict_maxexs: -1\n",
            "16:29:10 INFO |     dict_maxtokens: -1\n",
            "16:29:10 INFO |     dict_minfreq: 0\n",
            "16:29:10 INFO |     dict_nulltoken: __null__\n",
            "16:29:10 INFO |     dict_starttoken: __start__\n",
            "16:29:10 INFO |     dict_textfields: text,labels\n",
            "16:29:10 INFO |     dict_tokenizer: bpe\n",
            "16:29:10 INFO |     dict_unktoken: __unk__\n",
            "16:29:10 INFO |     display_add_fields: \n",
            "16:29:10 INFO |     display_examples: False\n",
            "16:29:10 INFO |     display_partner_persona: True\n",
            "16:29:10 INFO |     display_prettify: False\n",
            "16:29:10 INFO |     download_path: None\n",
            "16:29:10 INFO |     dropout: 0.1\n",
            "16:29:10 INFO |     dynamic_batching: None\n",
            "16:29:10 INFO |     embedding_projection: random\n",
            "16:29:10 INFO |     embedding_size: 512\n",
            "16:29:10 INFO |     embedding_type: random\n",
            "16:29:10 INFO |     embeddings_scale: True\n",
            "16:29:10 INFO |     eval_batchsize: None\n",
            "16:29:10 INFO |     eval_dynamic_batching: None\n",
            "16:29:10 INFO |     evaltask: None\n",
            "16:29:10 INFO |     ffn_size: 2048\n",
            "16:29:10 INFO |     final_extra_opt: \n",
            "16:29:10 INFO |     force_fp16_tokens: True\n",
            "16:29:10 INFO |     fp16: True\n",
            "16:29:10 INFO |     fp16_impl: safe\n",
            "16:29:10 INFO |     fromfile_datapath: fr_finet\n",
            "16:29:10 INFO |     fromfile_datatype_extension: True\n",
            "16:29:10 INFO |     gpu: -1\n",
            "16:29:10 INFO |     gradient_clip: 0.1\n",
            "16:29:10 INFO |     hide_labels: False\n",
            "16:29:10 INFO |     history_add_global_end_token: None\n",
            "16:29:10 INFO |     history_reversed: False\n",
            "16:29:10 INFO |     history_size: -1\n",
            "16:29:10 INFO |     image_cropsize: 224\n",
            "16:29:10 INFO |     image_mode: raw\n",
            "16:29:10 INFO |     image_size: 256\n",
            "16:29:10 INFO |     include_initial_utterances: False\n",
            "16:29:10 INFO |     include_personas: True\n",
            "16:29:10 INFO |     inference: greedy\n",
            "16:29:10 INFO |     init_model: /content/ParlAI/data/models/tutorial_transformer_generator/model\n",
            "16:29:10 INFO |     init_opt: None\n",
            "16:29:10 INFO |     interactive_mode: True\n",
            "16:29:10 INFO |     interactive_task: True\n",
            "16:29:10 INFO |     invsqrt_lr_decay_gamma: -1\n",
            "16:29:10 INFO |     is_debug: False\n",
            "16:29:10 INFO |     label_truncate: 128\n",
            "16:29:10 INFO |     learn_positional_embeddings: True\n",
            "16:29:10 INFO |     learningrate: 0.0001\n",
            "16:29:10 INFO |     local_human_candidates_file: None\n",
            "16:29:10 INFO |     log_every_n_secs: -1\n",
            "16:29:10 INFO |     log_every_n_steps: 50\n",
            "16:29:10 INFO |     loglevel: info\n",
            "16:29:10 INFO |     lr_scheduler: reduceonplateau\n",
            "16:29:10 INFO |     lr_scheduler_decay: 0.5\n",
            "16:29:10 INFO |     lr_scheduler_patience: 3\n",
            "16:29:10 INFO |     max_train_steps: -1\n",
            "16:29:10 INFO |     max_train_time: -1\n",
            "16:29:10 INFO |     metrics: default\n",
            "16:29:10 INFO |     model: transformer/generator\n",
            "16:29:10 INFO |     model_file: model/test_train_90M\n",
            "16:29:10 INFO |     model_parallel: False\n",
            "16:29:10 INFO |     momentum: 0\n",
            "16:29:10 INFO |     multitask_weights: '[1.0, 3.0, 3.0, 3.0]'\n",
            "16:29:10 INFO |     mutators: None\n",
            "16:29:10 INFO |     n_decoder_layers: -1\n",
            "16:29:10 INFO |     n_encoder_layers: -1\n",
            "16:29:10 INFO |     n_heads: 16\n",
            "16:29:10 INFO |     n_layers: 8\n",
            "16:29:10 INFO |     n_positions: 512\n",
            "16:29:10 INFO |     n_segments: 0\n",
            "16:29:10 INFO |     nesterov: True\n",
            "16:29:10 INFO |     no_cuda: False\n",
            "16:29:10 INFO |     num_epochs: 2.0\n",
            "16:29:10 INFO |     num_workers: 0\n",
            "16:29:10 INFO |     nus: [0.7]\n",
            "16:29:10 INFO |     optimizer: adamax\n",
            "16:29:10 INFO |     output_scaling: 1.0\n",
            "16:29:10 INFO |     override: \"{'task': 'blended_skill_talk', 'model_file': 'model/test_train_90M'}\"\n",
            "16:29:10 INFO |     parlai_home: /content/ParlAI\n",
            "16:29:10 INFO |     person_tokens: False\n",
            "16:29:10 INFO |     rank_candidates: False\n",
            "16:29:10 INFO |     relu_dropout: 0.0\n",
            "16:29:10 INFO |     safe_personas_only: True\n",
            "16:29:10 INFO |     safety: all\n",
            "16:29:10 INFO |     save_after_valid: True\n",
            "16:29:10 INFO |     save_every_n_secs: 60.0\n",
            "16:29:10 INFO |     share_word_embeddings: True\n",
            "16:29:10 INFO |     short_final_eval: False\n",
            "16:29:10 INFO |     single_turn: False\n",
            "16:29:10 INFO |     skip_generation: True\n",
            "16:29:10 INFO |     special_tok_lst: None\n",
            "16:29:10 INFO |     split_lines: False\n",
            "16:29:10 INFO |     starttime: Nov29_14-13\n",
            "16:29:10 INFO |     task: blended_skill_talk\n",
            "16:29:10 INFO |     temperature: 1.0\n",
            "16:29:10 INFO |     tensorboard_log: False\n",
            "16:29:10 INFO |     tensorboard_logdir: None\n",
            "16:29:10 INFO |     text_truncate: 512\n",
            "16:29:10 INFO |     topk: 10\n",
            "16:29:10 INFO |     topp: 0.9\n",
            "16:29:10 INFO |     truncate: -1\n",
            "16:29:10 INFO |     update_freq: 1\n",
            "16:29:10 INFO |     use_reply: label\n",
            "16:29:10 INFO |     validation_cutoff: 1.0\n",
            "16:29:10 INFO |     validation_every_n_epochs: 0.25\n",
            "16:29:10 INFO |     validation_every_n_secs: -1\n",
            "16:29:10 INFO |     validation_every_n_steps: -1\n",
            "16:29:10 INFO |     validation_max_exs: 20000\n",
            "16:29:10 INFO |     validation_metric: ppl\n",
            "16:29:10 INFO |     validation_metric_mode: min\n",
            "16:29:10 INFO |     validation_patience: 15\n",
            "16:29:10 INFO |     validation_share_agent: False\n",
            "16:29:10 INFO |     variant: xlm\n",
            "16:29:10 INFO |     verbose: False\n",
            "16:29:10 INFO |     wandb_entity: None\n",
            "16:29:10 INFO |     wandb_log: False\n",
            "16:29:10 INFO |     wandb_name: None\n",
            "16:29:10 INFO |     wandb_project: None\n",
            "16:29:10 INFO |     warmup_rate: 0.0001\n",
            "16:29:10 INFO |     warmup_updates: -1\n",
            "16:29:10 INFO |     weight_decay: None\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "16:29:10 INFO | building data: /content/ParlAI/data/OffensiveLanguage\n",
            "16:29:10 INFO | Downloading http://parl.ai/downloads/offensive_language/OffensiveLanguage.txt to /content/ParlAI/data/OffensiveLanguage/OffensiveLanguage.txt\n",
            "Downloading OffensiveLanguage.txt: 0.00B [00:00, ?B/s]\n",
            "16:29:11 INFO | building data: /content/ParlAI/data/models/dialogue_safety/safety_models_v1.tgz\n",
            "16:29:11 INFO | Downloading http://parl.ai/downloads/_models/dialogue_safety/safety_models_v1.tgz to /content/ParlAI/data/models/dialogue_safety/safety_models_v1.tgz\n",
            "Downloading safety_models_v1.tgz: 100% 2.23G/2.23G [01:10<00:00, 31.5MB/s]\n",
            "16:31:18 WARN | Overriding opt[\"model\"] to transformer/classifier (previously: transformer_classifier)\n",
            "16:31:18 WARN | Overriding opt[\"model_file\"] to /content/ParlAI/data/models/dialogue_safety/single_turn/model (previously: /checkpoint/edinan/20190828/safety_reddit/contiguous-dropout=0_multitask-weights=0.5,0.1,0.1,0.4,0.2_lr=5e-05_lr-scheduler-patience=3_lr-scheduler-decay=0.9_warmupupdates=1000/model)\n",
            "16:31:18 WARN | Overriding opt[\"print_scores\"] to True (previously: False)\n",
            "16:31:18 WARN | Overriding opt[\"data_parallel\"] to False (previously: True)\n",
            "16:31:18 INFO | Using CUDA\n",
            "16:31:18 INFO | loading dictionary from /content/ParlAI/data/models/dialogue_safety/single_turn/model.dict\n",
            "16:31:18 INFO | num words = 54944\n",
            "16:31:22 INFO | Loading existing model parameters from /content/ParlAI/data/models/dialogue_safety/single_turn/model\n",
            "16:31:24 INFO | Total parameters: 128,042,498 (128,042,498 trainable)\n",
            "16:31:26 INFO | creating task(s): blended_skill_talk\n",
            "[ loading personas.. ]\n",
            "\n",
            "  [NOTE: In the BST paper both partners have a persona.\n",
            "         You can choose to ignore yours, the model never sees it.\n",
            "         In the Blender paper, this was not used for humans.\n",
            "         You can also turn personas off with --include-personas False]\n",
            "\n",
            "[building data: /content/ParlAI/data/blended_skill_talk]\n",
            "16:31:26 INFO | Downloading http://parl.ai/downloads/blended_skill_talk/blended_skill_talk.tar.gz to /content/ParlAI/data/blended_skill_talk/blended_skill_talk.tar.gz\n",
            "Downloading blended_skill_talk.tar.gz: 100% 38.1M/38.1M [00:02<00:00, 16.3MB/s]\n",
            "16:31:30 INFO | Downloading http://parl.ai/downloads/blended_skill_talk/personas_list.txt to /content/ParlAI/data/blended_skill_talk/persona_list.txt\n",
            "Downloading persona_list.txt: 0.00B [00:01, ?B/s]\n",
            "16:31:31 INFO | Downloading http://parl.ai/downloads/blended_skill_talk/topic_to_persona_list.txt to /content/ParlAI/data/blended_skill_talk/topic_to_persona_list.txt\n",
            "Downloading topic_to_persona_list.txt: 0.00B [00:00, ?B/s]\n",
            "16:31:32 INFO | Downloading http://parl.ai/downloads/blended_skill_talk/ed_persona_topicifier__train__both_sides.json to /content/ParlAI/data/blended_skill_talk/ed_persona_topicifier__train__both_sides.json\n",
            "Downloading ed_persona_topicifier__train__both_sides.json: 0.00B [00:02, ?B/s]\n",
            "16:31:34 INFO | Downloading http://parl.ai/downloads/blended_skill_talk/ed_persona_topicifier__train__experiencer_only.json to /content/ParlAI/data/blended_skill_talk/ed_persona_topicifier__train__experiencer_only.json\n",
            "Downloading ed_persona_topicifier__train__experiencer_only.json: 0.00B [00:01, ?B/s]\n",
            "16:31:36 INFO | Downloading http://parl.ai/downloads/blended_skill_talk/ed_persona_topicifier__valid__experiencer_only.json to /content/ParlAI/data/blended_skill_talk/ed_persona_topicifier__valid__experiencer_only.json\n",
            "Downloading ed_persona_topicifier__valid__experiencer_only.json: 0.00B [00:03, ?B/s]\n",
            "16:31:40 INFO | Downloading http://parl.ai/downloads/blended_skill_talk/ed_persona_topicifier__test__experiencer_only.json to /content/ParlAI/data/blended_skill_talk/ed_persona_topicifier__test__experiencer_only.json\n",
            "Downloading ed_persona_topicifier__test__experiencer_only.json: 0.00B [00:03, ?B/s]\n",
            "16:31:44 INFO | Downloading http://parl.ai/downloads/blended_skill_talk/safe_personas_2.txt to /content/ParlAI/data/blended_skill_talk/safe_personas.txt\n",
            "Downloading safe_personas.txt: 0.00B [00:01, ?B/s]\n",
            "16:31:46 INFO | Downloading http://parl.ai/downloads/blended_skill_talk/human_annotations.json to /content/ParlAI/data/blended_skill_talk/human_annotations.json\n",
            "Downloading human_annotations.json: 0.00B [00:00, ?B/s]\n",
            "Loading /content/ParlAI/data/blended_skill_talk/train.json.\n",
            "Saving to /content/ParlAI/data/blended_skill_talk/train.txt\n",
            "Loading /content/ParlAI/data/blended_skill_talk/valid.json.\n",
            "Saving to /content/ParlAI/data/blended_skill_talk/valid.txt\n",
            "Loading /content/ParlAI/data/blended_skill_talk/test.json.\n",
            "Saving to /content/ParlAI/data/blended_skill_talk/test.txt\n",
            "\u001b[0;34m[context]:\u001b[0;0m \u001b[1myour persona: i have a poster of neil tyson degrasse on my wall.\n",
            "your persona: my two dogs are the best friends a girl could have.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m Bonjour\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mbonjour , je suis un chef de cakes .\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m aimez-vous les gâteaux?\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1moui , je suis un chef de cakes .\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m quel est ton nom?\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mje suis un chef de cakes , je suis un chef de cakes .\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m je suis etudiante\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mje suis un chef de cakes , je suis un chef de cakes .\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m et ?\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mje suis un chef de cakes , je suis un chef de cakes .\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m bonjour\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mje suis un chef de cakes , je suis un chef de cakes .\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m Traceback (most recent call last):\n",
            "  File \"ParlAI/parlai/scripts/safe_interactive.py\", line 87, in <module>\n",
            "    SafeInteractive.main()\n",
            "  File \"/content/ParlAI/parlai/core/script.py\", line 129, in main\n",
            "    return cls._run_args(None)\n",
            "  File \"/content/ParlAI/parlai/core/script.py\", line 101, in _run_args\n",
            "    return cls._run_from_parser_and_opt(opt, parser)\n",
            "  File \"/content/ParlAI/parlai/core/script.py\", line 108, in _run_from_parser_and_opt\n",
            "    return script.run()\n",
            "  File \"ParlAI/parlai/scripts/safe_interactive.py\", line 82, in run\n",
            "    return safe_interactive(self.opt)\n",
            "  File \"ParlAI/parlai/scripts/safe_interactive.py\", line 62, in safe_interactive\n",
            "    world.parley()\n",
            "  File \"/content/ParlAI/parlai/tasks/interactive/worlds.py\", line 75, in parley\n",
            "  File \"/content/ParlAI/parlai/agents/safe_local_human/safe_local_human.py\", line 119, in act\n",
            "    reply_text = self.get_reply()\n",
            "  File \"/content/ParlAI/parlai/agents/safe_local_human/safe_local_human.py\", line 105, in get_reply\n",
            "    reply_text = input(colorize('Enter Your Message:', 'field') + ' ')\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python ParlAI/parlai/scripts/safe_interactive.py -t blended_skill_talk -mf model/test_train_90M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGP2HiPgdUV_"
      },
      "source": [
        "# 8.HuggingFace models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paU8Ou7votOe",
        "outputId": "e3939931-7372-4833-e80f-46c48c9f9085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 10.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 35.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBn011zHwm9z"
      },
      "source": [
        " link: https://huggingface.co/transformers/model_doc/blenderbot.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310,
          "referenced_widgets": [
            "5eb7047640544f9f82429f570a855a3d",
            "eb3bb2b868d942dfb9f239ad0e418e1a",
            "82c506f4b0354b6382533ac5d99b65be",
            "8eb299d7e51045fb89aa2bba902c75b3",
            "e4a94e72d0bd4e52a23b2b7e29e3a171",
            "1fcd9124ca0b411ea097aa126751a65a",
            "157dbec4c26b4de99a2e6c4ea4c5ef4e"
          ]
        },
        "id": "EX0hARHToqLs",
        "outputId": "cf00891d-24f7-4ccc-ef3d-fa52f62a0a0b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5eb7047640544f9f82429f570a855a3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb3bb2b868d942dfb9f239ad0e418e1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBlenderbotForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBlenderbotForConditionalGeneration were initialized from the model checkpoint at facebook/blenderbot-400M-distill.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBlenderbotForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82c506f4b0354b6382533ac5d99b65be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/124k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8eb299d7e51045fb89aa2bba902c75b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/61.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4a94e72d0bd4e52a23b2b7e29e3a171",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.13k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fcd9124ca0b411ea097aa126751a65a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/16.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "157dbec4c26b4de99a2e6c4ea4c5ef4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BlenderbotTokenizer, TFBlenderbotForConditionalGeneration\n",
        "mname = 'facebook/blenderbot-400M-distill'\n",
        "model = TFBlenderbotForConditionalGeneration.from_pretrained(mname)\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8Mq6Pxxo1EA",
        "outputId": "1a51874e-64df-4105-a85c-b2f486644770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:   I am a student at the University of Pittsburgh. What are you studying?\n"
          ]
        }
      ],
      "source": [
        "NEXT_UTTERANCE = (\n",
        "\"Hi, I am a master student at Lorraine University in France, what about you? </s>\"\n",
        "\"<s> I am a student at the University of Pittsburgh. What are you studying? </s>\"\n",
        ")\n",
        "# print(\"Human: \", UTTERANCE)\n",
        "inputs = tokenizer([NEXT_UTTERANCE], return_tensors='tf')\n",
        "reply_ids = model.generate(**inputs)\n",
        "ans = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
        "print(\"Bot: \", ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kqfl1uQCptq1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkN5zF87wba7"
      },
      "source": [
        "link: https://huggingface.co/cedpsam/chatbot_fr?text=Si+vous+pouviez+d%C3%AEner+avec+une+personne%2C+morte+ou+vivante%2C+qui+serait-ce+%3F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9ok_uZawcdH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cedpsam/chatbot_fr\")\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(\"cedpsam/chatbot_fr\")\n",
        "\n",
        "for step in range(6):\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
        "    # print(new_user_input_ids)\n",
        "\n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "    # generated a response while limiting the total chat history to 1000 tokens, \n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids, max_length=30,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_p=0.92, top_k = 10\n",
        "    )\n",
        "    \n",
        "    # pretty print last ouput tokens from bot\n",
        "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB82PnXIwgu4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlX1r5rrZTnc"
      },
      "source": [
        "# 9.Self-chat (Blender)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcnNZmyahSs9"
      },
      "source": [
        "Blender define persona : https://github.com/facebookresearch/ParlAI/issues/2872"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8m7g9w4j-AK"
      },
      "outputs": [],
      "source": [
        "!pip install parlai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qzzjl1JWhVnB",
        "outputId": "8c2604eb-b112-4be0-8758-d158c8766409"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14:50:51 | Using CUDA\n",
            "14:50:51 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict\n",
            "14:50:51 | num words = 54944\n",
            "14:50:53 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "14:50:53 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/parlai/core/torch_generator_agent.py:1610: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  hyp_ids = best_idxs // voc_size\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bot\n",
            "hi , i am doing well . i am having a couple of episodes of panic attacks . how about you ?\n",
            "user\n",
            "oh no , i ' m sorry to hear that . i ' ve never had a panic attack .\n",
            "bot\n",
            "it is not as bad as it used to be . do you have any kids ? i have 2 .\n",
            "user\n",
            "i don ' t have any children , but i ' d love to have a child one day .\n",
            "bot\n",
            "it is a lot of work , but it is worth it . what do you do for fun ?\n",
            "user\n",
            "i like to read , play video games , and spend time with my family . what about yourself ?\n",
            "bot\n",
            "i love to read as well . what kind of video games do you like to play ? i am a stay at home mom .\n",
            "user\n",
            "that ' s great that you are a stay - at - home mom ! i love playing call of duty .\n",
            "bot\n",
            "i have never played it , but my husband and i play a lot . what is your favorite game ?\n",
            "user\n",
            "i really like halo . it ' s one of my favorite games of all time . what are your hobbies ?\n",
            "bot\n",
            "i like reading and hanging out with my husband . i also like to watch tv . how old are you ?\n",
            "user\n",
            "i am in my early 30 ' s . how about you ? what tv shows do you watch ?\n",
            "bot\n",
            "i am 40 . i am watching game of thrones right now . it is so good .\n",
            "user\n",
            "i ' ve never heard of it . i ' ll have to check it out . do you have a favorite character ?\n",
            "bot\n",
            "i ' d have to say jon snow . he ' s my favorite character . what ' s yours ?\n",
            "user\n",
            "i love george r . r . martin . he is such a great writer . i love how he writes his own stories .\n",
            "bot\n",
            "he is a great author . i wish i had more time to read . i have been having panic attacks lately .\n",
            "user\n",
            "oh no ! i ' m sorry to hear that . i hope you are able to get through it .\n",
            "bot\n",
            "thank you . it has been a rough year for me . what do you do for a living ?\n",
            "user\n",
            "i am a stay at home mom . i spend most of my time at home with my family .\n",
            "bot\n",
            "that sounds like a lot of fun . how long have you been a stay - at - home mom ?\n",
            "user\n",
            "since i can remember . it ' s been a long time . i miss my family a lot .\n",
            "bot\n",
            "i am sure you miss them . what is your favorite thing to do with your time with them ?\n",
            "user\n",
            "i like to go to the movies and play video games . what about you ? what are your hobbies ?\n",
            "bot\n",
            "i love to read and watch tv . i ' ve been having some issues with my upper back and chest .\n",
            "user\n",
            "oh no . that ' s terrible . do you have any exercises you like to do to help ?\n",
            "bot\n",
            "i try to keep my anxiety and depression at bay . it helps me keep my mind off things .\n",
            "user\n",
            "i am glad you are doing something to help . i wish you the best of luck with your family .\n",
            "bot\n",
            "thank you so much . i hope you have a good rest of your day . i have had some bad episodes this year .\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e8ee49f6599a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mblender_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_turn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'episode_done'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;31m# model produces a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblender_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2141\u001b[0m         \u001b[0;31m# BatchWorld handles calling self_observe, but we're in a Hogwild or Interactive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0;31m# world, so we need to handle this ourselves.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2143\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_observe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36mbatch_act\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m   2237\u001b[0m                 \u001b[0;31m# save memory and compute by disabling autograd.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0;31m# use `with torch.enable_grad()` to gain back gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2239\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36meval_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mprefix_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prefix_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             beam_preds_scores, beams = self._generate(\n\u001b[0;32m--> 876\u001b[0;31m                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m             )\n\u001b[1;32m    878\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbeam_preds_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, batch, beam_size, max_ts, prefix_tokens)\u001b[0m\n\u001b[1;32m   1124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m             \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincr_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincr_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m             \u001b[0;31m# only need the final hidden state to make the word prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/agents/transformer/modules/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, encoder_state, incr_state, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         tensor, new_incr_state = self.forward_layers(\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincr_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         )\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/agents/transformer/modules/decoder.py\u001b[0m in \u001b[0;36mforward_layers\u001b[0;34m(self, tensor, encoder_output, encoder_mask, incr_state, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0mencoder_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                     \u001b[0mincr_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mincr_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m                 )\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/agents/transformer/modules/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_output, encoder_mask, incr_state, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mincr_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mincr_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoder_attn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mstatic_kv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         )[:2]\n\u001b[1;32m    148\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# --dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/agents/transformer/modules/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, incr_state, static_kv, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# --attention-dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mattentioned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         attentioned = (\n\u001b[1;32m    259\u001b[0m             \u001b[0mattentioned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from parlai.core.agents import create_agent_from_model_file\n",
        "blender_agent = create_agent_from_model_file(\"zoo:blender/blender_90M/model\") #zoo:dodecadialogue/empathetic_dialogues_ft/model\n",
        "\n",
        "# forget everything. Important if you run this multiple times.\n",
        "# blender_agent.reset()\n",
        "\n",
        "# concatenate the persona and the first thing the human says\n",
        "bot_persona =  \"\\n\".join([\n",
        "    \"your persona: I'm 40, married and a mum of 2.\",\n",
        "    \"your persona: To keep it simple, I have suffered panic attacks since I was 18. Spent most of my twenties with few episodes.\",\n",
        "    \"your persona: Thirties, a few episodes of depression/anxiety and panic.\",\n",
        "    \"your persona: I turned 40 this year and my anxiety has never been worse.\",\n",
        "    \"your persona: I am currently experiencing chest pains. It feels like my chest muscles and upper back are torn and sore. My heart has been racing for days, sometimes out of control.\",\n",
        "])\n",
        "\n",
        "\n",
        "first_turn = [\"Hi, how are you?\"]\n",
        "while True:\n",
        "  # user\n",
        "  # Model actually witnesses the human's text\n",
        "  \n",
        "  blender_agent.observe({'text': bot_persona + \"\\n\"+  \"\\n\".join(first_turn[-10:]), 'episode_done': True})\n",
        "  # model produces a response\n",
        "  response = blender_agent.act()\n",
        "  first_turn.append(response['text'])\n",
        "  print(\"bot\")\n",
        "  print(first_turn[-1])\n",
        "\n",
        "  # bot + persona\n",
        "  # Model actually witnesses the human's text\n",
        "  blender_agent.observe({'text': \"\\n\".join(first_turn[-10:]), 'episode_done': True})\n",
        "  # model produces a response\n",
        "  response = blender_agent.act()\n",
        "  print(\"user\")\n",
        "  print(response['text'])\n",
        "  first_turn.append(response['text'])\n",
        "print()\n",
        "print(blender_agent.history.get_history_str())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdBQ0uWJbvCD"
      },
      "source": [
        "# 10.RAG (retrieval augmented generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOKfx9mScLVo",
        "outputId": "1dbddc48-287b-4249-9239-c50d739f839f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏬ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:31\n",
            "🔁 Restarting kernel...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - faiss-gpu\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2021.10.8  |       ha878542_0         139 KB  conda-forge\n",
            "    certifi-2021.10.8          |   py37h89c1867_1         145 KB  conda-forge\n",
            "    conda-4.11.0               |   py37h89c1867_0        16.9 MB  conda-forge\n",
            "    cudatoolkit-11.1.1         |       h6406543_8        1.20 GB  conda-forge\n",
            "    faiss-1.7.1                |py37cuda111h7f21d35_1_cuda         2.0 MB  conda-forge\n",
            "    faiss-gpu-1.7.1            |       h788eb59_1          15 KB  conda-forge\n",
            "    libblas-3.9.0              |11_linux64_openblas          12 KB  conda-forge\n",
            "    libcblas-3.9.0             |11_linux64_openblas          11 KB  conda-forge\n",
            "    libfaiss-1.7.1             |cuda111hf54f04a_1_cuda        80.2 MB  conda-forge\n",
            "    libfaiss-avx2-1.7.1        |cuda111h1234567_1_cuda        80.3 MB  conda-forge\n",
            "    libgfortran-ng-11.2.0      |      h69a702a_11          19 KB  conda-forge\n",
            "    libgfortran5-11.2.0        |      h5c6108e_11         1.7 MB  conda-forge\n",
            "    liblapack-3.9.0            |11_linux64_openblas          11 KB  conda-forge\n",
            "    libopenblas-0.3.17         |pthreads_h8fe5266_1         9.2 MB  conda-forge\n",
            "    numpy-1.20.3               |   py37h038b26d_1         5.7 MB  conda-forge\n",
            "    python_abi-3.7             |          2_cp37m           4 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        1.40 GB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  cudatoolkit        conda-forge/linux-64::cudatoolkit-11.1.1-h6406543_8\n",
            "  faiss              conda-forge/linux-64::faiss-1.7.1-py37cuda111h7f21d35_1_cuda\n",
            "  faiss-gpu          conda-forge/linux-64::faiss-gpu-1.7.1-h788eb59_1\n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-11_linux64_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-11_linux64_openblas\n",
            "  libfaiss           conda-forge/linux-64::libfaiss-1.7.1-cuda111hf54f04a_1_cuda\n",
            "  libfaiss-avx2      conda-forge/linux-64::libfaiss-avx2-1.7.1-cuda111h1234567_1_cuda\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-11.2.0-h69a702a_11\n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-11.2.0-h5c6108e_11\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-11_linux64_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.17-pthreads_h8fe5266_1\n",
            "  numpy              conda-forge/linux-64::numpy-1.20.3-py37h038b26d_1\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                      2020.12.5-ha878542_0 --> 2021.10.8-ha878542_0\n",
            "  certifi                          2020.12.5-py37h89c1867_1 --> 2021.10.8-py37h89c1867_1\n",
            "  conda                                4.9.2-py37h89c1867_0 --> 4.11.0-py37h89c1867_0\n",
            "  python_abi                                    3.7-1_cp37m --> 3.7-2_cp37m\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "certifi-2021.10.8    | 145 KB    | : 100% 1.0/1 [00:00<00:00,  7.98it/s]               \n",
            "faiss-gpu-1.7.1      | 15 KB     | : 100% 1.0/1 [00:00<00:00,  7.44it/s]\n",
            "libfaiss-avx2-1.7.1  | 80.3 MB   | : 100% 1.0/1 [00:17<00:00, 17.44s/it]               \n",
            "conda-4.11.0         | 16.9 MB   | : 100% 1.0/1 [00:03<00:00,  3.84s/it]               \n",
            "libfaiss-1.7.1       | 80.2 MB   | : 100% 1.0/1 [00:17<00:00, 17.26s/it]               \n",
            "liblapack-3.9.0      | 11 KB     | : 100% 1.0/1 [00:00<00:00, 15.44it/s]\n",
            "numpy-1.20.3         | 5.7 MB    | : 100% 1.0/1 [00:01<00:00,  1.27s/it]\n",
            "faiss-1.7.1          | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.15it/s]               \n",
            "ca-certificates-2021 | 139 KB    | : 100% 1.0/1 [00:00<00:00, 18.26it/s]\n",
            "libgfortran-ng-11.2. | 19 KB     | : 100% 1.0/1 [00:00<00:00, 15.92it/s]\n",
            "libcblas-3.9.0       | 11 KB     | : 100% 1.0/1 [00:00<00:00, 19.91it/s]\n",
            "libblas-3.9.0        | 12 KB     | : 100% 1.0/1 [00:00<00:00, 21.16it/s]\n",
            "libgfortran5-11.2.0  | 1.7 MB    | : 100% 1.0/1 [00:00<00:00,  2.79it/s]\n",
            "python_abi-3.7       | 4 KB      | : 100% 1.0/1 [00:00<00:00, 17.20it/s]\n",
            "libopenblas-0.3.17   | 9.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.62s/it]\n",
            "cudatoolkit-11.1.1   | 1.20 GB   | : 100% 1.0/1 [02:52<00:00, 172.16s/it]              \n",
            "Preparing transaction: | \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "\b\b| \b\bdone\n"
          ]
        }
      ],
      "source": [
        "# !pip install -U datasets\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda install -c pytorch faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miVGDdR9bw7t",
        "outputId": "4c68db36-23d3-4bee-cd1c-8140f5511083"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bart/configuration_bart.py:178: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
            "  f\"Please make sure the config includes `forced_bos_token_id={self.bos_token_id}` in future versions. \"\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizerFast'.\n",
            "Using custom data configuration dummy.psgs_w100.nq.no_index-dummy=True,with_index=False\n",
            "Reusing dataset wiki_dpr (/root/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.no_index-dummy=True,with_index=False/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n",
            "Using custom data configuration dummy.psgs_w100.nq.exact-df1b7a7f4307b5db\n",
            "Reusing dataset wiki_dpr (/root/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.exact-df1b7a7f4307b5db/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n",
            "Some weights of RagTokenForGeneration were not initialized from the model checkpoint at facebook/rag-token-nq and are newly initialized: ['rag.generator.lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/rag/tokenization_rag.py:97: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1733: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " george h.w. bush\n"
          ]
        }
      ],
      "source": [
        "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
        "\n",
        "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
        "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
        "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
        "\n",
        "input_dict = tokenizer.prepare_seq2seq_batch(\"who is the president of united states?\", return_tensors=\"pt\") \n",
        "\n",
        "generated = model.generate(input_ids=input_dict[\"input_ids\"]) \n",
        "print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pci7AOXqcL5"
      },
      "source": [
        "# 11.Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2VyWDleqjHJ"
      },
      "outputs": [],
      "source": [
        "finetuned_model_path = f'{mydrive_path}finetuned-dict'\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "init_model = 'zoo:blender/blender_90M/model'\n",
        "dict_file  = 'fr_finetuned_train.dict'\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file=dict_file,  #'zoo:tutorial_transformer_generator/model.dict',\n",
        "    # learn_positional_embeddings=True,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    # use a small learning rate with ADAM optimizer\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    # early stopping on perplexity\n",
        "    validation_metric='ppl',\n",
        "    # train at most 10 minutes, and validate every 0.25 epochs\n",
        "    # max_train_time=5*60,\n",
        "    validation_every_n_epochs=0.25,\n",
        "\n",
        "    num_epochs =3,\n",
        "    # save_after_valid = True,\n",
        "    # save_every_n_secs = 60,\n",
        "    verbose = True,\n",
        "    \n",
        "    # depend on your gpu. If you have a V100, this is good\n",
        "    batchsize=8, fp16=True, fp16_impl='mem_efficient',\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        "\n",
        "    # --beam-min-length 20 --beam-block-ngram 3 --beam-context-block-ngram 3 --beam-size 10 --inference beam\n",
        "    beam_min_length= 20,\n",
        "    beam_block_ngram= 3,\n",
        "    beam_context_block_ngram= 3,\n",
        "    beam_size= 10,\n",
        "\n",
        "    inference= \"beam\"\n",
        "    # beam_block_full_context= True,\n",
        "    # beam_block_list_filename= None,\n",
        "    # beam_delay= 30,\n",
        "    # beam_length_penalty= 0.65,\n",
        "    \n",
        "    # truncate= 10\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pIjKC2PuaiN8",
        "hmFp5DT2Nk7M",
        "IRfDXAOxQX-8",
        "Njr5h8lQ5BoA"
      ],
      "name": "Blender-finetune the models.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}