{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blender-90m-with-double-sized-data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SONwSWMp6qPv",
        "lVPS6p4XzPh4",
        "iGoTqdFD7x_3"
      ],
      "authorship_tag": "ABX9TyNLU+o8dlDHWUR8uEYIWOJj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirpaia/blenderbot/blob/main/blender_90m_with_double_sized_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.Installing prerequisites"
      ],
      "metadata": {
        "id": "SONwSWMp6qPv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d-SZ_On6Kxg"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "0BegaSUz6iUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7SzGRXHQ6kDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydrive_path = '/content/drive/MyDrive/colabs/blender-models/'\n",
        "# !pip uninstall -q parlai\n",
        "!pip install -q parlai\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "AWDakYmy6mIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preparing Data"
      ],
      "metadata": {
        "id": "YIJEq9_r63hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_list_of_turns_to_dialog(d):\n",
        "    if len(d)%2 !=0: d = d[:-1]\n",
        "    t = \"\"\n",
        "    for i in range(0,len(d),2):\n",
        "        u1 = d[i]\n",
        "        u2 = d[i+1]\n",
        "\n",
        "        if (i+2) != len(d):\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "        else:\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "    return t\n",
        "\n",
        "def transfer_list_of_pairs_to_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, text_label_pair in enumerate(d):\n",
        "    u1 = text_label_pair[0]\n",
        "    u2 = text_label_pair[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "    \n",
        "t = ['hello','how are you','good','bye','test']\n",
        "print(transfer_list_of_turns_to_dialog(t))\n",
        "\n",
        "t = [['hello','how are you'],['good','bye']]\n",
        "print(transfer_list_of_pairs_to_dialog(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47u8M3RK65m_",
        "outputId": "e0e53685-52a6-4f54-e0f2-48dd202c7c3e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n",
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XPersona"
      ],
      "metadata": {
        "id": "XE45ZyKC8WjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HLTCHKUST/Xpersona.git\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_train_corrected.json','r') as f:\n",
        "   train_data = json.load(f)\n",
        "\n",
        "dialogs_train = pd.DataFrame(train_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_valid_human_annotated.json','r') as f:\n",
        "   valid_data = json.load(f)\n",
        "\n",
        "dialogs_valid = pd.DataFrame(valid_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_test_human_annotated.json','r') as f:\n",
        "   test_data = json.load(f)\n",
        "\n",
        "dialogs_test = pd.DataFrame(test_data)['dialogue'].tolist()\n",
        "\n",
        "# dialogs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZmSJtQG8Y1o",
        "outputId": "b9a1b297-12d4-4a9a-ed9a-87bc3bf36b34"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/dataset_french_xpersona/': No such file or directory\n",
            "fatal: destination path 'Xpersona' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/dataset_french_xpersona/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "\n",
        "#region Training\n",
        "data_train = \"\"\n",
        "for d in dialogs_train:\n",
        "  data_train += transfer_list_of_pairs_to_dialog(d)\n",
        "\n",
        "file_train = open(f\"{data_path}train.txt\",\"w\")\n",
        "print(\"Training Set:\", file_train.write(data_train))\n",
        "#endregion \n",
        "    \n",
        "#region Validation\n",
        "data_valid = \"\"\n",
        "for d in dialogs_valid:\n",
        "  data_valid += transfer_list_of_pairs_to_dialog(d)\n",
        "\n",
        "file_valid = open(f\"{data_path}valid.txt\",\"w\")\n",
        "print(\"Validation Set:\", file_valid.write(data_valid))\n",
        "#endregion\n",
        "\n",
        "#region Test\n",
        "data_test = \"\"\n",
        "for d in dialogs_test:\n",
        "  data_test += transfer_list_of_pairs_to_dialog(d)\n",
        "\n",
        "file_test = open(f\"{data_path}test.txt\",\"w\")\n",
        "print(\"Test Set:\", file_test.write(data_test))\n",
        "#endregion \n",
        "\n",
        "# print(len(data_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKlGi0f08zQc",
        "outputId": "203a3fbb-b7fe-457e-eef4-98c9a3ddedbd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set: 15776427\n",
            "Validation Set: 257114\n",
            "Test Set: 262425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ED"
      ],
      "metadata": {
        "id": "DoSv5zhs8ZIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/\"\n",
        "data_path = \"/content/dataset_french_ed/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "!cp -rv $googledrive_data_path* $data_path\n"
      ],
      "metadata": {
        "id": "mISEjZbP8dN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BST"
      ],
      "metadata": {
        "id": "WUxXs_Oe8dda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/\"\n",
        "data_path = \"/content/dataset_french_bst/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "!cp -rv $googledrive_data_path* $data_path\n",
        "\n"
      ],
      "metadata": {
        "id": "kIRVhsq88e7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Creating new Tasks"
      ],
      "metadata": {
        "id": "8PQfT57Q8I2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#region XPersona\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_xpersona/'\n",
        "!mkdir $task_path'french_xpersona'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/* $task_path'french_xpersona/'\n",
        "#endregion\n",
        "\n",
        "\n",
        "#region ED\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_empathetic_dialogues/'\n",
        "!mkdir $task_path'french_empathetic_dialogues'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/* $task_path'french_empathetic_dialogues/'\n",
        "#endregion\n",
        "\n",
        "\n",
        "#region BST\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_blended_skill_talk/'\n",
        "!mkdir $task_path'french_blended_skill_talk'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/* $task_path'french_blended_skill_talk/'\n",
        "#endregion"
      ],
      "metadata": {
        "id": "6oVPKrhO8ISe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Display Model"
      ],
      "metadata": {
        "id": "lVPS6p4XzPh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file= f'{from_scratch_model_path}/model',\n",
        "    num_examples=8,\n",
        "    # skip_generation=False,\n",
        ")"
      ],
      "metadata": {
        "id": "A9FBtnZZzPPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Finetuning"
      ],
      "metadata": {
        "id": "iGoTqdFD7x_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model_path = f'{mydrive_path}finetuned-multitask-90m'\n",
        "init_model = 'zoo:blender/blender_90M/model'\n",
        "dict_file  = 'zoo:blender/blender_90M/model.dict'"
      ],
      "metadata": {
        "id": "65rK6pfj70Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath='fr_finetuned',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    # use a small learning rate with ADAM optimizer\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    validation_metric='ppl',\n",
        "    validation_every_n_epochs=0.25,\n",
        "    num_epochs = 3,\n",
        "    verbose = True,\n",
        "    \n",
        "    # depend on your gpu. If you have a V100, this is good\n",
        "    batchsize=8, fp16=True, fp16_impl='mem_efficient',\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        "\n",
        "    # --beam-min-length 20 --beam-block-ngram 3 --beam-context-block-ngram 3 --beam-size 10 --inference beam\n",
        "    beam_min_length= 20,\n",
        "    beam_block_ngram= 3,\n",
        "    beam_context_block_ngram= 3,\n",
        "    beam_size= 10,\n",
        "\n",
        "    inference= \"beam\"\n",
        ")"
      ],
      "metadata": {
        "id": "X7vLiYpF8HbF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}