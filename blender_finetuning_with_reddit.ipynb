{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirpaia/blenderbot/blob/main/blender_finetuning_with_reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SONwSWMp6qPv"
      },
      "source": [
        "# 0.Installing prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d-SZ_On6Kxg",
        "outputId": "8249ba4e-2e12-4877-d4c7-9049d76e4bae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Jun  7 17:03:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BegaSUz6iUz",
        "outputId": "0b301c08-3cbc-4886-ddd5-35e88e84ed7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SzGRXHQ6kDQ",
        "outputId": "b7aab073-a3ac-490e-c086-afd464ec6749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "mydrive_path = '/content/drive/MyDrive/colabs/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWDakYmy6mIQ",
        "outputId": "aed736bf-a1fc-4e3a-dd61-391064fd0fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 33.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 175 kB 73.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 77.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 248 kB 83.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 82.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 69.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 208 kB 73.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 346 kB 79.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 235 kB 86.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 125 kB 86.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 23.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.9 MB 69.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 74 kB 825 kB/s \n",
            "\u001b[K     |████████████████████████████████| 95 kB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 79.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 86.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 80.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 547 kB 91.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 91.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 749 kB 76.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 95.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 82.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 70.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 82.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 68.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 110 kB 2.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 9.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 121 kB 70.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 100 kB 10.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 75.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 83.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 964 kB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 71.1 MB/s \n",
            "\u001b[?25h  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 32.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.19.2\n"
          ]
        }
      ],
      "source": [
        "# !pip uninstall -q parlai\n",
        "!pip install -q parlai\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIJEq9_r63hi"
      },
      "source": [
        "# 1.Preparing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R-fgBUdcPX5"
      },
      "source": [
        "## Genreal Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47u8M3RK65m_",
        "outputId": "5e1f4da2-a403-418f-ec2e-04f1eeefe57e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n",
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def transfer_list_of_turns_to_dialog(d):\n",
        "    if len(d)%2 !=0: d = d[:-1]\n",
        "    t = \"\"\n",
        "    for i in range(0,len(d),2):\n",
        "        u1 = d[i]\n",
        "        u2 = d[i+1]\n",
        "\n",
        "        if (i+2) != len(d):\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "        else:\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "    return t\n",
        "\n",
        "def transfer_list_of_pairs_to_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, text_label_pair in enumerate(d):\n",
        "    u1 = text_label_pair[0]\n",
        "    u2 = text_label_pair[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "\n",
        "def convert_parlai_format_to_list_of_turns(lines):\n",
        "    result = []\n",
        "    for line in lines:\n",
        "        text_label = line.split(\"\\t\")\n",
        "        result.append(text_label[0].replace(\"text:\", \"\"))\n",
        "        result.append(text_label[1].replace(\"labels:\", \"\").replace(\"\\n\",\"\"))\n",
        "    return result\n",
        "\n",
        "t = ['hello','how are you','good','bye','test']\n",
        "print(transfer_list_of_turns_to_dialog(t))\n",
        "\n",
        "t = [['hello','how are you'],['good','bye']]\n",
        "print(transfer_list_of_pairs_to_dialog(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE45ZyKC8WjC"
      },
      "source": [
        "## French Reddit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8ZmSJtQG8Y1o",
        "outputId": "86ce96e9-070b-4cd5-bae0-731d5edcb70f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_path = f\"{mydrive_path}aliae-workspace/datasets/french_reddit/\"\n",
        "data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UexnzWURWKSI",
        "outputId": "ba4b3058-703e-4338-e82b-6d8bb2357e91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"text:Pour ceux qui jouent à LoL et qui attendaient impatiemment le URF, ça sera au final le NURF qui sera là. L'inverse du Urf : cooldown augmentés, conso mana/énergie augmentée, dégâts des minions augmentés... Voilà, enjoy, moi je vais pleurer dans un coin parce que le urf était le mode qui me poussait à garder LoL installé. ^^^^quelqu'un ^^^^n'aime ^^^^pas ^^^^LoL ^^^^? ^^^^:)\\tlabels:Ça, ou alors demain c'est le 1^er avril.\\n\",\n",
              " \"text:Le vrai mode URF était déjà une blague du 1er avril. Sauf que ça a bien marché et ils ont laissé le mode 2 semaines.On verra bien demain de toute façon :(\\tlabels:Et tu penses vraiment qu'ils vont remplacer un mode de jeu que tout le monde attend depuis 1 an par un truc pas fun du tout ?C'est clairement une blague.\\n\",\n",
              " \"text:Riot est tellement une vaste blague comme entreprise qu'ils en seraient capable D:\\tlabels:Une vaste blague comme entreprise ? Les mecs ont réussi, en pompant le concept d'un jeu existant à créer le jeu en ligne le plus joué au monde, à se faire un fric fou et ont énormément contribué à la progression de l'esport. C'est plutôt des génies oui.Si tu veux te convaincres que c'est une blague regarde ça : Units critically strike on 150% of attackset explique moi ce que ça veut dire.\\tepisode_done:True\\n\",\n",
              " \"text:Il y a une phrase prétendument de Pasqua qui tourne sur twitter en ce moment : *Quand on est emmerdé par une affaire, il faut susciter une affaire dans l’affaire, et si nécessaire une autre affaire dans l’affaire de l’affaire, jusqu’à ce que personne n’y comprenne plus rien.* Même si elle n'est pas de lui, elle s'applique bien à la situation actuelle. C'est bien joué de la part de la droite (bien aidé par la nullité du gouvernement) d'avoir su allumer des contres feux pour sauver le cul de Sarkozy. Assez marrant aussi de voir le léchage de boules en faveur de Sarko dans les articles de la page d'accueil de Valeurs Actuelles. C'est surement un hasard que ce soit ce journal qui sorte cette affaire.\\tlabels:Il y a une phrase prétendument de Pasqua qui tourne sur twitter en ce moment : Quand on est emmerdé par une affaire, il faut susciter une affaire dans l’affaire, et si nécessaire une autre affaire dans l’affaire de l’affaire, jusqu’à ce que personne n’y comprenne plus rien.Par exemple quand le gouvernement est emmerdé par ses résultats déplorables, il lance des diversions ? Après le mariage pour tous, Dieudonné, Méric, pas de bol cette fois-ci c'est un adversaire qui connait la chanson et apporte une réponse appropriée.Ma position n'a pas changé : le gouvernement ferait mieux de [s’occuper des Français](-Minute-Taubira-une-strategie-de-diversion-du-gouvernement-FN-1710677/).\\n\",\n",
              " \"text:Le mariage pour tous ? Lol ? Tout le bordel à ce sujet à été crée par l'opposition. \\tlabels:... C'est l'opposition qui propose les lois maintenant ?\\n\",\n",
              " \"text:Parce qu'on est en période de crise économique, le gouvernement démocratiquement élu n'aurait pas le droit de légiférer sur des questions non-économiques ? Arrête.\\tlabels: Parce qu'on est en période de crise économique, le gouvernement démocratiquement élu n'aurait pas le droit de légiférer sur des questions non-économiques ?Il a tout à fait le droit de faire diversion. D'ailleurs il le fait très bien.\\n\",\n",
              " \"text:Ta mauvaise foi est sans limite.Il ne fait aucune diversion dans la mesure où 1. Il est légitime à légiférer sur des sujets non-économiques2. Le mariage gay était inscrit au programme de Hollande avant son élection. Il s'agissait donc d'une promesse de campagne, et certaines personnes, j'en suis sûr, on voté Hollande en raison du mariage gay.Enfin, si l'opposition avait le même sentiment que toi, elle n'aurait pas  monopolisé l'attention via la manif pour tous et aurait recentrée le débat sur les questions économiques.\\tlabels: Il ne fait aucune diversion dans la mesure où 1. Il est légitime à légiférer sur des sujets non-économiquesEn quoi une diversion ne serait pas légitime ?J'ai déjà dit qu'il avait tout à fait le droit de le faire.Le mariage gay était inscrit au programme de Hollande avant son élection.Oui, et en quoi ça n'en fait pas une diversion ?si l'opposition avait le même sentiment que toi, elle n'aurait pas monopolisé l'attention via la manif pour tousC'est ce qu'a fait le FN. La véritable opposition est le FN, pas l'UMP.\\n\",\n",
              " 'text: C\\'est ce qu\\'a fait le FN.Ma mémoire n\\'est peut-être pas très bonne, mais je crois me souvenir de nombreux cadres du FN manifestant avec la Manif Pour Tous, signant sa charte et participant à cette grande \"diversion\".\\tlabels: Ma mémoire n\\'est peut-être pas très bonne, mais je crois me souvenir de nombreux cadres du FN manifestant avec la Manif Pour Tous, signant sa charte et participant à cette grande \"diversion\".C\\'est vrai, mais en quoi cela revient à \"monopoliser l\\'attention via la manif pour tous\" ?La ligne du FN [est très claire là-dessus]().\\tepisode_done:True\\n',\n",
              " 'text:Je viens de terminer une épreuve dite \"marathon\", on nous a filé un texte à traduire à 8h45 ce matin et on doit le rendre à 16h45 au plus tard.Eh bien figurez-vous que le cancer de la prostate, c\\'est pas rigolo !\\tlabels:Tu fais quoi, des études de trad ?\\n',\n",
              " 'text:Oui, je suis en traduction audiovisuelle.\\tlabels:Ah tu dois être à Strasbourg ? Je me souviens de ça, tout le monde flippait de devoir passer la journée sur une traduction, j\\'étais parmi les \"un peu plus vieux\" qui avaient déjà bossé avant et ça me faisait rouler des yeux.\\n']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(f\"{data_path}/data_train.txt\") as f:\n",
        "    lines = f.readlines()\n",
        "lines[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGoTqdFD7x_3"
      },
      "source": [
        "# 2.Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "65rK6pfj70Px",
        "outputId": "62482593-e972-4b81-e60f-815670cdcc0c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/colabs/blender-models/finetuned-reddit-400m/'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetuned_model_path = f'{mydrive_path}blender-models/finetuned-reddit-400m/'\n",
        "init_model = 'zoo:blender/blender_400Mdistill/model'\n",
        "dict_file  = 'zoo:blender/blender_400Mdistill/model.dict'\n",
        "# init_model = 'zoo:blender/blender_90M/model'\n",
        "# dict_file  = 'zoo:blender/blender_90M/model.dict'\n",
        "finetuned_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7vLiYpF8HbF"
      },
      "outputs": [],
      "source": [
        "# 90M settings\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    # task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "    # multitask_weights= \"1,3,3\",\n",
        "\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    \n",
        "    # depend on your gpu. \n",
        "    validation_every_n_epochs=0.25,\n",
        "    num_epochs = 5,\n",
        "    log_every_n_secs= 60,\n",
        "    verbose = True,\n",
        "    batchsize= 8, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        "\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    attention_dropout= 0.0, \n",
        "    model_parallel= False,\n",
        "    warmup_updates=100,\n",
        "\n",
        "    # customized parameters\n",
        "    # inference= \"beam\"\n",
        "    # beam_min_length= 20,\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_size= 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxL0-1x1YsQe"
      },
      "outputs": [],
      "source": [
        "# mydrive_path = '/content/finetuned-multitask-400m-double-sided-2epochs'\n",
        "# mydrive_path = '/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-double-sided/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNZE5ta2pO-X",
        "outputId": "d31f149e-e6b3-45e3-c578-9c014a453dec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17:30:26 | building dictionary first...\n",
            "17:30:26 | No model with opt yet at: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-400m//model(.opt)\n",
            "17:30:26 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: download_path: None,verbose: True,is_debug: False,datapath: /usr/local/lib/python3.7/dist-packages/data,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,world_logs: ,save_format: conversations,log_keep_fields: all,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data,fromfile_datatype_extension: True,checkpoint_activations: False,interactive_mode: False\u001b[0m\n",
            "17:30:26 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--init-opt /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt --allow-missing-init-opts True --task blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 8 --dynamic-batching full --eval-batchsize 8 --num-epochs -1 --validation-every-n-secs 900.0 --save-after-valid True --validation-every-n-epochs -1.0 --validation-patience 20 --log-every-n-secs 10.0 --distributed-world-size 8 --port 61337 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --chosen-topic-delimiter \n",
            " --num-topics 5 --train-experiencer-only False --remove-political-convos False --teacher-model /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model --task-loss-coeff 1.0 --encoder-loss-coeff 24.0 --hidden-loss-coeff 5.0 --pred-loss-coeff 8.0 --embedding-loss-coeff 0.35 --self-attn-loss-coeff 0.6 --enc-dec-attn-loss-coeff 3.0 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --inference beam --learningrate 0.0004 --gpu 0 --bpe-vocab /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json --bpe-merge /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt --bpe-add-prefix-space True --max-lr-steps -1 --parlai-home /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI --show-advanced-args False --numthreads 1 --rank 0 --dict-loaded True\u001b[0m\n",
            "17:30:26 | Using CUDA\n",
            "17:30:26 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict\n",
            "17:30:26 | num words = 8008\n",
            "17:30:32 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "17:30:32 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model\n",
            "17:30:34 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
            "17:30:34 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
            "17:30:35 | Opt:\n",
            "17:30:35 |     activation: gelu\n",
            "17:30:35 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "17:30:35 |     adam_eps: 1e-08\n",
            "17:30:35 |     add_p1_after_newln: False\n",
            "17:30:35 |     aggregate_micro: False\n",
            "17:30:35 |     allow_missing_init_opts: False\n",
            "17:30:35 |     attention_dropout: 0.0\n",
            "17:30:35 |     batchsize: 16\n",
            "17:30:35 |     beam_block_full_context: True\n",
            "17:30:35 |     beam_block_list_filename: None\n",
            "17:30:35 |     beam_block_ngram: -1\n",
            "17:30:35 |     beam_context_block_ngram: -1\n",
            "17:30:35 |     beam_delay: 30\n",
            "17:30:35 |     beam_length_penalty: 0.65\n",
            "17:30:35 |     beam_min_length: 1\n",
            "17:30:35 |     beam_size: 1\n",
            "17:30:35 |     betas: '(0.9, 0.999)'\n",
            "17:30:35 |     bpe_add_prefix_space: None\n",
            "17:30:35 |     bpe_debug: False\n",
            "17:30:35 |     bpe_dropout: None\n",
            "17:30:35 |     bpe_merge: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict-merges.txt\n",
            "17:30:35 |     bpe_vocab: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict-vocab.json\n",
            "17:30:36 |     checkpoint_activations: False\n",
            "17:30:36 |     compute_tokenized_bleu: False\n",
            "17:30:36 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "17:30:36 |     datatype: train\n",
            "17:30:36 |     delimiter: '  '\n",
            "17:30:36 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "17:30:36 |     dict_endtoken: __end__\n",
            "17:30:36 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict\n",
            "17:30:36 |     dict_include_test: False\n",
            "17:30:36 |     dict_include_valid: False\n",
            "17:30:36 |     dict_initpath: None\n",
            "17:30:36 |     dict_language: english\n",
            "17:30:36 |     dict_loaded: True\n",
            "17:30:36 |     dict_lower: False\n",
            "17:30:36 |     dict_max_ngram_size: -1\n",
            "17:30:36 |     dict_maxexs: -1\n",
            "17:30:36 |     dict_maxtokens: -1\n",
            "17:30:36 |     dict_minfreq: 0\n",
            "17:30:36 |     dict_nulltoken: __null__\n",
            "17:30:36 |     dict_starttoken: __start__\n",
            "17:30:36 |     dict_textfields: text,labels\n",
            "17:30:36 |     dict_tokenizer: bytelevelbpe\n",
            "17:30:36 |     dict_unktoken: __unk__\n",
            "17:30:36 |     display_examples: False\n",
            "17:30:36 |     download_path: None\n",
            "17:30:36 |     dropout: 0.1\n",
            "17:30:36 |     dynamic_batching: None\n",
            "17:30:36 |     embedding_projection: random\n",
            "17:30:36 |     embedding_size: 1280\n",
            "17:30:36 |     embedding_type: random\n",
            "17:30:36 |     embeddings_scale: True\n",
            "17:30:36 |     eval_batchsize: None\n",
            "17:30:36 |     eval_dynamic_batching: None\n",
            "17:30:36 |     evaltask: None\n",
            "17:30:36 |     ffn_size: 5120\n",
            "17:30:36 |     final_extra_opt: \n",
            "17:30:36 |     force_fp16_tokens: False\n",
            "17:30:36 |     fp16: True\n",
            "17:30:36 |     fp16_impl: mem_efficient\n",
            "17:30:36 |     fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data\n",
            "17:30:36 |     fromfile_datatype_extension: True\n",
            "17:30:36 |     gpu: -1\n",
            "17:30:36 |     gradient_clip: 0.1\n",
            "17:30:36 |     hide_labels: False\n",
            "17:30:36 |     history_add_global_end_token: end\n",
            "17:30:36 |     history_reversed: False\n",
            "17:30:36 |     history_size: -1\n",
            "17:30:36 |     image_cropsize: 224\n",
            "17:30:36 |     image_mode: raw\n",
            "17:30:36 |     image_size: 256\n",
            "17:30:36 |     inference: greedy\n",
            "17:30:36 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model\n",
            "17:30:36 |     init_opt: None\n",
            "17:30:36 |     interactive_mode: False\n",
            "17:30:36 |     invsqrt_lr_decay_gamma: -1\n",
            "17:30:36 |     is_debug: False\n",
            "17:30:36 |     label_truncate: 128\n",
            "17:30:36 |     learn_positional_embeddings: False\n",
            "17:30:36 |     learningrate: 7e-06\n",
            "17:30:36 |     load_from_checkpoint: True\n",
            "17:30:36 |     log_every_n_secs: 300.0\n",
            "17:30:36 |     log_every_n_steps: 50\n",
            "17:30:36 |     log_keep_fields: all\n",
            "17:30:36 |     loglevel: info\n",
            "17:30:36 |     lr_scheduler: reduceonplateau\n",
            "17:30:36 |     lr_scheduler_decay: 0.5\n",
            "17:30:36 |     lr_scheduler_patience: 3\n",
            "17:30:36 |     max_train_steps: -1\n",
            "17:30:36 |     max_train_time: -1\n",
            "17:30:36 |     metrics: default\n",
            "17:30:36 |     model: transformer/generator\n",
            "17:30:36 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-400m//model\n",
            "17:30:36 |     model_parallel: False\n",
            "17:30:36 |     momentum: 0\n",
            "17:30:36 |     multitask_weights: [1]\n",
            "17:30:36 |     mutators: None\n",
            "17:30:36 |     n_decoder_layers: 12\n",
            "17:30:36 |     n_encoder_layers: 2\n",
            "17:30:36 |     n_heads: 32\n",
            "17:30:36 |     n_layers: 2\n",
            "17:30:36 |     n_positions: 128\n",
            "17:30:36 |     n_segments: 0\n",
            "17:30:36 |     nesterov: True\n",
            "17:30:36 |     no_cuda: False\n",
            "17:30:36 |     num_epochs: 0.25\n",
            "17:30:36 |     num_workers: 0\n",
            "17:30:36 |     nus: (0.7,)\n",
            "17:30:36 |     optimizer: mem_eff_adam\n",
            "17:30:36 |     output_scaling: 1.0\n",
            "17:30:36 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data', 'fromfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-reddit-400m//model', 'init_model': 'zoo:blender/blender_400Mdistill/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict', 'validation_every_n_epochs': 0.25, 'num_epochs': 0.25, 'log_every_n_secs': 300.0, 'verbose': True, 'attention_dropout': 0.0, 'batchsize': 16, 'fp16': True, 'fp16_impl': 'mem_efficient', 'embedding_size': 1280, 'ffn_size': 5120, 'variant': 'prelayernorm', 'n_heads': 32, 'n_positions': 128, 'n_encoder_layers': 2, 'n_decoder_layers': 12, 'label_truncate': 128, 'text_truncate': 128, 'truncate': 128, 'activation': 'gelu', 'history_add_global_end_token': 'end', 'delimiter': '  ', 'dict_tokenizer': 'bytelevelbpe', 'dropout': 0.1, 'learningrate': 7e-06, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'optimizer': 'mem_eff_adam', 'relu_dropout': 0.0, 'model_parallel': False, 'warmup_updates': 100, 'update_freq': 2, 'gradient_clip': 0.1, 'skip_generation': True, 'validation_patience': 10, 'validation_metric': 'ppl', 'validation_metric_mode': 'min'}\"\n",
            "17:30:36 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "17:30:36 |     person_tokens: False\n",
            "17:30:36 |     rank_candidates: False\n",
            "17:30:36 |     relu_dropout: 0.0\n",
            "17:30:36 |     save_after_valid: False\n",
            "17:30:36 |     save_every_n_secs: -1\n",
            "17:30:36 |     save_format: conversations\n",
            "17:30:36 |     share_word_embeddings: True\n",
            "17:30:36 |     short_final_eval: False\n",
            "17:30:36 |     skip_generation: True\n",
            "17:30:36 |     special_tok_lst: None\n",
            "17:30:36 |     split_lines: False\n",
            "17:30:36 |     starttime: Jun07_17-30\n",
            "17:30:36 |     task: fromfile:parlaiformat\n",
            "17:30:36 |     temperature: 1.0\n",
            "17:30:36 |     tensorboard_log: False\n",
            "17:30:36 |     tensorboard_logdir: None\n",
            "17:30:36 |     text_truncate: 128\n",
            "17:30:36 |     topk: 10\n",
            "17:30:36 |     topp: 0.9\n",
            "17:30:36 |     truncate: 128\n",
            "17:30:36 |     update_freq: 2\n",
            "17:30:36 |     use_reply: label\n",
            "17:30:36 |     validation_cutoff: 1.0\n",
            "17:30:36 |     validation_every_n_epochs: 0.25\n",
            "17:30:36 |     validation_every_n_secs: -1\n",
            "17:30:36 |     validation_every_n_steps: -1\n",
            "17:30:36 |     validation_max_exs: -1\n",
            "17:30:36 |     validation_metric: ppl\n",
            "17:30:36 |     validation_metric_mode: min\n",
            "17:30:36 |     validation_patience: 10\n",
            "17:30:36 |     validation_share_agent: False\n",
            "17:30:36 |     variant: prelayernorm\n",
            "17:30:36 |     verbose: True\n",
            "17:30:36 |     wandb_entity: None\n",
            "17:30:36 |     wandb_log: False\n",
            "17:30:36 |     wandb_name: None\n",
            "17:30:36 |     wandb_project: None\n",
            "17:30:36 |     warmup_rate: 0.0001\n",
            "17:30:36 |     warmup_updates: 100\n",
            "17:30:36 |     weight_decay: None\n",
            "17:30:36 |     world_logs: \n",
            "17:30:36 | creating task(s): fromfile:parlaiformat\n",
            "17:30:36 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data_train.txt\n",
            "17:30:41 | training...\n",
            "17:30:42 | Overflow: setting loss scale to 65536.0\n",
            "17:30:43 | Overflow: setting loss scale to 32768.0\n",
            "17:31:22 | time:41s total_exs:1600 total_steps:50 epochs:0.00 time_left:2093s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   330.5 .9600  1835  4505   .7675      215.8 39.27 1600             33423     17    .5352 80.98 6.042 3.5e-06  1071  2628   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1906      14.07 420.7     .08856         0                   50 2906 7133 1.227\n",
            "\n",
            "17:32:05 | time:84s total_exs:3200 total_steps:100 epochs:0.01 time_left:2110s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     330     1  1833  4321   .7650      215.5 37.73 1600             32768  7.167    .5352  81.5 5.463 6.93e-06  1066  2514   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1806      14.87 235.8      .1076         0                  100 2899 6835 1.179\n",
            "\n",
            "17:32:47 | time:125s total_exs:4800 total_steps:150 epochs:0.01 time_left:2065s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   321.5     1  1829  4389   .7594      207.2 38.39 1600             32768  3.724    .5352 78.75 4.918 6.93e-06  1041  2498   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "     .1737      13.67 136.7      .1560         0                  150 2871 6887  1.2\n",
            "\n",
            "17:33:29 | time:168s total_exs:6400 total_steps:200 epochs:0.02 time_left:2026s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   322.1     1  1846  4391   .7712      206.8 38.05 1600             32768  3.582    .5352 78.61 4.696 6.93e-06  1034  2460   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "     .1762      13.98 109.5      .1821         0                  200 2880 6850 1.19\n",
            "\n",
            "17:34:11 | time:209s total_exs:8000 total_steps:250 epochs:0.02 time_left:1984s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   325.9     1  1826  4360   .7606      211.8 38.21 1600             32768  3.415    .5352 81.68 4.596 6.93e-06  1076  2570   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1875      14.41 99.05      .1886         0                  250 2902 6931 1.194\n",
            "\n",
            "17:34:52 | time:251s total_exs:9600 total_steps:300 epochs:0.03 time_left:1942s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   330.7     1  1842  4402   .7712      215.6 38.24 1600             32768  3.446    .5352 81.07 4.517 6.93e-06  1075  2570   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1869      13.88 91.54      .1949         0                  300 2917 6971 1.195\n",
            "\n",
            "17:35:34 | time:293s total_exs:11200 total_steps:350 epochs:0.03 time_left:1901s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     318     1  1822  4336   .7562      204.1 38.08 1600             32768  3.538    .5353  78.4 4.452 6.93e-06  1045  2488   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "     .1800      13.07 85.83      .2031         0                  350 2867 6823 1.19\n",
            "\n",
            "17:36:17 | time:335s total_exs:12800 total_steps:400 epochs:0.04 time_left:1860s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   337.6     1  1829  4346   .7606      223.3 38.02 1600             32768   3.55    .5353 78.96  4.38 6.93e-06  1043  2478   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1825      13.78 79.86      .2098         0                  400 2872 6824 1.189\n",
            "\n",
            "17:36:59 | time:378s total_exs:14400 total_steps:450 epochs:0.04 time_left:1819s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   325.4     1  1839  4376   .7631      210.4 38.07 1600             32768  3.533    .5353 81.71  4.34 6.93e-06  1063  2529   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "     .2031      15.29 76.72      .2161         0                  450 2902 6905 1.19\n",
            "\n",
            "17:37:41 | time:420s total_exs:16000 total_steps:500 epochs:0.05 time_left:1777s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   324.4     1  1822  4331   .7450      210.6 38.04 1600             32768  3.558    .5353 83.89 4.358 6.93e-06  1076  2558   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2056      16.65 78.07      .2137         0                  500 2898 6889 1.189\n",
            "\n",
            "17:38:23 | time:462s total_exs:17600 total_steps:550 epochs:0.05 time_left:1736s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   335.6     1  1840  4381   .7688      220.6  38.1 1600             32768  3.441    .5353 82.45 4.291 6.93e-06  1076  2563   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1931      15.18 73.02      .2191         0                  550 2916 6944 1.191\n",
            "\n",
            "17:39:05 | time:504s total_exs:19200 total_steps:600 epochs:0.06 time_left:1693s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   319.7     1  1822  4352   .7462      205.9 38.23 1600             32768  3.613    .5353 82.94 4.246 6.93e-06  1073  2564   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2044      15.86 69.85      .2244         0                  600 2895 6916 1.195\n",
            "\n",
            "17:39:46 | time:545s total_exs:20800 total_steps:650 epochs:0.06 time_left:1651s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   315.1     1  1832  4382   .7531      200.6 38.28 1600             32768  3.563    .5354 79.28 4.255 6.93e-06  1046  2503   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1750      13.89 70.48      .2250         0                  650 2878 6885 1.196\n",
            "\n",
            "17:40:28 | time:587s total_exs:22400 total_steps:700 epochs:0.07 time_left:1609s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   336.7     1  1852  4406   .7800        221 38.07 1600             32768  3.605    .5354 82.42 4.174 6.93e-06  1076  2559   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "     .1869      15.19 64.97      .2340         0                  700 2927 6965 1.19\n",
            "\n",
            "17:41:11 | time:630s total_exs:24000 total_steps:750 epochs:0.07 time_left:1568s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   323.8     1  1842  4380   .7675      208.7 38.06 1600             32768  3.704    .5354 78.26 4.149 6.93e-06  1055  2509   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "     .1781      12.34 63.4      .2353         0                  750 2896 6889 1.19\n",
            "\n",
            "17:41:53 | time:672s total_exs:25600 total_steps:800 epochs:0.08 time_left:1527s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   339.5     1  1847  4372   .7744      224.1 37.87 1600             32768  3.548    .5354 83.34 4.132 6.93e-06  1100  2603   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1988      14.61 62.32      .2364         0                  800 2947 6975 1.184\n",
            "\n",
            "17:42:35 | time:714s total_exs:27200 total_steps:850 epochs:0.08 time_left:1485s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     335     1  1837  4362   .7669      220.2    38 1600             32768  3.671    .5354 82.11 4.129 6.93e-06  1074  2551   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2044      14.99 62.1      .2377         0                  850 2911 6913 1.188\n",
            "\n",
            "17:43:17 | time:756s total_exs:28800 total_steps:900 epochs:0.09 time_left:1443s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   321.9     1  1836  4372   .7619      207.2 38.11 1600             32768   3.67    .5354 81.02 4.116 6.93e-06  1062  2530   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1919      14.63 61.34      .2404         0                  900 2898 6903 1.191\n",
            "\n",
            "17:43:59 | time:798s total_exs:30400 total_steps:950 epochs:0.09 time_left:1401s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   324.5     1  1838  4383   .7556      209.6 38.15 1600             32768   3.67    .5354 80.15 4.109 6.93e-06  1053  2511   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1894      14.34 60.86      .2401    .00125                  950 2891 6894 1.193\n",
            "\n",
            "17:44:41 | time:840s total_exs:32000 total_steps:1000 epochs:0.10 time_left:1359s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   330.1     1  1844  4389   .7712      214.9 38.07 1600             32768  3.669    .5354 82.05 4.082 6.93e-06  1062  2527   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "     .1919      15.66 59.26      .2451         0                 1000 2906 6916 1.19\n",
            "\n",
            "17:45:23 | time:882s total_exs:33600 total_steps:1050 epochs:0.10 time_left:1317s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   336.1     1  1848  4390   .7750      220.5 38.01 1600             32768  3.602    .5355 84.11  4.06 6.93e-06  1096  2603   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2013      15.61 57.98      .2443         0                 1050 2944 6994 1.188\n",
            "\n",
            "17:46:05 | time:924s total_exs:35200 total_steps:1100 epochs:0.11 time_left:1275s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   333.2     1  1819  4309   .7581      219.5  37.9 1600             32768  3.533    .5355 86.28 4.027 6.93e-06  1142  2704   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2194      14.93 56.07      .2483   .000625                 1100 2960 7013 1.185\n",
            "\n",
            "17:46:47 | time:966s total_exs:36800 total_steps:1150 epochs:0.11 time_left:1233s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   317.7     1  1830  4357   .7656      203.4 38.09 1600             32768  3.682    .5355 79.02 4.056 6.93e-06  1045  2489   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1819      13.69 57.73      .2476         0                 1150 2875 6845 1.191\n",
            "\n",
            "17:47:29 | time:1008s total_exs:38400 total_steps:1200 epochs:0.11 time_left:1191s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   319.4     1  1841  4386   .7631      204.3 38.12 1600             32768  3.733    .5355 81.07 3.999 6.93e-06  1058  2522   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1925      14.91 54.54      .2532    .00125                 1200 2900 6908 1.192\n",
            "\n",
            "17:48:11 | time:1050s total_exs:40000 total_steps:1250 epochs:0.12 time_left:1149s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   339.8     1  1829  4357   .7638      225.5 38.11 1600             32768  3.596    .5356 83.91  3.99 6.93e-06  1097  2612   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2050      15.37 54.06      .2527         0                 1250 2926 6969 1.191\n",
            "\n",
            "17:48:53 | time:1092s total_exs:41600 total_steps:1300 epochs:0.12 time_left:1107s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   326.1     1  1830  4359   .7638      211.7 38.11 1600             32768  3.606    .5356 81.74  3.99 6.93e-06  1070  2548   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1963      14.87 54.04      .2546         0                 1300 2900 6907 1.191\n",
            "\n",
            "17:49:35 | time:1134s total_exs:43200 total_steps:1350 epochs:0.13 time_left:1065s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   318.5     1  1826  4354   .7506      204.4 38.16 1600             32768   3.69    .5356 78.32 3.985 6.93e-06  1046  2494   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1781      12.96 53.8      .2569         0                 1350 2872 6848 1.193\n",
            "\n",
            "17:50:17 | time:1176s total_exs:44800 total_steps:1400 epochs:0.13 time_left:1023s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   318.8     1  1831  4381   .7588      204.3 38.27 1600             32768  3.784    .5356 79.55 3.956 6.93e-06  1050  2511   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1719      13.95 52.24      .2600         0                 1400 2881 6892 1.196\n",
            "\n",
            "17:50:59 | time:1218s total_exs:46400 total_steps:1450 epochs:0.14 time_left:981s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   339.6     1  1829  4350   .7594      225.3 38.05 1600             32768  3.696    .5356 84.21 3.963 6.93e-06  1089  2591   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2181      16.13 52.62      .2566         0                 1450 2918 6940 1.189\n",
            "\n",
            "17:51:41 | time:1260s total_exs:48000 total_steps:1500 epochs:0.14 time_left:939s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     323     1  1825  4359   .7562      208.9 38.21 1600             32768  3.686    .5356 81.12 3.917 6.93e-06  1064  2542   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1938      14.59 50.24      .2652         0                 1500 2890 6901 1.194\n",
            "\n",
            "17:52:23 | time:1302s total_exs:49600 total_steps:1550 epochs:0.15 time_left:897s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   312.2     1  1813  4318   .7431      198.9  38.1 1600             32768  3.679    .5356 80.28 3.926 6.93e-06  1059  2523   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1844      14.07 50.71      .2622         0                 1550 2873 6840 1.191\n",
            "\n",
            "17:53:05 | time:1344s total_exs:51200 total_steps:1600 epochs:0.15 time_left:855s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   328.6     1  1847  4399   .7700      213.2 38.11 1600             32768  3.713    .5357 81.21 3.922 6.93e-06  1069  2546   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1919      14.41 50.49      .2619         0                 1600 2916 6945 1.191\n",
            "\n",
            "17:53:47 | time:1386s total_exs:52800 total_steps:1650 epochs:0.16 time_left:813s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   319.5     1  1831  4353   .7688      205.1 38.03 1600             32768  3.722    .5357 83.04 3.929 6.93e-06  1081  2570   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2044      15.48 50.85      .2615         0                 1650 2912 6923 1.189\n",
            "\n",
            "17:54:29 | time:1428s total_exs:54400 total_steps:1700 epochs:0.16 time_left:771s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   330.4     1  1837  4376   .7662      215.6 38.11 1600             32768  3.649    .5357 83.48 3.907 6.93e-06  1080  2573   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2031      15.97 49.77      .2619         0                 1700 2918 6949 1.191\n",
            "\n",
            "17:55:11 | time:1470s total_exs:56000 total_steps:1750 epochs:0.17 time_left:729s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   323.6     1  1833  4375   .7625        209 38.19 1600             32768   3.73    .5358 81.31 3.916 6.93e-06  1066  2545   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1913      14.67 50.2      .2643         0                 1750 2899 6920 1.194\n",
            "\n",
            "17:55:53 | time:1512s total_exs:57600 total_steps:1800 epochs:0.17 time_left:687s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   321.8     1  1830  4369   .7644      207.5  38.2 1600             32768  3.892    .5357  77.9 3.848 6.93e-06  1024  2446   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1881      13.88 46.91      .2732         0                 1800 2854 6815 1.194\n",
            "\n",
            "17:56:35 | time:1554s total_exs:59200 total_steps:1850 epochs:0.18 time_left:645s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   337.9     1  1843  4408   .7656      222.7 38.26 1600             32768  3.688    .5358 82.65 3.871 6.93e-06  1069  2557   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2131      15.81 47.98      .2689         0                 1850 2913 6966 1.196\n",
            "\n",
            "17:57:17 | time:1596s total_exs:60800 total_steps:1900 epochs:0.18 time_left:603s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   318.7     1  1826  4348   .7525      204.5 38.09 1600             32768  3.725    .5358 79.39 3.879 6.93e-06  1051  2503   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1719       13.7 48.39      .2671         0                 1900 2877 6851 1.191\n",
            "\n",
            "17:57:59 | time:1638s total_exs:62400 total_steps:1950 epochs:0.19 time_left:561s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   329.9     1  1836  4366   .7688      215.1 38.05 1600             32768  3.721    .5358 81.69 3.837 6.93e-06  1065  2532   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "     .1956      15.14 46.38      .2715         0                 1950 2901 6898 1.19\n",
            "\n",
            "17:58:41 | time:1680s total_exs:64000 total_steps:2000 epochs:0.19 time_left:519s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   347.7     1  1852  4397   .7775      231.9 37.99 1600             32768  3.557    .5358 89.64 3.816 6.93e-06  1146  2721   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2306      18.01 45.42      .2731         0                 2000 2998 7118 1.187\n",
            "\n",
            "17:59:23 | time:1722s total_exs:65600 total_steps:2050 epochs:0.20 time_left:477s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   342.6     1  1852  4406   .7831      226.8 38.07 1600             64881  3.684    .5358 83.89 3.849 6.93e-06  1088  2589   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  ups  \n",
            "     .2100      15.87 46.94      .2723         0                 2050 2940 6995 1.19\n",
            "\n",
            "18:00:05 | time:1764s total_exs:67200 total_steps:2100 epochs:0.20 time_left:435s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     322     1  1815  4315   .7506      208.5 38.04 1600             65536  3.703    .5358 81.38 3.838 6.93e-06  1065  2532   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1862      14.84 46.44      .2712         0                 2100 2880 6847 1.189\n",
            "\n",
            "18:00:47 | time:1806s total_exs:68800 total_steps:2150 epochs:0.21 time_left:393s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     324     1  1827  4350   .7588      209.8 38.11 1600             65536  3.715    .5359 81.53 3.815 6.93e-06  1060  2525   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1900      15.26 45.37      .2767         0                 2150 2887 6876 1.191\n",
            "\n",
            "18:01:29 | time:1848s total_exs:70400 total_steps:2200 epochs:0.21 time_left:351s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   336.8     1  1844  4390   .7719      221.5  38.1 1600             65536  3.754    .5359 80.21 3.794 6.93e-06  1061  2526   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1856      13.91 44.44      .2805         0                 2200 2905 6916 1.191\n",
            "\n",
            "18:02:11 | time:1890s total_exs:72000 total_steps:2250 epochs:0.21 time_left:309s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   332.1     1  1858  4430   .7863        216 38.15 1600             65536  3.713    .5359 80.14 3.802 6.93e-06  1057  2520   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1812       14.1 44.81      .2774         0                 2250 2914 6950 1.193\n",
            "\n",
            "18:02:54 | time:1932s total_exs:73600 total_steps:2300 epochs:0.22 time_left:267s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     321     1  1835  4360   .7631      206.3 38.01 1600             65536  3.679    .5359 84.71  3.82 6.93e-06  1098  2609   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2044      16.07 45.6      .2775         0                 2300 2933 6968 1.188\n",
            "\n",
            "18:03:36 | time:1975s total_exs:75200 total_steps:2350 epochs:0.22 time_left:225s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   331.7     1  1839  4370   .7669      216.8 38.03 1600             65536  3.557    .5359 82.86 3.796 6.93e-06  1097  2608   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2013      14.29 44.52      .2787         0                 2350 2936 6978 1.189\n",
            "\n",
            "18:04:18 | time:2017s total_exs:76800 total_steps:2400 epochs:0.23 time_left:183s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   350.1     1  1847  4382   .7738      234.7 37.96 1600             65536  3.725    .5360 85.26 3.755 6.93e-06  1095  2599   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2081       16.8 42.72      .2816         0                 2400 2942 6981 1.187\n",
            "\n",
            "18:05:00 | time:2059s total_exs:78400 total_steps:2450 epochs:0.23 time_left:141s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   333.4     1  1840  4380   .7681      218.4 38.09 1600             65536  3.756    .5359 81.55 3.788 6.93e-06  1048  2494   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2013      16.07 44.17      .2794         0                 2450 2887 6875 1.191\n",
            "\n",
            "18:05:42 | time:2101s total_exs:80000 total_steps:2500 epochs:0.24 time_left:99s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   317.7     1  1836  4372   .7575        203 38.09 1600             65536  3.709    .5359 80.31 3.769 6.93e-06  1052  2505   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1875      14.54 43.35      .2812         0                 2500 2889 6877 1.191\n",
            "\n",
            "18:06:24 | time:2143s total_exs:81600 total_steps:2550 epochs:0.24 time_left:57s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   310.7     1  1814  4324   .7525      197.4 38.14 1600             65536  3.805    .5360  78.5  3.75 6.93e-06  1050  2504   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1812      12.87 42.53      .2854         0                 2550 2864 6828 1.192\n",
            "\n",
            "18:07:06 | time:2185s total_exs:83200 total_steps:2600 epochs:0.25 time_left:15s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   346.3     1  1850  4394   .7788      230.6 37.99 1600             65536  3.709    .5360 82.57 3.763 6.93e-06  1088  2583   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1994      14.57 43.07      .2843         0                 2600 2938 6977 1.187\n",
            "\n",
            "18:07:21 | time:2200s total_exs:83776 total_steps:2618 epochs:0.25 time_left:0s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     301     1  1824  4356   .7604        187  38.2  576             65536  3.847    .5359 72.15 3.766 6.93e-06 985.3  2353   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1389      10.57 43.19      .2851         0                 2618 2810 6709 1.195\n",
            "\n",
            "18:07:21 | num_epochs completed:0.25 time elapsed:2200.021808862686s\n",
            "18:07:21 | Saving dictionary to /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-400m//model.dict\n",
            "18:07:36 | \u001b[33mOverriding opt[\"init_model\"] to zoo:blender/blender_400Mdistill/model (previously: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model)\u001b[0m\n",
            "18:07:36 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,world_logs: ,save_format: conversations,log_keep_fields: all,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data,fromfile_datatype_extension: True,checkpoint_activations: False,verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "18:07:36 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--init-opt /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt --allow-missing-init-opts True --task blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 8 --dynamic-batching full --eval-batchsize 8 --num-epochs -1 --validation-every-n-secs 900.0 --save-after-valid True --validation-every-n-epochs -1.0 --validation-patience 20 --log-every-n-secs 10.0 --distributed-world-size 8 --port 61337 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --chosen-topic-delimiter \n",
            " --num-topics 5 --train-experiencer-only False --remove-political-convos False --teacher-model /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model --task-loss-coeff 1.0 --encoder-loss-coeff 24.0 --hidden-loss-coeff 5.0 --pred-loss-coeff 8.0 --embedding-loss-coeff 0.35 --self-attn-loss-coeff 0.6 --enc-dec-attn-loss-coeff 3.0 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --inference beam --force-fp16-tokens False --learningrate 0.0004 --gpu 0 --bpe-vocab /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json --bpe-merge /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt --bpe-add-prefix-space True --max-lr-steps -1 --parlai-home /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI --show-advanced-args False --numthreads 1 --rank 0\u001b[0m\n",
            "18:07:36 | Using CUDA\n",
            "18:07:36 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-400m//model.dict\n",
            "18:07:36 | num words = 8008\n",
            "18:07:43 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "18:07:43 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-400m//model\n",
            "18:07:53 | creating task(s): fromfile:parlaiformat\n",
            "18:07:53 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data_valid.txt\n",
            "18:07:55 | running eval: valid\n",
            "18:07:55 | \u001b[33m--skip-generation true produces limited metrics\u001b[0m\n",
            "18:13:32 | eval completed in 337.04s\n",
            "18:13:32 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   324.8  1829 14227   .7601      210.3 124.3 41899    .2818 80.77 3.658 6.93e-06  1062  8260   .1888      14.33 38.79   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .3014 9.547e-05                 2618 2890 22488\n",
            "\u001b[0m\n",
            "18:13:32 | creating task(s): fromfile:parlaiformat\n",
            "18:13:32 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data_test.txt\n",
            "18:13:33 | running eval: test\n",
            "18:19:08 | eval completed in 334.29s\n",
            "18:19:08 | \u001b[1mtest:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   324.8  1828 14299   .7584      210.5   125 41798    .2819 81.55 3.653 6.93e-06  1068  8358   .1929      14.71 38.6   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .3023 4.785e-05                 2618 2896 22657\n",
            "\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'clen': AverageMetric(324.8),\n",
              "  'ctpb': GlobalAverageMetric(1829),\n",
              "  'ctps': GlobalTimerMetric(1.423e+04),\n",
              "  'ctrunc': AverageMetric(0.7601),\n",
              "  'ctrunclen': AverageMetric(210.3),\n",
              "  'exps': GlobalTimerMetric(124.3),\n",
              "  'exs': SumMetric(4.19e+04),\n",
              "  'gpu_mem': GlobalAverageMetric(0.2818),\n",
              "  'llen': AverageMetric(80.77),\n",
              "  'loss': AverageMetric(3.658),\n",
              "  'lr': GlobalAverageMetric(6.93e-06),\n",
              "  'ltpb': GlobalAverageMetric(1062),\n",
              "  'ltps': GlobalTimerMetric(8260),\n",
              "  'ltrunc': AverageMetric(0.1888),\n",
              "  'ltrunclen': AverageMetric(14.33),\n",
              "  'ppl': PPLMetric(38.79),\n",
              "  'token_acc': AverageMetric(0.3014),\n",
              "  'token_em': AverageMetric(9.547e-05),\n",
              "  'total_train_updates': GlobalFixedMetric(2618),\n",
              "  'tpb': GlobalAverageMetric(2890),\n",
              "  'tps': GlobalTimerMetric(2.249e+04)},\n",
              " {'clen': AverageMetric(324.8),\n",
              "  'ctpb': GlobalAverageMetric(1828),\n",
              "  'ctps': GlobalTimerMetric(1.43e+04),\n",
              "  'ctrunc': AverageMetric(0.7584),\n",
              "  'ctrunclen': AverageMetric(210.5),\n",
              "  'exps': GlobalTimerMetric(125),\n",
              "  'exs': SumMetric(4.18e+04),\n",
              "  'gpu_mem': GlobalAverageMetric(0.2819),\n",
              "  'llen': AverageMetric(81.55),\n",
              "  'loss': AverageMetric(3.653),\n",
              "  'lr': GlobalAverageMetric(6.93e-06),\n",
              "  'ltpb': GlobalAverageMetric(1068),\n",
              "  'ltps': GlobalTimerMetric(8358),\n",
              "  'ltrunc': AverageMetric(0.1929),\n",
              "  'ltrunclen': AverageMetric(14.71),\n",
              "  'ppl': PPLMetric(38.6),\n",
              "  'token_acc': AverageMetric(0.3023),\n",
              "  'token_em': AverageMetric(4.785e-05),\n",
              "  'total_train_updates': GlobalFixedMetric(2618),\n",
              "  'tpb': GlobalAverageMetric(2896),\n",
              "  'tps': GlobalTimerMetric(2.266e+04)})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 400M settings\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    # task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "    # multitask_weights= \"1,3,3\",\n",
        "\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 0.25,\n",
        "    log_every_n_secs= 300,\n",
        "    verbose = True,\n",
        "    attention_dropout= 0.0, \n",
        "    batchsize= 16, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    # save_after_valid= True,\n",
        "\n",
        "    # arguments we get from the pretrained model. \"from recipes page for 2.7B model\" \n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    variant= \"prelayernorm\",\n",
        "    n_heads= 32, n_positions= 128, \n",
        "    n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "    dropout= 0.1,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    relu_dropout= 0.0, \n",
        "    model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    # inference = 'topk', \n",
        "    # temperature = 0.7, \n",
        "    # topk=30, \n",
        "    # beam_length_penalty=1.03\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1S0kRk3W63i"
      },
      "outputs": [],
      "source": [
        "# !cp -rv /content/finetuned-multitask-400m-double-sided-2epochs/* /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-double-sided/\n",
        "# !ls -lah /content/finetuned-multitask-400m-double-sided-2epochs/\n",
        "# !ls -lah /content/finetuned-multitask-400m-double-sided/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVPS6p4XzPh4"
      },
      "source": [
        "# 4.Display Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9FBtnZZzPPg",
        "outputId": "44957071-1838-468e-c821-b4974ad2b54d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15:05:29 | \u001b[33mOverriding opt[\"datatype\"] to test (previously: train)\u001b[0m\n",
            "15:05:29 | \u001b[33mOverriding opt[\"model_file\"] to /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m//model (previously: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m/model)\u001b[0m\n",
            "15:05:29 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "15:05:29 | \u001b[33mOverriding opt[\"beam_block_ngram\"] to 2 (previously: -1)\u001b[0m\n",
            "15:05:29 | \u001b[33mOverriding opt[\"beam_context_block_ngram\"] to 3 (previously: -1)\u001b[0m\n",
            "15:05:29 | \u001b[33mOverriding opt[\"beam_length_penalty\"] to 1.0 (previously: 0.65)\u001b[0m\n",
            "15:05:29 | \u001b[33mOverriding opt[\"beam_min_length\"] to 10 (previously: 1)\u001b[0m\n",
            "15:05:29 | \u001b[33mOverriding opt[\"beam_size\"] to 20 (previously: 1)\u001b[0m\n",
            "15:05:29 | \u001b[33mOverriding opt[\"inference\"] to topk (previously: greedy)\u001b[0m\n",
            "15:05:29 | \u001b[33mOverriding opt[\"temperature\"] to 0.5 (previously: 1.0)\u001b[0m\n",
            "15:05:29 | \u001b[33mOverriding opt[\"topk\"] to 20 (previously: 10)\u001b[0m\n",
            "15:05:29 | Using CUDA\n",
            "15:05:29 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m//model.dict\n",
            "15:05:29 | num words = 54944\n",
            "15:05:31 | Total parameters: 87,508,992 (86,984,704 trainable)\n",
            "15:05:31 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m//model\n",
            "15:05:32 | creating task(s): fromfile:parlaiformat\n",
            "15:05:32 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data_test.txt\n",
            "15:05:33 | Opt:\n",
            "15:05:33 |     activation: gelu\n",
            "15:05:33 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "15:05:33 |     adam_eps: 1e-08\n",
            "15:05:33 |     add_p1_after_newln: False\n",
            "15:05:33 |     aggregate_micro: False\n",
            "15:05:33 |     allow_missing_init_opts: False\n",
            "15:05:33 |     attention_dropout: 0.0\n",
            "15:05:33 |     batchsize: 8\n",
            "15:05:33 |     beam_block_full_context: True\n",
            "15:05:33 |     beam_block_list_filename: None\n",
            "15:05:33 |     beam_block_ngram: 2\n",
            "15:05:33 |     beam_context_block_ngram: 3\n",
            "15:05:33 |     beam_delay: 30\n",
            "15:05:33 |     beam_length_penalty: 1.0\n",
            "15:05:33 |     beam_min_length: 10\n",
            "15:05:33 |     beam_size: 20\n",
            "15:05:33 |     betas: '[0.9, 0.999]'\n",
            "15:05:33 |     bpe_add_prefix_space: None\n",
            "15:05:33 |     bpe_debug: False\n",
            "15:05:33 |     bpe_dropout: None\n",
            "15:05:33 |     bpe_merge: None\n",
            "15:05:33 |     bpe_vocab: None\n",
            "15:05:33 |     checkpoint_activations: False\n",
            "15:05:33 |     compute_tokenized_bleu: False\n",
            "15:05:33 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "15:05:33 |     datatype: test\n",
            "15:05:33 |     delimiter: '\\n'\n",
            "15:05:33 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "15:05:33 |     dict_endtoken: __end__\n",
            "15:05:33 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m//model.dict\n",
            "15:05:33 |     dict_include_test: False\n",
            "15:05:33 |     dict_include_valid: False\n",
            "15:05:33 |     dict_initpath: None\n",
            "15:05:33 |     dict_language: english\n",
            "15:05:33 |     dict_loaded: True\n",
            "15:05:33 |     dict_lower: True\n",
            "15:05:33 |     dict_max_ngram_size: -1\n",
            "15:05:33 |     dict_maxexs: -1\n",
            "15:05:33 |     dict_maxtokens: -1\n",
            "15:05:33 |     dict_minfreq: 0\n",
            "15:05:33 |     dict_nulltoken: __null__\n",
            "15:05:33 |     dict_starttoken: __start__\n",
            "15:05:33 |     dict_textfields: text,labels\n",
            "15:05:33 |     dict_tokenizer: bpe\n",
            "15:05:33 |     dict_unktoken: __unk__\n",
            "15:05:33 |     display_add_fields: \n",
            "15:05:33 |     display_examples: False\n",
            "15:05:33 |     download_path: None\n",
            "15:05:33 |     dropout: 0.0\n",
            "15:05:33 |     dynamic_batching: full\n",
            "15:05:33 |     embedding_projection: random\n",
            "15:05:33 |     embedding_size: 512\n",
            "15:05:33 |     embedding_type: random\n",
            "15:05:33 |     embeddings_scale: True\n",
            "15:05:33 |     eval_batchsize: None\n",
            "15:05:33 |     eval_dynamic_batching: None\n",
            "15:05:33 |     evaltask: None\n",
            "15:05:33 |     ffn_size: 2048\n",
            "15:05:33 |     final_extra_opt: \n",
            "15:05:33 |     force_fp16_tokens: True\n",
            "15:05:33 |     fp16: True\n",
            "15:05:33 |     fp16_impl: mem_efficient\n",
            "15:05:33 |     fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data\n",
            "15:05:33 |     fromfile_datatype_extension: True\n",
            "15:05:33 |     gpu: -1\n",
            "15:05:33 |     gradient_clip: 0.1\n",
            "15:05:33 |     hide_labels: False\n",
            "15:05:33 |     history_add_global_end_token: None\n",
            "15:05:33 |     history_reversed: False\n",
            "15:05:33 |     history_size: -1\n",
            "15:05:33 |     image_cropsize: 224\n",
            "15:05:33 |     image_mode: raw\n",
            "15:05:33 |     image_size: 256\n",
            "15:05:33 |     inference: topk\n",
            "15:05:33 |     init_model: zoo:blender/blender_90M/model\n",
            "15:05:33 |     init_opt: None\n",
            "15:05:33 |     interactive_mode: False\n",
            "15:05:33 |     invsqrt_lr_decay_gamma: -1\n",
            "15:05:33 |     is_debug: False\n",
            "15:05:33 |     label_truncate: 128\n",
            "15:05:33 |     learn_positional_embeddings: False\n",
            "15:05:33 |     learningrate: 1e-05\n",
            "15:05:33 |     log_every_n_secs: 60.0\n",
            "15:05:33 |     log_every_n_steps: 50\n",
            "15:05:33 |     log_keep_fields: all\n",
            "15:05:33 |     loglevel: info\n",
            "15:05:33 |     lr_scheduler: reduceonplateau\n",
            "15:05:33 |     lr_scheduler_decay: 0.5\n",
            "15:05:33 |     lr_scheduler_patience: 3\n",
            "15:05:33 |     max_train_steps: -1\n",
            "15:05:33 |     max_train_time: -1\n",
            "15:05:33 |     metrics: default\n",
            "15:05:33 |     model: transformer/generator\n",
            "15:05:33 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m//model\n",
            "15:05:33 |     model_parallel: False\n",
            "15:05:33 |     momentum: 0\n",
            "15:05:33 |     multitask_weights: [1]\n",
            "15:05:33 |     mutators: None\n",
            "15:05:33 |     n_decoder_layers: -1\n",
            "15:05:33 |     n_encoder_layers: -1\n",
            "15:05:33 |     n_heads: 16\n",
            "15:05:33 |     n_layers: 8\n",
            "15:05:33 |     n_positions: 512\n",
            "15:05:33 |     n_segments: 0\n",
            "15:05:33 |     nesterov: True\n",
            "15:05:33 |     no_cuda: False\n",
            "15:05:33 |     num_epochs: 5.0\n",
            "15:05:33 |     num_examples: 20\n",
            "15:05:33 |     num_workers: 0\n",
            "15:05:33 |     nus: [0.7]\n",
            "15:05:33 |     optimizer: mem_eff_adam\n",
            "15:05:33 |     output_scaling: 1.0\n",
            "15:05:33 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data', 'fromfile_datatype_extension': True, 'datatype': 'test', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m//model', 'num_examples': '20', 'skip_generation': False, 'beam_block_ngram': 2, 'beam_context_block_ngram': 3, 'beam_length_penalty': 1.0, 'beam_min_length': 10, 'beam_size': 20, 'inference': 'topk', 'temperature': 0.5, 'topk': 20, 'topp': 0.9}\"\n",
            "15:05:33 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "15:05:33 |     person_tokens: False\n",
            "15:05:33 |     rank_candidates: False\n",
            "15:05:33 |     relu_dropout: 0.0\n",
            "15:05:33 |     save_after_valid: False\n",
            "15:05:33 |     save_every_n_secs: -1\n",
            "15:05:33 |     save_format: conversations\n",
            "15:05:33 |     share_word_embeddings: True\n",
            "15:05:33 |     short_final_eval: False\n",
            "15:05:33 |     skip_generation: False\n",
            "15:05:33 |     special_tok_lst: None\n",
            "15:05:33 |     split_lines: False\n",
            "15:05:33 |     starttime: Jun03_23-03\n",
            "15:05:33 |     task: fromfile:parlaiformat\n",
            "15:05:33 |     temperature: 0.5\n",
            "15:05:33 |     tensorboard_log: False\n",
            "15:05:33 |     tensorboard_logdir: None\n",
            "15:05:33 |     text_truncate: 512\n",
            "15:05:33 |     topk: 20\n",
            "15:05:33 |     topp: 0.9\n",
            "15:05:33 |     truncate: -1\n",
            "15:05:33 |     update_freq: 1\n",
            "15:05:33 |     use_reply: label\n",
            "15:05:33 |     validation_cutoff: 1.0\n",
            "15:05:33 |     validation_every_n_epochs: 0.25\n",
            "15:05:33 |     validation_every_n_secs: -1\n",
            "15:05:33 |     validation_every_n_steps: -1\n",
            "15:05:33 |     validation_max_exs: -1\n",
            "15:05:33 |     validation_metric: ppl\n",
            "15:05:33 |     validation_metric_mode: min\n",
            "15:05:33 |     validation_patience: 10\n",
            "15:05:33 |     validation_share_agent: False\n",
            "15:05:33 |     variant: xlm\n",
            "15:05:33 |     verbose: False\n",
            "15:05:33 |     wandb_entity: None\n",
            "15:05:33 |     wandb_log: False\n",
            "15:05:33 |     wandb_name: None\n",
            "15:05:33 |     wandb_project: None\n",
            "15:05:33 |     warmup_rate: 0.0001\n",
            "15:05:33 |     warmup_updates: 100\n",
            "15:05:33 |     weight_decay: None\n",
            "15:05:33 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mTiens Fauve vient de sortir leur nouvel album. J'l'écoute, j'vais me suicider et je reviens. A toute.Edit : p'tain c'est tellement la même chose, j'ai du vérifier que c'était bien nouveau.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Y'a un an et demi, j'avais accroché à mort. Aujourd'hui, ça ne me touche plus. Du tout. C'est des geignards et c'est tout.Ça m'a fait pareil avec Orelsan. Au début j'adorais. Un ou deux ans plus tard, je suis tellement en désaccord avec son message que je ne supporte plus. C'est dingue ce qu'on évolue...\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis suicidala là - bas en fait .\u001b[0;0m\n",
            "\u001b[0mC'est quoi le message d'Orelsan ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Bois, fume, ne fout rien, ne prend pas tes responsabilités, de toute façon, c'est le monde qui est pourri, pas toi qui est trop flemmard pour te sortir les doigts du cul. Une ode à la flemmardise.\u001b[0;0m\n",
            "\u001b[0;95m     model: le genre de trucs qui se trouvent évidemment célérément déjà dans le métro . c ’ est un peu comme quoi .\u001b[0;0m\n",
            "\u001b[0mAutant Orelsan est un rappeur de merde, autant tu te fourre le doigt dans l'oeil quand a son \"message\".Mais quand l'rideau tombe, et qu'ta conscience te tyranniseTu sais qu'celui qui vit comme un égoïste finira tristeLa dualité de son \"personnage\" est la seule chose intéressante dans son oeuvre, alors mieux vaut éviter de passer a coté.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Pour moi ça reste le mec de \"suce ma bite pour la st valentin\", j'ai pas compris quand il a reviré sérieux. Pourquoi ? C'est comme si Didier Super se mettait à chanter du Laurent Voulzy, bon sauf que didier Super est un génie et Orelsan un petit con mais quand même...\u001b[0;0m\n",
            "\u001b[0;95m     model: les gens de la médecine sont toujours pas responsable de toi .\u001b[0;0m\n",
            "\u001b[0mIl en parle dans une de ses chansons (flemme de chercher) un peu plus sérieuse où il dit qu'il était un branleur quand il était jeune. Saint-Valentin/Sale Pute c'était il y a 9 ans, les gens changent.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Au moins c'était drôle.\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' etait quoi les gens qui ont changé de changement ?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mJ'attends fébrilement le Mal Parler, où normalement je devrais prendre  méga cher. Mais ça promet d'être drôle :D\u001b[0;0m\n",
            "\u001b[1;94m    labels: Tu fais quoi dans ta vie?\u001b[0;0m\n",
            "\u001b[0;95m     model: je sais pas trop comment tu veux faire .\u001b[0;0m\n",
            "\u001b[0mNe sois pas pressé jeune padawan. Pas de faux départ sur ce topic :D\u001b[0;0m\n",
            "\u001b[1;94m    labels: En tout cas, tu pars bien, t'es downvoté d'entrée de jeu.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne suis pas d ' accord avec toi .\u001b[0;0m\n",
            "\u001b[0mNormalement on ne downvote pas sur un MAL PARLER.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Normalement on ne downvote pas les gens avec lesquels on est pas d'accord non plus.\u001b[0;0m\n",
            "\u001b[0;95m     model: j ' ai pas vu que tu downvote .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mVous pensez qu'on va dépasser la barre des 600 messages?     Moi je pense qu'on va dépasser la barre des 600 messages\u001b[0;0m\n",
            "\u001b[1;94m    labels: Si ya pas d'AskFrance de grosse qualité qui sort, on va les passer large.\u001b[0;0m\n",
            "\u001b[0;95m     model: on peut pas dire que ce soit une bonne partie des messages .\u001b[0;0m\n",
            "\u001b[0mJe n'ose espérer passer la barre des 700. Ce serait irréalisable. N'est-ce pas? Hein?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Pour les 700, il faudrait un gros troll de qualité, du niveau du mec au CDI. Mais ce serait historique.\u001b[0;0m\n",
            "\u001b[0;95m     model: si tu as des 700 , tu peux toujours te faire des messages avec des personnes qui font des réactions .\u001b[0;0m\n",
            "\u001b[0mMoi j'aimerais juste que sally-draper reposte un truc sur sa maman morte. On dépasserait la barre des 1000 avec ses conneries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: T'as quand même pas eu le courage de la taguer, alors frime pas trop.\u001b[0;0m\n",
            "\u001b[0;95m     model: t ' as pas eu le courage d ' être littérature sur ton cdi ?\u001b[0;0m\n",
            "\u001b[0mMec, même quand tu t'amuses à écraser les cafards, tu vas pas jusqu'à souhaiter que ta maison soit infestée non?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je connais pas assez bien le drama de son départ et ce qui l'a précédé pour comprendre ce que t'entends par là, et de ce que j'ai vu parfois tu sembles être d'accord avec elle, parfois non. Je ne sais que penser.\u001b[0;0m\n",
            "\u001b[0;95m     model: j ' ai pas compris pourquoi c ' était le cas là .\u001b[0;0m\n",
            "\u001b[0mJe suis d'accord avec beaucoup de ses points et de ce qu'elle dit sur le fond.Maintenant si elle pouvait ne pas être une immonde connasse, ça serait un bonus.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je suis pas d'accord avec immonde connasse. Crétine égocentrique convient beaucoup mieux selon moi.\u001b[0;0m\n",
            "\u001b[0;95m     model: j ' aime bien , je trouve ça assez génial .\u001b[0;0m\n",
            "\u001b[0mImmonde crétine égocentrique? On coupe la poire en deux?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Ok c'est moi qui ai demandé, mais j'apprécie pas les insultes personnelles même quand elles sont pas dirigées vers moi :(\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est pas le genre de truc qui va faire du coup .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mTiens, je ne connaissais pas ces types:  dela de l'aspect politique de cette décision, à mon sens, ils auraient du obtenir la nationalité depuis un bon bout de temps\u001b[0;0m\n",
            "\u001b[1;94m    labels:   ils auraient du obtenir la nationalité depuis un bon bout de tempsPour l'avoir il suffit de remplir [les critères](-public.fr/particuliers/F2213.xhtml) et de la demander. S'ils ne l'ont pas c'est soit parce qu'ils ne remplissent pas ces critères soit parce qu'ils n'en veulent pas.\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est un peu comme les redditeurs .\u001b[0;0m\n",
            "\u001b[0mca c'est la théorie, dans la pratique, on ne connait pas leur histoire ni les démarches entreprisent\u001b[0;0m\n",
            "\u001b[1;94m    labels: Et donc on fait une loi sans connaître cela...\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis d ' accord , mais je pense que ce n ' est pas le cas .\u001b[0;0m\n",
            "\u001b[0mLe cas est sûrement connu par l administration française au vu des nombreuses commissions et associations qui ont travaille sur ce cas. Encore une fois, tu as des détails ou tu ne fais que spéculer par pur biais idéologique ?Est ce que sur le principe ces personnes méritent la nationalité a ton sens ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Encore une fois, tu as des détails ou tu ne fais que spéculer par pur biais idéologique ?Trouvé ! On a déjà fait une étude sur leur cas.www.assemblee-nationale.fr/14/rap-info/i1214.aspAu passage on retrouve tous les stéréotypes sur l'immigration qui sont complètement faux (les immigrés viennent faire le travail que les feignasses de Français ne veulent pas faire... l'immigration a été tellement bénéfique pour notre économie... ils sont victimes de tout ce qu'on peut reprocher au méchant état français... tout ça dit sans contradiction à l'Assemblée Nationale.)Est ce que sur le principe ces personnes méritent la nationalité a ton sens ?Les critères actuels sont corrects. J'ajouterais l'abandon obligatoire de la nationalité d'origine sauf accord bilatéral avec le pays, et la maîtrise d'un niveau de français B2 au lieu de B1 actuellement.\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' était pas une question de détail . je ne suis pas d ' accord avec ton post .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mEt pendant ce temps, ΣΥΡΙΖΑ (gauche radicale grecque) remporte entre 26 et 30% des voix, devant Νέα Δημοκρατία (centre-droit, parti du Premier Ministre Samaras) qui obtiendrait entre 23 et 27% des voix et surtout le ΠΑΣΟΚ et ses 7-9% attendus. L'Aube Dorée ne fait \"que\" 8-10%.Comme quoi, tout espoir n'est peut-être pas perdu.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Depuis quand le communisme, c'est mieux que le nationalisme?\u001b[0;0m\n",
            "\u001b[0;95m     model: depuis quand les communes , ça fait quoi ?\u001b[0;0m\n",
            "\u001b[0mDepuis que ce sont les communistes qui ont fait renaître et libéré la France. Alors que les nationalistes de Doriot, Pétain, Laval, et cie se réjouissaient de l'occupation hitlérienne.\u001b[0;0m\n",
            "\u001b[1;94m    labels:  Depuis que ce sont les communistes qui ont fait renaître et libéré la France.Voui, disons qu'il y a eu le léger problème du [pacte germano soviétique](-sovi%C3%A9tique) , avec les [sabotages](#Seconde_Guerre_mondiale_.281939-1945.29) qui ont précédé la guerre, et la confusion avec les [trotskistes](#D.C3.A9faitisme_r.C3.A9volutionnaire_et_d.C3.A9sorientation_d.27une_partie_des_militants_trotskistes) qui n'arrange pas les choses.Ensuite, [Laval]() n'était pas du tout nationaliste et ne l'a jamais été. Il était pacifiste et a été constant là-dessus.Pour [Doriot](), il devient fasciste nationaliste à partir de 1936, deux ans après son départ du parti communiste, mais encore une fois c'était avant tout un pacifiste.Après, ce sont tous des traîtres, ce qui est dans mon idée assez incompatible avec un quelconque nationalisme.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne suis pas d ' accord avec ton position .\u001b[0;0m\n",
            "\u001b[0m Après, ce sont tous des traîtres, ce qui est dans mon idée assez incompatible avec un quelconque nationalisme.Enfin quelqu'un de cohérent sur ce point ! Combien j'ai pu croiser de nationalistes exaltés qui les voient comme des héros sans comprendre l'absurdité de la posture...\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'ai jamais compris d'ailleurs comment des soi-disant nationalistes ont pu collaborer.Tous ceux de ma famille qui étaient en âge d'avoir une action politique ont passé la guerre en prison, où sont revenus après 1943 (soit après la fin de la zone libre), y compris mon arrière-grand-père qui était agé de plus de 60 ans et a été déporté \"par précaution\"...Ma conclusion : ceux qui ont eu le temps de collaborer étaient des déserteurs ou des pacifistes, donc bien plus responsables de la défaite que n'importe qui d'autre. De Gaulle a été bien gentil avec eux pendant la réconciliation...\u001b[0;0m\n",
            "\u001b[0;95m     model: pour tous les groupes de guerres qui ont l ' air élus , y ' a rien de découverte .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mMerci à tous ceux qui se sont inquiétés pour moi hier 3 j'ai pas passé une journée de fou mais j'ai bien dormi et ça va mieux.Du coup vous pouvez continuer à me PM, j'me sens important et tout.Sinon j'ai décidé de m'acheter un appareil de sport quelconque pour mon appart, je galère trop à sortir courir le matin parce que j'aime pas sortir en fait. Y'a des trucs pliables, des trucs ultra-compacts, mais j'ai déjà décidé d'éviter le rameur ou le tapis de course, c'est beaucoup trop gros. Du coup, qu'est-ce qui est meilleur pour le cardio: le vélo ou la machine elliptique?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Ou sinon, tu fais des pompes, des abdos et des squats  tous les jours. Pas besoin de matos. \u001b[0;0m\n",
            "\u001b[0;95m     model: il faut pas que tu fasse me permettre un article de cinq journées de cardio .\u001b[0;0m\n"
          ]
        }
      ],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    # task='french_blended_skill_talk',\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "    datatype= \"test\",\n",
        "\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    # the result of grid search on 400M model and BST dataset when inference=topk\n",
        "    beam_block_ngram= 2,\n",
        "\tbeam_context_block_ngram= 3,\n",
        "\tbeam_length_penalty= 1,\n",
        "\tbeam_min_length= 10,\n",
        "\tbeam_size= 20,\n",
        "\tinference= \"topk\",\n",
        "\ttemperature= 0.5,\n",
        "\ttopk= 20,\n",
        "\ttopp= 0.9\n",
        "\n",
        "    # # Farnaz sent me\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_min_length= 20, \n",
        "    # beam_size= 10,\n",
        "    # inference =  'topk',  \n",
        "    # topk=20, \n",
        "    # temperature = 0.5, \n",
        "    # beam_length_penalty=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap-cP0uzFF4y",
        "outputId": "2f52fc81-fe8d-49cd-cb2c-a056ea9e5e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:07:59 | Opt:\n",
            "09:07:59 |     allow_missing_init_opts: False\n",
            "09:07:59 |     batchsize: 1\n",
            "09:07:59 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "09:07:59 |     datatype: train:ordered\n",
            "09:07:59 |     dict_class: None\n",
            "09:07:59 |     display_add_fields: \n",
            "09:07:59 |     download_path: None\n",
            "09:07:59 |     dynamic_batching: None\n",
            "09:07:59 |     fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data\n",
            "09:07:59 |     fromfile_datatype_extension: True\n",
            "09:07:59 |     hide_labels: False\n",
            "09:07:59 |     ignore_agent_reply: True\n",
            "09:07:59 |     image_cropsize: 224\n",
            "09:07:59 |     image_mode: raw\n",
            "09:07:59 |     image_size: 256\n",
            "09:07:59 |     init_model: None\n",
            "09:07:59 |     init_opt: None\n",
            "09:07:59 |     is_debug: False\n",
            "09:07:59 |     loglevel: info\n",
            "09:07:59 |     max_display_len: 1000\n",
            "09:07:59 |     model: None\n",
            "09:07:59 |     model_file: None\n",
            "09:07:59 |     multitask_weights: [1]\n",
            "09:07:59 |     mutators: None\n",
            "09:07:59 |     num_examples: 20\n",
            "09:07:59 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data', 'fromfile_datatype_extension': True, 'num_examples': 20}\"\n",
            "09:07:59 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "09:07:59 |     starttime: Jun07_09-07\n",
            "09:07:59 |     task: fromfile:parlaiformat\n",
            "09:07:59 |     verbose: False\n",
            "09:07:59 | creating task(s): fromfile:parlaiformat\n",
            "09:07:59 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data_train.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mPour ceux qui jouent à LoL et qui attendaient impatiemment le URF, ça sera au final le NURF qui sera là. L'inverse du Urf : cooldown augmentés, conso mana/énergie augmentée, dégâts des minions augmentés... Voilà, enjoy, moi je vais pleurer dans un coin parce que le urf était le mode qui me poussait à garder LoL installé. ^^^^quelqu'un ^^^^n'aime ^^^^pas ^^^^LoL ^^^^? ^^^^:)\u001b[0;0m\n",
            "   \u001b[1;94mÇa, ou alors demain c'est le 1^er avril.\u001b[0;0m\n",
            "\u001b[0mLe vrai mode URF était déjà une blague du 1er avril. Sauf que ça a bien marché et ils ont laissé le mode 2 semaines.On verra bien demain de toute façon :(\u001b[0;0m\n",
            "   \u001b[1;94mEt tu penses vraiment qu'ils vont remplacer un mode de jeu que tout le monde attend depuis 1 an par un truc pas fun du tout ?C'est clairement une blague.\u001b[0;0m\n",
            "\u001b[0mRiot est tellement une vaste blague comme entreprise qu'ils en seraient capable D:\u001b[0;0m\n",
            "   \u001b[1;94mUne vaste blague comme entreprise ? Les mecs ont réussi, en pompant le concept d'un jeu existant à créer le jeu en ligne le plus joué au monde, à se faire un fric fou et ont énormément contribué à la progression de l'esport. C'est plutôt des génies oui.Si tu veux te convaincres que c'est une blague regarde ça : Units critically strike on 150% of attackset explique moi ce que ça veut dire.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mIl y a une phrase prétendument de Pasqua qui tourne sur twitter en ce moment : *Quand on est emmerdé par une affaire, il faut susciter une affaire dans l’affaire, et si nécessaire une autre affaire dans l’affaire de l’affaire, jusqu’à ce que personne n’y comprenne plus rien.* Même si elle n'est pas de lui, elle s'applique bien à la situation actuelle. C'est bien joué de la part de la droite (bien aidé par la nullité du gouvernement) d'avoir su allumer des contres feux pour sauver le cul de Sarkozy. Assez marrant aussi de voir le léchage de boules en faveur de Sarko dans les articles de la page d'accueil de Valeurs Actuelles. C'est surement un hasard que ce soit ce journal qui sorte cette affaire.\u001b[0;0m\n",
            "   \u001b[1;94mIl y a une phrase prétendument de Pasqua qui tourne sur twitter en ce moment : Quand on est emmerdé par une affaire, il faut susciter une affaire dans l’affaire, et si nécessaire une autre affaire dans l’affaire de l’affaire, jusqu’à ce que personne n’y comprenne plus rien.Par exemple quand le gouvernement est emmerdé par ses résultats déplorables, il lance des diversions ? Après le mariage pour tous, Dieudonné, Méric, pas de bol cette fois-ci c'est un adversaire qui connait la chanson et apporte une réponse appropriée.Ma position n'a pas changé : le gouvernement ferait mieux de [s’occuper des Français](-Minute-Taubira-une-strategie-de-diversion-du-gouvernement-FN-1710677/).\u001b[0;0m\n",
            "\u001b[0mLe mariage pour tous ? Lol ? Tout le bordel à ce sujet à été crée par l'opposition. \u001b[0;0m\n",
            "   \u001b[1;94m... C'est l'opposition qui propose les lois maintenant ?\u001b[0;0m\n",
            "\u001b[0mParce qu'on est en période de crise économique, le gouvernement démocratiquement élu n'aurait pas le droit de légiférer sur des questions non-économiques ? Arrête.\u001b[0;0m\n",
            "   \u001b[1;94m Parce qu'on est en période de crise économique, le gouvernement démocratiquement élu n'aurait pas le droit de légiférer sur des questions non-économiques ?Il a tout à fait le droit de faire diversion. D'ailleurs il le fait très bien.\u001b[0;0m\n",
            "\u001b[0mTa mauvaise foi est sans limite.Il ne fait aucune diversion dans la mesure où 1. Il est légitime à légiférer sur des sujets non-économiques2. Le mariage gay était inscrit au programme de Hollande avant son élection. Il s'agissait donc d'une promesse de campagne, et certaines personnes, j'en suis sûr, on voté Hollande en raison du mariage gay.Enfin, si l'opposition avait le même sentiment que toi, elle n'aurait pas  monopolisé l'attention via la manif pour tous et aurait recentrée le débat sur les questions économiques.\u001b[0;0m\n",
            "   \u001b[1;94m Il ne fait aucune diversion dans la mesure où 1. Il est légitime à légiférer sur des sujets non-économiquesEn quoi une diversion ne serait pas légitime ?J'ai déjà dit qu'il avait tout à fait le droit de le faire.Le mariage gay était inscrit au programme de Hollande avant son élection.Oui, et en quoi ça n'en fait pas une diversion ?si l'opposition avait le même sentiment que toi, elle n'aurait pas monopolisé l'attention via la manif pour tousC'est ce qu'a fait le FN. La véritable opposition est le FN, pas l'UMP.\u001b[0;0m\n",
            "\u001b[0m C'est ce qu'a fait le FN.Ma mémoire n'est peut-être pas très bonne, mais je crois me souvenir de nombreux cadres du FN manifestant avec la Manif Pour Tous, signant sa charte et participant à cette grande \"diversion\".\u001b[0;0m\n",
            "   \u001b[1;94m Ma mémoire n'est peut-être pas très bonne, mais je crois me souvenir de nombreux cadres du FN manifestant avec la Manif Pour Tous, signant sa charte et participant à cette grande \"diversion\".C'est vrai, mais en quoi cela revient à \"monopoliser l'attention via la manif pour tous\" ?La ligne du FN [est très claire là-dessus]().\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mJe viens de terminer une épreuve dite \"marathon\", on nous a filé un texte à traduire à 8h45 ce matin et on doit le rendre à 16h45 au plus tard.Eh bien figurez-vous que le cancer de la prostate, c'est pas rigolo !\u001b[0;0m\n",
            "   \u001b[1;94mTu fais quoi, des études de trad ?\u001b[0;0m\n",
            "\u001b[0mOui, je suis en traduction audiovisuelle.\u001b[0;0m\n",
            "   \u001b[1;94mAh tu dois être à Strasbourg ? Je me souviens de ça, tout le monde flippait de devoir passer la journée sur une traduction, j'étais parmi les \"un peu plus vieux\" qui avaient déjà bossé avant et ça me faisait rouler des yeux.\u001b[0;0m\n",
            "\u001b[0mOui !Je t'avais posé des questions sur ton AMA il y a quelques mois. :)Je dois avouer que je faisais pas le malin ce matin. Mais bon, c'est passé assez facilement en fait.\u001b[0;0m\n",
            "   \u001b[1;94mAh oui c'est vrai... on avait eu un txt sur la sclérose en plaque... ils sont joyeux ces gens. C'est toujours la prof blonde gentille en trad médicale, avec la super ambiance de merde où tout le monde se fait chier ?\u001b[0;0m\n",
            "\u001b[0mLa prof est blonde et gentille mais l'ambiance était vraiment bonne, si on considère les sujets des textes...\u001b[0;0m\n",
            "   \u001b[1;94mAh ben nous c'était mort, mais j'aimais à peu près personne dans ma promo. J'espère que t'auras bien réussi ton truc en tout cas, tu me diras. T'as toujours pas rencontré M. Volclair ?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mAutant c'est ridicule, autant je suis sûr qu'à 13 ans j'ai dû dire des conneries aussi grosses que celle-ci.\u001b[0;0m\n",
            "   \u001b[1;94mIl y a une légère différence entre dire quelque chose à 13 ans et le prendre en compte pour l'imprimer à 30 ans dans un \"journal\" pour que tout le monde le voit. \u001b[0;0m\n",
            "\u001b[0mBof. Ça peut être intéressant/marrant de voir les conneries que peuvent raconter les ados, de temps en temps. J'veux dire c'est pas comme si le magazine imprimait ça pour dire \"voici nos conseils sentimentaux, offerts par notre experte en la matière\". Tout lecteur équipé de plus de trois neurones lira ça sur le mode \"ah ah, ils sont marrants ces petits, ils ont encore tant de choses à apprendre sur la vie\".\u001b[0;0m\n",
            "   \u001b[1;94mSauf que la plupart du bétail francais prend pour vérité absolu tout ce qu'il voit à la télé et dans un journal, si on as plus de 3 neurones ca peut nous faire marrer mais si tu as plus de 10 neurones tu te rendra compte du mal que ca peut faire.\u001b[0;0m\n",
            "\u001b[0m Sauf que la plupart du bétail francais prend pour vérité absolu tout ce qu'il voit à la télé et dans un journalBen voyons. Le \"bétail français\" va lire un conseil \"trompe avant d'être trompé\" **signé par une gamine de 13 ans** et se dire \"ouais elle a raison\" ? Tu dois pas sortir souvent, pour penser que les gens sont vraiment aussi cons.\u001b[0;0m\n",
            "   \u001b[1;94mOk toi tu vis dans ta bulle sociale tu es fiché direct, c'est pas parce que tu vis dans un milieu ou les gens sont un minimum conscient que c'est le cas partout. Sors un peu, fréquente différentes couches sociales, et surtout va te balader dans les coins de france ou on trouve le bon redneck francais de base qui parle de tout ce quil lit au journal ou ce qu'il voit au info comme une vérité général, tu va perdre foi en l'humanité direct et revenir à la réalité.Malheuresement oui les gens sont aussi cons, le manque de culture ca fais des ravages terribles, les gens ne remettent pas grand chose en question si on les y aide pas.Cet article en particulier ne prouve rien et est\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mOn sait bien ce que ça va donner ce mal parlé.  Les gros bouzeux incultes vont s'apercevoir qu'ils ont tout en commun : l'alcoolisme, l'inculture, la jalousie, et que vont ils faire ? Ils vont faire une bonne grosse alliance de paysans, façon coopérative de l'inculture de France et essayer de trashtalk leur belle, superbe et délicate capitale :  #PARIS !\u001b[0;0m\n",
            "   \u001b[1;94mC'est cool de payer un loyer à 4 chiffres pour un studio ?  C'est comment le métro/Rer tous les matins ?  C'est pas trop cher 4 euros pour un demi ?  C'est chiant de pas pouvoir ce baigner quand il fait 38 ° ?J'entends déjà les \"Paris et ses expositions/concerts/musés/\" un vrai parisien n'y va jamais.\u001b[0;0m\n",
            "\u001b[0mC'est comment le métro/Rer tous les matins ?Tu veux dire avoir des transports en commun globalement fiables et bien foutus, abordables, et très étendus ? Plutôt pas mal, merci.\u001b[0;0m\n",
            "   \u001b[1;94mJe veux surtout avoir votre avis sur le fait de passer 2 sur les 18 heures de tes journées dans un métro .\u001b[0;0m\n",
            "\u001b[0mMieux vaut ça que 8 heures par jour sur un tracteur ?\u001b[0;0m\n",
            "   \u001b[1;94mMoi j'aime bien, surtout quand je suis au milieu de la route et que j'ai un connard de parisien en vacances derrière moi qui klaxonne parce qu'il est beaucoup trop pressé. C'est mon petit plaisir.\u001b[0;0m\n",
            "\u001b[0mAttends tu déconnes, les parisiens en vacances ils roulent encore moins vite que les tracteurs.\u001b[0;0m\n",
            "   \u001b[1;94mNormal, le reste de la France prend plaisir à aggro la moindre plaque 75 qu'ils voient, pour se branler sur \"les parisiens conduisent trop mal\" le reste de l'année, entre deux accidents mortels sur départementales\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mC'est quoi la proportion homme femme sur le sub r/france ?\u001b[0;0m\n",
            "   \u001b[1;94mC'est quoi le rapport ?\u001b[0;0m\n",
            "09:08:04 | loaded 96865 episodes with a total of 335085 examples\n"
          ]
        }
      ],
      "source": [
        "# from parlai.scripts.display_data import DisplayData\n",
        "# DisplayData.main(task='empathetic_dialogues', num_examples=10)\n",
        "\n",
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "    # model_file= f'{finetuned_model_path}/model',\n",
        "    # dict_file= dict_file,\n",
        "\n",
        "    num_examples=20,\n",
        "    # skip_generation=False,\n",
        "\n",
        "    # beam_min_length= 20,\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_size= 10,\n",
        "\n",
        "    # inference= \"beam\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "blender-finetuning-with-reddit.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPzoHMfjXYW1l9kkulhcjvX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}