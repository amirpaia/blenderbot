{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirpaia/blenderbot/blob/main/blender_finetuning_with_reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SONwSWMp6qPv"
      },
      "source": [
        "# 0.Installing prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d-SZ_On6Kxg",
        "outputId": "980ff9d1-9dc8-4811-cced-1aff0ec9394e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun 20 22:30:54 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BegaSUz6iUz",
        "outputId": "58a4ccf4-3338-437e-8f42-f07318888b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SzGRXHQ6kDQ",
        "outputId": "56ca889f-6642-4a7b-a2bd-d6f9cf021458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "mydrive_path = '/content/drive/MyDrive/colabs/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWDakYmy6mIQ",
        "outputId": "d5d0046d-ceab-47d0-ba01-eea18aca9e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed transformers-4.20.0\n"
          ]
        }
      ],
      "source": [
        "# !pip uninstall -q parlai\n",
        "!pip install -q parlai\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIJEq9_r63hi"
      },
      "source": [
        "# 1.Preparing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R-fgBUdcPX5"
      },
      "source": [
        "## Genreal Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47u8M3RK65m_",
        "outputId": "fbb4c07d-1a54-4465-8f72-b6e24dc15743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n",
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def transfer_list_of_turns_to_dialog(d):\n",
        "    if len(d)%2 !=0: d = d[:-1]\n",
        "    t = \"\"\n",
        "    for i in range(0,len(d),2):\n",
        "        u1 = d[i]\n",
        "        u2 = d[i+1]\n",
        "\n",
        "        if (i+2) != len(d):\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "        else:\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "    return t\n",
        "\n",
        "def transfer_list_of_pairs_to_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, text_label_pair in enumerate(d):\n",
        "    u1 = text_label_pair[0]\n",
        "    u2 = text_label_pair[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "\n",
        "def convert_parlai_format_to_list_of_turns(lines):\n",
        "    result = []\n",
        "    for line in lines:\n",
        "        text_label = line.split(\"\\t\")\n",
        "        result.append(text_label[0].replace(\"text:\", \"\"))\n",
        "        result.append(text_label[1].replace(\"labels:\", \"\").replace(\"\\n\",\"\"))\n",
        "    return result\n",
        "\n",
        "t = ['hello','how are you','good','bye','test']\n",
        "print(transfer_list_of_turns_to_dialog(t))\n",
        "\n",
        "t = [['hello','how are you'],['good','bye']]\n",
        "print(transfer_list_of_pairs_to_dialog(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE45ZyKC8WjC"
      },
      "source": [
        "## French Reddit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8ZmSJtQG8Y1o",
        "outputId": "d35a6a51-d4fc-4454-920a-62ef8c3bf75b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data_path = f\"{mydrive_path}aliae-workspace/datasets/french_reddit_LELU/\"\n",
        "data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UexnzWURWKSI",
        "outputId": "580b86b9-6829-4591-b849-56aaba15cba1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text:Désinformation ! Amnesty, complice de la fachosphère !\\tlabels:Ah non tu te trompes, Amnesty est financé par Soros, complice du nouvel ordre mondial et des lobbies crypto-massonique et reptilo-sionnistes!\\tepisode_done:True\\n',\n",
              " \"text:Le tirage au sort a mis Fillon en dernier sur les panneaux, mais curieusement ils ont placé les panneaux de droite à gauche...\\tlabels:A coté de chez moi, des petits malins ont tagué des barreaux de prison sur fillon. �� Edit: Et barré tous les autres d'une croix sauf MLP qui a eu un gros coeur autour de la tête.. Certainement du brigading pro-peluche. �� \\tepisode_done:True\\n\",\n",
              " 'text:Le croissant fertile.\\tlabels:Il y a officiellement un nom pour cette région : la banane bleue! \\tepisode_done:True\\n',\n",
              " \"text:C'est la ruine ! Vous m'entendez ? LA RUINE !\\tlabels:c'est ça enfin je dépense près de 50€ par semaines en manga :q\\tepisode_done:True\\n\",\n",
              " 'text:Du coup je sais plus, c\\'est qui les racistes, nous les \"sales blancs\", ou eux ?\\tlabels:Les racistes sont les blancs qui ont des yeux marrons et ne se sentent pas français.\\tepisode_done:True\\n',\n",
              " \"text: Arrête de télécharger illégalement des choses françaisesPas que. J'ai été gaulé pour Amy Winehouse.\\tlabels:Le film ou la musique ?La musique semble être le truc le plus surveillé par Hadopi.\\tepisode_done:True\\n\",\n",
              " \"text:Y a quand même une bonne partie de la communauté scientifique (et je dis pas ça en l'air) qui est bien plus pessimiste que ça sur le réchauffement climatique. Y a un paquet de projections maintenant qui tablent sur 4 à 6°C d'augmentation, et c'est le concenssus général que la fenètre d'action pour les 2°C est passée.Alors certes, il faut commencer quelque part, mais quand la coque du bateau à un trou béant, si tu te ramènes avec un verre d'eau pour écopper, tu pourrais aussi bien faire autre chose. Je crois que même la nouvelle génération, pourtant sensibilisée à ça, ne se rend pas compte de la gravité de la situation.Après, bon, on va pas bouder un truc qui va dans le bon sens, c'est sur. \\tlabels: Je crois que même la nouvelle génération, pourtant sensibilisée à ça, ne se rend pas compte de la gravité de la situation.De quelle nouvelle génération tu parles ?\\n\",\n",
              " \"text:Pour être très large, disons les moins de 40 ans ?\\tlabels:C'est plus large que l'Océan Pacifique là !  Franchement je suis pas si sûr que ça qu'ils soient vraiment sensibilisés. Par exemple, perso j'ai 21 ans, je n'ai **jamais** entendu parler d'écologie (que ce soit par les profs ou les camarades) pendant tout mon parcours scolaire.\\tepisode_done:True\\n\",\n",
              " \"text:Hé mon père s'appelle Christian ,je suis assez blessé par ton affirmation !\\tlabels:Moi c'est Chritiano\\tepisode_done:True\\n\",\n",
              " \"text: Et avec 25%, le pays est en crise et vote extrême-gauche apparemment.C'est quand le pays en question n'a pas subi une immigration massive. L'extrême-droite en France monte pour des raisons identitaires.\\tlabels:Faux et cela se voit très bien si tu superposes cette carte avec celle du FN. Elles sont presque identique. \\tepisode_done:True\\n\"]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "with open(f\"{data_path}/data_train.txt\") as f:\n",
        "    lines = f.readlines()\n",
        "lines[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGoTqdFD7x_3"
      },
      "source": [
        "# 2.Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "65rK6pfj70Px",
        "outputId": "6bfc0183-e730-4a4e-e6cd-543866c64232"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-400m/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# finetuned_model_path = f'{mydrive_path}blender-models/finetuned-reddit-400m/'\n",
        "finetuned_model_path = f'{mydrive_path}blender-models/finetuned-reddit_LELU-400m/'\n",
        "init_model = 'zoo:blender/blender_400Mdistill/model'\n",
        "dict_file  = 'zoo:blender/blender_400Mdistill/model.dict'\n",
        "# init_model = 'zoo:blender/blender_90M/model'\n",
        "# dict_file  = 'zoo:blender/blender_90M/model.dict'\n",
        "finetuned_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7vLiYpF8HbF"
      },
      "outputs": [],
      "source": [
        "# 90M settings\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    # task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "    # multitask_weights= \"1,3,3\",\n",
        "\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    \n",
        "    # depend on your gpu. \n",
        "    validation_every_n_epochs=0.1,\n",
        "    num_epochs = 0.1,\n",
        "    log_every_n_secs= 60,\n",
        "    verbose = True,\n",
        "    batchsize= 8, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        "\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    attention_dropout= 0.0, \n",
        "    model_parallel= False,\n",
        "    warmup_updates=100,\n",
        "\n",
        "    # customized parameters\n",
        "    # inference= \"beam\"\n",
        "    # beam_min_length= 20,\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_size= 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxL0-1x1YsQe"
      },
      "outputs": [],
      "source": [
        "# mydrive_path = '/content/finetuned-multitask-400m-double-sided-2epochs'\n",
        "# mydrive_path = '/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-double-sided/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNZE5ta2pO-X",
        "outputId": "fec16e0e-58a9-4a65-fe51-247aac3019fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                                 /@&%###%&&@@#\n",
            "                      .,*/((((##@@@&%%#%%&&@@@&%%#/*.\n",
            "             #@@&&&%%%%##(((///*****//(((###%%%&&&@@@@@&&%#%%#.\n",
            "         .%&@@@@@&&&%%%####((((////((((####%%%&&&@@@@@&&%%#%%####,\n",
            "           ./,,#(//**,,.....,,,,***////((((########%%%%%%%%###(((\n",
            "              /*(//**,,,....,,,,***////((((########%%%%%%%%###(#%*\n",
            "               (*,...      ...,,,***//////((((((///////(/*...,/#@@@(\n",
            "               **,,..         ...,,,,,,,,,,........,,*///*...*(#@@@@&&*\n",
            "               ./,,..          ...,,,,,,,,,........,,*//*,...*#/,,,,,/%#\n",
            "                (*,..          ...,,,,,,,,,........,,*//*,..,/(      .,#(\n",
            "                **,..          ...,,,,,,,,,.........,*//*,..,((       .,(#\n",
            "                 /*,..          ....,,,,,,,.....  ..,***,,,,(#         ..#&\n",
            "                 **,..          ....,,,,,,,....   ..,***,,,*#.         .,%@\n",
            "                 ./,...       B l e n d e r B o t ...***,,,*#          .*%@\n",
            "                  /*,..          ...,,,,,,,....    .,**,,,,/#         ..(%/\n",
            "                  /*,,..         ...,,,,,,,...    ..,*,,,,,(.         ..#&\n",
            "                  ,/*,..         ...,,,,,,,...    ..,*,,,,*#         ..*%(\n",
            "                   /*,..         ...,,,,,.....    ..,*,*,,/(         ..#&\n",
            "                   /**,..        ...,,,,.....    ...,***,*(.       ,,(%.\n",
            "                    (/*,,..      ....,,.....     ...,****(&@@@&&&#,\n",
            "                     (/*,,...   .....,,......     ..,****#@,\n",
            "                     *(/*,,/....*(###%(,(%%##(*.  ./,,**(\n",
            "                      ,//**(,........,/((#.........*,**(\n",
            "                      .(#//*,,,,,,.*.,/((%/,,.....,,*/@\n",
            "                    ((######//****,/.,/(#%#***,***(&@@@@@(\n",
            "                   *&%%#####%%%%%%%#//(#%&%%&&@@@@@@@@@@@@*\n",
            "                   &&%%%###((((((####%%%%&&&&&@@@@@@@@@@&&@.\n",
            "                  *##%%%##(((((((####%%%%%&&&&@@@@@@@@@&#/*,\n",
            "                 .(##%#/,  .,*((##%%%&&&&%%%#####%&&@&&%#(/*.\n",
            "                 /(###(,   .,*/(##%%%&&&&%%%######%&&&&%#(/*,\n",
            "                */((((*.  ..,//((##%%%%%%%%#######%&&&&%%#(/*,\n",
            "               .//(((/,   .,*//((###%%%%%%########%%&&&%%#((/,.\n",
            "              .&####(((((((((######%%%%%%%%&&&&&&&@@@@@@@@@@@@@#\n",
            "               *&#.   .*/((((#######%%%%%%&&&&&&&@@@@@#/.   (&/\n",
            "22:35:12 | building data: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/BST400Mdistill_v1.1.tgz\n",
            "22:35:12 | Downloading http://parl.ai/downloads/_models/blender/BST400Mdistill_v1.1.tgz to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/BST400Mdistill_v1.1.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading BST400Mdistill_v1.1.tgz:   2%|▏         | 58.5M/3.71G [00:05<02:47, 21.8MB/s]"
          ]
        }
      ],
      "source": [
        "# 400M settings\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    # task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "    # multitask_weights= \"1,3,3\",\n",
        "\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    validation_every_n_epochs=0.1, # veps= 0.25, \n",
        "    num_epochs = 0.1,\n",
        "    log_every_n_secs= 300,\n",
        "    verbose = True,\n",
        "    attention_dropout= 0.0, \n",
        "    batchsize= 16, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    # save_after_valid= True,\n",
        "\n",
        "    # arguments we get from the pretrained model. \"from recipes page for 2.7B model\" \n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    variant= \"prelayernorm\",\n",
        "    n_heads= 32, n_positions= 128, \n",
        "    n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "    dropout= 0.1,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    relu_dropout= 0.0, \n",
        "    model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    # inference = 'topk', \n",
        "    # temperature = 0.7, \n",
        "    # topk=30, \n",
        "    # beam_length_penalty=1.03\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1S0kRk3W63i"
      },
      "outputs": [],
      "source": [
        "# !cp -rv /content/finetuned-multitask-400m-double-sided-2epochs/* /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-double-sided/\n",
        "# !ls -lah /content/finetuned-multitask-400m-double-sided-2epochs/\n",
        "# !ls -lah /content/finetuned-multitask-400m-double-sided/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVPS6p4XzPh4"
      },
      "source": [
        "# 4.Display Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9FBtnZZzPPg"
      },
      "outputs": [],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    # task='french_blended_skill_talk',\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "    datatype= \"test\",\n",
        "\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    # the result of grid search on 400M model and BST dataset when inference=topk\n",
        "    beam_block_ngram= 2,\n",
        "\tbeam_context_block_ngram= 3,\n",
        "\tbeam_length_penalty= 1,\n",
        "\tbeam_min_length= 10,\n",
        "\tbeam_size= 20,\n",
        "\tinference= \"topk\",\n",
        "\ttemperature= 0.5,\n",
        "\ttopk= 20,\n",
        "\ttopp= 0.9\n",
        "\n",
        "    # # Farnaz sent me\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_min_length= 20, \n",
        "    # beam_size= 10,\n",
        "    # inference =  'topk',  \n",
        "    # topk=20, \n",
        "    # temperature = 0.5, \n",
        "    # beam_length_penalty=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap-cP0uzFF4y",
        "outputId": "2f52fc81-fe8d-49cd-cb2c-a056ea9e5e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:07:59 | Opt:\n",
            "09:07:59 |     allow_missing_init_opts: False\n",
            "09:07:59 |     batchsize: 1\n",
            "09:07:59 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "09:07:59 |     datatype: train:ordered\n",
            "09:07:59 |     dict_class: None\n",
            "09:07:59 |     display_add_fields: \n",
            "09:07:59 |     download_path: None\n",
            "09:07:59 |     dynamic_batching: None\n",
            "09:07:59 |     fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data\n",
            "09:07:59 |     fromfile_datatype_extension: True\n",
            "09:07:59 |     hide_labels: False\n",
            "09:07:59 |     ignore_agent_reply: True\n",
            "09:07:59 |     image_cropsize: 224\n",
            "09:07:59 |     image_mode: raw\n",
            "09:07:59 |     image_size: 256\n",
            "09:07:59 |     init_model: None\n",
            "09:07:59 |     init_opt: None\n",
            "09:07:59 |     is_debug: False\n",
            "09:07:59 |     loglevel: info\n",
            "09:07:59 |     max_display_len: 1000\n",
            "09:07:59 |     model: None\n",
            "09:07:59 |     model_file: None\n",
            "09:07:59 |     multitask_weights: [1]\n",
            "09:07:59 |     mutators: None\n",
            "09:07:59 |     num_examples: 20\n",
            "09:07:59 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data', 'fromfile_datatype_extension': True, 'num_examples': 20}\"\n",
            "09:07:59 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "09:07:59 |     starttime: Jun07_09-07\n",
            "09:07:59 |     task: fromfile:parlaiformat\n",
            "09:07:59 |     verbose: False\n",
            "09:07:59 | creating task(s): fromfile:parlaiformat\n",
            "09:07:59 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data_train.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mPour ceux qui jouent à LoL et qui attendaient impatiemment le URF, ça sera au final le NURF qui sera là. L'inverse du Urf : cooldown augmentés, conso mana/énergie augmentée, dégâts des minions augmentés... Voilà, enjoy, moi je vais pleurer dans un coin parce que le urf était le mode qui me poussait à garder LoL installé. ^^^^quelqu'un ^^^^n'aime ^^^^pas ^^^^LoL ^^^^? ^^^^:)\u001b[0;0m\n",
            "   \u001b[1;94mÇa, ou alors demain c'est le 1^er avril.\u001b[0;0m\n",
            "\u001b[0mLe vrai mode URF était déjà une blague du 1er avril. Sauf que ça a bien marché et ils ont laissé le mode 2 semaines.On verra bien demain de toute façon :(\u001b[0;0m\n",
            "   \u001b[1;94mEt tu penses vraiment qu'ils vont remplacer un mode de jeu que tout le monde attend depuis 1 an par un truc pas fun du tout ?C'est clairement une blague.\u001b[0;0m\n",
            "\u001b[0mRiot est tellement une vaste blague comme entreprise qu'ils en seraient capable D:\u001b[0;0m\n",
            "   \u001b[1;94mUne vaste blague comme entreprise ? Les mecs ont réussi, en pompant le concept d'un jeu existant à créer le jeu en ligne le plus joué au monde, à se faire un fric fou et ont énormément contribué à la progression de l'esport. C'est plutôt des génies oui.Si tu veux te convaincres que c'est une blague regarde ça : Units critically strike on 150% of attackset explique moi ce que ça veut dire.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mIl y a une phrase prétendument de Pasqua qui tourne sur twitter en ce moment : *Quand on est emmerdé par une affaire, il faut susciter une affaire dans l’affaire, et si nécessaire une autre affaire dans l’affaire de l’affaire, jusqu’à ce que personne n’y comprenne plus rien.* Même si elle n'est pas de lui, elle s'applique bien à la situation actuelle. C'est bien joué de la part de la droite (bien aidé par la nullité du gouvernement) d'avoir su allumer des contres feux pour sauver le cul de Sarkozy. Assez marrant aussi de voir le léchage de boules en faveur de Sarko dans les articles de la page d'accueil de Valeurs Actuelles. C'est surement un hasard que ce soit ce journal qui sorte cette affaire.\u001b[0;0m\n",
            "   \u001b[1;94mIl y a une phrase prétendument de Pasqua qui tourne sur twitter en ce moment : Quand on est emmerdé par une affaire, il faut susciter une affaire dans l’affaire, et si nécessaire une autre affaire dans l’affaire de l’affaire, jusqu’à ce que personne n’y comprenne plus rien.Par exemple quand le gouvernement est emmerdé par ses résultats déplorables, il lance des diversions ? Après le mariage pour tous, Dieudonné, Méric, pas de bol cette fois-ci c'est un adversaire qui connait la chanson et apporte une réponse appropriée.Ma position n'a pas changé : le gouvernement ferait mieux de [s’occuper des Français](-Minute-Taubira-une-strategie-de-diversion-du-gouvernement-FN-1710677/).\u001b[0;0m\n",
            "\u001b[0mLe mariage pour tous ? Lol ? Tout le bordel à ce sujet à été crée par l'opposition. \u001b[0;0m\n",
            "   \u001b[1;94m... C'est l'opposition qui propose les lois maintenant ?\u001b[0;0m\n",
            "\u001b[0mParce qu'on est en période de crise économique, le gouvernement démocratiquement élu n'aurait pas le droit de légiférer sur des questions non-économiques ? Arrête.\u001b[0;0m\n",
            "   \u001b[1;94m Parce qu'on est en période de crise économique, le gouvernement démocratiquement élu n'aurait pas le droit de légiférer sur des questions non-économiques ?Il a tout à fait le droit de faire diversion. D'ailleurs il le fait très bien.\u001b[0;0m\n",
            "\u001b[0mTa mauvaise foi est sans limite.Il ne fait aucune diversion dans la mesure où 1. Il est légitime à légiférer sur des sujets non-économiques2. Le mariage gay était inscrit au programme de Hollande avant son élection. Il s'agissait donc d'une promesse de campagne, et certaines personnes, j'en suis sûr, on voté Hollande en raison du mariage gay.Enfin, si l'opposition avait le même sentiment que toi, elle n'aurait pas  monopolisé l'attention via la manif pour tous et aurait recentrée le débat sur les questions économiques.\u001b[0;0m\n",
            "   \u001b[1;94m Il ne fait aucune diversion dans la mesure où 1. Il est légitime à légiférer sur des sujets non-économiquesEn quoi une diversion ne serait pas légitime ?J'ai déjà dit qu'il avait tout à fait le droit de le faire.Le mariage gay était inscrit au programme de Hollande avant son élection.Oui, et en quoi ça n'en fait pas une diversion ?si l'opposition avait le même sentiment que toi, elle n'aurait pas monopolisé l'attention via la manif pour tousC'est ce qu'a fait le FN. La véritable opposition est le FN, pas l'UMP.\u001b[0;0m\n",
            "\u001b[0m C'est ce qu'a fait le FN.Ma mémoire n'est peut-être pas très bonne, mais je crois me souvenir de nombreux cadres du FN manifestant avec la Manif Pour Tous, signant sa charte et participant à cette grande \"diversion\".\u001b[0;0m\n",
            "   \u001b[1;94m Ma mémoire n'est peut-être pas très bonne, mais je crois me souvenir de nombreux cadres du FN manifestant avec la Manif Pour Tous, signant sa charte et participant à cette grande \"diversion\".C'est vrai, mais en quoi cela revient à \"monopoliser l'attention via la manif pour tous\" ?La ligne du FN [est très claire là-dessus]().\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mJe viens de terminer une épreuve dite \"marathon\", on nous a filé un texte à traduire à 8h45 ce matin et on doit le rendre à 16h45 au plus tard.Eh bien figurez-vous que le cancer de la prostate, c'est pas rigolo !\u001b[0;0m\n",
            "   \u001b[1;94mTu fais quoi, des études de trad ?\u001b[0;0m\n",
            "\u001b[0mOui, je suis en traduction audiovisuelle.\u001b[0;0m\n",
            "   \u001b[1;94mAh tu dois être à Strasbourg ? Je me souviens de ça, tout le monde flippait de devoir passer la journée sur une traduction, j'étais parmi les \"un peu plus vieux\" qui avaient déjà bossé avant et ça me faisait rouler des yeux.\u001b[0;0m\n",
            "\u001b[0mOui !Je t'avais posé des questions sur ton AMA il y a quelques mois. :)Je dois avouer que je faisais pas le malin ce matin. Mais bon, c'est passé assez facilement en fait.\u001b[0;0m\n",
            "   \u001b[1;94mAh oui c'est vrai... on avait eu un txt sur la sclérose en plaque... ils sont joyeux ces gens. C'est toujours la prof blonde gentille en trad médicale, avec la super ambiance de merde où tout le monde se fait chier ?\u001b[0;0m\n",
            "\u001b[0mLa prof est blonde et gentille mais l'ambiance était vraiment bonne, si on considère les sujets des textes...\u001b[0;0m\n",
            "   \u001b[1;94mAh ben nous c'était mort, mais j'aimais à peu près personne dans ma promo. J'espère que t'auras bien réussi ton truc en tout cas, tu me diras. T'as toujours pas rencontré M. Volclair ?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mAutant c'est ridicule, autant je suis sûr qu'à 13 ans j'ai dû dire des conneries aussi grosses que celle-ci.\u001b[0;0m\n",
            "   \u001b[1;94mIl y a une légère différence entre dire quelque chose à 13 ans et le prendre en compte pour l'imprimer à 30 ans dans un \"journal\" pour que tout le monde le voit. \u001b[0;0m\n",
            "\u001b[0mBof. Ça peut être intéressant/marrant de voir les conneries que peuvent raconter les ados, de temps en temps. J'veux dire c'est pas comme si le magazine imprimait ça pour dire \"voici nos conseils sentimentaux, offerts par notre experte en la matière\". Tout lecteur équipé de plus de trois neurones lira ça sur le mode \"ah ah, ils sont marrants ces petits, ils ont encore tant de choses à apprendre sur la vie\".\u001b[0;0m\n",
            "   \u001b[1;94mSauf que la plupart du bétail francais prend pour vérité absolu tout ce qu'il voit à la télé et dans un journal, si on as plus de 3 neurones ca peut nous faire marrer mais si tu as plus de 10 neurones tu te rendra compte du mal que ca peut faire.\u001b[0;0m\n",
            "\u001b[0m Sauf que la plupart du bétail francais prend pour vérité absolu tout ce qu'il voit à la télé et dans un journalBen voyons. Le \"bétail français\" va lire un conseil \"trompe avant d'être trompé\" **signé par une gamine de 13 ans** et se dire \"ouais elle a raison\" ? Tu dois pas sortir souvent, pour penser que les gens sont vraiment aussi cons.\u001b[0;0m\n",
            "   \u001b[1;94mOk toi tu vis dans ta bulle sociale tu es fiché direct, c'est pas parce que tu vis dans un milieu ou les gens sont un minimum conscient que c'est le cas partout. Sors un peu, fréquente différentes couches sociales, et surtout va te balader dans les coins de france ou on trouve le bon redneck francais de base qui parle de tout ce quil lit au journal ou ce qu'il voit au info comme une vérité général, tu va perdre foi en l'humanité direct et revenir à la réalité.Malheuresement oui les gens sont aussi cons, le manque de culture ca fais des ravages terribles, les gens ne remettent pas grand chose en question si on les y aide pas.Cet article en particulier ne prouve rien et est\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mOn sait bien ce que ça va donner ce mal parlé.  Les gros bouzeux incultes vont s'apercevoir qu'ils ont tout en commun : l'alcoolisme, l'inculture, la jalousie, et que vont ils faire ? Ils vont faire une bonne grosse alliance de paysans, façon coopérative de l'inculture de France et essayer de trashtalk leur belle, superbe et délicate capitale :  #PARIS !\u001b[0;0m\n",
            "   \u001b[1;94mC'est cool de payer un loyer à 4 chiffres pour un studio ?  C'est comment le métro/Rer tous les matins ?  C'est pas trop cher 4 euros pour un demi ?  C'est chiant de pas pouvoir ce baigner quand il fait 38 ° ?J'entends déjà les \"Paris et ses expositions/concerts/musés/\" un vrai parisien n'y va jamais.\u001b[0;0m\n",
            "\u001b[0mC'est comment le métro/Rer tous les matins ?Tu veux dire avoir des transports en commun globalement fiables et bien foutus, abordables, et très étendus ? Plutôt pas mal, merci.\u001b[0;0m\n",
            "   \u001b[1;94mJe veux surtout avoir votre avis sur le fait de passer 2 sur les 18 heures de tes journées dans un métro .\u001b[0;0m\n",
            "\u001b[0mMieux vaut ça que 8 heures par jour sur un tracteur ?\u001b[0;0m\n",
            "   \u001b[1;94mMoi j'aime bien, surtout quand je suis au milieu de la route et que j'ai un connard de parisien en vacances derrière moi qui klaxonne parce qu'il est beaucoup trop pressé. C'est mon petit plaisir.\u001b[0;0m\n",
            "\u001b[0mAttends tu déconnes, les parisiens en vacances ils roulent encore moins vite que les tracteurs.\u001b[0;0m\n",
            "   \u001b[1;94mNormal, le reste de la France prend plaisir à aggro la moindre plaque 75 qu'ils voient, pour se branler sur \"les parisiens conduisent trop mal\" le reste de l'année, entre deux accidents mortels sur départementales\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mC'est quoi la proportion homme femme sur le sub r/france ?\u001b[0;0m\n",
            "   \u001b[1;94mC'est quoi le rapport ?\u001b[0;0m\n",
            "09:08:04 | loaded 96865 episodes with a total of 335085 examples\n"
          ]
        }
      ],
      "source": [
        "# from parlai.scripts.display_data import DisplayData\n",
        "# DisplayData.main(task='empathetic_dialogues', num_examples=10)\n",
        "\n",
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "    # model_file= f'{finetuned_model_path}/model',\n",
        "    # dict_file= dict_file,\n",
        "\n",
        "    num_examples=20,\n",
        "    # skip_generation=False,\n",
        "\n",
        "    # beam_min_length= 20,\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_size= 10,\n",
        "\n",
        "    # inference= \"beam\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "blender-finetuning-with-reddit.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMXfXSrmuYYQdlsEJmI1KX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}