{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirpaia/blenderbot/blob/main/blender_finetuning_with_reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SONwSWMp6qPv"
      },
      "source": [
        "# 0.Installing prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d-SZ_On6Kxg",
        "outputId": "a8923f41-2066-4d52-8634-11a0189b7e8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun  7 07:54:41 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BegaSUz6iUz",
        "outputId": "f1aa2835-561b-4192-f694-d7526b0553d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SzGRXHQ6kDQ",
        "outputId": "a0193aab-a05c-4b7e-ff78-f8d70aadf8e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "mydrive_path = '/content/drive/MyDrive/colabs/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWDakYmy6mIQ",
        "outputId": "7a11834b-2186-4853-c19b-d2e9864df683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 248 kB 70.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 125 kB 52.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 54.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 53.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 36.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 346 kB 36.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 208 kB 58.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 62.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 235 kB 68.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 749 kB 48.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 95 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 57.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 50.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.9 MB 43.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 52.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 61.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 547 kB 53.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 175 kB 63.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 67.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 64.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 55.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 62.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 70.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 110 kB 44.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 121 kB 58.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 100 kB 10.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 9.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 65.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 52.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 839 kB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 55.5 MB/s \n",
            "\u001b[?25h  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 9.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.26.9)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.19.2\n"
          ]
        }
      ],
      "source": [
        "# !pip uninstall -q parlai\n",
        "!pip install -q parlai\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIJEq9_r63hi"
      },
      "source": [
        "# 1.Preparing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R-fgBUdcPX5"
      },
      "source": [
        "## Genreal Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47u8M3RK65m_",
        "outputId": "e0f43a3d-2231-4a73-dfba-6164a68f509c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n",
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def transfer_list_of_turns_to_dialog(d):\n",
        "    if len(d)%2 !=0: d = d[:-1]\n",
        "    t = \"\"\n",
        "    for i in range(0,len(d),2):\n",
        "        u1 = d[i]\n",
        "        u2 = d[i+1]\n",
        "\n",
        "        if (i+2) != len(d):\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "        else:\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "    return t\n",
        "\n",
        "def transfer_list_of_pairs_to_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, text_label_pair in enumerate(d):\n",
        "    u1 = text_label_pair[0]\n",
        "    u2 = text_label_pair[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "\n",
        "def convert_parlai_format_to_list_of_turns(lines):\n",
        "    result = []\n",
        "    for line in lines:\n",
        "        text_label = line.split(\"\\t\")\n",
        "        result.append(text_label[0].replace(\"text:\", \"\"))\n",
        "        result.append(text_label[1].replace(\"labels:\", \"\").replace(\"\\n\",\"\"))\n",
        "    return result\n",
        "\n",
        "t = ['hello','how are you','good','bye','test']\n",
        "print(transfer_list_of_turns_to_dialog(t))\n",
        "\n",
        "t = [['hello','how are you'],['good','bye']]\n",
        "print(transfer_list_of_pairs_to_dialog(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE45ZyKC8WjC"
      },
      "source": [
        "## French Reddit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8ZmSJtQG8Y1o",
        "outputId": "07b5a06a-6911-40e6-b144-25a694e72159"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data_path = f\"{mydrive_path}aliae-workspace/datasets/french_reddit/\"\n",
        "data_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{data_path}/data_train.txt\") as f:\n",
        "    lines = f.readlines()\n",
        "lines[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UexnzWURWKSI",
        "outputId": "a90fec44-390b-4a04-d23c-97ae41540323"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"text:Pour ceux qui jouent à LoL et qui attendaient impatiemment le URF, ça sera au final le NURF qui sera là. L'inverse du Urf : cooldown augmentés, conso mana/énergie augmentée, dégâts des minions augmentés... Voilà, enjoy, moi je vais pleurer dans un coin parce que le urf était le mode qui me poussait à garder LoL installé. ^^^^quelqu'un ^^^^n'aime ^^^^pas ^^^^LoL ^^^^? ^^^^:)\\tlabels:Ça, ou alors demain c'est le 1^er avril.\\n\",\n",
              " \"text:Le vrai mode URF était déjà une blague du 1er avril. Sauf que ça a bien marché et ils ont laissé le mode 2 semaines.On verra bien demain de toute façon :(\\tlabels:Et tu penses vraiment qu'ils vont remplacer un mode de jeu que tout le monde attend depuis 1 an par un truc pas fun du tout ?C'est clairement une blague.\\n\",\n",
              " \"text:Riot est tellement une vaste blague comme entreprise qu'ils en seraient capable D:\\tlabels:Une vaste blague comme entreprise ? Les mecs ont réussi, en pompant le concept d'un jeu existant à créer le jeu en ligne le plus joué au monde, à se faire un fric fou et ont énormément contribué à la progression de l'esport. C'est plutôt des génies oui.Si tu veux te convaincres que c'est une blague regarde ça : Units critically strike on 150% of attackset explique moi ce que ça veut dire.\\tepisode_done:True\\n\",\n",
              " \"text:Il y a une phrase prétendument de Pasqua qui tourne sur twitter en ce moment : *Quand on est emmerdé par une affaire, il faut susciter une affaire dans l’affaire, et si nécessaire une autre affaire dans l’affaire de l’affaire, jusqu’à ce que personne n’y comprenne plus rien.* Même si elle n'est pas de lui, elle s'applique bien à la situation actuelle. C'est bien joué de la part de la droite (bien aidé par la nullité du gouvernement) d'avoir su allumer des contres feux pour sauver le cul de Sarkozy. Assez marrant aussi de voir le léchage de boules en faveur de Sarko dans les articles de la page d'accueil de Valeurs Actuelles. C'est surement un hasard que ce soit ce journal qui sorte cette affaire.\\tlabels:Il y a une phrase prétendument de Pasqua qui tourne sur twitter en ce moment : Quand on est emmerdé par une affaire, il faut susciter une affaire dans l’affaire, et si nécessaire une autre affaire dans l’affaire de l’affaire, jusqu’à ce que personne n’y comprenne plus rien.Par exemple quand le gouvernement est emmerdé par ses résultats déplorables, il lance des diversions ? Après le mariage pour tous, Dieudonné, Méric, pas de bol cette fois-ci c'est un adversaire qui connait la chanson et apporte une réponse appropriée.Ma position n'a pas changé : le gouvernement ferait mieux de [s’occuper des Français](-Minute-Taubira-une-strategie-de-diversion-du-gouvernement-FN-1710677/).\\n\",\n",
              " \"text:Le mariage pour tous ? Lol ? Tout le bordel à ce sujet à été crée par l'opposition. \\tlabels:... C'est l'opposition qui propose les lois maintenant ?\\n\",\n",
              " \"text:Parce qu'on est en période de crise économique, le gouvernement démocratiquement élu n'aurait pas le droit de légiférer sur des questions non-économiques ? Arrête.\\tlabels: Parce qu'on est en période de crise économique, le gouvernement démocratiquement élu n'aurait pas le droit de légiférer sur des questions non-économiques ?Il a tout à fait le droit de faire diversion. D'ailleurs il le fait très bien.\\n\",\n",
              " \"text:Ta mauvaise foi est sans limite.Il ne fait aucune diversion dans la mesure où 1. Il est légitime à légiférer sur des sujets non-économiques2. Le mariage gay était inscrit au programme de Hollande avant son élection. Il s'agissait donc d'une promesse de campagne, et certaines personnes, j'en suis sûr, on voté Hollande en raison du mariage gay.Enfin, si l'opposition avait le même sentiment que toi, elle n'aurait pas  monopolisé l'attention via la manif pour tous et aurait recentrée le débat sur les questions économiques.\\tlabels: Il ne fait aucune diversion dans la mesure où 1. Il est légitime à légiférer sur des sujets non-économiquesEn quoi une diversion ne serait pas légitime ?J'ai déjà dit qu'il avait tout à fait le droit de le faire.Le mariage gay était inscrit au programme de Hollande avant son élection.Oui, et en quoi ça n'en fait pas une diversion ?si l'opposition avait le même sentiment que toi, elle n'aurait pas monopolisé l'attention via la manif pour tousC'est ce qu'a fait le FN. La véritable opposition est le FN, pas l'UMP.\\n\",\n",
              " 'text: C\\'est ce qu\\'a fait le FN.Ma mémoire n\\'est peut-être pas très bonne, mais je crois me souvenir de nombreux cadres du FN manifestant avec la Manif Pour Tous, signant sa charte et participant à cette grande \"diversion\".\\tlabels: Ma mémoire n\\'est peut-être pas très bonne, mais je crois me souvenir de nombreux cadres du FN manifestant avec la Manif Pour Tous, signant sa charte et participant à cette grande \"diversion\".C\\'est vrai, mais en quoi cela revient à \"monopoliser l\\'attention via la manif pour tous\" ?La ligne du FN [est très claire là-dessus]().\\tepisode_done:True\\n',\n",
              " 'text:Je viens de terminer une épreuve dite \"marathon\", on nous a filé un texte à traduire à 8h45 ce matin et on doit le rendre à 16h45 au plus tard.Eh bien figurez-vous que le cancer de la prostate, c\\'est pas rigolo !\\tlabels:Tu fais quoi, des études de trad ?\\n',\n",
              " 'text:Oui, je suis en traduction audiovisuelle.\\tlabels:Ah tu dois être à Strasbourg ? Je me souviens de ça, tout le monde flippait de devoir passer la journée sur une traduction, j\\'étais parmi les \"un peu plus vieux\" qui avaient déjà bossé avant et ça me faisait rouler des yeux.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGoTqdFD7x_3"
      },
      "source": [
        "# 2.Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "65rK6pfj70Px",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "72aafedd-3bb9-4280-a9a2-2324c822efad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "finetuned_model_path = f'{mydrive_path}blender-models/finetuned-reddit-90m/'\n",
        "# init_model = 'zoo:blender/blender_400Mdistill/model'\n",
        "# dict_file  = 'zoo:blender/blender_400Mdistill/model.dict'\n",
        "init_model = 'zoo:blender/blender_90M/model'\n",
        "dict_file  = 'zoo:blender/blender_90M/model.dict'\n",
        "finetuned_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7vLiYpF8HbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f398763d-7673-43c5-dd48-a2811d5ebb78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                                 /@&%###%&&@@#\n",
            "                      .,*/((((##@@@&%%#%%&&@@@&%%#/*.\n",
            "             #@@&&&%%%%##(((///*****//(((###%%%&&&@@@@@&&%#%%#.\n",
            "         .%&@@@@@&&&%%%####((((////((((####%%%&&&@@@@@&&%%#%%####,\n",
            "           ./,,#(//**,,.....,,,,***////((((########%%%%%%%%###(((\n",
            "              /*(//**,,,....,,,,***////((((########%%%%%%%%###(#%*\n",
            "               (*,...      ...,,,***//////((((((///////(/*...,/#@@@(\n",
            "               **,,..         ...,,,,,,,,,,........,,*///*...*(#@@@@&&*\n",
            "               ./,,..          ...,,,,,,,,,........,,*//*,...*#/,,,,,/%#\n",
            "                (*,..          ...,,,,,,,,,........,,*//*,..,/(      .,#(\n",
            "                **,..          ...,,,,,,,,,.........,*//*,..,((       .,(#\n",
            "                 /*,..          ....,,,,,,,.....  ..,***,,,,(#         ..#&\n",
            "                 **,..          ....,,,,,,,....   ..,***,,,*#.         .,%@\n",
            "                 ./,...       B l e n d e r B o t ...***,,,*#          .*%@\n",
            "                  /*,..          ...,,,,,,,....    .,**,,,,/#         ..(%/\n",
            "                  /*,,..         ...,,,,,,,...    ..,*,,,,,(.         ..#&\n",
            "                  ,/*,..         ...,,,,,,,...    ..,*,,,,*#         ..*%(\n",
            "                   /*,..         ...,,,,,.....    ..,*,*,,/(         ..#&\n",
            "                   /**,..        ...,,,,.....    ...,***,*(.       ,,(%.\n",
            "                    (/*,,..      ....,,.....     ...,****(&@@@&&&#,\n",
            "                     (/*,,...   .....,,......     ..,****#@,\n",
            "                     *(/*,,/....*(###%(,(%%##(*.  ./,,**(\n",
            "                      ,//**(,........,/((#.........*,**(\n",
            "                      .(#//*,,,,,,.*.,/((%/,,.....,,*/@\n",
            "                    ((######//****,/.,/(#%#***,***(&@@@@@(\n",
            "                   *&%%#####%%%%%%%#//(#%&%%&&@@@@@@@@@@@@*\n",
            "                   &&%%%###((((((####%%%%&&&&&@@@@@@@@@@&&@.\n",
            "                  *##%%%##(((((((####%%%%%&&&&@@@@@@@@@&#/*,\n",
            "                 .(##%#/,  .,*((##%%%&&&&%%%#####%&&@&&%#(/*.\n",
            "                 /(###(,   .,*/(##%%%&&&&%%%######%&&&&%#(/*,\n",
            "                */((((*.  ..,//((##%%%%%%%%#######%&&&&%%#(/*,\n",
            "               .//(((/,   .,*//((###%%%%%%########%%&&&%%#((/,.\n",
            "              .&####(((((((((######%%%%%%%%&&&&&&&@@@@@@@@@@@@@#\n",
            "               *&#.   .*/((((#######%%%%%%&&&&&&&@@@@@#/.   (&/\n",
            "07:59:40 | building data: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/BST0B.tgz\n",
            "07:59:40 | Downloading http://parl.ai/downloads/_models/blender/BST0B.tgz to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/BST0B.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading BST0B.tgz: 100%|██████████| 161M/161M [00:03<00:00, 41.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07:59:47 | building dictionary first...\n",
            "07:59:47 | \u001b[33mOverriding opt[\"init_model\"] to zoo:blender/blender_90M/model (previously: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model)\u001b[0m\n",
            "07:59:47 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "07:59:47 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: full,is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data,fromfile_datatype_extension: True,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,beam_block_full_context: True,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,dict_loaded: True,verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "07:59:47 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues --numthreads 1 --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 16 --num-epochs -1 --save-every-n-secs 60.0 --save-after-valid True --validation-max-exs 20000 --validation-patience 15 --log-every-n-secs 2 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --num-topics 5 --train-experiencer-only False --dropout 0.1 --learn-positional-embeddings True --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --skip-generation False --inference beam --fp16-impl apex --force-fp16-tokens False --optimizer adamax --learningrate 7.5e-06 --max-lr-steps -1 --warmup-updates -1 --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
            "07:59:48 | Using CUDA\n",
            "07:59:48 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m/model.dict\n",
            "07:59:49 | num words = 54944\n",
            "07:59:50 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
            "08:00:03 | Total parameters: 87,508,992 (86,984,704 trainable)\n",
            "08:00:03 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m/model\n",
            "08:00:15 | Opt:\n",
            "08:00:15 |     activation: gelu\n",
            "08:00:15 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "08:00:15 |     adam_eps: 1e-08\n",
            "08:00:15 |     add_p1_after_newln: False\n",
            "08:00:15 |     aggregate_micro: False\n",
            "08:00:15 |     allow_missing_init_opts: False\n",
            "08:00:15 |     attention_dropout: 0.0\n",
            "08:00:15 |     batchsize: 8\n",
            "08:00:15 |     beam_block_full_context: True\n",
            "08:00:15 |     beam_block_list_filename: None\n",
            "08:00:15 |     beam_block_ngram: -1\n",
            "08:00:15 |     beam_context_block_ngram: -1\n",
            "08:00:15 |     beam_delay: 30\n",
            "08:00:15 |     beam_length_penalty: 0.65\n",
            "08:00:15 |     beam_min_length: 1\n",
            "08:00:15 |     beam_size: 1\n",
            "08:00:15 |     betas: '[0.9, 0.999]'\n",
            "08:00:15 |     bpe_add_prefix_space: None\n",
            "08:00:15 |     bpe_debug: False\n",
            "08:00:15 |     bpe_dropout: None\n",
            "08:00:15 |     bpe_merge: None\n",
            "08:00:15 |     bpe_vocab: None\n",
            "08:00:15 |     checkpoint_activations: False\n",
            "08:00:15 |     compute_tokenized_bleu: False\n",
            "08:00:15 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "08:00:15 |     datatype: train\n",
            "08:00:15 |     delimiter: '\\n'\n",
            "08:00:15 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "08:00:15 |     dict_endtoken: __end__\n",
            "08:00:15 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m/model.dict\n",
            "08:00:15 |     dict_include_test: False\n",
            "08:00:15 |     dict_include_valid: False\n",
            "08:00:15 |     dict_initpath: None\n",
            "08:00:15 |     dict_language: english\n",
            "08:00:15 |     dict_loaded: True\n",
            "08:00:15 |     dict_lower: True\n",
            "08:00:15 |     dict_max_ngram_size: -1\n",
            "08:00:15 |     dict_maxexs: -1\n",
            "08:00:15 |     dict_maxtokens: -1\n",
            "08:00:15 |     dict_minfreq: 0\n",
            "08:00:15 |     dict_nulltoken: __null__\n",
            "08:00:15 |     dict_starttoken: __start__\n",
            "08:00:15 |     dict_textfields: text,labels\n",
            "08:00:15 |     dict_tokenizer: bpe\n",
            "08:00:15 |     dict_unktoken: __unk__\n",
            "08:00:15 |     display_examples: False\n",
            "08:00:15 |     download_path: None\n",
            "08:00:15 |     dropout: 0.0\n",
            "08:00:15 |     dynamic_batching: full\n",
            "08:00:15 |     embedding_projection: random\n",
            "08:00:15 |     embedding_size: 512\n",
            "08:00:15 |     embedding_type: random\n",
            "08:00:15 |     embeddings_scale: True\n",
            "08:00:15 |     eval_batchsize: None\n",
            "08:00:15 |     eval_dynamic_batching: None\n",
            "08:00:15 |     evaltask: None\n",
            "08:00:15 |     ffn_size: 2048\n",
            "08:00:15 |     final_extra_opt: \n",
            "08:00:15 |     force_fp16_tokens: True\n",
            "08:00:15 |     fp16: True\n",
            "08:00:15 |     fp16_impl: mem_efficient\n",
            "08:00:15 |     fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data\n",
            "08:00:15 |     fromfile_datatype_extension: True\n",
            "08:00:15 |     gpu: -1\n",
            "08:00:15 |     gradient_clip: 0.1\n",
            "08:00:15 |     hide_labels: False\n",
            "08:00:15 |     history_add_global_end_token: None\n",
            "08:00:15 |     history_reversed: False\n",
            "08:00:15 |     history_size: -1\n",
            "08:00:15 |     image_cropsize: 224\n",
            "08:00:15 |     image_mode: raw\n",
            "08:00:15 |     image_size: 256\n",
            "08:00:15 |     inference: greedy\n",
            "08:00:15 |     init_model: zoo:blender/blender_90M/model\n",
            "08:00:15 |     init_opt: None\n",
            "08:00:15 |     interactive_mode: False\n",
            "08:00:15 |     invsqrt_lr_decay_gamma: -1\n",
            "08:00:15 |     is_debug: False\n",
            "08:00:15 |     label_truncate: 128\n",
            "08:00:15 |     learn_positional_embeddings: False\n",
            "08:00:15 |     learningrate: 1e-05\n",
            "08:00:15 |     load_from_checkpoint: True\n",
            "08:00:15 |     log_every_n_secs: 60.0\n",
            "08:00:15 |     log_every_n_steps: 50\n",
            "08:00:15 |     log_keep_fields: all\n",
            "08:00:15 |     loglevel: info\n",
            "08:00:15 |     lr_scheduler: reduceonplateau\n",
            "08:00:15 |     lr_scheduler_decay: 0.5\n",
            "08:00:15 |     lr_scheduler_patience: 3\n",
            "08:00:15 |     max_train_steps: -1\n",
            "08:00:15 |     max_train_time: -1\n",
            "08:00:15 |     metrics: default\n",
            "08:00:15 |     model: transformer/generator\n",
            "08:00:15 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m/model\n",
            "08:00:15 |     model_parallel: False\n",
            "08:00:15 |     momentum: 0\n",
            "08:00:15 |     multitask_weights: [1]\n",
            "08:00:15 |     mutators: None\n",
            "08:00:15 |     n_decoder_layers: -1\n",
            "08:00:15 |     n_encoder_layers: -1\n",
            "08:00:15 |     n_heads: 16\n",
            "08:00:15 |     n_layers: 8\n",
            "08:00:15 |     n_positions: 512\n",
            "08:00:15 |     n_segments: 0\n",
            "08:00:15 |     nesterov: True\n",
            "08:00:15 |     no_cuda: False\n",
            "08:00:15 |     num_epochs: 5.0\n",
            "08:00:15 |     num_workers: 0\n",
            "08:00:15 |     nus: [0.7]\n",
            "08:00:15 |     optimizer: mem_eff_adam\n",
            "08:00:15 |     output_scaling: 1.0\n",
            "08:00:15 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data', 'fromfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m/model', 'init_model': 'zoo:blender/blender_90M/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'validation_every_n_epochs': 0.25, 'num_epochs': 5.0, 'log_every_n_secs': 60.0, 'verbose': True, 'batchsize': 8, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'validation_patience': 10, 'validation_metric': 'ppl', 'validation_metric_mode': 'min', 'dynamic_batching': 'full', 'learningrate': 1e-05, 'optimizer': 'adam', 'attention_dropout': 0.0, 'model_parallel': False, 'warmup_updates': 100}\"\n",
            "08:00:15 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "08:00:15 |     person_tokens: False\n",
            "08:00:15 |     rank_candidates: False\n",
            "08:00:15 |     relu_dropout: 0.0\n",
            "08:00:15 |     save_after_valid: False\n",
            "08:00:15 |     save_every_n_secs: -1\n",
            "08:00:15 |     save_format: conversations\n",
            "08:00:15 |     share_word_embeddings: True\n",
            "08:00:15 |     short_final_eval: False\n",
            "08:00:15 |     skip_generation: True\n",
            "08:00:15 |     special_tok_lst: None\n",
            "08:00:15 |     split_lines: False\n",
            "08:00:15 |     starttime: Jun03_23-03\n",
            "08:00:15 |     task: fromfile:parlaiformat\n",
            "08:00:15 |     temperature: 1.0\n",
            "08:00:15 |     tensorboard_log: False\n",
            "08:00:15 |     tensorboard_logdir: None\n",
            "08:00:15 |     text_truncate: 512\n",
            "08:00:15 |     topk: 10\n",
            "08:00:15 |     topp: 0.9\n",
            "08:00:15 |     truncate: -1\n",
            "08:00:15 |     update_freq: 1\n",
            "08:00:15 |     use_reply: label\n",
            "08:00:15 |     validation_cutoff: 1.0\n",
            "08:00:15 |     validation_every_n_epochs: 0.25\n",
            "08:00:15 |     validation_every_n_secs: -1\n",
            "08:00:15 |     validation_every_n_steps: -1\n",
            "08:00:15 |     validation_max_exs: -1\n",
            "08:00:15 |     validation_metric: ppl\n",
            "08:00:15 |     validation_metric_mode: min\n",
            "08:00:15 |     validation_patience: 10\n",
            "08:00:15 |     validation_share_agent: False\n",
            "08:00:15 |     variant: xlm\n",
            "08:00:15 |     verbose: True\n",
            "08:00:15 |     wandb_entity: None\n",
            "08:00:15 |     wandb_log: False\n",
            "08:00:15 |     wandb_name: None\n",
            "08:00:15 |     wandb_project: None\n",
            "08:00:15 |     warmup_rate: 0.0001\n",
            "08:00:15 |     warmup_updates: 100\n",
            "08:00:15 |     weight_decay: None\n",
            "08:00:15 |     world_logs: \n",
            "08:00:15 | creating task(s): fromfile:parlaiformat\n",
            "08:00:15 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data_train.txt\n",
            "08:00:35 | training...\n",
            "08:00:52 | time:7422s total_exs:252428 total_steps:20006 epochs:0.75 time_left:41839s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   101.8     1  2206  6926       0          0 68.05 1084              8192  8.344    .3089 58.06 2.438 9.9e-06  1154  3622   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "    .09133      4.835 11.45      .5130  .0009225                20006 3360 10548 3.139\n",
            "\n",
            "08:01:08 | time:7438s total_exs:253172 total_steps:20056 epochs:0.76 time_left:41784s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   195.3     1  2881  9141  .01882      1.691 47.21  744              8192  9.918    .2855 66.04 2.382 9.9e-06 871.6  2766   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1398      7.466 10.82      .5205         0                20056 3752 11906 3.173\n",
            "\n",
            "08:01:24 | time:7454s total_exs:253824 total_steps:20106 epochs:0.76 time_left:41748s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   229.2     1  2931  9118  .04141      4.377 40.56  652              8192  8.935    .2974 68.02 2.459 9.9e-06 788.7  2453   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1334      7.531 11.69      .5084         0                20106 3720 11571 3.111\n",
            "\n",
            "08:01:40 | time:7470s total_exs:254488 total_steps:20156 epochs:0.76 time_left:41709s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   239.6     1  3083  9665  .06476      7.462 41.63  664              8192  10.33    .2974 66.86 2.448 9.9e-06 761.2  2386   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1491      9.545 11.56      .5108         0                20156 3844 12051 3.135\n",
            "\n",
            "08:01:56 | time:7486s total_exs:255144 total_steps:20206 epochs:0.76 time_left:41670s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   224.5     1  2893  9193  .03659      3.968 41.69  656              8192  9.089    .3076 62.25 2.472 9.9e-06 734.6  2334   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1098      6.258 11.85      .5039   .001524                20206 3628 11527 3.178\n",
            "\n",
            "08:02:12 | time:7502s total_exs:255776 total_steps:20256 epochs:0.76 time_left:41639s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   263.8     1  3182  9796  .09335      12.01 38.91  632              8192  9.095    .2855 59.38 2.407 9.9e-06 695.8  2142   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "    .08228      4.328 11.1      .5119         0                20256 3878 11937 3.078\n",
            "\n",
            "08:02:28 | time:7518s total_exs:256344 total_steps:20306 epochs:0.77 time_left:41617s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   291.5     1  3097  9894   .1444      18.92  36.3  568              8192  9.707    .2855 65.48 2.409 9.9e-06 653.5  2088   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1373      7.956 11.12      .5125         0                20306 3750 11982 3.195\n",
            "\n",
            "08:02:44 | time:7534s total_exs:256964 total_steps:20356 epochs:0.77 time_left:41589s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   277.4     1  3153  9668   .1452      23.13 38.02  620              8192  9.408    .2974 66.46 2.473 9.9e-06 717.6  2201   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1355      8.587 11.86      .5050         0                20356 3870 11868 3.067\n",
            "\n",
            "08:03:00 | time:7550s total_exs:257636 total_steps:20406 epochs:0.77 time_left:41548s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   237.6     1  2987  9441  .09226       15.3 42.47  672              8192  9.495    .2855 63.15 2.494 9.9e-06 768.2  2428   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1042      5.996 12.11      .5024         0                20406 3755 11868 3.161\n",
            "\n",
            "08:03:16 | time:7566s total_exs:258336 total_steps:20456 epochs:0.77 time_left:41502s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   223.6     1  2998  9466  .05714      9.427  44.2  700              8192  9.036    .2855  61.9 2.426 9.9e-06 772.2  2438   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1171      6.743 11.32      .5152   .004286                20456 3770 11904 3.158\n",
            "\n",
            "08:03:32 | time:7582s total_exs:258936 total_steps:20506 epochs:0.77 time_left:41476s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   270.1     1  3052  9509   .0750      15.74 37.39  600              8192  9.648    .2974 67.85 2.433 9.9e-06   713  2221   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1533      8.435 11.39      .5128         0                20506 3765 11731 3.116\n",
            "\n",
            "08:03:49 | time:7598s total_exs:259532 total_steps:20556 epochs:0.77 time_left:41453s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   292.7     1  3163  9612   .1359      27.43 36.23  596              8192  8.691    .3124 80.18 2.495 9.9e-06 810.8  2464   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1913      12.17 12.12      .5062   .001678                20556 3973 12076 3.04\n",
            "\n",
            "08:04:05 | time:7615s total_exs:260196 total_steps:20606 epochs:0.78 time_left:41416s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   255.1     1  3084  9530   .1370      22.84 41.04  664              8192  9.406    .3039 68.85 2.394 9.9e-06 787.4  2433   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1506      9.554 10.96      .5168   .001506                20606 3871 11963 3.09\n",
            "\n",
            "08:04:21 | time:7631s total_exs:260820 total_steps:20656 epochs:0.78 time_left:41388s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   261.5     1  3093  9446  .08974      13.64 38.11  624              8192  9.417    .2856 73.52 2.374 9.9e-06 781.2  2386   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1635      10.92 10.74      .5289         0                20656 3874 11832 3.054\n",
            "\n",
            "08:04:37 | time:7647s total_exs:261424 total_steps:20706 epochs:0.78 time_left:41362s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     275     1  3124  9677  .09768      16.43 37.42  604              8192  9.534    .2856 71.39 2.414 9.9e-06 739.6  2291   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1474      10.16 11.18      .5157   .003311                20706 3863 11968 3.098\n",
            "\n",
            "08:04:54 | time:7664s total_exs:262076 total_steps:20756 epochs:0.78 time_left:41330s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   248.2     1  2958  8925   .1150      21.43 39.35  652              8192  9.571    .3123 67.25 2.465 9.9e-06 774.4  2337   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1288      7.863 11.77      .5069         0                20756 3732 11262 3.018\n",
            "\n",
            "08:05:10 | time:7680s total_exs:262688 total_steps:20806 epochs:0.78 time_left:41303s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   290.7     1  3237 10024   .1569      26.22  37.9  612              8192  10.87    .3038 69.58 2.406 9.9e-06 726.3  2249   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1454      10.24 11.09      .5134   .001634                20806 3964 12273 3.097\n",
            "\n",
            "08:05:26 | time:7696s total_exs:263320 total_steps:20856 epochs:0.79 time_left:41271s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   269.5     1  3074  9618   .1392      26.29 39.55  632              8192  9.659    .2855 65.93 2.334 9.9e-06 734.1  2297   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1456      7.853 10.32      .5298         0                20856 3808 11915 3.129\n",
            "\n",
            "08:05:42 | time:7712s total_exs:263936 total_steps:20906 epochs:0.79 time_left:41243s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   282.5     1  3106  9620   .1266      30.42 38.16  616              8192  9.037    .2855 71.71 2.416 9.9e-06   765  2370   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1558       9.62 11.2      .5154   .006494                20906 3870 11990 3.098\n",
            "\n",
            "08:05:58 | time:7728s total_exs:264624 total_steps:20956 epochs:0.79 time_left:41199s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   211.9     1  2821  9078  .03343      6.924 44.28  688              8192  8.797    .2886 65.03 2.403 9.9e-06 785.9  2529   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1337      7.916 11.05      .5147         0                20956 3607 11607 3.218\n",
            "\n",
            "08:06:14 | time:7744s total_exs:265256 total_steps:21006 epochs:0.79 time_left:41169s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   264.9     1  3104  9545   .1108      19.28 38.87  632              8192  9.319    .3038  69.9 2.418 9.9e-06 776.1  2386   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1392        8.5 11.23      .5137         0                21006 3880 11931 3.075\n",
            "\n",
            "08:06:30 | time:7760s total_exs:265832 total_steps:21056 epochs:0.79 time_left:41146s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   291.3     1  3102  9894   .1250      22.03 36.74  576              8192  10.52    .2883  71.4 2.401 9.9e-06 697.6  2225   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1510      10.85 11.03      .5187         0                21056 3800 12119 3.19\n",
            "\n",
            "08:06:46 | time:7776s total_exs:266396 total_steps:21106 epochs:0.80 time_left:41129s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   320.1     1  3246  9996   .1507      32.27 34.73  564              8192  9.445    .2855  73.2 2.407 9.9e-06 699.6  2154   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1702      11.18 11.1      .5194         0                21106 3946 12150 3.079\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 90M settings\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    # task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "    # multitask_weights= \"1,3,3\",\n",
        "\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    \n",
        "    # depend on your gpu. \n",
        "    validation_every_n_epochs=0.25,\n",
        "    num_epochs = 5,\n",
        "    log_every_n_secs= 60,\n",
        "    verbose = True,\n",
        "    batchsize= 8, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        "\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    attention_dropout= 0.0, \n",
        "    model_parallel= False,\n",
        "    warmup_updates=100,\n",
        "\n",
        "    # customized parameters\n",
        "    # inference= \"beam\"\n",
        "    # beam_min_length= 20,\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_size= 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mydrive_path = '/content/finetuned-multitask-400m-double-sided-2epochs'\n",
        "# mydrive_path = '/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-double-sided/'"
      ],
      "metadata": {
        "id": "mxL0-1x1YsQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNZE5ta2pO-X"
      },
      "outputs": [],
      "source": [
        "# 400M settings\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "    multitask_weights= \"1,3,3\",\n",
        "\n",
        "    # task='fromfile:parlaiformat', \n",
        "    # fromfile_datapath='copied_dataset_french_bst/',\n",
        "    # fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 5,\n",
        "    log_every_n_secs= 60,\n",
        "    verbose = True,\n",
        "    attention_dropout= 0.0, \n",
        "    batchsize= 16, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    # save_after_valid= True,\n",
        "\n",
        "    # arguments we get from the pretrained model. \"from recipes page for 2.7B model\" \n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    variant= \"prelayernorm\",\n",
        "    n_heads= 32, n_positions= 128, \n",
        "    n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "    dropout= 0.1,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    relu_dropout= 0.0, \n",
        "    model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    # inference = 'topk', \n",
        "    # temperature = 0.7, \n",
        "    # topk=30, \n",
        "    # beam_length_penalty=1.03\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -rv /content/finetuned-multitask-400m-double-sided-2epochs/* /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-double-sided/\n",
        "# !ls -lah /content/finetuned-multitask-400m-double-sided-2epochs/\n",
        "# !ls -lah /content/finetuned-multitask-400m-double-sided/"
      ],
      "metadata": {
        "id": "K1S0kRk3W63i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVPS6p4XzPh4"
      },
      "source": [
        "# 4.Display Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9FBtnZZzPPg",
        "outputId": "c2343265-c6b1-4870-8cc4-6c58863b76de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15:06:36 | \u001b[33mOverriding opt[\"task\"] to french_blended_skill_talk (previously: french_blended_skill_talk,french_xpersona,french_empathetic_dialogues)\u001b[0m\n",
            "15:06:36 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "15:06:36 | Using CUDA\n",
            "15:06:36 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m-double-sided/model.dict\n",
            "15:06:36 | num words = 54944\n",
            "15:06:38 | Total parameters: 87,508,992 (86,984,704 trainable)\n",
            "15:06:38 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m-double-sided/model\n",
            "15:06:42 | creating task(s): french_blended_skill_talk\n",
            "15:06:42 | Loading ParlAI text data: /content/dataset_french_bst/valid.txt\n",
            "15:06:42 | Opt:\n",
            "15:06:42 |     activation: gelu\n",
            "15:06:42 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "15:06:42 |     adam_eps: 1e-08\n",
            "15:06:42 |     add_p1_after_newln: False\n",
            "15:06:42 |     aggregate_micro: False\n",
            "15:06:42 |     allow_missing_init_opts: False\n",
            "15:06:42 |     attention_dropout: 0.0\n",
            "15:06:42 |     batchsize: 8\n",
            "15:06:42 |     beam_block_full_context: True\n",
            "15:06:42 |     beam_block_list_filename: None\n",
            "15:06:42 |     beam_block_ngram: -1\n",
            "15:06:42 |     beam_context_block_ngram: -1\n",
            "15:06:42 |     beam_delay: 30\n",
            "15:06:42 |     beam_length_penalty: 0.65\n",
            "15:06:42 |     beam_min_length: 1\n",
            "15:06:42 |     beam_size: 1\n",
            "15:06:42 |     betas: '[0.9, 0.999]'\n",
            "15:06:42 |     bpe_add_prefix_space: None\n",
            "15:06:42 |     bpe_debug: False\n",
            "15:06:42 |     bpe_dropout: None\n",
            "15:06:42 |     bpe_merge: None\n",
            "15:06:42 |     bpe_vocab: None\n",
            "15:06:42 |     checkpoint_activations: False\n",
            "15:06:42 |     compute_tokenized_bleu: False\n",
            "15:06:42 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "15:06:42 |     datatype: train\n",
            "15:06:42 |     delimiter: '\\n'\n",
            "15:06:42 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "15:06:42 |     dict_endtoken: __end__\n",
            "15:06:42 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m-double-sided/model.dict\n",
            "15:06:42 |     dict_include_test: False\n",
            "15:06:42 |     dict_include_valid: False\n",
            "15:06:42 |     dict_initpath: None\n",
            "15:06:42 |     dict_language: english\n",
            "15:06:42 |     dict_loaded: True\n",
            "15:06:42 |     dict_lower: True\n",
            "15:06:42 |     dict_max_ngram_size: -1\n",
            "15:06:42 |     dict_maxexs: -1\n",
            "15:06:42 |     dict_maxtokens: -1\n",
            "15:06:42 |     dict_minfreq: 0\n",
            "15:06:42 |     dict_nulltoken: __null__\n",
            "15:06:42 |     dict_starttoken: __start__\n",
            "15:06:42 |     dict_textfields: text,labels\n",
            "15:06:42 |     dict_tokenizer: bpe\n",
            "15:06:42 |     dict_unktoken: __unk__\n",
            "15:06:42 |     display_add_fields: \n",
            "15:06:42 |     display_examples: False\n",
            "15:06:42 |     download_path: None\n",
            "15:06:42 |     dropout: 0.0\n",
            "15:06:42 |     dynamic_batching: full\n",
            "15:06:42 |     embedding_projection: random\n",
            "15:06:42 |     embedding_size: 512\n",
            "15:06:42 |     embedding_type: random\n",
            "15:06:42 |     embeddings_scale: True\n",
            "15:06:42 |     eval_batchsize: None\n",
            "15:06:42 |     eval_dynamic_batching: None\n",
            "15:06:42 |     evaltask: None\n",
            "15:06:42 |     ffn_size: 2048\n",
            "15:06:42 |     final_extra_opt: \n",
            "15:06:42 |     force_fp16_tokens: True\n",
            "15:06:42 |     fp16: True\n",
            "15:06:42 |     fp16_impl: mem_efficient\n",
            "15:06:42 |     gpu: -1\n",
            "15:06:42 |     gradient_clip: 0.1\n",
            "15:06:42 |     hide_labels: False\n",
            "15:06:42 |     history_add_global_end_token: None\n",
            "15:06:42 |     history_reversed: False\n",
            "15:06:42 |     history_size: -1\n",
            "15:06:42 |     image_cropsize: 224\n",
            "15:06:42 |     image_mode: raw\n",
            "15:06:42 |     image_size: 256\n",
            "15:06:42 |     inference: greedy\n",
            "15:06:42 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model\n",
            "15:06:42 |     init_opt: None\n",
            "15:06:42 |     interactive_mode: False\n",
            "15:06:42 |     invsqrt_lr_decay_gamma: -1\n",
            "15:06:42 |     is_debug: False\n",
            "15:06:42 |     label_truncate: 128\n",
            "15:06:42 |     learn_positional_embeddings: False\n",
            "15:06:42 |     learningrate: 1e-05\n",
            "15:06:42 |     log_every_n_secs: 60.0\n",
            "15:06:42 |     log_every_n_steps: 50\n",
            "15:06:42 |     log_keep_fields: all\n",
            "15:06:42 |     loglevel: info\n",
            "15:06:42 |     lr_scheduler: reduceonplateau\n",
            "15:06:42 |     lr_scheduler_decay: 0.5\n",
            "15:06:43 |     lr_scheduler_patience: 3\n",
            "15:06:43 |     max_train_steps: -1\n",
            "15:06:43 |     max_train_time: -1\n",
            "15:06:43 |     metrics: default\n",
            "15:06:43 |     model: transformer/generator\n",
            "15:06:43 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m-double-sided/model\n",
            "15:06:43 |     model_parallel: False\n",
            "15:06:43 |     momentum: 0\n",
            "15:06:43 |     multitask_weights: '[1.0, 3.0, 3.0]'\n",
            "15:06:43 |     mutators: None\n",
            "15:06:43 |     n_decoder_layers: -1\n",
            "15:06:43 |     n_encoder_layers: -1\n",
            "15:06:43 |     n_heads: 16\n",
            "15:06:43 |     n_layers: 8\n",
            "15:06:43 |     n_positions: 512\n",
            "15:06:43 |     n_segments: 0\n",
            "15:06:43 |     nesterov: True\n",
            "15:06:43 |     no_cuda: False\n",
            "15:06:43 |     num_epochs: 5.0\n",
            "15:06:43 |     num_examples: 20\n",
            "15:06:43 |     num_workers: 0\n",
            "15:06:43 |     nus: [0.7]\n",
            "15:06:43 |     optimizer: mem_eff_adam\n",
            "15:06:43 |     output_scaling: 1.0\n",
            "15:06:43 |     override: \"{'task': 'french_blended_skill_talk', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m-double-sided/model', 'num_examples': '20', 'skip_generation': False}\"\n",
            "15:06:43 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "15:06:43 |     person_tokens: False\n",
            "15:06:43 |     rank_candidates: False\n",
            "15:06:43 |     relu_dropout: 0.0\n",
            "15:06:43 |     save_after_valid: False\n",
            "15:06:43 |     save_every_n_secs: -1\n",
            "15:06:43 |     save_format: conversations\n",
            "15:06:43 |     share_word_embeddings: True\n",
            "15:06:43 |     short_final_eval: False\n",
            "15:06:43 |     skip_generation: False\n",
            "15:06:43 |     special_tok_lst: None\n",
            "15:06:43 |     split_lines: False\n",
            "15:06:43 |     starttime: May25_08-36\n",
            "15:06:43 |     task: french_blended_skill_talk\n",
            "15:06:43 |     temperature: 1.0\n",
            "15:06:43 |     tensorboard_log: False\n",
            "15:06:43 |     tensorboard_logdir: None\n",
            "15:06:43 |     text_truncate: 512\n",
            "15:06:43 |     topk: 10\n",
            "15:06:43 |     topp: 0.9\n",
            "15:06:43 |     truncate: -1\n",
            "15:06:43 |     update_freq: 1\n",
            "15:06:43 |     use_reply: label\n",
            "15:06:43 |     validation_cutoff: 1.0\n",
            "15:06:43 |     validation_every_n_epochs: 0.25\n",
            "15:06:43 |     validation_every_n_secs: -1\n",
            "15:06:43 |     validation_every_n_steps: -1\n",
            "15:06:43 |     validation_max_exs: -1\n",
            "15:06:43 |     validation_metric: ppl\n",
            "15:06:43 |     validation_metric_mode: min\n",
            "15:06:43 |     validation_patience: 10\n",
            "15:06:43 |     validation_share_agent: False\n",
            "15:06:43 |     variant: xlm\n",
            "15:06:43 |     verbose: False\n",
            "15:06:43 |     wandb_entity: None\n",
            "15:06:43 |     wandb_log: False\n",
            "15:06:43 |     wandb_name: None\n",
            "15:06:43 |     wandb_project: None\n",
            "15:06:43 |     warmup_rate: 0.0001\n",
            "15:06:43 |     warmup_updates: 100\n",
            "15:06:43 |     weight_decay: None\n",
            "15:06:43 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk- - -\u001b[0;0m\n",
            "\u001b[0mCela semble dangereux. Cela vaut-il la peine de faire un travail aussi dangereux ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Wekk, c'est bon si vous êtes bien formé.  Il y a trois niveaux \u001b[0;0m\n",
            "\u001b[0;95m     model: oui , c ' est vrai . je ne sais pas si je peux m ' en empêcher .\u001b[0;0m\n",
            "\u001b[0mA quel niveau êtes-vous ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'ai reçu une formation sur le tas lorsque j'ai commencé à travailler.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un grand fan de la musique country .\u001b[0;0m\n",
            "\u001b[0mC'est génial ! Depuis combien de temps faites-vous ce travail ? \u001b[0;0m\n",
            "\u001b[1;94m    labels: Depuis un bon nombre d'années maintenant.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un étudiant de la marine .\u001b[0;0m\n",
            "\u001b[0mOn dirait que ça peut être un travail dangereux parfois.\u001b[0;0m\n",
            "\u001b[1;94m    labels: C'est vrai, surtout si vous ne prenez pas les mesures appropriées.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui , c ' est vrai .\u001b[0;0m\n",
            "\u001b[0mEh bien, vous êtes entré dans un métier qui vous garantira toujours du travail.\u001b[0;0m\n",
            "\u001b[1;94m    labels: C'est vrai, surtout dans cette économie\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un peu déçu , mais je suis un peu plus âgé que moi .\u001b[0;0m\n",
            "\u001b[0mJ'ai moi-même un emploi assez stable, mais surtout du travail de bureau.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Est-ce que ça vous plaît ?\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est bien , je suis content que tu aies pu être stable .\u001b[0;0m\n",
            "\u001b[0mJe le fais vraiment, mais parfois j'aimerais être plus actif et debout.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je vous comprends. S'étirer le long des couloirs\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est bien , je suis sûr que vous vous êtes amusés .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk- - -\u001b[0;0m\n",
            "\u001b[0mWekk, c'est bon si vous êtes bien formé.  Il y a trois niveaux \u001b[0;0m\n",
            "\u001b[1;94m    labels: A quel niveau êtes-vous ?\u001b[0;0m\n",
            "\u001b[0;95m     model: oui , je suis un grand fan de la musique .\u001b[0;0m\n",
            "\u001b[0mJ'ai reçu une formation sur le tas lorsque j'ai commencé à travailler.\u001b[0;0m\n",
            "\u001b[1;94m    labels: C'est génial ! Depuis combien de temps faites-vous ce travail ? \u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est génial . je suis sûr que vous avez __unk__ is à faire ça .\u001b[0;0m\n",
            "\u001b[0mDepuis un bon nombre d'années maintenant.\u001b[0;0m\n",
            "\u001b[1;94m    labels: On dirait que ça peut être un travail dangereux parfois.\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est génial ! je suis sûr que vous avez beaucoup de temps pour travailler .\u001b[0;0m\n",
            "\u001b[0mC'est vrai, surtout si vous ne prenez pas les mesures appropriées.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Eh bien, vous êtes entré dans un métier qui vous garantira toujours du travail.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis d ' accord . je suis un peu déçu .\u001b[0;0m\n",
            "\u001b[0mC'est vrai, surtout dans cette économie\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'ai moi-même un emploi assez stable, mais surtout du travail de bureau.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis sûr que vous avez beaucoup de chance .\u001b[0;0m\n",
            "\u001b[0mEst-ce que ça vous plaît ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je le fais vraiment, mais parfois j'aimerais être plus actif et debout.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui , je suis un peu déçu .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk- - -\u001b[0;0m\n",
            "\u001b[0mJe me sens très bien dans ma relation avec mon partenaire. Mon partenaire apprécie mon opinion sur les choses.\u001b[0;0m\n",
            "\u001b[1;94m    labels: ok, si doux\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est bien . je suis sûr que tu es très fier de lui .\u001b[0;0m\n",
            "\u001b[0mEt vous, quelqu'un vers qui vous pouvez vous tourner ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Bien sûr, j'ai des amis avec qui je suis toujours libre de partager.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne suis pas sûr .\u001b[0;0m\n",
            "\u001b[0mJ'aime faire du vélo tout terrain avec mes amis.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je n'ai jamais fait ça.  Je ne suis pas vraiment du genre à faire de l'activité physique, mais je serais prêt à essayer, j'imagine.\u001b[0;0m\n",
            "\u001b[0;95m     model: j ' aime aussi faire du vélo .\u001b[0;0m\n",
            "\u001b[0mJ'aime aussi aller prendre un thé à bulles, c'est tellement amusant.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Ça, c'est plus mon truc. Ça a un goût un peu étrange, mais j'aime ça.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne suis pas sûr de ce que vous aimez faire .\u001b[0;0m\n",
            "\u001b[0mExactement, c'est bizarre mais je continue à en redemander. Tellement étrange.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Oui, c'est juste une de ces choses. Une fois que vous commencez, vous ne pouvez plus vous arrêter.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis sûr que vous avez __unk__ is à faire ça .\u001b[0;0m\n",
            "\u001b[0mY a-t-il quelque chose que vous aimez vraiment faire ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'aime me détendre avec des amis, boire un petit verre, ce genre de choses.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne suis pas sûr , mais je suis sûr que je vais faire ça .\u001b[0;0m\n",
            "\u001b[0mÇa semble être une chose assez cool à faire, je suis assez décontracté aussi.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Yea. Je veux dire, le monde bouge si vite, parfois vous devez juste prendre du recul et laisser le passé s'accomplir.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui , c ' est vrai .\u001b[0;0m\n"
          ]
        }
      ],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='french_blended_skill_talk',\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap-cP0uzFF4y",
        "outputId": "d35ec469-4e4a-40ef-f059-ecad54e1ee19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12:46:14 | Opt:\n",
            "12:46:14 |     allow_missing_init_opts: False\n",
            "12:46:14 |     batchsize: 1\n",
            "12:46:14 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "12:46:14 |     datatype: train:ordered\n",
            "12:46:14 |     dict_class: None\n",
            "12:46:14 |     display_add_fields: \n",
            "12:46:14 |     download_path: None\n",
            "12:46:14 |     dynamic_batching: None\n",
            "12:46:14 |     fromfile_datapath: copied_dataset_french_bst/\n",
            "12:46:14 |     fromfile_datatype_extension: True\n",
            "12:46:14 |     hide_labels: False\n",
            "12:46:14 |     ignore_agent_reply: True\n",
            "12:46:14 |     image_cropsize: 224\n",
            "12:46:14 |     image_mode: raw\n",
            "12:46:14 |     image_size: 256\n",
            "12:46:14 |     init_model: None\n",
            "12:46:14 |     init_opt: None\n",
            "12:46:14 |     is_debug: False\n",
            "12:46:14 |     loglevel: info\n",
            "12:46:14 |     max_display_len: 1000\n",
            "12:46:14 |     model: None\n",
            "12:46:14 |     model_file: None\n",
            "12:46:14 |     multitask_weights: [1]\n",
            "12:46:14 |     mutators: None\n",
            "12:46:14 |     num_examples: 20\n",
            "12:46:14 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'copied_dataset_french_bst/', 'fromfile_datatype_extension': True, 'num_examples': 20}\"\n",
            "12:46:14 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "12:46:14 |     starttime: May24_12-46\n",
            "12:46:14 |     task: fromfile:parlaiformat\n",
            "12:46:14 |     verbose: False\n",
            "12:46:14 | creating task(s): fromfile:parlaiformat\n",
            "12:46:14 | Loading ParlAI text data: copied_dataset_french_bst/_train.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: copied_dataset_french_bst/_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mJ'aime la musique live, c'est pourquoi j'essaie d'aller aux concerts.\u001b[0;0m\n",
            "   \u001b[1;94mMoi aussi. Qu'est-ce que tu aimes ?\u001b[0;0m\n",
            "\u001b[0mJ'aime jouer la comédie, j'espère être un acteur, et vous ?\u001b[0;0m\n",
            "   \u001b[1;94mC'est bon. Vous avez des enfants ?\u001b[0;0m\n",
            "\u001b[0mNon, mais un jour.\u001b[0;0m\n",
            "   \u001b[1;94mc'est bien. J'ai 2\u001b[0;0m\n",
            "\u001b[0mLorsque j'aurai terminé mes études, je compte fonder une famille.\u001b[0;0m\n",
            "   \u001b[1;94mc'est génial ! tu seras prête\u001b[0;0m\n",
            "\u001b[0mJe l'espère, quel âge ont vos enfants ?\u001b[0;0m\n",
            "   \u001b[1;94m5 & 7. Ils me prennent beaucoup de temps.\u001b[0;0m\n",
            "\u001b[0mJ'imagine. Je suis sûr qu'ils sont de grands enfants.\u001b[0;0m\n",
            "   \u001b[1;94mheureusement, ils aiment les fleurs tout autant que moi. Nous passons beaucoup de temps dans le jardin.\u001b[0;0m\n",
            "\u001b[0mJ'aimerais avoir plus de temps pour faire ce genre de choses. L'école de médecine est épuisante. \u001b[0;0m\n",
            "   \u001b[1;94mOn dirait bien. As-tu trouvé un travail d'actrice, cependant ?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: copied_dataset_french_bst/_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mMoi aussi. Qu'est-ce que tu aimes ?\u001b[0;0m\n",
            "   \u001b[1;94mJ'aime jouer la comédie, j'espère être un acteur, et vous ?\u001b[0;0m\n",
            "\u001b[0mC'est bon. Vous avez des enfants ?\u001b[0;0m\n",
            "   \u001b[1;94mNon, mais un jour.\u001b[0;0m\n",
            "\u001b[0mc'est bien. J'ai 2\u001b[0;0m\n",
            "   \u001b[1;94mLorsque j'aurai terminé mes études, je compte fonder une famille.\u001b[0;0m\n",
            "\u001b[0mc'est génial ! tu seras prête\u001b[0;0m\n",
            "   \u001b[1;94mJe l'espère, quel âge ont vos enfants ?\u001b[0;0m\n",
            "\u001b[0m5 & 7. Ils me prennent beaucoup de temps.\u001b[0;0m\n",
            "   \u001b[1;94mJ'imagine. Je suis sûr qu'ils sont de grands enfants.\u001b[0;0m\n",
            "\u001b[0mheureusement, ils aiment les fleurs tout autant que moi. Nous passons beaucoup de temps dans le jardin.\u001b[0;0m\n",
            "   \u001b[1;94mJ'aimerais avoir plus de temps pour faire ce genre de choses. L'école de médecine est épuisante. \u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: copied_dataset_french_bst/_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mOh, j'adore les lasagnes. Je fais mes propres nouilles ainsi que la sauce. \u001b[0;0m\n",
            "   \u001b[1;94mWow.  C'est incroyable.  J'ai lu que les lasagnes sont nées en Italie au Moyen-âge.  \u001b[0;0m\n",
            "\u001b[0mOh vraiment ? C'est intéressant. En fait, je suis moi-même italien.\u001b[0;0m\n",
            "   \u001b[1;94mGénial. Moi et mon partenaire venons d'acheter une maison. Je suis impatient de cuisiner dans ma cuisine.\u001b[0;0m\n",
            "\u001b[0mDéménager dans un nouvel endroit peut être très amusant. Êtes-vous un bon cuisinier ?\u001b[0;0m\n",
            "   \u001b[1;94mJ'aime à le penser. J'aime aussi faire du café pour le plaisir après le repas.\u001b[0;0m\n",
            "\u001b[0mMmm. Ça a l'air délicieux en ce moment.\u001b[0;0m\n",
            "   \u001b[1;94mQu'est-ce que vous aimez faire ?\u001b[0;0m\n",
            "\u001b[0mEh bien j'aime les tatouages et les piercings, je travaille sur mon prochain en ce moment.\u001b[0;0m\n",
            "   \u001b[1;94mLes piercings sont cool. Mais je n'ai pas de tatouages. J'ai trop peur. Je veux en avoir\u001b[0;0m\n",
            "\u001b[0mQue prendriez-vous ?\u001b[0;0m\n",
            "   \u001b[1;94mPeut-être quelque chose pour mes enfants. J'ai toujours voulu un symbole d'anarchie.\u001b[0;0m\n",
            "\u001b[0mHaha c'est une idée cool.\u001b[0;0m\n",
            "   \u001b[1;94mJ'aime penser que je suis cool aussi. Avec un peu de chance, un jour.\u001b[0;0m\n",
            "12:46:15 | loaded 9638 episodes with a total of 59700 examples\n"
          ]
        }
      ],
      "source": [
        "# from parlai.scripts.display_data import DisplayData\n",
        "# DisplayData.main(task='empathetic_dialogues', num_examples=10)\n",
        "\n",
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(\n",
        "    task='fromfile:parlaiformat',\n",
        "    fromfile_datapath='copied_dataset_french_bst/',\n",
        "    fromfile_datatype_extension=True,\n",
        "    # model_file= f'{finetuned_model_path}/model',\n",
        "    # dict_file= dict_file,\n",
        "\n",
        "    num_examples=20,\n",
        "    # skip_generation=False,\n",
        "\n",
        "    # beam_min_length= 20,\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_size= 10,\n",
        "\n",
        "    # inference= \"beam\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lVPS6p4XzPh4"
      ],
      "machine_shape": "hm",
      "name": "blender-finetuning-with-reddit.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOZeaUyb0HcrxEHGSZRTasx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}