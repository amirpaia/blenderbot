{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirpaia/blenderbot/blob/main/blender_finetuning_with_reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SONwSWMp6qPv"
      },
      "source": [
        "# 0.Installing prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d-SZ_On6Kxg",
        "outputId": "0dde2085-926e-4ebb-8501-391f598c8c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun 22 13:42:03 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BegaSUz6iUz",
        "outputId": "d7694d61-ddc6-46e8-9416-44254926e4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SzGRXHQ6kDQ",
        "outputId": "75711054-e4d8-43dd-a21e-bae8a2d4ff56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "mydrive_path = '/content/drive/MyDrive/colabs/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWDakYmy6mIQ",
        "outputId": "a9ce74fb-a257-4b52-ee1e-564ad45e3860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 79 kB 8.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 962 kB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25h  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.28.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.20.1\n"
          ]
        }
      ],
      "source": [
        "# !pip uninstall -q parlai\n",
        "!pip install -q parlai\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIJEq9_r63hi"
      },
      "source": [
        "# 1.Preparing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R-fgBUdcPX5"
      },
      "source": [
        "## Genreal Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47u8M3RK65m_",
        "outputId": "011dfdee-7f27-4f0c-8a22-93272c2c3dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n",
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def transfer_list_of_turns_to_dialog(d):\n",
        "    if len(d)%2 !=0: d = d[:-1]\n",
        "    t = \"\"\n",
        "    for i in range(0,len(d),2):\n",
        "        u1 = d[i]\n",
        "        u2 = d[i+1]\n",
        "\n",
        "        if (i+2) != len(d):\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "        else:\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "    return t\n",
        "\n",
        "def transfer_list_of_pairs_to_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, text_label_pair in enumerate(d):\n",
        "    u1 = text_label_pair[0]\n",
        "    u2 = text_label_pair[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "\n",
        "def convert_parlai_format_to_list_of_turns(lines):\n",
        "    result = []\n",
        "    for line in lines:\n",
        "        text_label = line.split(\"\\t\")\n",
        "        result.append(text_label[0].replace(\"text:\", \"\"))\n",
        "        result.append(text_label[1].replace(\"labels:\", \"\").replace(\"\\n\",\"\"))\n",
        "    return result\n",
        "\n",
        "t = ['hello','how are you','good','bye','test']\n",
        "print(transfer_list_of_turns_to_dialog(t))\n",
        "\n",
        "t = [['hello','how are you'],['good','bye']]\n",
        "print(transfer_list_of_pairs_to_dialog(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE45ZyKC8WjC"
      },
      "source": [
        "## French Reddit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8ZmSJtQG8Y1o",
        "outputId": "9f65f53d-b173-434e-cc58-588222cba466"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data_path = f\"{mydrive_path}aliae-workspace/datasets/french_reddit_LELU/\"\n",
        "data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UexnzWURWKSI",
        "outputId": "40041ff4-a1d0-4984-e2b4-8a3365170f04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text:Désinformation ! Amnesty, complice de la fachosphère !\\tlabels:Ah non tu te trompes, Amnesty est financé par Soros, complice du nouvel ordre mondial et des lobbies crypto-massonique et reptilo-sionnistes!\\tepisode_done:True\\n',\n",
              " \"text:Le tirage au sort a mis Fillon en dernier sur les panneaux, mais curieusement ils ont placé les panneaux de droite à gauche...\\tlabels:A coté de chez moi, des petits malins ont tagué des barreaux de prison sur fillon. �� Edit: Et barré tous les autres d'une croix sauf MLP qui a eu un gros coeur autour de la tête.. Certainement du brigading pro-peluche. �� \\tepisode_done:True\\n\",\n",
              " 'text:Le croissant fertile.\\tlabels:Il y a officiellement un nom pour cette région : la banane bleue! \\tepisode_done:True\\n',\n",
              " \"text:C'est la ruine ! Vous m'entendez ? LA RUINE !\\tlabels:c'est ça enfin je dépense près de 50€ par semaines en manga :q\\tepisode_done:True\\n\",\n",
              " 'text:Du coup je sais plus, c\\'est qui les racistes, nous les \"sales blancs\", ou eux ?\\tlabels:Les racistes sont les blancs qui ont des yeux marrons et ne se sentent pas français.\\tepisode_done:True\\n',\n",
              " \"text: Arrête de télécharger illégalement des choses françaisesPas que. J'ai été gaulé pour Amy Winehouse.\\tlabels:Le film ou la musique ?La musique semble être le truc le plus surveillé par Hadopi.\\tepisode_done:True\\n\",\n",
              " \"text:Y a quand même une bonne partie de la communauté scientifique (et je dis pas ça en l'air) qui est bien plus pessimiste que ça sur le réchauffement climatique. Y a un paquet de projections maintenant qui tablent sur 4 à 6°C d'augmentation, et c'est le concenssus général que la fenètre d'action pour les 2°C est passée.Alors certes, il faut commencer quelque part, mais quand la coque du bateau à un trou béant, si tu te ramènes avec un verre d'eau pour écopper, tu pourrais aussi bien faire autre chose. Je crois que même la nouvelle génération, pourtant sensibilisée à ça, ne se rend pas compte de la gravité de la situation.Après, bon, on va pas bouder un truc qui va dans le bon sens, c'est sur. \\tlabels: Je crois que même la nouvelle génération, pourtant sensibilisée à ça, ne se rend pas compte de la gravité de la situation.De quelle nouvelle génération tu parles ?\\n\",\n",
              " \"text:Pour être très large, disons les moins de 40 ans ?\\tlabels:C'est plus large que l'Océan Pacifique là !  Franchement je suis pas si sûr que ça qu'ils soient vraiment sensibilisés. Par exemple, perso j'ai 21 ans, je n'ai **jamais** entendu parler d'écologie (que ce soit par les profs ou les camarades) pendant tout mon parcours scolaire.\\tepisode_done:True\\n\",\n",
              " \"text:Hé mon père s'appelle Christian ,je suis assez blessé par ton affirmation !\\tlabels:Moi c'est Chritiano\\tepisode_done:True\\n\",\n",
              " \"text: Et avec 25%, le pays est en crise et vote extrême-gauche apparemment.C'est quand le pays en question n'a pas subi une immigration massive. L'extrême-droite en France monte pour des raisons identitaires.\\tlabels:Faux et cela se voit très bien si tu superposes cette carte avec celle du FN. Elles sont presque identique. \\tepisode_done:True\\n\"]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "with open(f\"{data_path}/data_train.txt\") as f:\n",
        "    lines = f.readlines()\n",
        "lines[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGoTqdFD7x_3"
      },
      "source": [
        "# 2.Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "65rK6pfj70Px",
        "outputId": "ab9e3a8d-0225-45f0-fb80-053027f2c259"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-90m/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# finetuned_model_path = f'{mydrive_path}blender-models/finetuned-reddit-400m/'\n",
        "finetuned_model_path = f'{mydrive_path}blender-models/finetuned-reddit_LELU-90m/'\n",
        "# init_model = 'zoo:blender/blender_400Mdistill/model'\n",
        "# dict_file  = 'zoo:blender/blender_400Mdistill/model.dict'\n",
        "init_model = 'zoo:blender/blender_90M/model'\n",
        "dict_file  = 'zoo:blender/blender_90M/model.dict'\n",
        "finetuned_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7vLiYpF8HbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e416ca55-fc18-4ae6-ac4d-4f7f3cd57c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14:14:05 | building dictionary first...\n",
            "14:14:05 | \u001b[33mOverriding opt[\"dict_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict (previously: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-90m/model.dict)\u001b[0m\n",
            "14:14:05 | \u001b[33mOverriding opt[\"validation_every_n_epochs\"] to 0.25 (previously: 0.05)\u001b[0m\n",
            "14:14:05 | \u001b[33mOverriding opt[\"num_epochs\"] to 5.0 (previously: 2.0)\u001b[0m\n",
            "14:14:05 | \u001b[33mOverriding opt[\"save_after_valid\"] to True (previously: False)\u001b[0m\n",
            "14:14:05 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "14:14:05 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: full,is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/data,fromfile_datatype_extension: True,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,beam_block_full_context: True,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,dict_loaded: True,verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "14:14:05 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues --numthreads 1 --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 16 --num-epochs -1 --save-every-n-secs 60.0 --validation-max-exs 20000 --validation-patience 15 --log-every-n-secs 2 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --num-topics 5 --train-experiencer-only False --dropout 0.1 --learn-positional-embeddings True --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --skip-generation False --inference beam --fp16-impl apex --force-fp16-tokens False --optimizer adamax --learningrate 7.5e-06 --max-lr-steps -1 --warmup-updates -1 --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
            "14:14:05 | Using CUDA\n",
            "14:14:05 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-90m/model.dict\n",
            "14:14:05 | num words = 54944\n",
            "14:14:07 | Total parameters: 87,508,992 (86,984,704 trainable)\n",
            "14:14:07 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-90m/model\n",
            "14:14:10 | Opt:\n",
            "14:14:10 |     activation: gelu\n",
            "14:14:10 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "14:14:10 |     adam_eps: 1e-08\n",
            "14:14:10 |     add_p1_after_newln: False\n",
            "14:14:10 |     aggregate_micro: False\n",
            "14:14:10 |     allow_missing_init_opts: False\n",
            "14:14:10 |     attention_dropout: 0.0\n",
            "14:14:10 |     batchsize: 8\n",
            "14:14:10 |     beam_block_full_context: True\n",
            "14:14:10 |     beam_block_list_filename: None\n",
            "14:14:10 |     beam_block_ngram: -1\n",
            "14:14:10 |     beam_context_block_ngram: -1\n",
            "14:14:10 |     beam_delay: 30\n",
            "14:14:10 |     beam_length_penalty: 0.65\n",
            "14:14:10 |     beam_min_length: 1\n",
            "14:14:10 |     beam_size: 1\n",
            "14:14:10 |     betas: '[0.9, 0.999]'\n",
            "14:14:10 |     bpe_add_prefix_space: None\n",
            "14:14:10 |     bpe_debug: False\n",
            "14:14:10 |     bpe_dropout: None\n",
            "14:14:10 |     bpe_merge: None\n",
            "14:14:10 |     bpe_vocab: None\n",
            "14:14:10 |     checkpoint_activations: False\n",
            "14:14:10 |     compute_tokenized_bleu: False\n",
            "14:14:10 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "14:14:10 |     datatype: train\n",
            "14:14:10 |     delimiter: '\\n'\n",
            "14:14:10 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "14:14:10 |     dict_endtoken: __end__\n",
            "14:14:10 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-90m/model.dict\n",
            "14:14:10 |     dict_include_test: False\n",
            "14:14:10 |     dict_include_valid: False\n",
            "14:14:10 |     dict_initpath: None\n",
            "14:14:10 |     dict_language: english\n",
            "14:14:10 |     dict_loaded: True\n",
            "14:14:10 |     dict_lower: True\n",
            "14:14:10 |     dict_max_ngram_size: -1\n",
            "14:14:10 |     dict_maxexs: -1\n",
            "14:14:10 |     dict_maxtokens: -1\n",
            "14:14:10 |     dict_minfreq: 0\n",
            "14:14:10 |     dict_nulltoken: __null__\n",
            "14:14:10 |     dict_starttoken: __start__\n",
            "14:14:10 |     dict_textfields: text,labels\n",
            "14:14:10 |     dict_tokenizer: bpe\n",
            "14:14:10 |     dict_unktoken: __unk__\n",
            "14:14:10 |     display_examples: False\n",
            "14:14:10 |     download_path: None\n",
            "14:14:10 |     dropout: 0.0\n",
            "14:14:10 |     dynamic_batching: full\n",
            "14:14:10 |     embedding_projection: random\n",
            "14:14:10 |     embedding_size: 512\n",
            "14:14:10 |     embedding_type: random\n",
            "14:14:10 |     embeddings_scale: True\n",
            "14:14:10 |     eval_batchsize: None\n",
            "14:14:10 |     eval_dynamic_batching: None\n",
            "14:14:10 |     evaltask: None\n",
            "14:14:10 |     ffn_size: 2048\n",
            "14:14:10 |     final_extra_opt: \n",
            "14:14:10 |     force_fp16_tokens: True\n",
            "14:14:10 |     fp16: True\n",
            "14:14:10 |     fp16_impl: mem_efficient\n",
            "14:14:10 |     fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/data\n",
            "14:14:10 |     fromfile_datatype_extension: True\n",
            "14:14:10 |     gpu: -1\n",
            "14:14:10 |     gradient_clip: 0.1\n",
            "14:14:10 |     hide_labels: False\n",
            "14:14:10 |     history_add_global_end_token: None\n",
            "14:14:10 |     history_reversed: False\n",
            "14:14:10 |     history_size: -1\n",
            "14:14:10 |     image_cropsize: 224\n",
            "14:14:10 |     image_mode: raw\n",
            "14:14:10 |     image_size: 256\n",
            "14:14:10 |     inference: greedy\n",
            "14:14:10 |     init_model: zoo:blender/blender_90M/model\n",
            "14:14:10 |     init_opt: None\n",
            "14:14:10 |     interactive_mode: False\n",
            "14:14:10 |     invsqrt_lr_decay_gamma: -1\n",
            "14:14:10 |     is_debug: False\n",
            "14:14:10 |     label_truncate: 128\n",
            "14:14:10 |     learn_positional_embeddings: False\n",
            "14:14:10 |     learningrate: 1e-05\n",
            "14:14:10 |     load_from_checkpoint: True\n",
            "14:14:10 |     log_every_n_secs: 60.0\n",
            "14:14:10 |     log_every_n_steps: 50\n",
            "14:14:10 |     log_keep_fields: all\n",
            "14:14:10 |     loglevel: info\n",
            "14:14:10 |     lr_scheduler: reduceonplateau\n",
            "14:14:10 |     lr_scheduler_decay: 0.5\n",
            "14:14:10 |     lr_scheduler_patience: 3\n",
            "14:14:10 |     max_train_steps: -1\n",
            "14:14:10 |     max_train_time: -1\n",
            "14:14:10 |     metrics: default\n",
            "14:14:10 |     model: transformer/generator\n",
            "14:14:10 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-90m/model\n",
            "14:14:10 |     model_parallel: False\n",
            "14:14:10 |     momentum: 0\n",
            "14:14:10 |     multitask_weights: [1]\n",
            "14:14:10 |     mutators: None\n",
            "14:14:10 |     n_decoder_layers: -1\n",
            "14:14:10 |     n_encoder_layers: -1\n",
            "14:14:10 |     n_heads: 16\n",
            "14:14:10 |     n_layers: 8\n",
            "14:14:10 |     n_positions: 512\n",
            "14:14:10 |     n_segments: 0\n",
            "14:14:10 |     nesterov: True\n",
            "14:14:10 |     no_cuda: False\n",
            "14:14:10 |     num_epochs: 5.0\n",
            "14:14:10 |     num_workers: 0\n",
            "14:14:10 |     nus: [0.7]\n",
            "14:14:10 |     optimizer: mem_eff_adam\n",
            "14:14:10 |     output_scaling: 1.0\n",
            "14:14:10 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/data', 'fromfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-90m/model', 'init_model': 'zoo:blender/blender_90M/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'validation_every_n_epochs': 0.25, 'num_epochs': 5.0, 'log_every_n_secs': 60.0, 'verbose': True, 'batchsize': 8, 'fp16': True, 'fp16_impl': 'mem_efficient', 'save_after_valid': True, 'skip_generation': True, 'validation_patience': 10, 'validation_metric': 'ppl', 'validation_metric_mode': 'min', 'dynamic_batching': 'full', 'learningrate': 1e-05, 'optimizer': 'adam', 'attention_dropout': 0.0, 'model_parallel': False, 'warmup_updates': 100}\"\n",
            "14:14:10 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "14:14:10 |     person_tokens: False\n",
            "14:14:10 |     rank_candidates: False\n",
            "14:14:10 |     relu_dropout: 0.0\n",
            "14:14:10 |     save_after_valid: True\n",
            "14:14:10 |     save_every_n_secs: -1\n",
            "14:14:10 |     save_format: conversations\n",
            "14:14:10 |     share_word_embeddings: True\n",
            "14:14:10 |     short_final_eval: False\n",
            "14:14:10 |     skip_generation: True\n",
            "14:14:10 |     special_tok_lst: None\n",
            "14:14:10 |     split_lines: False\n",
            "14:14:10 |     starttime: Jun21_07-57\n",
            "14:14:10 |     task: fromfile:parlaiformat\n",
            "14:14:10 |     temperature: 1.0\n",
            "14:14:10 |     tensorboard_log: False\n",
            "14:14:10 |     tensorboard_logdir: None\n",
            "14:14:10 |     text_truncate: 512\n",
            "14:14:10 |     topk: 10\n",
            "14:14:10 |     topp: 0.9\n",
            "14:14:10 |     truncate: -1\n",
            "14:14:10 |     update_freq: 1\n",
            "14:14:10 |     use_reply: label\n",
            "14:14:10 |     validation_cutoff: 1.0\n",
            "14:14:10 |     validation_every_n_epochs: 0.25\n",
            "14:14:10 |     validation_every_n_secs: -1\n",
            "14:14:10 |     validation_every_n_steps: -1\n",
            "14:14:10 |     validation_max_exs: -1\n",
            "14:14:10 |     validation_metric: ppl\n",
            "14:14:10 |     validation_metric_mode: min\n",
            "14:14:10 |     validation_patience: 10\n",
            "14:14:10 |     validation_share_agent: False\n",
            "14:14:10 |     variant: xlm\n",
            "14:14:10 |     verbose: True\n",
            "14:14:10 |     wandb_entity: None\n",
            "14:14:10 |     wandb_log: False\n",
            "14:14:10 |     wandb_name: None\n",
            "14:14:10 |     wandb_project: None\n",
            "14:14:10 |     warmup_rate: 0.0001\n",
            "14:14:10 |     warmup_updates: 100\n",
            "14:14:10 |     weight_decay: None\n",
            "14:14:10 |     world_logs: \n",
            "14:14:10 | creating task(s): fromfile:parlaiformat\n",
            "14:14:10 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/data_train.txt\n",
            "14:15:09 | training...\n",
            "14:15:26 | time:292s total_exs:7188 total_steps:289 epochs:0.01 time_left:114049s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   63.36     1  1855  5867       0          0 92.59 1464             16384  5.821    .5375 58.89 3.386 9.9e-06  1500  4743   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1031      7.666 29.55      .3915         0                  289 3355 10610 3.163\n",
            "\n",
            "14:15:41 | time:307s total_exs:8352 total_steps:339 epochs:0.01 time_left:103232s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   89.51     1  2084  6856       0          0 76.59 1164             16384  6.232    .5427 69.91 3.305 9.9e-06  1355  4459   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1572      11.69 27.25      .4010         0                  339 3439 11314 3.29\n",
            "\n",
            "14:15:56 | time:322s total_exs:9500 total_steps:389 epochs:0.02 time_left:95250s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   90.75     1  2051  6706  .01045      1.422 75.07 1148             16384  6.416    .5468 67.76  3.24 9.9e-06  1306  4270   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1490      10.89 25.53      .4124  .0008711                  389 3357 10975 3.27\n",
            "\n",
            "14:16:11 | time:338s total_exs:10596 total_steps:439 epochs:0.02 time_left:89379s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   96.89     1  2124  7022       0          0 72.47 1096             16384  6.442    .5292 65.86 3.233 9.9e-06  1231  4071   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1314      9.696 25.36      .4099         0                  439 3355 11092 3.307\n",
            "\n",
            "14:16:27 | time:353s total_exs:11700 total_steps:489 epochs:0.02 time_left:84631s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   94.35     1  2062  6661 .007246      .9837 71.34 1104             16384  6.385    .5468 68.18 3.216 9.9e-06  1260  4070   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1368      11.13 24.92      .4125         0                  489 3321 10730 3.231\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 90M settings\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    # task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "    # multitask_weights= \"1,3,3\",\n",
        "\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    \n",
        "    # depend on your gpu. \n",
        "    validation_every_n_epochs=0.25,\n",
        "    num_epochs = 5,\n",
        "    log_every_n_secs= 60,\n",
        "    verbose = True,\n",
        "    batchsize= 8, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    save_after_valid = True,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        "\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    attention_dropout= 0.0, \n",
        "    model_parallel= False,\n",
        "    warmup_updates=100,\n",
        "\n",
        "    # customized parameters\n",
        "    # inference= \"beam\"\n",
        "    # beam_min_length= 20,\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_size= 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxL0-1x1YsQe"
      },
      "outputs": [],
      "source": [
        "# mydrive_path = '/content/finetuned-multitask-400m-double-sided-2epochs'\n",
        "# mydrive_path = '/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-double-sided/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNZE5ta2pO-X",
        "outputId": "4db3ae34-04c3-4ffd-b2df-78ff8bca71c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22:44:36 | building dictionary first...\n",
            "22:44:36 | No model with opt yet at: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-400m/model(.opt)\n",
            "22:44:36 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: download_path: None,verbose: True,is_debug: False,datapath: /usr/local/lib/python3.7/dist-packages/data,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,world_logs: ,save_format: conversations,log_keep_fields: all,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/data,fromfile_datatype_extension: True,checkpoint_activations: False,interactive_mode: False\u001b[0m\n",
            "22:44:36 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--init-opt /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt --allow-missing-init-opts True --task blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 8 --dynamic-batching full --eval-batchsize 8 --num-epochs -1 --validation-every-n-secs 900.0 --save-after-valid True --validation-every-n-epochs -1.0 --validation-patience 20 --log-every-n-secs 10.0 --distributed-world-size 8 --port 61337 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --chosen-topic-delimiter \n",
            " --num-topics 5 --train-experiencer-only False --remove-political-convos False --teacher-model /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model --task-loss-coeff 1.0 --encoder-loss-coeff 24.0 --hidden-loss-coeff 5.0 --pred-loss-coeff 8.0 --embedding-loss-coeff 0.35 --self-attn-loss-coeff 0.6 --enc-dec-attn-loss-coeff 3.0 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --inference beam --learningrate 0.0004 --gpu 0 --bpe-vocab /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json --bpe-merge /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt --bpe-add-prefix-space True --max-lr-steps -1 --parlai-home /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI --show-advanced-args False --numthreads 1 --rank 0 --dict-loaded True\u001b[0m\n",
            "22:44:36 | Using CUDA\n",
            "22:44:36 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict\n",
            "22:44:36 | num words = 8008\n",
            "22:44:42 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "22:44:42 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model\n",
            "22:44:45 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
            "22:44:45 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
            "22:44:45 | Opt:\n",
            "22:44:45 |     activation: gelu\n",
            "22:44:45 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "22:44:45 |     adam_eps: 1e-08\n",
            "22:44:45 |     add_p1_after_newln: False\n",
            "22:44:45 |     aggregate_micro: False\n",
            "22:44:45 |     allow_missing_init_opts: False\n",
            "22:44:45 |     attention_dropout: 0.0\n",
            "22:44:45 |     batchsize: 16\n",
            "22:44:45 |     beam_block_full_context: True\n",
            "22:44:45 |     beam_block_list_filename: None\n",
            "22:44:45 |     beam_block_ngram: -1\n",
            "22:44:45 |     beam_context_block_ngram: -1\n",
            "22:44:45 |     beam_delay: 30\n",
            "22:44:45 |     beam_length_penalty: 0.65\n",
            "22:44:45 |     beam_min_length: 1\n",
            "22:44:45 |     beam_size: 1\n",
            "22:44:45 |     betas: '(0.9, 0.999)'\n",
            "22:44:45 |     bpe_add_prefix_space: None\n",
            "22:44:45 |     bpe_debug: False\n",
            "22:44:45 |     bpe_dropout: None\n",
            "22:44:45 |     bpe_merge: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict-merges.txt\n",
            "22:44:45 |     bpe_vocab: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict-vocab.json\n",
            "22:44:45 |     checkpoint_activations: False\n",
            "22:44:45 |     compute_tokenized_bleu: False\n",
            "22:44:45 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "22:44:45 |     datatype: train\n",
            "22:44:45 |     delimiter: '  '\n",
            "22:44:45 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "22:44:45 |     dict_endtoken: __end__\n",
            "22:44:45 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict\n",
            "22:44:45 |     dict_include_test: False\n",
            "22:44:45 |     dict_include_valid: False\n",
            "22:44:45 |     dict_initpath: None\n",
            "22:44:45 |     dict_language: english\n",
            "22:44:45 |     dict_loaded: True\n",
            "22:44:45 |     dict_lower: False\n",
            "22:44:45 |     dict_max_ngram_size: -1\n",
            "22:44:45 |     dict_maxexs: -1\n",
            "22:44:45 |     dict_maxtokens: -1\n",
            "22:44:45 |     dict_minfreq: 0\n",
            "22:44:45 |     dict_nulltoken: __null__\n",
            "22:44:45 |     dict_starttoken: __start__\n",
            "22:44:45 |     dict_textfields: text,labels\n",
            "22:44:45 |     dict_tokenizer: bytelevelbpe\n",
            "22:44:45 |     dict_unktoken: __unk__\n",
            "22:44:45 |     display_examples: False\n",
            "22:44:45 |     download_path: None\n",
            "22:44:45 |     dropout: 0.1\n",
            "22:44:45 |     dynamic_batching: None\n",
            "22:44:45 |     embedding_projection: random\n",
            "22:44:45 |     embedding_size: 1280\n",
            "22:44:45 |     embedding_type: random\n",
            "22:44:45 |     embeddings_scale: True\n",
            "22:44:45 |     eval_batchsize: None\n",
            "22:44:45 |     eval_dynamic_batching: None\n",
            "22:44:45 |     evaltask: None\n",
            "22:44:45 |     ffn_size: 5120\n",
            "22:44:45 |     final_extra_opt: \n",
            "22:44:45 |     force_fp16_tokens: False\n",
            "22:44:45 |     fp16: True\n",
            "22:44:45 |     fp16_impl: mem_efficient\n",
            "22:44:45 |     fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/data\n",
            "22:44:45 |     fromfile_datatype_extension: True\n",
            "22:44:45 |     gpu: -1\n",
            "22:44:45 |     gradient_clip: 0.1\n",
            "22:44:45 |     hide_labels: False\n",
            "22:44:45 |     history_add_global_end_token: end\n",
            "22:44:45 |     history_reversed: False\n",
            "22:44:45 |     history_size: -1\n",
            "22:44:45 |     image_cropsize: 224\n",
            "22:44:45 |     image_mode: raw\n",
            "22:44:45 |     image_size: 256\n",
            "22:44:45 |     inference: greedy\n",
            "22:44:45 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model\n",
            "22:44:45 |     init_opt: None\n",
            "22:44:45 |     interactive_mode: False\n",
            "22:44:45 |     invsqrt_lr_decay_gamma: -1\n",
            "22:44:45 |     is_debug: False\n",
            "22:44:45 |     label_truncate: 128\n",
            "22:44:45 |     learn_positional_embeddings: False\n",
            "22:44:45 |     learningrate: 7e-06\n",
            "22:44:45 |     load_from_checkpoint: True\n",
            "22:44:45 |     log_every_n_secs: 300.0\n",
            "22:44:45 |     log_every_n_steps: 50\n",
            "22:44:45 |     log_keep_fields: all\n",
            "22:44:45 |     loglevel: info\n",
            "22:44:45 |     lr_scheduler: reduceonplateau\n",
            "22:44:45 |     lr_scheduler_decay: 0.5\n",
            "22:44:45 |     lr_scheduler_patience: 3\n",
            "22:44:45 |     max_train_steps: -1\n",
            "22:44:45 |     max_train_time: -1\n",
            "22:44:45 |     metrics: default\n",
            "22:44:45 |     model: transformer/generator\n",
            "22:44:45 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-400m/model\n",
            "22:44:45 |     model_parallel: False\n",
            "22:44:45 |     momentum: 0\n",
            "22:44:45 |     multitask_weights: [1]\n",
            "22:44:45 |     mutators: None\n",
            "22:44:45 |     n_decoder_layers: 12\n",
            "22:44:45 |     n_encoder_layers: 2\n",
            "22:44:45 |     n_heads: 32\n",
            "22:44:45 |     n_layers: 2\n",
            "22:44:45 |     n_positions: 128\n",
            "22:44:45 |     n_segments: 0\n",
            "22:44:45 |     nesterov: True\n",
            "22:44:45 |     no_cuda: False\n",
            "22:44:45 |     num_epochs: 0.02\n",
            "22:44:45 |     num_workers: 0\n",
            "22:44:45 |     nus: (0.7,)\n",
            "22:44:45 |     optimizer: mem_eff_adam\n",
            "22:44:45 |     output_scaling: 1.0\n",
            "22:44:45 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/data', 'fromfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-400m/model', 'init_model': 'zoo:blender/blender_400Mdistill/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict', 'validation_every_n_epochs': 0.01, 'num_epochs': 0.02, 'log_every_n_secs': 300.0, 'verbose': True, 'attention_dropout': 0.0, 'batchsize': 16, 'fp16': True, 'fp16_impl': 'mem_efficient', 'embedding_size': 1280, 'ffn_size': 5120, 'variant': 'prelayernorm', 'n_heads': 32, 'n_positions': 128, 'n_encoder_layers': 2, 'n_decoder_layers': 12, 'label_truncate': 128, 'text_truncate': 128, 'truncate': 128, 'activation': 'gelu', 'history_add_global_end_token': 'end', 'delimiter': '  ', 'dict_tokenizer': 'bytelevelbpe', 'dropout': 0.1, 'learningrate': 7e-06, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'optimizer': 'mem_eff_adam', 'relu_dropout': 0.0, 'model_parallel': False, 'warmup_updates': 100, 'update_freq': 2, 'gradient_clip': 0.1, 'skip_generation': True, 'validation_patience': 10, 'validation_metric': 'ppl', 'validation_metric_mode': 'min'}\"\n",
            "22:44:45 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "22:44:45 |     person_tokens: False\n",
            "22:44:45 |     rank_candidates: False\n",
            "22:44:45 |     relu_dropout: 0.0\n",
            "22:44:45 |     save_after_valid: False\n",
            "22:44:45 |     save_every_n_secs: -1\n",
            "22:44:45 |     save_format: conversations\n",
            "22:44:45 |     share_word_embeddings: True\n",
            "22:44:45 |     short_final_eval: False\n",
            "22:44:45 |     skip_generation: True\n",
            "22:44:45 |     special_tok_lst: None\n",
            "22:44:45 |     split_lines: False\n",
            "22:44:45 |     starttime: Jun20_22-44\n",
            "22:44:45 |     task: fromfile:parlaiformat\n",
            "22:44:45 |     temperature: 1.0\n",
            "22:44:45 |     tensorboard_log: False\n",
            "22:44:45 |     tensorboard_logdir: None\n",
            "22:44:45 |     text_truncate: 128\n",
            "22:44:45 |     topk: 10\n",
            "22:44:45 |     topp: 0.9\n",
            "22:44:45 |     truncate: 128\n",
            "22:44:45 |     update_freq: 2\n",
            "22:44:45 |     use_reply: label\n",
            "22:44:45 |     validation_cutoff: 1.0\n",
            "22:44:45 |     validation_every_n_epochs: 0.01\n",
            "22:44:45 |     validation_every_n_secs: -1\n",
            "22:44:45 |     validation_every_n_steps: -1\n",
            "22:44:45 |     validation_max_exs: -1\n",
            "22:44:45 |     validation_metric: ppl\n",
            "22:44:45 |     validation_metric_mode: min\n",
            "22:44:45 |     validation_patience: 10\n",
            "22:44:45 |     validation_share_agent: False\n",
            "22:44:45 |     variant: prelayernorm\n",
            "22:44:45 |     verbose: True\n",
            "22:44:45 |     wandb_entity: None\n",
            "22:44:45 |     wandb_log: False\n",
            "22:44:45 |     wandb_name: None\n",
            "22:44:45 |     wandb_project: None\n",
            "22:44:45 |     warmup_rate: 0.0001\n",
            "22:44:45 |     warmup_updates: 100\n",
            "22:44:45 |     weight_decay: None\n",
            "22:44:45 |     world_logs: \n",
            "22:44:46 | creating task(s): fromfile:parlaiformat\n",
            "22:44:46 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/data_train.txt\n",
            "22:44:55 | training...\n",
            "22:44:56 | Overflow: setting loss scale to 65536.0\n",
            "22:44:58 | Overflow: setting loss scale to 32768.0\n",
            "22:46:09 | time:75s total_exs:1600 total_steps:50 epochs:0.00 time_left:393s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   126.7 .9600  1272  1703   .3287      47.15 21.42 1600             33423  17.74    .7156 83.39 6.081 3.5e-06  1048  1403   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1869       17.9 437.3     .08593         0                   50 2320 3106 .6695\n",
            "\n",
            "22:47:24 | time:150s total_exs:3200 total_steps:100 epochs:0.01 time_left:319s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   128.7     1  1307  1746   .3444         47 21.38 1600             32768  7.732    .7156 82.47  5.48 6.93e-06  1033  1381   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1931       17.9 239.8      .1081         0                  100 2340 3127 .6681\n",
            "\n",
            "22:48:39 | time:224s total_exs:4800 total_steps:150 epochs:0.01 time_left:244s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   129.7     1  1284  1715   .3331      49.45 21.37 1600             32768  3.708    .7156 85.51 4.935 6.93e-06  1059  1414   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .2056      19.32 139.1      .1549         0                  150 2343 3130 .6679\n",
            "\n",
            "22:48:50 | time:235s total_exs:5024 total_steps:157 epochs:0.01 time_left:233s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   107.4     1  1241  1672   .2946      29.87 21.56  224             32768  3.675    .7156 69.46 4.823 6.93e-06 938.8  1265   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1250      10.79 124.3      .1700         0                  157 2180 2937 .6739\n",
            "\n",
            "22:48:50 | creating task(s): fromfile:parlaiformat\n",
            "22:48:50 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/data_valid.txt\n",
            "22:48:52 | running eval: valid\n",
            "22:48:52 | \u001b[33m--skip-generation true produces limited metrics\u001b[0m\n",
            "23:05:41 | eval completed in 1008.79s\n",
            "23:05:41 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "     127  1286  4979   .3311      46.57 61.95 62490    .5229 81.91 4.689 6.93e-06  1030  3988   .1895      17.53 108.7   \n",
            "    token_acc  token_em  total_train_updates  tpb  tps  \n",
            "        .1823         0                  157 2315 8967\n",
            "\u001b[0m\n",
            "23:05:41 | \u001b[1;32mnew best ppl: 108.7\u001b[0m\n",
            "23:05:41 | saving best valid model: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-400m/model\n",
            "23:05:41 | Saving dictionary to /content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-400m/model.dict\n",
            "23:07:13 | time:1338s total_exs:6624 total_steps:207 epochs:0.01 time_left:685s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   129.7     1  1313  1743   .3356      47.67 21.24 1600             32768  3.499    .7165 83.36 4.706 6.93e-06  1050  1394   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1944      17.73 110.6      .1781         0                  207 2363 3137 .6638\n",
            "\n",
            "23:08:27 | time:1413s total_exs:8224 total_steps:257 epochs:0.02 time_left:308s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   119.8     1  1249  1673   .3194      41.73 21.43 1600             32768  3.553    .7165 81.59 4.628 6.93e-06  1014  1359   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1881      18.18 102.3      .1866         0                  257 2264 3032 .6697\n",
            "\n",
            "23:09:42 | time:1488s total_exs:9824 total_steps:307 epochs:0.02 time_left:29s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "     135     1  1308  1745   .3525      53.21 21.34 1600             32768  3.608    .7165 81.12 4.519 6.93e-06  1024  1366   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1825      17.12 91.7      .1982         0                  307 2332 3111 .6670\n",
            "\n",
            "23:09:52 | time:1497s total_exs:10032 total_steps:313 epochs:0.02 time_left:0s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss       lr  ltpb  ltps  \\\n",
            "   142.4     1  1339  1851   .3606      58.73 22.11  208             32768  3.514    .7165 77.66 4.518 6.93e-06  1015  1403   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "     .1587      14.22 91.68      .1927         0                  313 2354 3254 .6668\n",
            "\n",
            "23:09:52 | num_epochs completed:0.02 time elapsed:1497.0432579517365s\n",
            "23:09:52 | \u001b[33mOverriding opt[\"init_model\"] to zoo:blender/blender_400Mdistill/model (previously: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model)\u001b[0m\n",
            "23:09:52 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,world_logs: ,save_format: conversations,log_keep_fields: all,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/data,fromfile_datatype_extension: True,checkpoint_activations: False,verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "23:09:52 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--init-opt /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt --allow-missing-init-opts True --task blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 8 --dynamic-batching full --eval-batchsize 8 --num-epochs -1 --validation-every-n-secs 900.0 --save-after-valid True --validation-every-n-epochs -1.0 --validation-patience 20 --log-every-n-secs 10.0 --distributed-world-size 8 --port 61337 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --chosen-topic-delimiter \n",
            " --num-topics 5 --train-experiencer-only False --remove-political-convos False --teacher-model /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model --task-loss-coeff 1.0 --encoder-loss-coeff 24.0 --hidden-loss-coeff 5.0 --pred-loss-coeff 8.0 --embedding-loss-coeff 0.35 --self-attn-loss-coeff 0.6 --enc-dec-attn-loss-coeff 3.0 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --inference beam --force-fp16-tokens False --learningrate 0.0004 --gpu 0 --bpe-vocab /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json --bpe-merge /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt --bpe-add-prefix-space True --max-lr-steps -1 --parlai-home /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI --show-advanced-args False --numthreads 1 --rank 0\u001b[0m\n",
            "23:09:52 | Using CUDA\n",
            "23:09:52 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-400m/model.dict\n",
            "23:09:52 | num words = 8008\n",
            "23:09:59 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "23:09:59 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit_LELU-400m/model\n",
            "23:10:07 | creating task(s): fromfile:parlaiformat\n",
            "23:10:07 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/data_valid.txt\n",
            "23:10:08 | running eval: valid\n",
            "23:26:57 | eval completed in 1008.38s\n",
            "23:26:57 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "     127  1286  4981   .3311      46.57 61.97 62490    .7416 81.91 4.689 6.93e-06  1030  3990   .1895      17.53 108.7   \n",
            "    token_acc  token_em  total_train_updates  tpb  tps  \n",
            "        .1823         0                  157 2315 8971\n",
            "\u001b[0m\n",
            "23:26:57 | creating task(s): fromfile:parlaiformat\n",
            "23:26:57 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit_LELU/data_test.txt\n",
            "23:26:59 | running eval: test\n",
            "23:43:52 | eval completed in 1013.85s\n",
            "23:43:52 | \u001b[1mtest:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   127.9  1290  4992   .3322      47.18 61.87 62726    .4779 81.79 4.689 6.93e-06  1031  3988   .1895      17.33 108.7   \n",
            "    token_acc  token_em  total_train_updates  tpb  tps  \n",
            "        .1826         0                  157 2321 8980\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'clen': AverageMetric(127),\n",
              "  'ctpb': GlobalAverageMetric(1286),\n",
              "  'ctps': GlobalTimerMetric(4981),\n",
              "  'ctrunc': AverageMetric(0.3311),\n",
              "  'ctrunclen': AverageMetric(46.57),\n",
              "  'exps': GlobalTimerMetric(61.97),\n",
              "  'exs': SumMetric(6.249e+04),\n",
              "  'gpu_mem': GlobalAverageMetric(0.7416),\n",
              "  'llen': AverageMetric(81.91),\n",
              "  'loss': AverageMetric(4.689),\n",
              "  'lr': GlobalAverageMetric(6.93e-06),\n",
              "  'ltpb': GlobalAverageMetric(1030),\n",
              "  'ltps': GlobalTimerMetric(3990),\n",
              "  'ltrunc': AverageMetric(0.1895),\n",
              "  'ltrunclen': AverageMetric(17.53),\n",
              "  'ppl': PPLMetric(108.7),\n",
              "  'token_acc': AverageMetric(0.1823),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(157),\n",
              "  'tpb': GlobalAverageMetric(2315),\n",
              "  'tps': GlobalTimerMetric(8971)},\n",
              " {'clen': AverageMetric(127.9),\n",
              "  'ctpb': GlobalAverageMetric(1290),\n",
              "  'ctps': GlobalTimerMetric(4992),\n",
              "  'ctrunc': AverageMetric(0.3322),\n",
              "  'ctrunclen': AverageMetric(47.18),\n",
              "  'exps': GlobalTimerMetric(61.87),\n",
              "  'exs': SumMetric(6.273e+04),\n",
              "  'gpu_mem': GlobalAverageMetric(0.4779),\n",
              "  'llen': AverageMetric(81.79),\n",
              "  'loss': AverageMetric(4.689),\n",
              "  'lr': GlobalAverageMetric(6.93e-06),\n",
              "  'ltpb': GlobalAverageMetric(1031),\n",
              "  'ltps': GlobalTimerMetric(3988),\n",
              "  'ltrunc': AverageMetric(0.1895),\n",
              "  'ltrunclen': AverageMetric(17.33),\n",
              "  'ppl': PPLMetric(108.7),\n",
              "  'token_acc': AverageMetric(0.1826),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(157),\n",
              "  'tpb': GlobalAverageMetric(2321),\n",
              "  'tps': GlobalTimerMetric(8980)})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# 400M settings\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    # task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues\",\n",
        "    # multitask_weights= \"1,3,3\",\n",
        "\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 5,\n",
        "    log_every_n_secs= 300,\n",
        "    verbose = True,\n",
        "    attention_dropout= 0.0, \n",
        "    batchsize= 16, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    save_after_valid= True,\n",
        "\n",
        "    # arguments we get from the pretrained model. \"from recipes page for 2.7B model\" \n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    variant= \"prelayernorm\",\n",
        "    n_heads= 32, n_positions= 128, \n",
        "    n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "    dropout= 0.1,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    relu_dropout= 0.0, \n",
        "    model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    # inference = 'topk', \n",
        "    # temperature = 0.7, \n",
        "    # topk=30, \n",
        "    # beam_length_penalty=1.03\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1S0kRk3W63i"
      },
      "outputs": [],
      "source": [
        "# !cp -rv /content/finetuned-multitask-400m-double-sided-2epochs/* /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-double-sided/\n",
        "# !ls -lah /content/finetuned-multitask-400m-double-sided-2epochs/\n",
        "# !ls -lah /content/finetuned-multitask-400m-double-sided/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVPS6p4XzPh4"
      },
      "source": [
        "# 4.Display Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9FBtnZZzPPg"
      },
      "outputs": [],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    # task='french_blended_skill_talk',\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "    datatype= \"test\",\n",
        "\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    # the result of grid search on 400M model and BST dataset when inference=topk\n",
        "    beam_block_ngram= 2,\n",
        "\tbeam_context_block_ngram= 3,\n",
        "\tbeam_length_penalty= 1,\n",
        "\tbeam_min_length= 10,\n",
        "\tbeam_size= 20,\n",
        "\tinference= \"topk\",\n",
        "\ttemperature= 0.5,\n",
        "\ttopk= 20,\n",
        "\ttopp= 0.9\n",
        "\n",
        "    # # Farnaz sent me\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_min_length= 20, \n",
        "    # beam_size= 10,\n",
        "    # inference =  'topk',  \n",
        "    # topk=20, \n",
        "    # temperature = 0.5, \n",
        "    # beam_length_penalty=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap-cP0uzFF4y",
        "outputId": "2f52fc81-fe8d-49cd-cb2c-a056ea9e5e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:07:59 | Opt:\n",
            "09:07:59 |     allow_missing_init_opts: False\n",
            "09:07:59 |     batchsize: 1\n",
            "09:07:59 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "09:07:59 |     datatype: train:ordered\n",
            "09:07:59 |     dict_class: None\n",
            "09:07:59 |     display_add_fields: \n",
            "09:07:59 |     download_path: None\n",
            "09:07:59 |     dynamic_batching: None\n",
            "09:07:59 |     fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data\n",
            "09:07:59 |     fromfile_datatype_extension: True\n",
            "09:07:59 |     hide_labels: False\n",
            "09:07:59 |     ignore_agent_reply: True\n",
            "09:07:59 |     image_cropsize: 224\n",
            "09:07:59 |     image_mode: raw\n",
            "09:07:59 |     image_size: 256\n",
            "09:07:59 |     init_model: None\n",
            "09:07:59 |     init_opt: None\n",
            "09:07:59 |     is_debug: False\n",
            "09:07:59 |     loglevel: info\n",
            "09:07:59 |     max_display_len: 1000\n",
            "09:07:59 |     model: None\n",
            "09:07:59 |     model_file: None\n",
            "09:07:59 |     multitask_weights: [1]\n",
            "09:07:59 |     mutators: None\n",
            "09:07:59 |     num_examples: 20\n",
            "09:07:59 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data', 'fromfile_datatype_extension': True, 'num_examples': 20}\"\n",
            "09:07:59 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "09:07:59 |     starttime: Jun07_09-07\n",
            "09:07:59 |     task: fromfile:parlaiformat\n",
            "09:07:59 |     verbose: False\n",
            "09:07:59 | creating task(s): fromfile:parlaiformat\n",
            "09:07:59 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data_train.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mPour ceux qui jouent à LoL et qui attendaient impatiemment le URF, ça sera au final le NURF qui sera là. L'inverse du Urf : cooldown augmentés, conso mana/énergie augmentée, dégâts des minions augmentés... Voilà, enjoy, moi je vais pleurer dans un coin parce que le urf était le mode qui me poussait à garder LoL installé. ^^^^quelqu'un ^^^^n'aime ^^^^pas ^^^^LoL ^^^^? ^^^^:)\u001b[0;0m\n",
            "   \u001b[1;94mÇa, ou alors demain c'est le 1^er avril.\u001b[0;0m\n",
            "\u001b[0mLe vrai mode URF était déjà une blague du 1er avril. Sauf que ça a bien marché et ils ont laissé le mode 2 semaines.On verra bien demain de toute façon :(\u001b[0;0m\n",
            "   \u001b[1;94mEt tu penses vraiment qu'ils vont remplacer un mode de jeu que tout le monde attend depuis 1 an par un truc pas fun du tout ?C'est clairement une blague.\u001b[0;0m\n",
            "\u001b[0mRiot est tellement une vaste blague comme entreprise qu'ils en seraient capable D:\u001b[0;0m\n",
            "   \u001b[1;94mUne vaste blague comme entreprise ? Les mecs ont réussi, en pompant le concept d'un jeu existant à créer le jeu en ligne le plus joué au monde, à se faire un fric fou et ont énormément contribué à la progression de l'esport. C'est plutôt des génies oui.Si tu veux te convaincres que c'est une blague regarde ça : Units critically strike on 150% of attackset explique moi ce que ça veut dire.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mIl y a une phrase prétendument de Pasqua qui tourne sur twitter en ce moment : *Quand on est emmerdé par une affaire, il faut susciter une affaire dans l’affaire, et si nécessaire une autre affaire dans l’affaire de l’affaire, jusqu’à ce que personne n’y comprenne plus rien.* Même si elle n'est pas de lui, elle s'applique bien à la situation actuelle. C'est bien joué de la part de la droite (bien aidé par la nullité du gouvernement) d'avoir su allumer des contres feux pour sauver le cul de Sarkozy. Assez marrant aussi de voir le léchage de boules en faveur de Sarko dans les articles de la page d'accueil de Valeurs Actuelles. C'est surement un hasard que ce soit ce journal qui sorte cette affaire.\u001b[0;0m\n",
            "   \u001b[1;94mIl y a une phrase prétendument de Pasqua qui tourne sur twitter en ce moment : Quand on est emmerdé par une affaire, il faut susciter une affaire dans l’affaire, et si nécessaire une autre affaire dans l’affaire de l’affaire, jusqu’à ce que personne n’y comprenne plus rien.Par exemple quand le gouvernement est emmerdé par ses résultats déplorables, il lance des diversions ? Après le mariage pour tous, Dieudonné, Méric, pas de bol cette fois-ci c'est un adversaire qui connait la chanson et apporte une réponse appropriée.Ma position n'a pas changé : le gouvernement ferait mieux de [s’occuper des Français](-Minute-Taubira-une-strategie-de-diversion-du-gouvernement-FN-1710677/).\u001b[0;0m\n",
            "\u001b[0mLe mariage pour tous ? Lol ? Tout le bordel à ce sujet à été crée par l'opposition. \u001b[0;0m\n",
            "   \u001b[1;94m... C'est l'opposition qui propose les lois maintenant ?\u001b[0;0m\n",
            "\u001b[0mParce qu'on est en période de crise économique, le gouvernement démocratiquement élu n'aurait pas le droit de légiférer sur des questions non-économiques ? Arrête.\u001b[0;0m\n",
            "   \u001b[1;94m Parce qu'on est en période de crise économique, le gouvernement démocratiquement élu n'aurait pas le droit de légiférer sur des questions non-économiques ?Il a tout à fait le droit de faire diversion. D'ailleurs il le fait très bien.\u001b[0;0m\n",
            "\u001b[0mTa mauvaise foi est sans limite.Il ne fait aucune diversion dans la mesure où 1. Il est légitime à légiférer sur des sujets non-économiques2. Le mariage gay était inscrit au programme de Hollande avant son élection. Il s'agissait donc d'une promesse de campagne, et certaines personnes, j'en suis sûr, on voté Hollande en raison du mariage gay.Enfin, si l'opposition avait le même sentiment que toi, elle n'aurait pas  monopolisé l'attention via la manif pour tous et aurait recentrée le débat sur les questions économiques.\u001b[0;0m\n",
            "   \u001b[1;94m Il ne fait aucune diversion dans la mesure où 1. Il est légitime à légiférer sur des sujets non-économiquesEn quoi une diversion ne serait pas légitime ?J'ai déjà dit qu'il avait tout à fait le droit de le faire.Le mariage gay était inscrit au programme de Hollande avant son élection.Oui, et en quoi ça n'en fait pas une diversion ?si l'opposition avait le même sentiment que toi, elle n'aurait pas monopolisé l'attention via la manif pour tousC'est ce qu'a fait le FN. La véritable opposition est le FN, pas l'UMP.\u001b[0;0m\n",
            "\u001b[0m C'est ce qu'a fait le FN.Ma mémoire n'est peut-être pas très bonne, mais je crois me souvenir de nombreux cadres du FN manifestant avec la Manif Pour Tous, signant sa charte et participant à cette grande \"diversion\".\u001b[0;0m\n",
            "   \u001b[1;94m Ma mémoire n'est peut-être pas très bonne, mais je crois me souvenir de nombreux cadres du FN manifestant avec la Manif Pour Tous, signant sa charte et participant à cette grande \"diversion\".C'est vrai, mais en quoi cela revient à \"monopoliser l'attention via la manif pour tous\" ?La ligne du FN [est très claire là-dessus]().\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mJe viens de terminer une épreuve dite \"marathon\", on nous a filé un texte à traduire à 8h45 ce matin et on doit le rendre à 16h45 au plus tard.Eh bien figurez-vous que le cancer de la prostate, c'est pas rigolo !\u001b[0;0m\n",
            "   \u001b[1;94mTu fais quoi, des études de trad ?\u001b[0;0m\n",
            "\u001b[0mOui, je suis en traduction audiovisuelle.\u001b[0;0m\n",
            "   \u001b[1;94mAh tu dois être à Strasbourg ? Je me souviens de ça, tout le monde flippait de devoir passer la journée sur une traduction, j'étais parmi les \"un peu plus vieux\" qui avaient déjà bossé avant et ça me faisait rouler des yeux.\u001b[0;0m\n",
            "\u001b[0mOui !Je t'avais posé des questions sur ton AMA il y a quelques mois. :)Je dois avouer que je faisais pas le malin ce matin. Mais bon, c'est passé assez facilement en fait.\u001b[0;0m\n",
            "   \u001b[1;94mAh oui c'est vrai... on avait eu un txt sur la sclérose en plaque... ils sont joyeux ces gens. C'est toujours la prof blonde gentille en trad médicale, avec la super ambiance de merde où tout le monde se fait chier ?\u001b[0;0m\n",
            "\u001b[0mLa prof est blonde et gentille mais l'ambiance était vraiment bonne, si on considère les sujets des textes...\u001b[0;0m\n",
            "   \u001b[1;94mAh ben nous c'était mort, mais j'aimais à peu près personne dans ma promo. J'espère que t'auras bien réussi ton truc en tout cas, tu me diras. T'as toujours pas rencontré M. Volclair ?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mAutant c'est ridicule, autant je suis sûr qu'à 13 ans j'ai dû dire des conneries aussi grosses que celle-ci.\u001b[0;0m\n",
            "   \u001b[1;94mIl y a une légère différence entre dire quelque chose à 13 ans et le prendre en compte pour l'imprimer à 30 ans dans un \"journal\" pour que tout le monde le voit. \u001b[0;0m\n",
            "\u001b[0mBof. Ça peut être intéressant/marrant de voir les conneries que peuvent raconter les ados, de temps en temps. J'veux dire c'est pas comme si le magazine imprimait ça pour dire \"voici nos conseils sentimentaux, offerts par notre experte en la matière\". Tout lecteur équipé de plus de trois neurones lira ça sur le mode \"ah ah, ils sont marrants ces petits, ils ont encore tant de choses à apprendre sur la vie\".\u001b[0;0m\n",
            "   \u001b[1;94mSauf que la plupart du bétail francais prend pour vérité absolu tout ce qu'il voit à la télé et dans un journal, si on as plus de 3 neurones ca peut nous faire marrer mais si tu as plus de 10 neurones tu te rendra compte du mal que ca peut faire.\u001b[0;0m\n",
            "\u001b[0m Sauf que la plupart du bétail francais prend pour vérité absolu tout ce qu'il voit à la télé et dans un journalBen voyons. Le \"bétail français\" va lire un conseil \"trompe avant d'être trompé\" **signé par une gamine de 13 ans** et se dire \"ouais elle a raison\" ? Tu dois pas sortir souvent, pour penser que les gens sont vraiment aussi cons.\u001b[0;0m\n",
            "   \u001b[1;94mOk toi tu vis dans ta bulle sociale tu es fiché direct, c'est pas parce que tu vis dans un milieu ou les gens sont un minimum conscient que c'est le cas partout. Sors un peu, fréquente différentes couches sociales, et surtout va te balader dans les coins de france ou on trouve le bon redneck francais de base qui parle de tout ce quil lit au journal ou ce qu'il voit au info comme une vérité général, tu va perdre foi en l'humanité direct et revenir à la réalité.Malheuresement oui les gens sont aussi cons, le manque de culture ca fais des ravages terribles, les gens ne remettent pas grand chose en question si on les y aide pas.Cet article en particulier ne prouve rien et est\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mOn sait bien ce que ça va donner ce mal parlé.  Les gros bouzeux incultes vont s'apercevoir qu'ils ont tout en commun : l'alcoolisme, l'inculture, la jalousie, et que vont ils faire ? Ils vont faire une bonne grosse alliance de paysans, façon coopérative de l'inculture de France et essayer de trashtalk leur belle, superbe et délicate capitale :  #PARIS !\u001b[0;0m\n",
            "   \u001b[1;94mC'est cool de payer un loyer à 4 chiffres pour un studio ?  C'est comment le métro/Rer tous les matins ?  C'est pas trop cher 4 euros pour un demi ?  C'est chiant de pas pouvoir ce baigner quand il fait 38 ° ?J'entends déjà les \"Paris et ses expositions/concerts/musés/\" un vrai parisien n'y va jamais.\u001b[0;0m\n",
            "\u001b[0mC'est comment le métro/Rer tous les matins ?Tu veux dire avoir des transports en commun globalement fiables et bien foutus, abordables, et très étendus ? Plutôt pas mal, merci.\u001b[0;0m\n",
            "   \u001b[1;94mJe veux surtout avoir votre avis sur le fait de passer 2 sur les 18 heures de tes journées dans un métro .\u001b[0;0m\n",
            "\u001b[0mMieux vaut ça que 8 heures par jour sur un tracteur ?\u001b[0;0m\n",
            "   \u001b[1;94mMoi j'aime bien, surtout quand je suis au milieu de la route et que j'ai un connard de parisien en vacances derrière moi qui klaxonne parce qu'il est beaucoup trop pressé. C'est mon petit plaisir.\u001b[0;0m\n",
            "\u001b[0mAttends tu déconnes, les parisiens en vacances ils roulent encore moins vite que les tracteurs.\u001b[0;0m\n",
            "   \u001b[1;94mNormal, le reste de la France prend plaisir à aggro la moindre plaque 75 qu'ils voient, pour se branler sur \"les parisiens conduisent trop mal\" le reste de l'année, entre deux accidents mortels sur départementales\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mC'est quoi la proportion homme femme sur le sub r/france ?\u001b[0;0m\n",
            "   \u001b[1;94mC'est quoi le rapport ?\u001b[0;0m\n",
            "09:08:04 | loaded 96865 episodes with a total of 335085 examples\n"
          ]
        }
      ],
      "source": [
        "# from parlai.scripts.display_data import DisplayData\n",
        "# DisplayData.main(task='empathetic_dialogues', num_examples=10)\n",
        "\n",
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "    # model_file= f'{finetuned_model_path}/model',\n",
        "    # dict_file= dict_file,\n",
        "\n",
        "    num_examples=20,\n",
        "    # skip_generation=False,\n",
        "\n",
        "    # beam_min_length= 20,\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_size= 10,\n",
        "\n",
        "    # inference= \"beam\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "blender-finetuning-with-reddit.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMTYdk5pqP6ZboZCKFtjw2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}