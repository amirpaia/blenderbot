{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirpaia/blenderbot/blob/main/blender_90m_400m_multitask_double_sided.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SONwSWMp6qPv"
      },
      "source": [
        "# 0.Installing prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d-SZ_On6Kxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945e2d15-e01b-4d38-8969-80e4c3a01df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun 16 07:43:35 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BegaSUz6iUz",
        "outputId": "6c172d85-e887-43c0-eb6d-74aac6a0d754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SzGRXHQ6kDQ",
        "outputId": "50c6419a-2615-4236-ab5b-2d13e4c495f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWDakYmy6mIQ"
      },
      "outputs": [],
      "source": [
        "mydrive_path = '/content/drive/MyDrive/colabs/blender-models/'\n",
        "# !pip uninstall -q parlai\n",
        "!pip install -q parlai\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -v /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/* /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m/"
      ],
      "metadata": {
        "id": "5ye1o1VhUSxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!parlai eval_model --task blended_skill_talk --model-file zoo:blender/blender_90M/model --batchsize 32"
      ],
      "metadata": {
        "id": "xofOAv4XPge4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIJEq9_r63hi"
      },
      "source": [
        "# 1.Preparing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R-fgBUdcPX5"
      },
      "source": [
        "## Genreal Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47u8M3RK65m_",
        "outputId": "97a1e2a8-f695-4b88-d6b7-facaed38ac59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n",
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def transfer_list_of_turns_to_dialog(d):\n",
        "    if len(d)%2 !=0: d = d[:-1]\n",
        "    t = \"\"\n",
        "    for i in range(0,len(d),2):\n",
        "        u1 = d[i]\n",
        "        u2 = d[i+1]\n",
        "\n",
        "        if (i+2) != len(d):\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "        else:\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "    return t\n",
        "\n",
        "def transfer_list_of_pairs_to_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, text_label_pair in enumerate(d):\n",
        "    u1 = text_label_pair[0]\n",
        "    u2 = text_label_pair[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "\n",
        "def convert_parlai_format_to_list_of_turns(lines):\n",
        "    result = []\n",
        "    for line in lines:\n",
        "        text_label = line.split(\"\\t\")\n",
        "        result.append(text_label[0].replace(\"text:\", \"\"))\n",
        "        result.append(text_label[1].replace(\"labels:\", \"\").replace(\"\\n\",\"\"))\n",
        "    return result\n",
        "\n",
        "t = ['hello','how are you','good','bye','test']\n",
        "print(transfer_list_of_turns_to_dialog(t))\n",
        "\n",
        "t = [['hello','how are you'],['good','bye']]\n",
        "print(transfer_list_of_pairs_to_dialog(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIROa77CJtG9"
      },
      "outputs": [],
      "source": [
        "def make_dataset_double_sided(lines, path, filename): \n",
        "    # train[:10]\n",
        "    all_dialogs_parlai_format = []\n",
        "    dialog = []\n",
        "    for line in lines:\n",
        "        dialog.append(line)\n",
        "        if 'episode_done:True' in line:\n",
        "            turns = convert_parlai_format_to_list_of_turns(dialog)\n",
        "            first_parlai_dialog = transfer_list_of_turns_to_dialog(turns)\n",
        "            second_parlai_dialog = transfer_list_of_turns_to_dialog(turns[1:])\n",
        "\n",
        "            all_dialogs_parlai_format.append(first_parlai_dialog)\n",
        "            all_dialogs_parlai_format.append(second_parlai_dialog)\n",
        "\n",
        "            dialog = []\n",
        "            # break\n",
        "    print(sum([1 for a in lines if 'episode_done:True' in a]), len(all_dialogs_parlai_format))\n",
        "\n",
        "    with open(f\"{path}{filename}\", \"w\") as f:\n",
        "        f.writelines(all_dialogs_parlai_format)\n",
        "\n",
        "# !mkdir /content/dataset_french_bst/\n",
        "# make_dataset_double_sided(lines,\"/content/dataset_french_bst/\", \"train.txt\")\n",
        "\n",
        "\n",
        "\n",
        "# # import os.path\n",
        "# # from os import path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE45ZyKC8WjC"
      },
      "source": [
        "## XPersona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZmSJtQG8Y1o",
        "outputId": "6bdbb9e3-1c44-42da-c4c8-07deeb0c2cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Xpersona'...\n",
            "remote: Enumerating objects: 285, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 285 (delta 6), reused 6 (delta 4), pack-reused 275\u001b[K\n",
            "Receiving objects: 100% (285/285), 45.01 MiB | 19.31 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HLTCHKUST/Xpersona.git\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_train_corrected.json','r') as f:\n",
        "   train_data = json.load(f)\n",
        "\n",
        "dialogs_train = pd.DataFrame(train_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_valid_human_annotated.json','r') as f:\n",
        "   valid_data = json.load(f)\n",
        "\n",
        "dialogs_valid = pd.DataFrame(valid_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_test_human_annotated.json','r') as f:\n",
        "   test_data = json.load(f)\n",
        "\n",
        "dialogs_test = pd.DataFrame(test_data)['dialogue'].tolist()\n",
        "\n",
        "# dialogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plw61KMBJU4I",
        "outputId": "7a29af81-59fe-4966-d0fc-43bf7b6b76bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/dataset_french_xpersona/': No such file or directory\n",
            "33756\n",
            "496\n",
            "498\n"
          ]
        }
      ],
      "source": [
        "data_path = \"/content/dataset_french_xpersona/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "\n",
        "def convert_xpersona_dialogs_to_parlai_format_file_and_double_sided(dialogs, filename):\n",
        "    all_dialogs_parlai_format = []\n",
        "    import numpy as np\n",
        "    for d in dialogs:\n",
        "        turns = np.reshape(d, (-1)).tolist()\n",
        "        all_dialogs_parlai_format.append(transfer_list_of_turns_to_dialog(turns))\n",
        "        all_dialogs_parlai_format.append(transfer_list_of_turns_to_dialog(turns[1:]))\n",
        "\n",
        "    print(len(all_dialogs_parlai_format))\n",
        "\n",
        "    with open(f\"{data_path}{filename}\",\"w\") as f:\n",
        "        f.writelines(all_dialogs_parlai_format)\n",
        "\n",
        "convert_xpersona_dialogs_to_parlai_format_file_and_double_sided(dialogs_train, \"train.txt\")\n",
        "convert_xpersona_dialogs_to_parlai_format_file_and_double_sided(dialogs_valid, \"valid.txt\")\n",
        "convert_xpersona_dialogs_to_parlai_format_file_and_double_sided(dialogs_test, \"test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKlGi0f08zQc"
      },
      "outputs": [],
      "source": [
        "# data_path = \"/content/dataset_french_xpersona/\"\n",
        "# !rm -R $data_path\n",
        "# !mkdir $data_path\n",
        "\n",
        "# #region Training\n",
        "# data_train = \"\"\n",
        "# for d in dialogs_train:\n",
        "#   data_train += transfer_list_of_pairs_to_dialog(d)\n",
        "\n",
        "# file_train = open(f\"{data_path}train.txt\",\"w\")\n",
        "# print(\"Training Set:\", file_train.write(data_train))\n",
        "# #endregion \n",
        "    \n",
        "# #region Validation\n",
        "# data_valid = \"\"\n",
        "# for d in dialogs_valid:\n",
        "#   data_valid += transfer_list_of_pairs_to_dialog(d)\n",
        "\n",
        "# file_valid = open(f\"{data_path}valid.txt\",\"w\")\n",
        "# print(\"Validation Set:\", file_valid.write(data_valid))\n",
        "# #endregion\n",
        "\n",
        "# #region Test\n",
        "# data_test = \"\"\n",
        "# for d in dialogs_test:\n",
        "#   data_test += transfer_list_of_pairs_to_dialog(d)\n",
        "\n",
        "# file_test = open(f\"{data_path}test.txt\",\"w\")\n",
        "# print(\"Test Set:\", file_test.write(data_test))\n",
        "# #endregion \n",
        "\n",
        "# # print(len(data_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoSv5zhs8ZIv"
      },
      "source": [
        "## ED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mISEjZbP8dN-",
        "outputId": "d5164a08-4894-4601-90a6-65f4e0d67769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/dataset_french_ed/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/test.txt' -> '/content/dataset_french_ed/test.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/train.txt' -> '/content/dataset_french_ed/train.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/valid.txt' -> '/content/dataset_french_ed/valid.txt'\n"
          ]
        }
      ],
      "source": [
        "googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/\"\n",
        "data_path = \"/content/dataset_french_ed/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "!cp -rv $googledrive_data_path* $data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNbq22L-Khmg",
        "outputId": "7183e24f-74a6-4386-c421-51b755e362c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64636\n",
            "39057 5537 5093 49687\n"
          ]
        }
      ],
      "source": [
        "# with open(f\"{googledrive_data_path}train.txt\") as f:\n",
        "#     train = f.readlines()\n",
        "\n",
        "# with open(f\"{googledrive_data_path}valid.txt\") as f:\n",
        "#     valid = f.readlines()\n",
        "\n",
        "# with open(f\"{googledrive_data_path}test.txt\") as f:\n",
        "#     test = f.readlines()\n",
        "\n",
        "\n",
        "# print(len(train))\n",
        "# a, b, c = sum([1 for a in train if 'episode_done:True' in a]), sum([1 for a in valid if 'episode_done:True' in a]), sum([1 for a in test if 'episode_done:True' in a])\n",
        "# print(a,b,c,a+b+c)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUxXs_Oe8dda"
      },
      "source": [
        "## BST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIRVhsq88e7A",
        "outputId": "067c6353-0b26-4c63-9039-ebfc91375c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/dataset_french_bst/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/test.txt' -> '/content/dataset_french_bst/test.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/train.txt' -> '/content/dataset_french_bst/train.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/valid.txt' -> '/content/dataset_french_bst/valid.txt'\n"
          ]
        }
      ],
      "source": [
        "googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/\"\n",
        "data_path = \"/content/dataset_french_bst/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "!cp -rv $googledrive_data_path* $data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuQo9hiTRT0Y",
        "outputId": "07a63e96-8dd6-438c-c509-68125be9bfa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59700\n",
            "9638 2018 1958 13614\n"
          ]
        }
      ],
      "source": [
        "googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/\"\n",
        "with open(f\"{googledrive_data_path}train.txt\") as f:\n",
        "    train = f.readlines()\n",
        "\n",
        "with open(f\"{googledrive_data_path}valid.txt\") as f:\n",
        "    valid = f.readlines()\n",
        "\n",
        "with open(f\"{googledrive_data_path}test.txt\") as f:\n",
        "    test = f.readlines()\n",
        "\n",
        "\n",
        "print(len(train))\n",
        "a, b, c = sum([1 for a in train if 'episode_done:True' in a]), sum([1 for a in valid if 'episode_done:True' in a]), sum([1 for a in test if 'episode_done:True' in a])\n",
        "print(a, b, c, a+b+c)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l6gr-_0Ohlg"
      },
      "source": [
        "## WoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv1tbBZOOnW9"
      },
      "outputs": [],
      "source": [
        "# googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_wizard_of_wikipedia/\"\n",
        "# data_path = \"/content/dataset_french_wow/\"\n",
        "# !rm -R $data_path\n",
        "# !mkdir $data_path\n",
        "# !cp -rv $googledrive_data_path* $data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vj3Ii8TAknd",
        "outputId": "842e5f00-28f9-42e4-f8d2-8772c78e1d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/dataset_french_wow/': No such file or directory\n",
            "17848 35696\n",
            "2231 4462\n",
            "2231 4462\n"
          ]
        }
      ],
      "source": [
        "# convert_xpersona_dialogs_to_parlai_format_file_and_double_sided(dialogs_train, \"train.txt\")\n",
        "googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_wizard_of_wikipedia/\"\n",
        "data_path = \"/content/dataset_french_wow/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "\n",
        "with open(f\"{googledrive_data_path}train.txt\") as f:\n",
        "    train_lines = f.readlines()\n",
        "with open(f\"{googledrive_data_path}valid.txt\") as f:\n",
        "    valid_lines = f.readlines()\n",
        "with open(f\"{googledrive_data_path}test.txt\") as f:\n",
        "    test_lines = f.readlines()\n",
        "\n",
        "\n",
        "make_dataset_double_sided(train_lines, data_path, \"train.txt\")\n",
        "make_dataset_double_sided(valid_lines, data_path, \"valid.txt\")\n",
        "make_dataset_double_sided(test_lines, data_path, \"test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jcuyswOBWrQ"
      },
      "outputs": [],
      "source": [
        "# googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_wizard_of_wikipedia/\"\n",
        "# with open(f\"{googledrive_data_path}train.txt\") as f:\n",
        "#     train = f.readlines()\n",
        "# with open(f\"{googledrive_data_path}valid.txt\") as f:\n",
        "#     valid = f.readlines()\n",
        "# with open(f\"{googledrive_data_path}test.txt\") as f:\n",
        "#     test = f.readlines()\n",
        "# print(len(train))\n",
        "# a, b, c = sum([1 for a in train if 'episode_done:True' in a]), sum([1 for a in valid if 'episode_done:True' in a]), sum([1 for a in test if 'episode_done:True' in a])\n",
        "# print(a, b, c, a+b+c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PQfT57Q8I2u"
      },
      "source": [
        "# 2.Creating new Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oVPKrhO8ISe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5eb632-518b-4283-92f0-07d2cc0af412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/agents.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/agents.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/build.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/build.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/__init__.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/__init__.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/LICENSE_DOCUMENTATION' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/LICENSE_DOCUMENTATION'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/README.md' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/README.md'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test/empathetic_dialogues_test.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test/empathetic_dialogues_test.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test/empathetic_dialogues_train.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test/empathetic_dialogues_train.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test/empathetic_dialogues_valid.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test/empathetic_dialogues_valid.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/worlds.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/worlds.py'\n",
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/agents.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/agents.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/build.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/build.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/__init__.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/__init__.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/LICENSE_DOCUMENTATION' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/LICENSE_DOCUMENTATION'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/README.md' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/README.md'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test/empathetic_dialogues_test.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test/empathetic_dialogues_test.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test/empathetic_dialogues_train.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test/empathetic_dialogues_train.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test/empathetic_dialogues_valid.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test/empathetic_dialogues_valid.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/worlds.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/worlds.py'\n",
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/agents.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/agents.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/build.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/build.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/__init__.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/__init__.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/LICENSE_DOCUMENTATION' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/LICENSE_DOCUMENTATION'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/README.md' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/README.md'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test/empathetic_dialogues_test.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test/empathetic_dialogues_test.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test/empathetic_dialogues_train.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test/empathetic_dialogues_train.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test/empathetic_dialogues_valid.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test/empathetic_dialogues_valid.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/worlds.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/worlds.py'\n",
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/agents.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/agents.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/build.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/build.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/__init__.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/__init__.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/LICENSE_DOCUMENTATION' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/LICENSE_DOCUMENTATION'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/README.md' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/README.md'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/test' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/test'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/test/empathetic_dialogues_test.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/test/empathetic_dialogues_test.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/test/empathetic_dialogues_train.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/test/empathetic_dialogues_train.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/test/empathetic_dialogues_valid.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/test/empathetic_dialogues_valid.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/test.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/test.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/worlds.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/worlds.py'\n"
          ]
        }
      ],
      "source": [
        "#region XPersona\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_xpersona/'\n",
        "!mkdir $task_path'french_xpersona'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/* $task_path'french_xpersona/'\n",
        "#endregion\n",
        "\n",
        "\n",
        "#region ED\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_empathetic_dialogues/'\n",
        "!mkdir $task_path'french_empathetic_dialogues'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/* $task_path'french_empathetic_dialogues/'\n",
        "#endregion\n",
        "\n",
        "\n",
        "#region BST\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_blended_skill_talk/'\n",
        "!mkdir $task_path'french_blended_skill_talk'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/* $task_path'french_blended_skill_talk/'\n",
        "#endregion\n",
        "\n",
        "#region WoW\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_wizard_of_wikipedia/'\n",
        "!mkdir $task_path'french_wizard_of_wikipedia'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/* $task_path'french_wizard_of_wikipedia/'\n",
        "#endregion "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDe0BBqqChqF"
      },
      "outputs": [],
      "source": [
        "# !parlai display_data --task french_empathetic_dialogues\n",
        "# !parlai display_data --task french_xpersona \n",
        "# !parlai display_data --task french_blended_skill_talk \n",
        "# !parlai display_data --task french_wizard_of_wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGoTqdFD7x_3"
      },
      "source": [
        "# 3.Finetuning + Multitasking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "65rK6pfj70Px",
        "outputId": "a8583c8b-0f2c-46bb-86fa-b215f2e9e520"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# finetuned_model_path = f'{mydrive_path}finetuned-reddit-90m--finetuned-4tasks-5epochs/'\n",
        "finetuned_model_path = f'{mydrive_path}finetuned-reddit_LELU-4tasks-90m'\n",
        "# init_model = 'zoo:blender/blender_400Mdistill/model'\n",
        "# dict_file  = 'zoo:blender/blender_400Mdistill/model.dict'\n",
        "init_model = 'zoo:blender/blender_90M/model'\n",
        "dict_file  = 'zoo:blender/blender_90M/model.dict'\n",
        "finetuned_model_path\n",
        "# from os import path\n",
        "# path.exists(f\"{finetuned_model_path}model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zArppuSCGS7"
      },
      "outputs": [],
      "source": [
        "# !mkdir copied_dataset_french_bst\n",
        "# !cp dataset_french_bst/* copied_dataset_french_bst\n",
        "# !mv copied_dataset_french_bst/test.txt copied_dataset_french_bst/_test.txt\n",
        "# !mv copied_dataset_french_bst/train.txt copied_dataset_french_bst/_train.txt\n",
        "# !mv copied_dataset_french_bst/valid.txt copied_dataset_french_bst/_valid.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pipze3fDlXpD",
        "outputId": "8b478d62-258c-46e8-dbae-5254d4589ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: shortuuid, setproctitle, sentry-sdk, pathtools, docker-pycreds, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 wandb-0.12.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7vLiYpF8HbF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "128fd40b58b1410aaef82c012c5c9ed3",
            "4c38cefd4fea487aba661ac2f6bb16f1",
            "52448a45dde44b8896446c69043ede4d",
            "56226e1618e8444ab41b43c2b7cac556",
            "5703fb09f21a4931aa2067a4d62502fa",
            "c4a694833b694a9199182e47ec6da791",
            "ad9d10e70e034784bff2250d38b60060",
            "664750a79215441ea7b43aca361c3e7c"
          ]
        },
        "outputId": "6b10ab7b-05c8-43c8-ed30-9a5ddaeda677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14:21:20 | building dictionary first...\n",
            "14:21:20 | \u001b[33mOverriding opt[\"multitask_weights\"] to (1.0, 3.0, 3.0, 3.0) (previously: [1.0, 3.0, 3.0, 3.0])\u001b[0m\n",
            "14:21:20 | \u001b[33mOverriding opt[\"init_model\"] to zoo:blender/blender_90M/model (previously: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.checkpoint)\u001b[0m\n",
            "14:21:20 | \u001b[33mOverriding opt[\"dict_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict (previously: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.checkpoint.dict)\u001b[0m\n",
            "14:21:20 | \u001b[33mOverriding opt[\"validation_every_n_epochs\"] to 0.1 (previously: 0.25)\u001b[0m\n",
            "14:21:20 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "14:21:20 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "14:21:20 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--validation-every-n-epochs 0.25 --optimizer mem_eff_adam\u001b[0m\n",
            "14:21:20 | Using CUDA\n",
            "14:21:20 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.checkpoint.dict\n",
            "14:21:20 | num words = 54944\n",
            "14:21:22 | Total parameters: 87,508,992 (86,984,704 trainable)\n",
            "14:21:22 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.checkpoint\n",
            "14:21:25 | Opt:\n",
            "14:21:25 |     activation: gelu\n",
            "14:21:25 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "14:21:25 |     adam_eps: 1e-08\n",
            "14:21:25 |     add_p1_after_newln: False\n",
            "14:21:25 |     aggregate_micro: False\n",
            "14:21:25 |     allow_missing_init_opts: False\n",
            "14:21:25 |     attention_dropout: 0.0\n",
            "14:21:25 |     batchsize: 8\n",
            "14:21:25 |     beam_block_full_context: True\n",
            "14:21:25 |     beam_block_list_filename: None\n",
            "14:21:25 |     beam_block_ngram: -1\n",
            "14:21:25 |     beam_context_block_ngram: -1\n",
            "14:21:25 |     beam_delay: 30\n",
            "14:21:25 |     beam_length_penalty: 0.65\n",
            "14:21:25 |     beam_min_length: 1\n",
            "14:21:25 |     beam_size: 1\n",
            "14:21:25 |     betas: '[0.9, 0.999]'\n",
            "14:21:25 |     bpe_add_prefix_space: None\n",
            "14:21:25 |     bpe_debug: False\n",
            "14:21:25 |     bpe_dropout: None\n",
            "14:21:25 |     bpe_merge: None\n",
            "14:21:25 |     bpe_vocab: None\n",
            "14:21:25 |     checkpoint_activations: False\n",
            "14:21:25 |     compute_tokenized_bleu: False\n",
            "14:21:25 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "14:21:25 |     datatype: train\n",
            "14:21:25 |     delimiter: '\\n'\n",
            "14:21:25 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "14:21:25 |     dict_endtoken: __end__\n",
            "14:21:25 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.checkpoint.dict\n",
            "14:21:25 |     dict_include_test: False\n",
            "14:21:25 |     dict_include_valid: False\n",
            "14:21:25 |     dict_initpath: None\n",
            "14:21:25 |     dict_language: english\n",
            "14:21:25 |     dict_loaded: True\n",
            "14:21:25 |     dict_lower: True\n",
            "14:21:25 |     dict_max_ngram_size: -1\n",
            "14:21:25 |     dict_maxexs: -1\n",
            "14:21:25 |     dict_maxtokens: -1\n",
            "14:21:25 |     dict_minfreq: 0\n",
            "14:21:25 |     dict_nulltoken: __null__\n",
            "14:21:25 |     dict_starttoken: __start__\n",
            "14:21:25 |     dict_textfields: text,labels\n",
            "14:21:25 |     dict_tokenizer: bpe\n",
            "14:21:25 |     dict_unktoken: __unk__\n",
            "14:21:25 |     display_examples: False\n",
            "14:21:25 |     download_path: None\n",
            "14:21:25 |     dropout: 0.0\n",
            "14:21:25 |     dynamic_batching: full\n",
            "14:21:25 |     embedding_projection: random\n",
            "14:21:25 |     embedding_size: 512\n",
            "14:21:25 |     embedding_type: random\n",
            "14:21:25 |     embeddings_scale: True\n",
            "14:21:25 |     eval_batchsize: None\n",
            "14:21:25 |     eval_dynamic_batching: None\n",
            "14:21:25 |     evaltask: None\n",
            "14:21:25 |     ffn_size: 2048\n",
            "14:21:25 |     final_extra_opt: \n",
            "14:21:25 |     force_fp16_tokens: True\n",
            "14:21:25 |     fp16: True\n",
            "14:21:25 |     fp16_impl: mem_efficient\n",
            "14:21:25 |     fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data\n",
            "14:21:25 |     fromfile_datatype_extension: True\n",
            "14:21:25 |     gpu: -1\n",
            "14:21:25 |     gradient_clip: 0.1\n",
            "14:21:25 |     hide_labels: False\n",
            "14:21:25 |     history_add_global_end_token: None\n",
            "14:21:25 |     history_reversed: False\n",
            "14:21:25 |     history_size: -1\n",
            "14:21:25 |     image_cropsize: 224\n",
            "14:21:25 |     image_mode: raw\n",
            "14:21:25 |     image_size: 256\n",
            "14:21:25 |     inference: greedy\n",
            "14:21:25 |     init_model: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.checkpoint\n",
            "14:21:25 |     init_opt: None\n",
            "14:21:25 |     interactive_mode: False\n",
            "14:21:25 |     invsqrt_lr_decay_gamma: -1\n",
            "14:21:25 |     is_debug: False\n",
            "14:21:25 |     label_truncate: 128\n",
            "14:21:25 |     learn_positional_embeddings: False\n",
            "14:21:25 |     learningrate: 1e-05\n",
            "14:21:25 |     load_from_checkpoint: True\n",
            "14:21:25 |     log_every_n_secs: 180.0\n",
            "14:21:25 |     log_every_n_steps: 50\n",
            "14:21:25 |     log_keep_fields: all\n",
            "14:21:25 |     loglevel: info\n",
            "14:21:25 |     lr_scheduler: reduceonplateau\n",
            "14:21:25 |     lr_scheduler_decay: 0.5\n",
            "14:21:25 |     lr_scheduler_patience: 3\n",
            "14:21:25 |     max_train_steps: -1\n",
            "14:21:25 |     max_train_time: -1\n",
            "14:21:25 |     metrics: default\n",
            "14:21:25 |     model: transformer/generator\n",
            "14:21:25 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model\n",
            "14:21:25 |     model_parallel: False\n",
            "14:21:25 |     momentum: 0\n",
            "14:21:25 |     multitask_weights: '(1.0, 3.0, 3.0, 3.0)'\n",
            "14:21:25 |     mutators: None\n",
            "14:21:25 |     n_decoder_layers: -1\n",
            "14:21:25 |     n_encoder_layers: -1\n",
            "14:21:25 |     n_heads: 16\n",
            "14:21:25 |     n_layers: 8\n",
            "14:21:25 |     n_positions: 512\n",
            "14:21:25 |     n_segments: 0\n",
            "14:21:25 |     nesterov: True\n",
            "14:21:25 |     no_cuda: False\n",
            "14:21:25 |     num_epochs: 10.0\n",
            "14:21:25 |     num_workers: 0\n",
            "14:21:25 |     nus: [0.7]\n",
            "14:21:25 |     optimizer: mem_eff_adam\n",
            "14:21:25 |     output_scaling: 1.0\n",
            "14:21:25 |     override: \"{'task': 'french_blended_skill_talk,french_wizard_of_wikipedia,french_xpersona,french_empathetic_dialogues', 'multitask_weights': (1.0, 3.0, 3.0, 3.0), 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model', 'init_model': 'zoo:blender/blender_90M/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'validation_every_n_epochs': 0.1, 'num_epochs': 10.0, 'log_every_n_secs': 180.0, 'verbose': True, 'batchsize': 8, 'fp16': True, 'fp16_impl': 'mem_efficient', 'save_after_valid': True, 'wandb_log': True, 'skip_generation': True, 'validation_patience': 10, 'validation_metric': 'ppl', 'validation_metric_mode': 'min', 'dynamic_batching': 'full', 'learningrate': 1e-05, 'optimizer': 'adam', 'attention_dropout': 0.0, 'model_parallel': False, 'warmup_updates': 100}\"\n",
            "14:21:25 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "14:21:25 |     person_tokens: False\n",
            "14:21:25 |     rank_candidates: False\n",
            "14:21:25 |     relu_dropout: 0.0\n",
            "14:21:25 |     save_after_valid: True\n",
            "14:21:25 |     save_every_n_secs: -1\n",
            "14:21:25 |     save_format: conversations\n",
            "14:21:25 |     share_word_embeddings: True\n",
            "14:21:25 |     short_final_eval: False\n",
            "14:21:25 |     skip_generation: True\n",
            "14:21:25 |     special_tok_lst: None\n",
            "14:21:25 |     split_lines: False\n",
            "14:21:25 |     starttime: Jun03_23-03\n",
            "14:21:25 |     task: french_blended_skill_talk,french_wizard_of_wikipedia,french_xpersona,french_empathetic_dialogues\n",
            "14:21:25 |     temperature: 1.0\n",
            "14:21:25 |     tensorboard_log: False\n",
            "14:21:25 |     tensorboard_logdir: None\n",
            "14:21:25 |     text_truncate: 512\n",
            "14:21:25 |     topk: 10\n",
            "14:21:25 |     topp: 0.9\n",
            "14:21:25 |     truncate: -1\n",
            "14:21:25 |     update_freq: 1\n",
            "14:21:25 |     use_reply: label\n",
            "14:21:25 |     validation_cutoff: 1.0\n",
            "14:21:25 |     validation_every_n_epochs: 0.1\n",
            "14:21:25 |     validation_every_n_secs: -1\n",
            "14:21:25 |     validation_every_n_steps: -1\n",
            "14:21:25 |     validation_max_exs: -1\n",
            "14:21:25 |     validation_metric: ppl\n",
            "14:21:25 |     validation_metric_mode: min\n",
            "14:21:25 |     validation_patience: 10\n",
            "14:21:25 |     validation_share_agent: False\n",
            "14:21:25 |     variant: xlm\n",
            "14:21:25 |     verbose: True\n",
            "14:21:25 |     wandb_entity: None\n",
            "14:21:25 |     wandb_log: True\n",
            "14:21:25 |     wandb_name: None\n",
            "14:21:25 |     wandb_project: None\n",
            "14:21:25 |     warmup_rate: 0.0001\n",
            "14:21:25 |     warmup_updates: 100\n",
            "14:21:25 |     weight_decay: None\n",
            "14:21:25 |     world_logs: \n",
            "14:21:26 | creating task(s): french_blended_skill_talk,french_wizard_of_wikipedia,french_xpersona,french_empathetic_dialogues\n",
            "14:21:26 | Loading ParlAI text data: /content/dataset_french_bst/train.txt\n",
            "14:21:26 | Loading ParlAI text data: /content/dataset_french_wow/train.txt\n",
            "14:21:28 | Loading ParlAI text data: /content/dataset_french_xpersona/train.txt\n",
            "14:21:31 | Loading ParlAI text data: /content/dataset_french_ed/train.txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.18"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/wandb/run-20220616_142150-1c1p1qsf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/aliae_research/2022-06-16-14-21/runs/1c1p1qsf\" target=\"_blank\">playful-moon-1</a></strong> to <a href=\"https://wandb.ai/aliae_research/2022-06-16-14-21\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14:21:52 | training...\n",
            "14:22:03 | time:43096s total_exs:4191034 total_steps:200030 epochs:8.41 time_left:8176s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         55.32     1  2357 11796       0          0 225.8 2256             16384  5.905    .4126 28.03   \n",
            "   french_blended_skill_talk   64.21                         0          0        184                                   29.52   \n",
            "   french_empathetic_dialogues 41.84                         0          0        735                                   26.95   \n",
            "   french_wizard_of_wikipedia  63.08                         0          0        507                                   33.78   \n",
            "   french_xpersona             52.14                         0          0        830                                   21.88   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.176 1.238e-06  1209  6050 .0008332     .04263 8.888      .5472   .001506   \n",
            "   french_blended_skill_talk   2.111                              0          0 8.253      .5540         0   \n",
            "   french_empathetic_dialogues 2.019                        .001361     .04626 7.533      .5608         0   \n",
            "   french_wizard_of_wikipedia  2.375                        .001972      .1243 10.76      .5244         0   \n",
            "   french_xpersona             2.198                              0          0 9.008      .5496   .006024   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       119998 3565 17846 5.006  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:22:14 | time:43107s total_exs:4192478 total_steps:200080 epochs:8.41 time_left:8160s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         112.7     1  2852 12654       0          0 128.1 1444             16384  7.924    .4072 30.03   \n",
            "   french_blended_skill_talk     141                         0          0         95                                   32.27   \n",
            "   french_empathetic_dialogues 52.84                         0          0        590                                   28.49   \n",
            "   french_wizard_of_wikipedia  121.6                         0          0        341                                   36.88   \n",
            "   french_xpersona             135.3                         0          0        418                                   22.48   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.191 1.238e-06   837  3714       0          0  9.01      .5395  .0004237               120048   \n",
            "   french_blended_skill_talk   2.118                             0          0 8.315      .5457         0                        \n",
            "   french_empathetic_dialogues 2.069                             0          0 7.919      .5509   .001695                        \n",
            "   french_wizard_of_wikipedia  2.387                             0          0 10.88      .5194         0                        \n",
            "   french_xpersona             2.188                             0          0 8.921      .5419         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3689 16368 4.438  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:22:24 | time:43117s total_exs:4193802 total_steps:200130 epochs:8.41 time_left:8146s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         136.7     1  2832 14384       0          0 134.5 1324             16384  7.227    .4108 31.32   \n",
            "   french_blended_skill_talk   187.3                         0          0         68                                   32.37   \n",
            "   french_empathetic_dialogues 55.77                         0          0        644                                   28.65   \n",
            "   french_wizard_of_wikipedia  143.2                         0          0        306                                   40.72   \n",
            "   french_xpersona             160.6                         0          0        306                                   23.55   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.229 1.238e-06 804.9  4088 .001634     .05964 9.357      .5353   .000817               120098   \n",
            "   french_blended_skill_talk   2.161                             0          0 8.679      .5420         0                        \n",
            "   french_empathetic_dialogues 2.096                             0          0 8.133      .5497         0                        \n",
            "   french_wizard_of_wikipedia  2.419                       .006536      .2386 11.23      .5133         0                        \n",
            "   french_xpersona             2.239                             0          0 9.382      .5363   .003268                        \n",
            "                                tpb   tps  ups  \n",
            "   all                         3637 18472 5.08  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "14:22:36 | time:43130s total_exs:4194930 total_steps:200180 epochs:8.41 time_left:8134s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         152.9     1  2949 11924       0          0 91.21 1128             16384  8.061    .4126 29.91   \n",
            "   french_blended_skill_talk   214.6                         0          0         88                                   31.15   \n",
            "   french_empathetic_dialogues 69.03                         0          0        443                                   28.32   \n",
            "   french_wizard_of_wikipedia  162.6                         0          0        272                                   37.92   \n",
            "   french_xpersona             165.4                         0          0        325                                   22.27   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.167 1.238e-06 654.7  2647 .001483     .07678 8.817      .5441         0               120148   \n",
            "   french_blended_skill_talk   2.065                             0          0 7.886      .5469         0                        \n",
            "   french_empathetic_dialogues 2.029                       .002257      .1196 7.603      .5615         0                        \n",
            "   french_wizard_of_wikipedia   2.38                       .003676      .1875 10.81      .5218         0                        \n",
            "   french_xpersona             2.194                             0          0  8.97      .5464         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3604 14571 4.043  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:22:46 | time:43140s total_exs:4196210 total_steps:200230 epochs:8.42 time_left:8121s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         136.7     1  2782 14020       0          0   129 1280             16384  7.476    .4256 30.26   \n",
            "   french_blended_skill_talk   199.4                         0          0         67                                   30.67   \n",
            "   french_empathetic_dialogues 54.65                         0          0        563                                   30.01   \n",
            "   french_wizard_of_wikipedia  150.3                         0          0        313                                   39.51   \n",
            "   french_xpersona             142.2                         0          0        337                                   20.83   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.188 1.238e-06 765.9  3860 .001332     .01776 8.998      .5451  .0007418               120198   \n",
            "   french_blended_skill_talk   2.096                             0          0 8.134      .5601         0                        \n",
            "   french_empathetic_dialogues 2.049                       .005329     .07105 7.759      .5562         0                        \n",
            "   french_wizard_of_wikipedia  2.396                             0          0 10.98      .5200         0                        \n",
            "   french_xpersona              2.21                             0          0 9.115      .5440   .002967                        \n",
            "                                tpb   tps  ups  \n",
            "   all                         3548 17880 5.04  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "14:22:55 | Overflow: setting loss scale to 8192.0\n",
            "14:22:58 | time:43151s total_exs:4197474 total_steps:200280 epochs:8.42 time_left:8107s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         148.9 .9800  2882 12711       0          0 111.5 1264             15401  7.693    .4126 30.51   \n",
            "   french_blended_skill_talk   239.3                         0          0         72                                   31.22   \n",
            "   french_empathetic_dialogues  51.3                         0          0        549                                   29.97   \n",
            "   french_wizard_of_wikipedia    146                         0          0        274                                   38.75   \n",
            "   french_xpersona               159                         0          0        369                                   22.11   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.247 1.238e-06 748.8  3303 .001366     .01685 9.512      .5317  .0006775               120248   \n",
            "   french_blended_skill_talk   2.324                             0          0 10.21      .5187         0                        \n",
            "   french_empathetic_dialogues 2.087                       .005464      .0674 8.062      .5453         0                        \n",
            "   french_wizard_of_wikipedia  2.367                             0          0 10.67      .5224         0                        \n",
            "   french_xpersona             2.209                             0          0 9.104      .5405    .00271                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3631 16014 4.411  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:23:07 | time:43161s total_exs:4198810 total_steps:200330 epochs:8.42 time_left:8093s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         132.4     1  2760 13922       0          0 134.8 1336              8192  7.683    .4072  29.2   \n",
            "   french_blended_skill_talk   195.9                         0          0         67                                   28.85   \n",
            "   french_empathetic_dialogues 50.99                         0          0        597                                   27.68   \n",
            "   french_wizard_of_wikipedia  144.6                         0          0        262                                   37.94   \n",
            "   french_xpersona               138                         0          0        410                                   22.31   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.252 1.238e-06   751  3788       0          0 9.609      .5357         0               120298   \n",
            "   french_blended_skill_talk    2.34                             0          0 10.39      .5266         0                        \n",
            "   french_empathetic_dialogues 2.059                             0          0 7.839      .5524         0                        \n",
            "   french_wizard_of_wikipedia  2.434                             0          0  11.4      .5131         0                        \n",
            "   french_xpersona             2.176                             0          0 8.808      .5507         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3511 17710 5.044  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:23:19 | time:43172s total_exs:4200058 total_steps:200380 epochs:8.42 time_left:8080s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         142.8     1  3034 13603       0          0 111.9 1248              8192  7.637    .4018  29.5   \n",
            "   french_blended_skill_talk   189.8                         0          0         93                                   30.15   \n",
            "   french_empathetic_dialogues  63.3                         0          0        501                                   26.15   \n",
            "   french_wizard_of_wikipedia  172.8                         0          0        267                                   39.48   \n",
            "   french_xpersona             145.3                         0          0        387                                   22.23   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.218 1.238e-06   701  3143       0          0 9.298      .5385         0               120348   \n",
            "   french_blended_skill_talk   2.159                             0          0 8.664      .5374         0                        \n",
            "   french_empathetic_dialogues  2.01                             0          0  7.46      .5677         0                        \n",
            "   french_wizard_of_wikipedia  2.425                             0          0  11.3      .5127         0                        \n",
            "   french_xpersona             2.279                             0          0 9.765      .5360         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3735 16746 4.484  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:23:29 | time:43182s total_exs:4201302 total_steps:200430 epochs:8.43 time_left:8066s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         140.3     1  3033 15410       0          0 126.4 1244              8192  8.182    .3969 28.97   \n",
            "   french_blended_skill_talk   191.6                         0          0         96                                   27.36   \n",
            "   french_empathetic_dialogues 56.16                         0          0        470                                    28.3   \n",
            "   french_wizard_of_wikipedia  143.9                         0          0        314                                   37.65   \n",
            "   french_xpersona             169.4                         0          0        364                                   22.57   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.235 1.238e-06 719.2  3654 .001592    .004777 9.428      .5356         0               120398   \n",
            "   french_blended_skill_talk   2.202                             0          0  9.04      .5333         0                        \n",
            "   french_empathetic_dialogues 2.055                             0          0 7.808      .5554         0                        \n",
            "   french_wizard_of_wikipedia  2.413                       .006369     .01911 11.17      .5185         0                        \n",
            "   french_xpersona             2.272                             0          0 9.696      .5351         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3752 19065 5.082  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:23:40 | time:43194s total_exs:4202682 total_steps:200480 epochs:8.43 time_left:8052s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         135.2     1  2775 12041  .03441      1.273 119.8 1380              8192  7.696    .4073 30.45   \n",
            "   french_blended_skill_talk   218.1                     .1310      4.726         84                                   31.51   \n",
            "   french_empathetic_dialogues 48.95                         0          0        643                                   28.26   \n",
            "   french_wizard_of_wikipedia  136.3                   .006689      .3645        299                                   38.58   \n",
            "   french_xpersona             137.6                         0          0        354                                   23.46   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.223 1.238e-06 810.4  3517 .001614      .0787 9.303      .5382         0               120448   \n",
            "   french_blended_skill_talk   2.128                             0          0 8.401      .5489         0                        \n",
            "   french_empathetic_dialogues   2.1                        .00311      .1275 8.165      .5467         0                        \n",
            "   french_wizard_of_wikipedia  2.409                       .003344      .1873 11.12      .5197         0                        \n",
            "   french_xpersona             2.254                             0          0 9.525      .5376         0                        \n",
            "                                tpb   tps  ups  \n",
            "   all                         3585 15558 4.34  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "14:23:50 | time:43204s total_exs:4203974 total_steps:200530 epochs:8.43 time_left:8038s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         131.5     1  2904 14721       0          0   131 1292              8192  7.537    .3983 29.68   \n",
            "   french_blended_skill_talk   159.8                         0          0         90                                   28.51   \n",
            "   french_empathetic_dialogues 59.19                         0          0        570                                   30.17   \n",
            "   french_wizard_of_wikipedia  149.1                         0          0        312                                   36.33   \n",
            "   french_xpersona             158.1                         0          0        320                                   23.69   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.185 1.238e-06   773  3918 .0004386     .01404 8.953      .5425  .0007813   \n",
            "   french_blended_skill_talk    2.13                              0          0 8.416      .5542         0   \n",
            "   french_empathetic_dialogues 2.038                        .001754     .05614 7.676      .5506         0   \n",
            "   french_wizard_of_wikipedia  2.347                              0          0 10.45      .5221         0   \n",
            "   french_xpersona             2.227                              0          0 9.269      .5431   .003125   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       120498 3677 18639 5.069  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:24:01 | time:43215s total_exs:4205222 total_steps:200580 epochs:8.43 time_left:8025s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         141.7     1  2854 12675  .00625      .5531 110.9 1248              8192  7.926    .4104 28.76   \n",
            "   french_blended_skill_talk   211.2                     .0250      2.212         80                                   27.73   \n",
            "   french_empathetic_dialogues 60.58                         0          0        527                                   28.94   \n",
            "   french_wizard_of_wikipedia  153.1                         0          0        279                                   37.16   \n",
            "   french_xpersona             141.8                         0          0        362                                    21.2   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.179 1.238e-06   709  3149 .001845     .04486 8.895      .5472  .0006906               120548   \n",
            "   french_blended_skill_talk   2.086                             0          0 8.054      .5649         0                        \n",
            "   french_empathetic_dialogues 2.082                       .003795     .03605 8.018      .5525         0                        \n",
            "   french_wizard_of_wikipedia   2.37                       .003584      .1434 10.69      .5214         0                        \n",
            "   french_xpersona             2.177                             0          0 8.817      .5499   .002762                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3563 15824 4.442  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:24:11 | time:43225s total_exs:4206406 total_steps:200630 epochs:8.44 time_left:8012s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         142.3     1  2743 14038       0          0 121.2 1184              8192    8.1    .4073  29.9   \n",
            "   french_blended_skill_talk   200.5                         0          0         96                                   28.62   \n",
            "   french_empathetic_dialogues 49.06                         0          0        507                                   30.36   \n",
            "   french_wizard_of_wikipedia  156.6                         0          0        257                                   38.79   \n",
            "   french_xpersona             162.9                         0          0        324                                   21.82   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.205 1.238e-06   698  3572 .002945      .1510 9.144      .5359         0               120598   \n",
            "   french_blended_skill_talk   2.101                             0          0 8.178      .5524         0                        \n",
            "   french_empathetic_dialogues 2.089                        .00789      .4990 8.076      .5479         0                        \n",
            "   french_wizard_of_wikipedia  2.392                       .003891      .1051 10.93      .5124         0                        \n",
            "   french_xpersona              2.24                             0          0  9.39      .5309         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3441 17611 5.119  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:24:22 | time:43236s total_exs:4207758 total_steps:200680 epochs:8.44 time_left:7998s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         123.9     1  2895 12885       0          0 120.3 1352              8192  7.386    .4068 29.01   \n",
            "   french_blended_skill_talk   163.9                         0          0         98                                   28.18   \n",
            "   french_empathetic_dialogues  56.3                         0          0        539                                   28.93   \n",
            "   french_wizard_of_wikipedia  139.6                         0          0        342                                   37.59   \n",
            "   french_xpersona             135.7                         0          0        373                                   21.35   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.204 1.238e-06 782.7  3483 .0004638     .01855 9.111      .5401  .0006702   \n",
            "   french_blended_skill_talk   2.122                              0          0 8.351      .5539         0   \n",
            "   french_empathetic_dialogues 2.085                        .001855     .07421 8.044      .5468         0   \n",
            "   french_wizard_of_wikipedia  2.357                              0          0 10.56      .5219         0   \n",
            "   french_xpersona              2.25                              0          0 9.486      .5376   .002681   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       120648 3678 16368 4.451  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:24:32 | time:43246s total_exs:4208990 total_steps:200730 epochs:8.44 time_left:7985s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         139.4     1  2755 13858       0          0 123.9 1232              8192  7.878    .4151 30.18   \n",
            "   french_blended_skill_talk   202.7                         0          0         58                                   30.16   \n",
            "   french_empathetic_dialogues  56.3                         0          0        523                                   28.37   \n",
            "   french_wizard_of_wikipedia  160.2                         0          0        298                                   41.04   \n",
            "   french_xpersona             138.2                         0          0        353                                   21.16   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.183 1.238e-06 724.7  3645 .000956     .02294 8.971      .5437   .001894               120698   \n",
            "   french_blended_skill_talk   2.068                             0          0  7.91      .5632         0                        \n",
            "   french_empathetic_dialogues 2.062                       .003824     .09178 7.858      .5552   .001912                        \n",
            "   french_wizard_of_wikipedia  2.417                             0          0 11.21      .5137         0                        \n",
            "   french_xpersona             2.187                             0          0 8.907      .5425   .005666                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3480 17504 5.031  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:24:44 | time:43257s total_exs:4210290 total_steps:200780 epochs:8.44 time_left:7971s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         127.4     1  2717 12234       0          0 117.1 1300              8192  7.453    .4165 30.63   \n",
            "   french_blended_skill_talk     171                         0          0         55                                   33.64   \n",
            "   french_empathetic_dialogues 57.96                         0          0        592                                   30.82   \n",
            "   french_wizard_of_wikipedia  130.4                         0          0        302                                   35.89   \n",
            "   french_xpersona             150.3                         0          0        351                                   22.17   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.189 1.238e-06 772.4  3478 .001689     .04012 9.014      .5405         0               120748   \n",
            "   french_blended_skill_talk   2.055                             0          0 7.803      .5535         0                        \n",
            "   french_empathetic_dialogues 2.103                       .006757      .1605  8.19      .5437         0                        \n",
            "   french_wizard_of_wikipedia  2.409                             0          0 11.12      .5170         0                        \n",
            "   french_xpersona             2.191                             0          0  8.94      .5477         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3489 15712 4.504  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:24:54 | time:43267s total_exs:4211606 total_steps:200830 epochs:8.45 time_left:7957s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         135.5     1  2782 14031       0          0 132.7 1316              8192  7.454    .4012  30.3   \n",
            "   french_blended_skill_talk   190.4                         0          0         60                                   32.35   \n",
            "   french_empathetic_dialogues 54.09                         0          0        627                                   29.74   \n",
            "   french_wizard_of_wikipedia  146.4                         0          0        271                                   37.36   \n",
            "   french_xpersona             151.1                         0          0        358                                   21.74   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.186 1.238e-06 768.6  3876 .0007974     .02751  9.03      .5418         0   \n",
            "   french_blended_skill_talk   2.026                              0          0 7.582      .5647         0   \n",
            "   french_empathetic_dialogues  2.06                         .00319      .1100  7.85      .5562         0   \n",
            "   french_wizard_of_wikipedia  2.454                              0          0 11.63      .5065         0   \n",
            "   french_xpersona             2.203                              0          0 9.055      .5398         0   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       120798 3551 17908 5.044  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:25:05 | time:43278s total_exs:4212638 total_steps:200880 epochs:8.45 time_left:7946s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         160.4     1  2918 13172       0          0 93.16 1032              8192  8.308    .4047 30.99   \n",
            "   french_blended_skill_talk     226                         0          0         94                                   33.51   \n",
            "   french_empathetic_dialogues  63.9                         0          0        351                                   31.13   \n",
            "   french_wizard_of_wikipedia  193.5                         0          0        267                                   38.07   \n",
            "   french_xpersona             158.1                         0          0        320                                   21.25   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                          2.17 1.238e-06 619.3  2795 .002849     .05342 8.848      .5472  .0007813               120848   \n",
            "   french_blended_skill_talk   2.037                             0          0 7.666      .5673         0                        \n",
            "   french_empathetic_dialogues 2.081                         .0114      .2137 8.012      .5561         0                        \n",
            "   french_wizard_of_wikipedia  2.392                             0          0 10.94      .5150         0                        \n",
            "   french_xpersona             2.172                             0          0 8.773      .5503   .003125                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3538 15968 4.514  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:25:14 | time:43288s total_exs:4213866 total_steps:200930 epochs:8.45 time_left:7933s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         135.3     1  2917 15024       0          0 126.5 1228              8192  7.634    .4105 29.31   \n",
            "   french_blended_skill_talk   173.9                         0          0         82                                   28.84   \n",
            "   french_empathetic_dialogues  59.3                         0          0        477                                   28.62   \n",
            "   french_wizard_of_wikipedia  146.6                         0          0        314                                   37.57   \n",
            "   french_xpersona             161.3                         0          0        355                                   22.23   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.181 1.238e-06 714.1  3678       0          0 8.945      .5428         0               120898   \n",
            "   french_blended_skill_talk   2.101                             0          0 8.176      .5488         0                        \n",
            "   french_empathetic_dialogues 2.012                             0          0  7.48      .5600         0                        \n",
            "   french_wizard_of_wikipedia  2.397                             0          0 10.99      .5254         0                        \n",
            "   french_xpersona             2.212                             0          0 9.138      .5369         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3631 18702 5.151  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:25:26 | time:43299s total_exs:4215158 total_steps:200980 epochs:8.45 time_left:7919s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         131.8     1  2844 12494       0          0 113.5 1292              8192  7.644    .3983 29.96   \n",
            "   french_blended_skill_talk   170.8                         0          0         68                                   30.22   \n",
            "   french_empathetic_dialogues 57.61                         0          0        571                                   28.88   \n",
            "   french_wizard_of_wikipedia  148.4                         0          0        294                                   38.71   \n",
            "   french_xpersona             150.6                         0          0        359                                   22.01   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.243 1.238e-06 756.6  3323       0          0 9.511      .5373         0               120948   \n",
            "   french_blended_skill_talk   2.202                             0          0 9.042      .5411         0                        \n",
            "   french_empathetic_dialogues 2.058                             0          0 7.833      .5562         0                        \n",
            "   french_wizard_of_wikipedia   2.44                             0          0 11.48      .5090         0                        \n",
            "   french_xpersona             2.271                             0          0 9.693      .5428         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3601 15818 4.393  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:25:36 | time:43309s total_exs:4216466 total_steps:201030 epochs:8.46 time_left:7905s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         136.4     1  2877 14668  .02074      1.346 133.4 1308              8192  7.649    .4047  29.8   \n",
            "   french_blended_skill_talk     219                    .07692      4.872         78                                   32.05   \n",
            "   french_empathetic_dialogues 53.43                         0          0        488                                   28.41   \n",
            "   french_wizard_of_wikipedia  137.5                   .006024      .5120        332                                   37.65   \n",
            "   french_xpersona             135.6                         0          0        410                                   21.08   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.209 1.238e-06 749.8  3823 .0005123    .007172 9.201      .5403         0   \n",
            "   french_blended_skill_talk   2.094                              0          0  8.12      .5568         0   \n",
            "   french_empathetic_dialogues 2.072                        .002049     .02869 7.937      .5514         0   \n",
            "   french_wizard_of_wikipedia  2.421                              0          0 11.26      .5147         0   \n",
            "   french_xpersona             2.249                              0          0 9.483      .5382         0   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       120998 3627 18491 5.099  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:25:47 | time:43321s total_exs:4217726 total_steps:201080 epochs:8.46 time_left:7892s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         138.3     1  2919 12986       0          0 112.1 1260              8192   8.28    .3996 30.18   \n",
            "   french_blended_skill_talk   189.1                         0          0         84                                   32.18   \n",
            "   french_empathetic_dialogues 56.31                         0          0        525                                   27.22   \n",
            "   french_wizard_of_wikipedia    150                         0          0        289                                   38.38   \n",
            "   french_xpersona             157.9                         0          0        362                                   22.94   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.206 1.238e-06 727.8  3238       0          0 9.114      .5409         0               121048   \n",
            "   french_blended_skill_talk   2.135                             0          0 8.455      .5527         0                        \n",
            "   french_empathetic_dialogues 2.118                             0          0  8.31      .5443         0                        \n",
            "   french_wizard_of_wikipedia  2.332                             0          0  10.3      .5243         0                        \n",
            "   french_xpersona              2.24                             0          0  9.39      .5423         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3647 16224 4.449  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:25:57 | time:43331s total_exs:4219018 total_steps:201130 epochs:8.46 time_left:7878s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         140.8     1  2806 14184  .01182      .3936 130.6 1292              8192  7.935    .4048 29.47   \n",
            "   french_blended_skill_talk   223.6                    .03797      1.304         79                                   30.66   \n",
            "   french_empathetic_dialogues 57.41                         0          0        587                                   28.72   \n",
            "   french_wizard_of_wikipedia  133.9                   .003831      .1801        261                                      37   \n",
            "   french_xpersona             148.4                   .005479     .09041        365                                   21.49   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.218 1.238e-06 732.5  3703  .00181      .1008 9.259      .5377         0               121098   \n",
            "   french_blended_skill_talk   2.139                             0          0 8.489      .5462         0                        \n",
            "   french_empathetic_dialogues 2.071                       .003407      .1618 7.935      .5544         0                        \n",
            "   french_wizard_of_wikipedia  2.393                       .003831      .2414 10.95      .5149         0                        \n",
            "   french_xpersona             2.269                             0          0 9.666      .5354         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3538 17888 5.056  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:26:08 | time:43342s total_exs:4220322 total_steps:201180 epochs:8.46 time_left:7864s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         143.8     1  2797 12595   .0167      1.173 117.4 1304              8192   8.37    .3920 30.98   \n",
            "   french_blended_skill_talk   225.8                    .05882      4.329         85                                   35.38   \n",
            "   french_empathetic_dialogues 53.98                         0          0        608                                   26.76   \n",
            "   french_wizard_of_wikipedia  165.2                   .007968      .3625        251                                    40.9   \n",
            "   french_xpersona             130.1                         0          0        360                                   20.87   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.246 1.238e-06 740.9  3336 .000996     .01295 9.532      .5342         0               121148   \n",
            "   french_blended_skill_talk   2.246                             0          0 9.452      .5318         0                        \n",
            "   french_empathetic_dialogues 2.071                             0          0 7.935      .5566         0                        \n",
            "   french_wizard_of_wikipedia  2.432                       .003984     .05179 11.38      .5084         0                        \n",
            "   french_xpersona             2.236                             0          0 9.355      .5400         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3538 15931 4.504  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:26:18 | time:43351s total_exs:4221662 total_steps:201230 epochs:8.47 time_left:7850s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         131.4     1  2850 14696       0          0 138.2 1340              8192  7.628    .4013    30   \n",
            "   french_blended_skill_talk   162.6                         0          0         60                                   32.27   \n",
            "   french_empathetic_dialogues 55.57                         0          0        653                                   27.58   \n",
            "   french_wizard_of_wikipedia  153.3                         0          0        263                                   39.09   \n",
            "   french_xpersona             154.3                         0          0        364                                   21.08   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.224 1.238e-06 757.5  3905 .0003828     .01034 9.297      .5379  .0006868   \n",
            "   french_blended_skill_talk    2.26                              0          0 9.581      .5351         0   \n",
            "   french_empathetic_dialogues 2.068                        .001531     .04135 7.912      .5506         0   \n",
            "   french_wizard_of_wikipedia  2.364                              0          0 10.63      .5206         0   \n",
            "   french_xpersona             2.204                              0          0  9.06      .5453   .002747   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       121198 3608 18601 5.156  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:26:29 | time:43363s total_exs:4222898 total_steps:201280 epochs:8.47 time_left:7837s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         139.6     1  2749 12126       0          0   109 1236              8192  7.649    .3984 30.48   \n",
            "   french_blended_skill_talk   213.8                         0          0         94                                    31.2   \n",
            "   french_empathetic_dialogues 52.46                         0          0        524                                   29.43   \n",
            "   french_wizard_of_wikipedia  150.3                         0          0        271                                   37.85   \n",
            "   french_xpersona             141.6                         0          0        347                                   23.44   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.245 1.238e-06   733  3233 .0004771     .04342 9.513      .5329  .0007205   \n",
            "   french_blended_skill_talk   2.185                              0          0 8.893      .5336         0   \n",
            "   french_empathetic_dialogues 2.113                        .001908      .1737 8.269      .5491         0   \n",
            "   french_wizard_of_wikipedia  2.447                              0          0 11.56      .5082         0   \n",
            "   french_xpersona             2.234                              0          0 9.336      .5406   .002882   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       121248 3482 15359 4.411  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:26:39 | time:43373s total_exs:4224066 total_steps:201330 epochs:8.47 time_left:7825s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         142.2     1  2859 14673       0          0 119.9 1168              8192  7.878    .3997 31.28   \n",
            "   french_blended_skill_talk   188.7                         0          0         68                                   31.44   \n",
            "   french_empathetic_dialogues 62.69                         0          0        463                                   30.58   \n",
            "   french_wizard_of_wikipedia    157                         0          0        326                                   39.25   \n",
            "   french_xpersona             160.5                         0          0        311                                   23.86   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.244 1.238e-06 727.9  3736 .002927     .06623 9.509      .5364         0               121298   \n",
            "   french_blended_skill_talk   2.206                             0          0 9.076      .5346         0                        \n",
            "   french_empathetic_dialogues 2.072                       .008639      .1944 7.941      .5530         0                        \n",
            "   french_wizard_of_wikipedia  2.421                       .003067     .07055 11.26      .5209         0                        \n",
            "   french_xpersona             2.278                             0          0 9.757      .5371         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3587 18409 5.133  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:26:50 | time:43384s total_exs:4225362 total_steps:201380 epochs:8.47 time_left:7811s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         145.7     1  2915 12910  .02335      1.377 114.8 1296              8192  7.916    .4001 29.32   \n",
            "   french_blended_skill_talk   238.3                    .06897      4.276         87                                   32.71   \n",
            "   french_empathetic_dialogues 54.98                         0          0        539                                   27.52   \n",
            "   french_wizard_of_wikipedia  151.5                     .0219      1.077        274                                   35.81   \n",
            "   french_xpersona             137.9                   .002525      .1566        396                                   21.25   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.167 1.238e-06 718.1  3181       0          0 8.772      .5454  .0006313               121348   \n",
            "   french_blended_skill_talk   2.119                             0          0 8.323      .5464         0                        \n",
            "   french_empathetic_dialogues  2.05                             0          0  7.77      .5589         0                        \n",
            "   french_wizard_of_wikipedia  2.312                             0          0  10.1      .5305         0                        \n",
            "   french_xpersona             2.186                             0          0 8.896      .5458   .002525                        \n",
            "                                tpb   tps  ups  \n",
            "   all                         3633 16091 4.43  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "14:27:00 | time:43394s total_exs:4226706 total_steps:201430 epochs:8.48 time_left:7796s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         135.3     1  2739 13806  .01114      .5393 135.5 1344              8192  7.711    .4048 29.78   \n",
            "   french_blended_skill_talk   213.8                    .03797      1.684         79                                   31.38   \n",
            "   french_empathetic_dialogues 49.17                         0          0        620                                   28.74   \n",
            "   french_wizard_of_wikipedia  136.1                   .006579      .4737        304                                   36.84   \n",
            "   french_xpersona             142.1                         0          0        341                                   22.16   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.194 1.238e-06 780.9  3937 .0004032    .002016  9.01      .5395         0   \n",
            "   french_blended_skill_talk    2.17                              0          0 8.761      .5385         0   \n",
            "   french_empathetic_dialogues  2.08                        .001613    .008065 8.002      .5535         0   \n",
            "   french_wizard_of_wikipedia  2.325                              0          0 10.22      .5236         0   \n",
            "   french_xpersona             2.203                              0          0 9.054      .5422         0   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       121398 3520 17743 5.041  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:27:11 | time:43405s total_exs:4227938 total_steps:201480 epochs:8.48 time_left:7783s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         144.8     1  2879 13011       0          0 111.4 1232              8192  7.657    .3828 31.07   \n",
            "   french_blended_skill_talk   216.6                         0          0         73                                   32.44   \n",
            "   french_empathetic_dialogues 60.95                         0          0        513                                   29.48   \n",
            "   french_wizard_of_wikipedia  157.1                         0          0        281                                   39.13   \n",
            "   french_xpersona             144.4                         0          0        365                                   23.25   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.228 1.238e-06 739.4  3341 .0004873   .0009747 9.388      .5376         0   \n",
            "   french_blended_skill_talk    2.08                              0          0 8.002      .5553         0   \n",
            "   french_empathetic_dialogues 2.096                        .001949    .003899 8.137      .5483         0   \n",
            "   french_wizard_of_wikipedia  2.452                              0          0 11.61      .5159         0   \n",
            "   french_xpersona             2.283                              0          0 9.802      .5308         0   \n",
            "                                total_train_updates  tpb   tps  ups  \n",
            "   all                                       121448 3618 16352 4.52  \n",
            "   french_blended_skill_talk                                         \n",
            "   french_empathetic_dialogues                                       \n",
            "   french_wizard_of_wikipedia                                        \n",
            "   french_xpersona\n",
            "\n",
            "14:27:21 | time:43415s total_exs:4229218 total_steps:201530 epochs:8.48 time_left:7770s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         142.1     1  2840 14519 .008772      .2061 130.9 1280              8192  7.798    .4128 31.04   \n",
            "   french_blended_skill_talk   213.5                    .03509      .8246         57                                   36.12   \n",
            "   french_empathetic_dialogues 53.89                         0          0        558                                   27.65   \n",
            "   french_wizard_of_wikipedia  153.8                         0          0        289                                   38.15   \n",
            "   french_xpersona             147.2                         0          0        376                                   22.24   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.182 1.238e-06 737.3  3770 .0008651    .008651 8.986      .5437         0   \n",
            "   french_blended_skill_talk   1.958                              0          0 7.084      .5794         0   \n",
            "   french_empathetic_dialogues 2.117                              0          0 8.302      .5449         0   \n",
            "   french_wizard_of_wikipedia  2.398                         .00346      .0346    11      .5172         0   \n",
            "   french_xpersona             2.258                              0          0 9.563      .5335         0   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       121498 3577 18289 5.113  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:27:33 | time:43426s total_exs:4230422 total_steps:201580 epochs:8.48 time_left:7757s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         142.3     1  2766 12125  .01499      .3690 105.6 1204              8192  7.843    .4075 31.58   \n",
            "   french_blended_skill_talk   196.7                    .04348      .8587         92                                   32.51   \n",
            "   french_empathetic_dialogues 56.59                         0          0        542                                   30.38   \n",
            "   french_wizard_of_wikipedia  152.3                     .0165      .6172        303                                    41.9   \n",
            "   french_xpersona             163.4                         0          0        267                                   21.53   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.185 1.238e-06   758  3323       0          0 8.991      .5438         0               121548   \n",
            "   french_blended_skill_talk   2.039                             0          0 7.687      .5664         0                        \n",
            "   french_empathetic_dialogues 2.071                             0          0  7.93      .5490         0                        \n",
            "   french_wizard_of_wikipedia  2.404                             0          0 11.07      .5190         0                        \n",
            "   french_xpersona             2.228                             0          0 9.279      .5410         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3524 15448 4.385  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:27:43 | time:43436s total_exs:4231662 total_steps:201630 epochs:8.49 time_left:7744s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         137.6     1  2831 14314   .0109      .5699 125.4 1240              8192  7.846    .4049 30.49   \n",
            "   french_blended_skill_talk   194.4                    .03093       2.01         97                                   29.28   \n",
            "   french_empathetic_dialogues 59.83                   .001919     .03647        521                                   29.32   \n",
            "   french_wizard_of_wikipedia  152.7                    .01075      .2330        279                                   40.84   \n",
            "   french_xpersona             143.3                         0          0        343                                   22.52   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.171 1.238e-06 744.6  3765 .0004798     .00144 8.868      .5425         0   \n",
            "   french_blended_skill_talk   2.054                              0          0 7.797      .5592         0   \n",
            "   french_empathetic_dialogues 2.015                        .001919    .005758 7.499      .5575         0   \n",
            "   french_wizard_of_wikipedia  2.403                              0          0 11.05      .5108         0   \n",
            "   french_xpersona             2.211                              0          0 9.126      .5424         0   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       121598 3575 18079 5.057  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:27:54 | time:43447s total_exs:4232998 total_steps:201680 epochs:8.49 time_left:7730s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                           126     1  2624 11891       0          0 121.1 1336              8192   7.55    .3943 30.51   \n",
            "   french_blended_skill_talk   160.7                         0          0         66                                   31.39   \n",
            "   french_empathetic_dialogues 52.38                         0          0        683                                   28.78   \n",
            "   french_wizard_of_wikipedia  152.4                         0          0        252                                   39.09   \n",
            "   french_xpersona             138.6                         0          0        335                                   22.78   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.213 1.238e-06   784  3552 .0007321    .004026 9.245      .5374         0   \n",
            "   french_blended_skill_talk   2.061                              0          0 7.851      .5526         0   \n",
            "   french_empathetic_dialogues 2.082                        .002928     .01611  8.02      .5513         0   \n",
            "   french_wizard_of_wikipedia  2.425                              0          0 11.31      .5168         0   \n",
            "   french_xpersona             2.283                              0          0 9.801      .5290         0   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       121648 3408 15443 4.532  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:28:04 | time:43457s total_exs:4234370 total_steps:201730 epochs:8.49 time_left:7715s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         131.2     1  2655 13564       0          0 140.2 1372              8192  7.257    .4167 29.14   \n",
            "   french_blended_skill_talk   186.3                         0          0         65                                   28.45   \n",
            "   french_empathetic_dialogues 56.43                         0          0        753                                   29.31   \n",
            "   french_wizard_of_wikipedia  140.1                         0          0        240                                   36.56   \n",
            "   french_xpersona             141.8                         0          0        314                                   22.26   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.159 1.238e-06 793.6  4054       0          0 8.751      .5475         0               121698   \n",
            "   french_blended_skill_talk   2.006                             0          0  7.43      .5684         0                        \n",
            "   french_empathetic_dialogues 2.049                             0          0 7.761      .5551         0                        \n",
            "   french_wizard_of_wikipedia  2.367                             0          0 10.66      .5240         0                        \n",
            "   french_xpersona             2.214                             0          0  9.15      .5423         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3449 17618 5.109  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:28:15 | time:43469s total_exs:4235630 total_steps:201780 epochs:8.49 time_left:7702s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         150.8     1  2890 12760 .006098     .08232 111.3 1260              8192  7.691    .4186 30.29   \n",
            "   french_blended_skill_talk   217.1                    .02439      .3293         82                                   31.95   \n",
            "   french_empathetic_dialogues 56.18                         0          0        617                                   30.25   \n",
            "   french_wizard_of_wikipedia  171.1                         0          0        241                                   36.56   \n",
            "   french_xpersona             158.8                         0          0        320                                   22.41   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.163 1.238e-06 744.2  3286 .0008104     .02229 8.777      .5476  .0007813   \n",
            "   french_blended_skill_talk    2.04                              0          0 7.691      .5626         0   \n",
            "   french_empathetic_dialogues 2.051                        .003241     .08914 7.777      .5547         0   \n",
            "   french_wizard_of_wikipedia  2.378                              0          0 10.78      .5236         0   \n",
            "   french_xpersona             2.181                              0          0 8.856      .5494   .003125   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       121748 3634 16046 4.415  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:28:25 | time:43479s total_exs:4236990 total_steps:201830 epochs:8.50 time_left:7687s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         128.6     1  2679 13361       0          0 135.6 1360              8192  7.182    .4055 30.39   \n",
            "   french_blended_skill_talk   176.6                         0          0         80                                   33.34   \n",
            "   french_empathetic_dialogues 52.64                         0          0        699                                   28.36   \n",
            "   french_wizard_of_wikipedia  139.3                         0          0        267                                    36.6   \n",
            "   french_xpersona               146                         0          0        314                                   23.27   \n",
            "                                loss        lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  \\\n",
            "   all                         2.197 1.238e-06 791.3  3946 .0003577    .002146 9.075      .5425   .001154   \n",
            "   french_blended_skill_talk   2.094                              0          0 8.121      .5501         0   \n",
            "   french_empathetic_dialogues  2.07                        .001431    .008584 7.923      .5543   .001431   \n",
            "   french_wizard_of_wikipedia    2.4                              0          0 11.03      .5213         0   \n",
            "   french_xpersona             2.223                              0          0 9.232      .5442   .003185   \n",
            "                                total_train_updates  tpb   tps   ups  \n",
            "   all                                       121798 3471 17307 4.987  \n",
            "   french_blended_skill_talk                                          \n",
            "   french_empathetic_dialogues                                        \n",
            "   french_wizard_of_wikipedia                                         \n",
            "   french_xpersona\n",
            "\n",
            "14:28:36 | time:43490s total_exs:4238222 total_steps:201880 epochs:8.50 time_left:7674s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         142.1     1  2785 12689 .006299      .1489 112.2 1232              8192  8.313    .3945 30.51   \n",
            "   french_blended_skill_talk     208                    .01429      .4429         70                                   33.31   \n",
            "   french_empathetic_dialogues 60.09                         0          0        549                                   27.93   \n",
            "   french_wizard_of_wikipedia    155                    .01091      .1527        275                                   38.95   \n",
            "   french_xpersona             145.5                         0          0        338                                   21.83   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                          2.19 1.238e-06   713  3248 .001366     .04736 8.999      .5446         0               121848   \n",
            "   french_blended_skill_talk   2.102                             0          0 8.184      .5600         0                        \n",
            "   french_empathetic_dialogues 2.062                       .005464      .1894 7.862      .5522         0                        \n",
            "   french_wizard_of_wikipedia  2.372                             0          0 10.72      .5253         0                        \n",
            "   french_xpersona             2.222                             0          0 9.226      .5408         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3498 15937 4.556  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:28:40 | time:43493s total_exs:4238654 total_steps:201899 epochs:8.50 time_left:7670s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         150.4     1  2957 15320       0          0 117.8  432              8192  7.443    .3961 31.08   \n",
            "   french_blended_skill_talk   215.5                         0          0         31                                   31.26   \n",
            "   french_empathetic_dialogues 69.87                         0          0        157                                   31.71   \n",
            "   french_wizard_of_wikipedia  159.8                         0          0        112                                   37.78   \n",
            "   french_xpersona             156.4                         0          0        132                                   23.58   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.195 1.238e-06 699.3  3623 .002232     .01116 9.039      .5473         0               121867   \n",
            "   french_blended_skill_talk   2.225                             0          0 9.256      .5542         0                        \n",
            "   french_empathetic_dialogues 2.026                             0          0 7.583      .5581         0                        \n",
            "   french_wizard_of_wikipedia  2.358                       .008929     .04464 10.57      .5270         0                        \n",
            "   french_xpersona             2.169                             0          0 8.752      .5500         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3656 18943 5.182  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "14:28:40 | creating task(s): french_blended_skill_talk\n",
            "14:28:40 | Loading ParlAI text data: /content/dataset_french_bst/valid.txt\n",
            "14:28:42 | creating task(s): french_wizard_of_wikipedia\n",
            "14:28:42 | Loading ParlAI text data: /content/dataset_french_wow/valid.txt\n",
            "14:28:45 | creating task(s): french_xpersona\n",
            "14:28:45 | Loading ParlAI text data: /content/dataset_french_xpersona/valid.txt\n",
            "14:28:47 | creating task(s): french_empathetic_dialogues\n",
            "14:28:47 | Loading ParlAI text data: /content/dataset_french_ed/valid.txt\n",
            "14:28:49 | running eval: valid\n",
            "14:31:00 | eval completed in 130.13s\n",
            "14:31:00 | \u001b[1mvalid:\n",
            "                                clen  ctpb  ctps   ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n",
            "   all                         144.9  3417 55408  .004689      .2258 278.6 43257    .1658 30.78 2.229 1.238e-06 532.4  8632   \n",
            "   french_blended_skill_talk   199.5               .01395      .6120       12476          30.98 2.172                         \n",
            "   french_empathetic_dialogues 60.69                    0          0        9308             31 2.103                         \n",
            "   french_wizard_of_wikipedia  150.9              .004257      .2811       17853          38.01 2.407                         \n",
            "   french_xpersona             168.6             .0005525     .01022        3620          23.13 2.233                         \n",
            "                                  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  \n",
            "   all                           .000955     .02212  9.35      .5371 2.686e-05               121867 3950 64040  \n",
            "   french_blended_skill_talk   8.015e-05   .0005611 8.779      .5431         0                                  \n",
            "   french_empathetic_dialogues   .001612     .04738 8.189      .5452  .0001074                                  \n",
            "   french_wizard_of_wikipedia    .002128     .04055  11.1      .5161         0                                  \n",
            "   french_xpersona                     0          0 9.332      .5437         0\n",
            "\u001b[0m\n",
            "14:31:00 | \u001b[1mdid not beat best ppl: 3.3923 impatience: 13\u001b[0m\n",
            "14:31:00 | saving model checkpoint: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.checkpoint\n",
            "14:31:04 | ran out of patience! stopping training.\n",
            "14:31:11 | \u001b[33mOverriding opt[\"multitask_weights\"] to (1.0, 3.0, 3.0, 3.0) (previously: [1.0, 3.0, 3.0, 3.0])\u001b[0m\n",
            "14:31:11 | \u001b[33mOverriding opt[\"init_model\"] to zoo:blender/blender_90M/model (previously: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.checkpoint)\u001b[0m\n",
            "14:31:11 | \u001b[33mOverriding opt[\"dict_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict (previously: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.checkpoint.dict)\u001b[0m\n",
            "14:31:11 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "14:31:11 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "14:31:11 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--optimizer mem_eff_adam\u001b[0m\n",
            "14:31:11 | Using CUDA\n",
            "14:31:11 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.checkpoint.dict\n",
            "14:31:11 | num words = 54944\n",
            "14:31:13 | Total parameters: 87,508,992 (86,984,704 trainable)\n",
            "14:31:13 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.checkpoint\n",
            "14:31:16 | creating task(s): french_blended_skill_talk\n",
            "14:31:16 | Loading ParlAI text data: /content/dataset_french_bst/valid.txt\n",
            "14:31:18 | creating task(s): french_wizard_of_wikipedia\n",
            "14:31:18 | Loading ParlAI text data: /content/dataset_french_wow/valid.txt\n",
            "14:31:21 | creating task(s): french_xpersona\n",
            "14:31:21 | Loading ParlAI text data: /content/dataset_french_xpersona/valid.txt\n",
            "14:31:23 | creating task(s): french_empathetic_dialogues\n",
            "14:31:23 | Loading ParlAI text data: /content/dataset_french_ed/valid.txt\n",
            "14:31:25 | running eval: valid\n",
            "14:33:38 | eval completed in 132.99s\n",
            "14:33:38 | \u001b[1mvalid:\n",
            "                                clen  ctpb  ctps   ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n",
            "   all                         144.9  3432 53388  .004689      .2258 268.5 43257    .1546 30.78 2.229 1.238e-06 534.6  8317   \n",
            "   french_blended_skill_talk   199.5               .01395      .6120       12476          30.98 2.172                         \n",
            "   french_empathetic_dialogues 60.69                    0          0        9308             31 2.103                         \n",
            "   french_wizard_of_wikipedia  150.9              .004257      .2811       17853          38.01 2.407                         \n",
            "   french_xpersona             168.6             .0005525     .01022        3620          23.13 2.233                         \n",
            "                                  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  \n",
            "   all                           .000955     .02212  9.35      .5370 2.686e-05               121867 3966 61705  \n",
            "   french_blended_skill_talk   8.015e-05   .0005611 8.779      .5431         0                                  \n",
            "   french_empathetic_dialogues   .001612     .04738 8.189      .5452  .0001074                                  \n",
            "   french_wizard_of_wikipedia    .002128     .04055  11.1      .5160         0                                  \n",
            "   french_xpersona                     0          0 9.332      .5437         0\n",
            "\u001b[0m\n",
            "14:33:38 | creating task(s): french_blended_skill_talk\n",
            "14:33:38 | Loading ParlAI text data: /content/dataset_french_bst/test.txt\n",
            "14:33:40 | creating task(s): french_wizard_of_wikipedia\n",
            "14:33:40 | Loading ParlAI text data: /content/dataset_french_wow/test.txt\n",
            "14:33:42 | creating task(s): french_xpersona\n",
            "14:33:42 | Loading ParlAI text data: /content/dataset_french_xpersona/test.txt\n",
            "14:33:44 | creating task(s): french_empathetic_dialogues\n",
            "14:33:44 | Loading ParlAI text data: /content/dataset_french_ed/test.txt\n",
            "14:33:46 | running eval: test\n",
            "14:35:56 | eval completed in 129.66s\n",
            "14:35:56 | \u001b[1mtest:\n",
            "                                clen  ctpb  ctps   ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n",
            "   all                         147.5  3429 54228  .005961      .3297 268.5 42087    .1545  31.5 2.244 1.238e-06 540.1  8541   \n",
            "   french_blended_skill_talk   203.1               .01939      1.094       12121          31.82 2.169                         \n",
            "   french_empathetic_dialogues 64.16             .0001187    .001187        8426          32.52 2.147                         \n",
            "   french_wizard_of_wikipedia  151.7              .003242      .2063       17889          38.25 2.408                         \n",
            "   french_xpersona               171              .001096      .0178        3651          23.41 2.254                         \n",
            "                                 ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  \n",
            "   all                         .0008269     .02161 9.485      .5340 5.762e-05               121867 3969 62768  \n",
            "   french_blended_skill_talk    .000165    .006023  8.75      .5451         0                                  \n",
            "   french_empathetic_dialogues  .002136     .05578 8.556      .5388  .0001187                                  \n",
            "   french_wizard_of_wikipedia   .001006     .02465 11.11      .5163  .0001118                                  \n",
            "   french_xpersona                    0          0 9.525      .5360         0\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "128fd40b58b1410aaef82c012c5c9ed3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>clen/train</td><td></td></tr><tr><td>clen/valid</td><td></td></tr><tr><td>clip/train</td><td></td></tr><tr><td>ctpb/train</td><td></td></tr><tr><td>ctpb/valid</td><td></td></tr><tr><td>ctps/train</td><td></td></tr><tr><td>ctps/valid</td><td></td></tr><tr><td>ctrunc/train</td><td></td></tr><tr><td>ctrunc/valid</td><td></td></tr><tr><td>ctrunclen/train</td><td></td></tr><tr><td>ctrunclen/valid</td><td></td></tr><tr><td>custom_step</td><td></td></tr><tr><td>exps/train</td><td></td></tr><tr><td>exps/valid</td><td></td></tr><tr><td>exs/train</td><td></td></tr><tr><td>exs/valid</td><td></td></tr><tr><td>fp16_loss_scalar/train</td><td></td></tr><tr><td>french_blended_skill_talk/clen/train</td><td></td></tr><tr><td>french_blended_skill_talk/clen/valid</td><td></td></tr><tr><td>french_blended_skill_talk/ctrunc/train</td><td></td></tr><tr><td>french_blended_skill_talk/ctrunc/valid</td><td></td></tr><tr><td>french_blended_skill_talk/ctrunclen/train</td><td></td></tr><tr><td>french_blended_skill_talk/ctrunclen/valid</td><td></td></tr><tr><td>french_blended_skill_talk/exs/train</td><td></td></tr><tr><td>french_blended_skill_talk/exs/valid</td><td></td></tr><tr><td>french_blended_skill_talk/llen/train</td><td></td></tr><tr><td>french_blended_skill_talk/llen/valid</td><td></td></tr><tr><td>french_blended_skill_talk/loss/train</td><td></td></tr><tr><td>french_blended_skill_talk/loss/valid</td><td></td></tr><tr><td>french_blended_skill_talk/ltrunc/train</td><td></td></tr><tr><td>french_blended_skill_talk/ltrunc/valid</td><td></td></tr><tr><td>french_blended_skill_talk/ltrunclen/train</td><td></td></tr><tr><td>french_blended_skill_talk/ltrunclen/valid</td><td></td></tr><tr><td>french_blended_skill_talk/ppl/train</td><td></td></tr><tr><td>french_blended_skill_talk/ppl/valid</td><td></td></tr><tr><td>french_blended_skill_talk/token_acc/train</td><td></td></tr><tr><td>french_blended_skill_talk/token_acc/valid</td><td></td></tr><tr><td>french_blended_skill_talk/token_em/train</td><td></td></tr><tr><td>french_blended_skill_talk/token_em/valid</td><td></td></tr><tr><td>french_empathetic_dialogues/clen/train</td><td></td></tr><tr><td>french_empathetic_dialogues/clen/valid</td><td></td></tr><tr><td>french_empathetic_dialogues/ctrunc/train</td><td></td></tr><tr><td>french_empathetic_dialogues/ctrunc/valid</td><td></td></tr><tr><td>french_empathetic_dialogues/ctrunclen/train</td><td></td></tr><tr><td>french_empathetic_dialogues/ctrunclen/valid</td><td></td></tr><tr><td>french_empathetic_dialogues/exs/train</td><td></td></tr><tr><td>french_empathetic_dialogues/exs/valid</td><td></td></tr><tr><td>french_empathetic_dialogues/llen/train</td><td></td></tr><tr><td>french_empathetic_dialogues/llen/valid</td><td></td></tr><tr><td>french_empathetic_dialogues/loss/train</td><td></td></tr><tr><td>french_empathetic_dialogues/loss/valid</td><td></td></tr><tr><td>french_empathetic_dialogues/ltrunc/train</td><td></td></tr><tr><td>french_empathetic_dialogues/ltrunc/valid</td><td></td></tr><tr><td>french_empathetic_dialogues/ltrunclen/train</td><td></td></tr><tr><td>french_empathetic_dialogues/ltrunclen/valid</td><td></td></tr><tr><td>french_empathetic_dialogues/ppl/train</td><td></td></tr><tr><td>french_empathetic_dialogues/ppl/valid</td><td></td></tr><tr><td>french_empathetic_dialogues/token_acc/train</td><td></td></tr><tr><td>french_empathetic_dialogues/token_acc/valid</td><td></td></tr><tr><td>french_empathetic_dialogues/token_em/train</td><td></td></tr><tr><td>french_empathetic_dialogues/token_em/valid</td><td></td></tr><tr><td>french_wizard_of_wikipedia/clen/train</td><td></td></tr><tr><td>french_wizard_of_wikipedia/clen/valid</td><td></td></tr><tr><td>french_wizard_of_wikipedia/ctrunc/train</td><td></td></tr><tr><td>french_wizard_of_wikipedia/ctrunc/valid</td><td></td></tr><tr><td>french_wizard_of_wikipedia/ctrunclen/train</td><td></td></tr><tr><td>french_wizard_of_wikipedia/ctrunclen/valid</td><td></td></tr><tr><td>french_wizard_of_wikipedia/exs/train</td><td></td></tr><tr><td>french_wizard_of_wikipedia/exs/valid</td><td></td></tr><tr><td>french_wizard_of_wikipedia/llen/train</td><td></td></tr><tr><td>french_wizard_of_wikipedia/llen/valid</td><td></td></tr><tr><td>french_wizard_of_wikipedia/loss/train</td><td></td></tr><tr><td>french_wizard_of_wikipedia/loss/valid</td><td></td></tr><tr><td>french_wizard_of_wikipedia/ltrunc/train</td><td></td></tr><tr><td>french_wizard_of_wikipedia/ltrunc/valid</td><td></td></tr><tr><td>french_wizard_of_wikipedia/ltrunclen/train</td><td></td></tr><tr><td>french_wizard_of_wikipedia/ltrunclen/valid</td><td></td></tr><tr><td>french_wizard_of_wikipedia/ppl/train</td><td></td></tr><tr><td>french_wizard_of_wikipedia/ppl/valid</td><td></td></tr><tr><td>french_wizard_of_wikipedia/token_acc/train</td><td></td></tr><tr><td>french_wizard_of_wikipedia/token_acc/valid</td><td></td></tr><tr><td>french_wizard_of_wikipedia/token_em/train</td><td></td></tr><tr><td>french_wizard_of_wikipedia/token_em/valid</td><td></td></tr><tr><td>french_xpersona/clen/train</td><td></td></tr><tr><td>french_xpersona/clen/valid</td><td></td></tr><tr><td>french_xpersona/ctrunc/train</td><td></td></tr><tr><td>french_xpersona/ctrunc/valid</td><td></td></tr><tr><td>french_xpersona/ctrunclen/train</td><td></td></tr><tr><td>french_xpersona/ctrunclen/valid</td><td></td></tr><tr><td>french_xpersona/exs/train</td><td></td></tr><tr><td>french_xpersona/exs/valid</td><td></td></tr><tr><td>french_xpersona/llen/train</td><td></td></tr><tr><td>french_xpersona/llen/valid</td><td></td></tr><tr><td>french_xpersona/loss/train</td><td></td></tr><tr><td>french_xpersona/loss/valid</td><td></td></tr><tr><td>french_xpersona/ltrunc/train</td><td></td></tr><tr><td>french_xpersona/ltrunc/valid</td><td></td></tr><tr><td>french_xpersona/ltrunclen/train</td><td></td></tr><tr><td>french_xpersona/ltrunclen/valid</td><td></td></tr><tr><td>french_xpersona/ppl/train</td><td></td></tr><tr><td>french_xpersona/ppl/valid</td><td></td></tr><tr><td>french_xpersona/token_acc/train</td><td></td></tr><tr><td>french_xpersona/token_acc/valid</td><td></td></tr><tr><td>french_xpersona/token_em/train</td><td></td></tr><tr><td>french_xpersona/token_em/valid</td><td></td></tr><tr><td>gnorm/train</td><td></td></tr><tr><td>gpu_mem/train</td><td></td></tr><tr><td>gpu_mem/valid</td><td></td></tr><tr><td>llen/train</td><td></td></tr><tr><td>llen/valid</td><td></td></tr><tr><td>loss/train</td><td></td></tr><tr><td>loss/valid</td><td></td></tr><tr><td>lr/train</td><td></td></tr><tr><td>lr/valid</td><td></td></tr><tr><td>ltpb/train</td><td></td></tr><tr><td>ltpb/valid</td><td></td></tr><tr><td>ltps/train</td><td></td></tr><tr><td>ltps/valid</td><td></td></tr><tr><td>ltrunc/train</td><td></td></tr><tr><td>ltrunc/valid</td><td></td></tr><tr><td>ltrunclen/train</td><td></td></tr><tr><td>ltrunclen/valid</td><td></td></tr><tr><td>ppl/train</td><td></td></tr><tr><td>ppl/valid</td><td></td></tr><tr><td>token_acc/train</td><td></td></tr><tr><td>token_acc/valid</td><td></td></tr><tr><td>token_em/train</td><td></td></tr><tr><td>token_em/valid</td><td></td></tr><tr><td>total_exs/valid</td><td></td></tr><tr><td>total_train_updates/train</td><td></td></tr><tr><td>total_train_updates/valid</td><td></td></tr><tr><td>tpb/train</td><td></td></tr><tr><td>tpb/valid</td><td></td></tr><tr><td>tps/train</td><td></td></tr><tr><td>tps/valid</td><td></td></tr><tr><td>ups/train</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>clen/test</td><td>147.49722</td></tr><tr><td>clen/train</td><td>150.37938</td></tr><tr><td>clen/valid</td><td>144.89979</td></tr><tr><td>clip/train</td><td>1.0</td></tr><tr><td>ctpb/test</td><td>3429.0098</td></tr><tr><td>ctpb/train</td><td>2957.10526</td></tr><tr><td>ctpb/valid</td><td>3431.57261</td></tr><tr><td>ctps/test</td><td>54227.72799</td></tr><tr><td>ctps/train</td><td>15320.26369</td></tr><tr><td>ctps/valid</td><td>53387.66791</td></tr><tr><td>ctrunc/test</td><td>0.00596</td></tr><tr><td>ctrunc/train</td><td>0.0</td></tr><tr><td>ctrunc/valid</td><td>0.00469</td></tr><tr><td>ctrunclen/test</td><td>0.32975</td></tr><tr><td>ctrunclen/train</td><td>0.0</td></tr><tr><td>ctrunclen/valid</td><td>0.22582</td></tr><tr><td>custom_step</td><td>201899</td></tr><tr><td>exps/test</td><td>268.46763</td></tr><tr><td>exps/train</td><td>117.79205</td></tr><tr><td>exps/valid</td><td>268.46199</td></tr><tr><td>exs/test</td><td>42087</td></tr><tr><td>exs/train</td><td>432</td></tr><tr><td>exs/valid</td><td>43257</td></tr><tr><td>fp16_loss_scalar/train</td><td>8192.0</td></tr><tr><td>french_blended_skill_talk/clen/test</td><td>203.08308</td></tr><tr><td>french_blended_skill_talk/clen/train</td><td>215.51613</td></tr><tr><td>french_blended_skill_talk/clen/valid</td><td>199.47595</td></tr><tr><td>french_blended_skill_talk/ctrunc/test</td><td>0.01939</td></tr><tr><td>french_blended_skill_talk/ctrunc/train</td><td>0.0</td></tr><tr><td>french_blended_skill_talk/ctrunc/valid</td><td>0.01395</td></tr><tr><td>french_blended_skill_talk/ctrunclen/test</td><td>1.09372</td></tr><tr><td>french_blended_skill_talk/ctrunclen/train</td><td>0.0</td></tr><tr><td>french_blended_skill_talk/ctrunclen/valid</td><td>0.61197</td></tr><tr><td>french_blended_skill_talk/exs/test</td><td>12121</td></tr><tr><td>french_blended_skill_talk/exs/train</td><td>31</td></tr><tr><td>french_blended_skill_talk/exs/valid</td><td>12476</td></tr><tr><td>french_blended_skill_talk/llen/test</td><td>31.81883</td></tr><tr><td>french_blended_skill_talk/llen/train</td><td>31.25806</td></tr><tr><td>french_blended_skill_talk/llen/valid</td><td>30.98148</td></tr><tr><td>french_blended_skill_talk/loss/test</td><td>2.16905</td></tr><tr><td>french_blended_skill_talk/loss/train</td><td>2.22527</td></tr><tr><td>french_blended_skill_talk/loss/valid</td><td>2.17239</td></tr><tr><td>french_blended_skill_talk/ltrunc/test</td><td>0.00017</td></tr><tr><td>french_blended_skill_talk/ltrunc/train</td><td>0.0</td></tr><tr><td>french_blended_skill_talk/ltrunc/valid</td><td>8e-05</td></tr><tr><td>french_blended_skill_talk/ltrunclen/test</td><td>0.00602</td></tr><tr><td>french_blended_skill_talk/ltrunclen/train</td><td>0.0</td></tr><tr><td>french_blended_skill_talk/ltrunclen/valid</td><td>0.00056</td></tr><tr><td>french_blended_skill_talk/ppl/test</td><td>8.74999</td></tr><tr><td>french_blended_skill_talk/ppl/train</td><td>9.25599</td></tr><tr><td>french_blended_skill_talk/ppl/valid</td><td>8.77925</td></tr><tr><td>french_blended_skill_talk/token_acc/test</td><td>0.54509</td></tr><tr><td>french_blended_skill_talk/token_acc/train</td><td>0.55418</td></tr><tr><td>french_blended_skill_talk/token_acc/valid</td><td>0.54314</td></tr><tr><td>french_blended_skill_talk/token_em/test</td><td>0.0</td></tr><tr><td>french_blended_skill_talk/token_em/train</td><td>0.0</td></tr><tr><td>french_blended_skill_talk/token_em/valid</td><td>0.0</td></tr><tr><td>french_empathetic_dialogues/clen/test</td><td>64.15796</td></tr><tr><td>french_empathetic_dialogues/clen/train</td><td>69.87261</td></tr><tr><td>french_empathetic_dialogues/clen/valid</td><td>60.68608</td></tr><tr><td>french_empathetic_dialogues/ctrunc/test</td><td>0.00012</td></tr><tr><td>french_empathetic_dialogues/ctrunc/train</td><td>0.0</td></tr><tr><td>french_empathetic_dialogues/ctrunc/valid</td><td>0.0</td></tr><tr><td>french_empathetic_dialogues/ctrunclen/test</td><td>0.00119</td></tr><tr><td>french_empathetic_dialogues/ctrunclen/train</td><td>0.0</td></tr><tr><td>french_empathetic_dialogues/ctrunclen/valid</td><td>0.0</td></tr><tr><td>french_empathetic_dialogues/exs/test</td><td>8426</td></tr><tr><td>french_empathetic_dialogues/exs/train</td><td>157</td></tr><tr><td>french_empathetic_dialogues/exs/valid</td><td>9308</td></tr><tr><td>french_empathetic_dialogues/llen/test</td><td>32.51756</td></tr><tr><td>french_empathetic_dialogues/llen/train</td><td>31.71338</td></tr><tr><td>french_empathetic_dialogues/llen/valid</td><td>30.99882</td></tr><tr><td>french_empathetic_dialogues/loss/test</td><td>2.14666</td></tr><tr><td>french_empathetic_dialogues/loss/train</td><td>2.02595</td></tr><tr><td>french_empathetic_dialogues/loss/valid</td><td>2.10276</td></tr><tr><td>french_empathetic_dialogues/ltrunc/test</td><td>0.00214</td></tr><tr><td>french_empathetic_dialogues/ltrunc/train</td><td>0.0</td></tr><tr><td>french_empathetic_dialogues/ltrunc/valid</td><td>0.00161</td></tr><tr><td>french_empathetic_dialogues/ltrunclen/test</td><td>0.05578</td></tr><tr><td>french_empathetic_dialogues/ltrunclen/train</td><td>0.0</td></tr><tr><td>french_empathetic_dialogues/ltrunclen/valid</td><td>0.04738</td></tr><tr><td>french_empathetic_dialogues/ppl/test</td><td>8.55621</td></tr><tr><td>french_empathetic_dialogues/ppl/train</td><td>7.58329</td></tr><tr><td>french_empathetic_dialogues/ppl/valid</td><td>8.18872</td></tr><tr><td>french_empathetic_dialogues/token_acc/test</td><td>0.53876</td></tr><tr><td>french_empathetic_dialogues/token_acc/train</td><td>0.55814</td></tr><tr><td>french_empathetic_dialogues/token_acc/valid</td><td>0.54517</td></tr><tr><td>french_empathetic_dialogues/token_em/test</td><td>0.00012</td></tr><tr><td>french_empathetic_dialogues/token_em/train</td><td>0.0</td></tr><tr><td>french_empathetic_dialogues/token_em/valid</td><td>0.00011</td></tr><tr><td>french_wizard_of_wikipedia/clen/test</td><td>151.73632</td></tr><tr><td>french_wizard_of_wikipedia/clen/train</td><td>159.75</td></tr><tr><td>french_wizard_of_wikipedia/clen/valid</td><td>150.88629</td></tr><tr><td>french_wizard_of_wikipedia/ctrunc/test</td><td>0.00324</td></tr><tr><td>french_wizard_of_wikipedia/ctrunc/train</td><td>0.0</td></tr><tr><td>french_wizard_of_wikipedia/ctrunc/valid</td><td>0.00426</td></tr><tr><td>french_wizard_of_wikipedia/ctrunclen/test</td><td>0.20627</td></tr><tr><td>french_wizard_of_wikipedia/ctrunclen/train</td><td>0.0</td></tr><tr><td>french_wizard_of_wikipedia/ctrunclen/valid</td><td>0.28107</td></tr><tr><td>french_wizard_of_wikipedia/exs/test</td><td>17889</td></tr><tr><td>french_wizard_of_wikipedia/exs/train</td><td>112</td></tr><tr><td>french_wizard_of_wikipedia/exs/valid</td><td>17853</td></tr><tr><td>french_wizard_of_wikipedia/llen/test</td><td>38.25317</td></tr><tr><td>french_wizard_of_wikipedia/llen/train</td><td>37.77679</td></tr><tr><td>french_wizard_of_wikipedia/llen/valid</td><td>38.00622</td></tr><tr><td>french_wizard_of_wikipedia/loss/test</td><td>2.40789</td></tr><tr><td>french_wizard_of_wikipedia/loss/train</td><td>2.35769</td></tr><tr><td>french_wizard_of_wikipedia/loss/valid</td><td>2.40705</td></tr><tr><td>french_wizard_of_wikipedia/ltrunc/test</td><td>0.00101</td></tr><tr><td>french_wizard_of_wikipedia/ltrunc/train</td><td>0.00893</td></tr><tr><td>french_wizard_of_wikipedia/ltrunc/valid</td><td>0.00213</td></tr><tr><td>french_wizard_of_wikipedia/ltrunclen/test</td><td>0.02465</td></tr><tr><td>french_wizard_of_wikipedia/ltrunclen/train</td><td>0.04464</td></tr><tr><td>french_wizard_of_wikipedia/ltrunclen/valid</td><td>0.04055</td></tr><tr><td>french_wizard_of_wikipedia/ppl/test</td><td>11.11046</td></tr><tr><td>french_wizard_of_wikipedia/ppl/train</td><td>10.56654</td></tr><tr><td>french_wizard_of_wikipedia/ppl/valid</td><td>11.10116</td></tr><tr><td>french_wizard_of_wikipedia/token_acc/test</td><td>0.51634</td></tr><tr><td>french_wizard_of_wikipedia/token_acc/train</td><td>0.52698</td></tr><tr><td>french_wizard_of_wikipedia/token_acc/valid</td><td>0.51603</td></tr><tr><td>french_wizard_of_wikipedia/token_em/test</td><td>0.00011</td></tr><tr><td>french_wizard_of_wikipedia/token_em/train</td><td>0.0</td></tr><tr><td>french_wizard_of_wikipedia/token_em/valid</td><td>0.0</td></tr><tr><td>french_xpersona/clen/test</td><td>171.0115</td></tr><tr><td>french_xpersona/clen/train</td><td>156.37879</td></tr><tr><td>french_xpersona/clen/valid</td><td>168.55083</td></tr><tr><td>french_xpersona/ctrunc/test</td><td>0.0011</td></tr><tr><td>french_xpersona/ctrunc/train</td><td>0.0</td></tr><tr><td>french_xpersona/ctrunc/valid</td><td>0.00055</td></tr><tr><td>french_xpersona/ctrunclen/test</td><td>0.0178</td></tr><tr><td>french_xpersona/ctrunclen/train</td><td>0.0</td></tr><tr><td>french_xpersona/ctrunclen/valid</td><td>0.01022</td></tr><tr><td>french_xpersona/exs/test</td><td>3651</td></tr><tr><td>french_xpersona/exs/train</td><td>132</td></tr><tr><td>french_xpersona/exs/valid</td><td>3620</td></tr><tr><td>french_xpersona/llen/test</td><td>23.41222</td></tr><tr><td>french_xpersona/llen/train</td><td>23.58333</td></tr><tr><td>french_xpersona/llen/valid</td><td>23.12983</td></tr><tr><td>french_xpersona/loss/test</td><td>2.25393</td></tr><tr><td>french_xpersona/loss/train</td><td>2.16929</td></tr><tr><td>french_xpersona/loss/valid</td><td>2.23348</td></tr><tr><td>french_xpersona/ltrunc/test</td><td>0.0</td></tr><tr><td>french_xpersona/ltrunc/train</td><td>0.0</td></tr><tr><td>french_xpersona/ltrunc/valid</td><td>0.0</td></tr><tr><td>french_xpersona/ltrunclen/test</td><td>0.0</td></tr><tr><td>french_xpersona/ltrunclen/train</td><td>0.0</td></tr><tr><td>french_xpersona/ltrunclen/valid</td><td>0.0</td></tr><tr><td>french_xpersona/ppl/test</td><td>9.52512</td></tr><tr><td>french_xpersona/ppl/train</td><td>8.75208</td></tr><tr><td>french_xpersona/ppl/valid</td><td>9.33233</td></tr><tr><td>french_xpersona/token_acc/test</td><td>0.53595</td></tr><tr><td>french_xpersona/token_acc/train</td><td>0.54995</td></tr><tr><td>french_xpersona/token_acc/valid</td><td>0.54374</td></tr><tr><td>french_xpersona/token_em/test</td><td>0.0</td></tr><tr><td>french_xpersona/token_em/train</td><td>0.0</td></tr><tr><td>french_xpersona/token_em/valid</td><td>0.0</td></tr><tr><td>gnorm/train</td><td>7.44326</td></tr><tr><td>gpu_mem/test</td><td>0.15446</td></tr><tr><td>gpu_mem/train</td><td>0.39607</td></tr><tr><td>gpu_mem/valid</td><td>0.15465</td></tr><tr><td>llen/test</td><td>31.50044</td></tr><tr><td>llen/train</td><td>31.08289</td></tr><tr><td>llen/valid</td><td>30.77909</td></tr><tr><td>loss/test</td><td>2.24438</td></tr><tr><td>loss/train</td><td>2.19455</td></tr><tr><td>loss/valid</td><td>2.22892</td></tr><tr><td>lr/test</td><td>0.0</td></tr><tr><td>lr/train</td><td>0.0</td></tr><tr><td>lr/valid</td><td>0.0</td></tr><tr><td>ltpb/test</td><td>540.06022</td></tr><tr><td>ltpb/train</td><td>699.31579</td></tr><tr><td>ltpb/valid</td><td>534.60304</td></tr><tr><td>ltps/test</td><td>8540.72353</td></tr><tr><td>ltps/train</td><td>3622.99478</td></tr><tr><td>ltps/valid</td><td>8317.22739</td></tr><tr><td>ltrunc/test</td><td>0.00083</td></tr><tr><td>ltrunc/train</td><td>0.00223</td></tr><tr><td>ltrunc/valid</td><td>0.00096</td></tr><tr><td>ltrunclen/test</td><td>0.02161</td></tr><tr><td>ltrunclen/train</td><td>0.01116</td></tr><tr><td>ltrunclen/valid</td><td>0.02212</td></tr><tr><td>ppl/test</td><td>9.48544</td></tr><tr><td>ppl/train</td><td>9.03948</td></tr><tr><td>ppl/valid</td><td>9.35037</td></tr><tr><td>token_acc/test</td><td>0.53404</td></tr><tr><td>token_acc/train</td><td>0.54731</td></tr><tr><td>token_acc/valid</td><td>0.53702</td></tr><tr><td>token_em/test</td><td>6e-05</td></tr><tr><td>token_em/train</td><td>0.0</td></tr><tr><td>token_em/valid</td><td>3e-05</td></tr><tr><td>total_exs/valid</td><td>4238654</td></tr><tr><td>total_train_updates/test</td><td>121867</td></tr><tr><td>total_train_updates/train</td><td>121867</td></tr><tr><td>total_train_updates/valid</td><td>121867</td></tr><tr><td>tpb/test</td><td>3969.07003</td></tr><tr><td>tpb/train</td><td>3656.42105</td></tr><tr><td>tpb/valid</td><td>3966.17566</td></tr><tr><td>tps/test</td><td>62768.456</td></tr><tr><td>tps/train</td><td>18943.32796</td></tr><tr><td>tps/valid</td><td>61704.91771</td></tr><tr><td>ups/train</td><td>5.18234</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">playful-moon-1</strong>: <a href=\"https://wandb.ai/aliae_research/2022-06-16-14-21/runs/1c1p1qsf\" target=\"_blank\">https://wandb.ai/aliae_research/2022-06-16-14-21/runs/1c1p1qsf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/wandb/run-20220616_142150-1c1p1qsf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'clen': MacroAverageMetric(144.9),\n",
              "  'ctpb': GlobalAverageMetric(3432),\n",
              "  'ctps': GlobalTimerMetric(5.339e+04),\n",
              "  'ctrunc': MacroAverageMetric(0.004689),\n",
              "  'ctrunclen': MacroAverageMetric(0.2258),\n",
              "  'exps': GlobalTimerMetric(268.5),\n",
              "  'exs': SumMetric(4.326e+04),\n",
              "  'french_blended_skill_talk/clen': AverageMetric(199.5),\n",
              "  'french_blended_skill_talk/ctrunc': AverageMetric(0.01395),\n",
              "  'french_blended_skill_talk/ctrunclen': AverageMetric(0.612),\n",
              "  'french_blended_skill_talk/exs': SumMetric(1.248e+04),\n",
              "  'french_blended_skill_talk/llen': AverageMetric(30.98),\n",
              "  'french_blended_skill_talk/loss': AverageMetric(2.172),\n",
              "  'french_blended_skill_talk/ltrunc': AverageMetric(8.015e-05),\n",
              "  'french_blended_skill_talk/ltrunclen': AverageMetric(0.0005611),\n",
              "  'french_blended_skill_talk/ppl': PPLMetric(8.779),\n",
              "  'french_blended_skill_talk/token_acc': AverageMetric(0.5431),\n",
              "  'french_blended_skill_talk/token_em': AverageMetric(0),\n",
              "  'french_empathetic_dialogues/clen': AverageMetric(60.69),\n",
              "  'french_empathetic_dialogues/ctrunc': AverageMetric(0),\n",
              "  'french_empathetic_dialogues/ctrunclen': AverageMetric(0),\n",
              "  'french_empathetic_dialogues/exs': SumMetric(9308),\n",
              "  'french_empathetic_dialogues/llen': AverageMetric(31),\n",
              "  'french_empathetic_dialogues/loss': AverageMetric(2.103),\n",
              "  'french_empathetic_dialogues/ltrunc': AverageMetric(0.001612),\n",
              "  'french_empathetic_dialogues/ltrunclen': AverageMetric(0.04738),\n",
              "  'french_empathetic_dialogues/ppl': PPLMetric(8.189),\n",
              "  'french_empathetic_dialogues/token_acc': AverageMetric(0.5452),\n",
              "  'french_empathetic_dialogues/token_em': AverageMetric(0.0001074),\n",
              "  'french_wizard_of_wikipedia/clen': AverageMetric(150.9),\n",
              "  'french_wizard_of_wikipedia/ctrunc': AverageMetric(0.004257),\n",
              "  'french_wizard_of_wikipedia/ctrunclen': AverageMetric(0.2811),\n",
              "  'french_wizard_of_wikipedia/exs': SumMetric(1.785e+04),\n",
              "  'french_wizard_of_wikipedia/llen': AverageMetric(38.01),\n",
              "  'french_wizard_of_wikipedia/loss': AverageMetric(2.407),\n",
              "  'french_wizard_of_wikipedia/ltrunc': AverageMetric(0.002128),\n",
              "  'french_wizard_of_wikipedia/ltrunclen': AverageMetric(0.04055),\n",
              "  'french_wizard_of_wikipedia/ppl': PPLMetric(11.1),\n",
              "  'french_wizard_of_wikipedia/token_acc': AverageMetric(0.516),\n",
              "  'french_wizard_of_wikipedia/token_em': AverageMetric(0),\n",
              "  'french_xpersona/clen': AverageMetric(168.6),\n",
              "  'french_xpersona/ctrunc': AverageMetric(0.0005525),\n",
              "  'french_xpersona/ctrunclen': AverageMetric(0.01022),\n",
              "  'french_xpersona/exs': SumMetric(3620),\n",
              "  'french_xpersona/llen': AverageMetric(23.13),\n",
              "  'french_xpersona/loss': AverageMetric(2.233),\n",
              "  'french_xpersona/ltrunc': AverageMetric(0),\n",
              "  'french_xpersona/ltrunclen': AverageMetric(0),\n",
              "  'french_xpersona/ppl': PPLMetric(9.332),\n",
              "  'french_xpersona/token_acc': AverageMetric(0.5437),\n",
              "  'french_xpersona/token_em': AverageMetric(0),\n",
              "  'gpu_mem': GlobalAverageMetric(0.1546),\n",
              "  'llen': MacroAverageMetric(30.78),\n",
              "  'loss': MacroAverageMetric(2.229),\n",
              "  'lr': GlobalAverageMetric(1.238e-06),\n",
              "  'ltpb': GlobalAverageMetric(534.6),\n",
              "  'ltps': GlobalTimerMetric(8317),\n",
              "  'ltrunc': MacroAverageMetric(0.000955),\n",
              "  'ltrunclen': MacroAverageMetric(0.02212),\n",
              "  'ppl': MacroAverageMetric(9.35),\n",
              "  'token_acc': MacroAverageMetric(0.537),\n",
              "  'token_em': MacroAverageMetric(2.686e-05),\n",
              "  'total_train_updates': GlobalFixedMetric(1.219e+05),\n",
              "  'tpb': GlobalAverageMetric(3966),\n",
              "  'tps': GlobalTimerMetric(6.17e+04)},\n",
              " {'clen': MacroAverageMetric(147.5),\n",
              "  'ctpb': GlobalAverageMetric(3429),\n",
              "  'ctps': GlobalTimerMetric(5.423e+04),\n",
              "  'ctrunc': MacroAverageMetric(0.005961),\n",
              "  'ctrunclen': MacroAverageMetric(0.3297),\n",
              "  'exps': GlobalTimerMetric(268.5),\n",
              "  'exs': SumMetric(4.209e+04),\n",
              "  'french_blended_skill_talk/clen': AverageMetric(203.1),\n",
              "  'french_blended_skill_talk/ctrunc': AverageMetric(0.01939),\n",
              "  'french_blended_skill_talk/ctrunclen': AverageMetric(1.094),\n",
              "  'french_blended_skill_talk/exs': SumMetric(1.212e+04),\n",
              "  'french_blended_skill_talk/llen': AverageMetric(31.82),\n",
              "  'french_blended_skill_talk/loss': AverageMetric(2.169),\n",
              "  'french_blended_skill_talk/ltrunc': AverageMetric(0.000165),\n",
              "  'french_blended_skill_talk/ltrunclen': AverageMetric(0.006023),\n",
              "  'french_blended_skill_talk/ppl': PPLMetric(8.75),\n",
              "  'french_blended_skill_talk/token_acc': AverageMetric(0.5451),\n",
              "  'french_blended_skill_talk/token_em': AverageMetric(0),\n",
              "  'french_empathetic_dialogues/clen': AverageMetric(64.16),\n",
              "  'french_empathetic_dialogues/ctrunc': AverageMetric(0.0001187),\n",
              "  'french_empathetic_dialogues/ctrunclen': AverageMetric(0.001187),\n",
              "  'french_empathetic_dialogues/exs': SumMetric(8426),\n",
              "  'french_empathetic_dialogues/llen': AverageMetric(32.52),\n",
              "  'french_empathetic_dialogues/loss': AverageMetric(2.147),\n",
              "  'french_empathetic_dialogues/ltrunc': AverageMetric(0.002136),\n",
              "  'french_empathetic_dialogues/ltrunclen': AverageMetric(0.05578),\n",
              "  'french_empathetic_dialogues/ppl': PPLMetric(8.556),\n",
              "  'french_empathetic_dialogues/token_acc': AverageMetric(0.5388),\n",
              "  'french_empathetic_dialogues/token_em': AverageMetric(0.0001187),\n",
              "  'french_wizard_of_wikipedia/clen': AverageMetric(151.7),\n",
              "  'french_wizard_of_wikipedia/ctrunc': AverageMetric(0.003242),\n",
              "  'french_wizard_of_wikipedia/ctrunclen': AverageMetric(0.2063),\n",
              "  'french_wizard_of_wikipedia/exs': SumMetric(1.789e+04),\n",
              "  'french_wizard_of_wikipedia/llen': AverageMetric(38.25),\n",
              "  'french_wizard_of_wikipedia/loss': AverageMetric(2.408),\n",
              "  'french_wizard_of_wikipedia/ltrunc': AverageMetric(0.001006),\n",
              "  'french_wizard_of_wikipedia/ltrunclen': AverageMetric(0.02465),\n",
              "  'french_wizard_of_wikipedia/ppl': PPLMetric(11.11),\n",
              "  'french_wizard_of_wikipedia/token_acc': AverageMetric(0.5163),\n",
              "  'french_wizard_of_wikipedia/token_em': AverageMetric(0.0001118),\n",
              "  'french_xpersona/clen': AverageMetric(171),\n",
              "  'french_xpersona/ctrunc': AverageMetric(0.001096),\n",
              "  'french_xpersona/ctrunclen': AverageMetric(0.0178),\n",
              "  'french_xpersona/exs': SumMetric(3651),\n",
              "  'french_xpersona/llen': AverageMetric(23.41),\n",
              "  'french_xpersona/loss': AverageMetric(2.254),\n",
              "  'french_xpersona/ltrunc': AverageMetric(0),\n",
              "  'french_xpersona/ltrunclen': AverageMetric(0),\n",
              "  'french_xpersona/ppl': PPLMetric(9.525),\n",
              "  'french_xpersona/token_acc': AverageMetric(0.536),\n",
              "  'french_xpersona/token_em': AverageMetric(0),\n",
              "  'gpu_mem': GlobalAverageMetric(0.1545),\n",
              "  'llen': MacroAverageMetric(31.5),\n",
              "  'loss': MacroAverageMetric(2.244),\n",
              "  'lr': GlobalAverageMetric(1.238e-06),\n",
              "  'ltpb': GlobalAverageMetric(540.1),\n",
              "  'ltps': GlobalTimerMetric(8541),\n",
              "  'ltrunc': MacroAverageMetric(0.0008269),\n",
              "  'ltrunclen': MacroAverageMetric(0.02161),\n",
              "  'ppl': MacroAverageMetric(9.485),\n",
              "  'token_acc': MacroAverageMetric(0.534),\n",
              "  'token_em': MacroAverageMetric(5.762e-05),\n",
              "  'total_train_updates': GlobalFixedMetric(1.219e+05),\n",
              "  'tpb': GlobalAverageMetric(3969),\n",
              "  'tps': GlobalTimerMetric(6.277e+04)})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# 90M settings\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    task= \"french_blended_skill_talk,french_wizard_of_wikipedia,french_xpersona,french_empathetic_dialogues\",\n",
        "    multitask_weights= \"1,3,3,3\",\n",
        "\n",
        "    # task='fromfile:parlaiformat', \n",
        "    # fromfile_datapath= f'{data_path}data',\n",
        "    # fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    \n",
        "    # depend on your gpu. \n",
        "    \n",
        "    validation_every_n_epochs=0.1,\n",
        "    num_epochs = 10,\n",
        "    log_every_n_secs= 180,\n",
        "    verbose = True,\n",
        "    batchsize= 8, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    save_after_valid = True,\n",
        "    wandb_log = True,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        "\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    attention_dropout= 0.0, \n",
        "    model_parallel= False,\n",
        "    warmup_updates=100,\n",
        "\n",
        "    # customized parameters\n",
        "    # inference= \"beam\"\n",
        "    # beam_min_length= 20,\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_size= 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxL0-1x1YsQe"
      },
      "outputs": [],
      "source": [
        "# mydrive_path = '/content/finetuned-multitask-400m-double-sided-2epochs'\n",
        "# mydrive_path = '/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-double-sided/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "id": "jNZE5ta2pO-X",
        "outputId": "850d4938-cdea-4a4d-da47-0c8ec8f89200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08:17:05 | building dictionary first...\n",
            "08:17:05 | \u001b[33mOverriding opt[\"multitask_weights\"] to (1.0, 3.0, 3.0, 3.0) (previously: [1.0, 3.0, 3.0, 3.0])\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"dict_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict (previously: /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.dict)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"num_epochs\"] to 10.0 (previously: 5.0)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"log_every_n_secs\"] to 300.0 (previously: 180.0)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"batchsize\"] to 16 (previously: 8)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"save_after_valid\"] to True (previously: False)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"embedding_size\"] to 1280 (previously: 512)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"ffn_size\"] to 5120 (previously: 2048)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"variant\"] to prelayernorm (previously: xlm)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"n_heads\"] to 32 (previously: 16)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"n_positions\"] to 128 (previously: 512)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"n_encoder_layers\"] to 2 (previously: -1)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"n_decoder_layers\"] to 12 (previously: -1)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"text_truncate\"] to 128 (previously: 512)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"truncate\"] to 128 (previously: -1)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"history_add_global_end_token\"] to end (previously: None)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"delimiter\"] to    (previously: \n",
            ")\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"dict_tokenizer\"] to bytelevelbpe (previously: bpe)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"dropout\"] to 0.1 (previously: 0.0)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"learningrate\"] to 7e-06 (previously: 1e-05)\u001b[0m\n",
            "08:17:05 | \u001b[33mOverriding opt[\"update_freq\"] to 2 (previously: 1)\u001b[0m\n",
            "08:17:05 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: full,is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data,fromfile_datatype_extension: True,n_encoder_layers: 2,n_decoder_layers: 12,model_parallel: False,checkpoint_activations: False,beam_block_full_context: True,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,history_reversed: False,history_add_global_end_token: end,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,dict_loaded: True,verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "08:17:05 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues --numthreads 1 --num-epochs -1 --save-every-n-secs 60.0 --validation-max-exs 20000 --validation-patience 15 --log-every-n-secs 2 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --num-topics 5 --train-experiencer-only False --embedding-size 512 --ffn-size 2048 --n-heads 16 --learn-positional-embeddings True --n-positions 512 --variant xlm --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --skip-generation False --inference beam --fp16-impl apex --force-fp16-tokens False --optimizer adamax --learningrate 7.5e-06 --truncate -1 --text-truncate 512 --delimiter \n",
            " --dict-tokenizer bpe --max-lr-steps -1 --warmup-updates -1 --update-freq 1 --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
            "08:17:05 | Using CUDA\n",
            "08:17:05 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-reddit-90m--finetuned-4tasks-5epochs/model.dict\n",
            "08:17:05 | num words = 54944\n",
            "08:17:05 | \u001b[33mAre you sure you want to lower case your BPE dictionary?\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f4f20dc6b33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mvp\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mvalidation_metric\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"ppl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#vmt = \"ppl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mvalidation_metric_mode\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# vmm= \"min\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# customized parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m# Create model and assign it to the specified task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/agents.py\u001b[0m in \u001b[0;36mcreate_agent\u001b[0;34m(opt, requireModelExists)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;31m# Attempt to load the model from the model file first (this way we do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;31m# not even have to specify the model name as a parameter)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_agent_from_opt_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/agents.py\u001b[0m in \u001b[0;36mcreate_agent_from_opt_file\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;31m# loaded ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0mcompare_init_model_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_from_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_from_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, opt, shared)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0minit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_finetune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_init_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'beam_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, opt, shared)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshared\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;31m# intialize any important structures from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fp16'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'force_fp16_tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36mbuild_dictionary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mplace\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdo\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \"\"\"\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_toks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/dict.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, opt, shared)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreebank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreebankWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'bpe'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bytelevelbpe'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'slow_bytelevel_bpe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpe_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_with_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/utils/bpe.py\u001b[0m in \u001b[0;36mbpe_factory\u001b[0;34m(opt, shared)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Attempt to instantiate HF tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mbpe_helper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingFaceBpeHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dict_loaded'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/utils/bpe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, opt, shared)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             raise IOError(\n\u001b[0;32m--> 836\u001b[0;31m                 \u001b[0;34m'--bpe-vocab and --bpe-merge are mandatory with '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m                 \u001b[0;34m'--dict-tokenizer bytelevelbpe'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             )\n",
            "\u001b[0;31mOSError\u001b[0m: --bpe-vocab and --bpe-merge are mandatory with --dict-tokenizer bytelevelbpe"
          ]
        }
      ],
      "source": [
        "# 400M settings\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    task= \"french_blended_skill_talk,french_wizard_of_wikipedia,french_xpersona,french_empathetic_dialogues\",\n",
        "    multitask_weights= \"1,3,3,3\",\n",
        "\n",
        "    # task='fromfile:parlaiformat', \n",
        "    # fromfile_datapath= f'{data_path}data',\n",
        "    # fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 10,\n",
        "    log_every_n_secs= 300,\n",
        "    verbose = True,\n",
        "    attention_dropout= 0.0, \n",
        "    batchsize= 16, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    # save_after_valid= True,\n",
        "\n",
        "    # arguments we get from the pretrained model. \"from recipes page for 2.7B model\" \n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    variant= \"prelayernorm\",\n",
        "    n_heads= 32, n_positions= 128, \n",
        "    n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "    dropout= 0.1,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    relu_dropout= 0.0, \n",
        "    model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    # inference = 'topk', \n",
        "    # temperature = 0.7, \n",
        "    # topk=30, \n",
        "    # beam_length_penalty=1.03\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1S0kRk3W63i"
      },
      "outputs": [],
      "source": [
        "# !cp -rv /content/finetuned-multitask-400m-double-sided-2epochs/* /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-double-sided/\n",
        "# !ls -lah /content/finetuned-multitask-400m-double-sided-2epochs/\n",
        "# !ls -lah /content/finetuned-multitask-400m-double-sided/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVPS6p4XzPh4"
      },
      "source": [
        "# 4.Display Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9FBtnZZzPPg"
      },
      "outputs": [],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='french_blended_skill_talk',\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model_path = f'{mydrive_path}finetuned-multitask-90m/'\n",
        "finetuned_model_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "O7IUsHgd-Jbq",
        "outputId": "a09e2773-1f14-4a33-9a52-a38033bffae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap-cP0uzFF4y",
        "outputId": "3c438278-a75a-4c92-b793-91baaee8bc0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14:42:28 | \u001b[33mOverriding opt[\"task\"] to french_blended_skill_talk (previously: french_blended_skill_talk,french_xpersona,french_empathetic_dialogues,french_wizard_of_wikipedia)\u001b[0m\n",
            "14:42:28 | \u001b[33mOverriding opt[\"datatype\"] to test (previously: train)\u001b[0m\n",
            "14:42:28 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "14:42:28 | \u001b[33mOverriding opt[\"beam_block_ngram\"] to 2 (previously: -1)\u001b[0m\n",
            "14:42:28 | \u001b[33mOverriding opt[\"beam_context_block_ngram\"] to 3 (previously: -1)\u001b[0m\n",
            "14:42:28 | \u001b[33mOverriding opt[\"beam_length_penalty\"] to 1.0 (previously: 0.65)\u001b[0m\n",
            "14:42:28 | \u001b[33mOverriding opt[\"beam_min_length\"] to 10 (previously: 1)\u001b[0m\n",
            "14:42:28 | \u001b[33mOverriding opt[\"beam_size\"] to 20 (previously: 1)\u001b[0m\n",
            "14:42:28 | \u001b[33mOverriding opt[\"inference\"] to topk (previously: greedy)\u001b[0m\n",
            "14:42:28 | \u001b[33mOverriding opt[\"temperature\"] to 0.5 (previously: 1.0)\u001b[0m\n",
            "14:42:28 | \u001b[33mOverriding opt[\"topk\"] to 20 (previously: 10)\u001b[0m\n",
            "14:42:28 | Using CUDA\n",
            "14:42:28 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m/model.dict\n",
            "14:42:28 | num words = 54944\n",
            "14:42:30 | Total parameters: 87,508,992 (86,984,704 trainable)\n",
            "14:42:30 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m/model\n",
            "14:42:32 | creating task(s): french_blended_skill_talk\n",
            "14:42:32 | Loading ParlAI text data: /content/dataset_french_bst/test.txt\n",
            "14:42:32 | Opt:\n",
            "14:42:32 |     activation: gelu\n",
            "14:42:32 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "14:42:32 |     adam_eps: 1e-08\n",
            "14:42:32 |     add_p1_after_newln: False\n",
            "14:42:32 |     aggregate_micro: False\n",
            "14:42:32 |     allow_missing_init_opts: False\n",
            "14:42:32 |     attention_dropout: 0.0\n",
            "14:42:32 |     batchsize: 8\n",
            "14:42:32 |     beam_block_full_context: True\n",
            "14:42:32 |     beam_block_list_filename: None\n",
            "14:42:32 |     beam_block_ngram: 2\n",
            "14:42:32 |     beam_context_block_ngram: 3\n",
            "14:42:32 |     beam_delay: 30\n",
            "14:42:32 |     beam_length_penalty: 1.0\n",
            "14:42:32 |     beam_min_length: 10\n",
            "14:42:32 |     beam_size: 20\n",
            "14:42:32 |     betas: '[0.9, 0.999]'\n",
            "14:42:32 |     bpe_add_prefix_space: None\n",
            "14:42:32 |     bpe_debug: False\n",
            "14:42:32 |     bpe_dropout: None\n",
            "14:42:32 |     bpe_merge: None\n",
            "14:42:32 |     bpe_vocab: None\n",
            "14:42:32 |     checkpoint_activations: False\n",
            "14:42:32 |     compute_tokenized_bleu: False\n",
            "14:42:32 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "14:42:32 |     datatype: test\n",
            "14:42:32 |     delimiter: '\\n'\n",
            "14:42:32 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "14:42:32 |     dict_endtoken: __end__\n",
            "14:42:32 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m/model.dict\n",
            "14:42:32 |     dict_include_test: False\n",
            "14:42:32 |     dict_include_valid: False\n",
            "14:42:32 |     dict_initpath: None\n",
            "14:42:32 |     dict_language: english\n",
            "14:42:32 |     dict_loaded: True\n",
            "14:42:32 |     dict_lower: True\n",
            "14:42:32 |     dict_max_ngram_size: -1\n",
            "14:42:32 |     dict_maxexs: -1\n",
            "14:42:32 |     dict_maxtokens: -1\n",
            "14:42:32 |     dict_minfreq: 0\n",
            "14:42:32 |     dict_nulltoken: __null__\n",
            "14:42:32 |     dict_starttoken: __start__\n",
            "14:42:32 |     dict_textfields: text,labels\n",
            "14:42:32 |     dict_tokenizer: bpe\n",
            "14:42:32 |     dict_unktoken: __unk__\n",
            "14:42:32 |     display_add_fields: \n",
            "14:42:32 |     display_examples: False\n",
            "14:42:32 |     download_path: None\n",
            "14:42:32 |     dropout: 0.0\n",
            "14:42:32 |     dynamic_batching: full\n",
            "14:42:32 |     embedding_projection: random\n",
            "14:42:32 |     embedding_size: 512\n",
            "14:42:32 |     embedding_type: random\n",
            "14:42:32 |     embeddings_scale: True\n",
            "14:42:32 |     eval_batchsize: None\n",
            "14:42:32 |     eval_dynamic_batching: None\n",
            "14:42:32 |     evaltask: None\n",
            "14:42:32 |     ffn_size: 2048\n",
            "14:42:32 |     final_extra_opt: \n",
            "14:42:32 |     force_fp16_tokens: True\n",
            "14:42:32 |     fp16: True\n",
            "14:42:32 |     fp16_impl: mem_efficient\n",
            "14:42:32 |     gpu: -1\n",
            "14:42:32 |     gradient_clip: 0.1\n",
            "14:42:32 |     hide_labels: False\n",
            "14:42:32 |     history_add_global_end_token: None\n",
            "14:42:32 |     history_reversed: False\n",
            "14:42:32 |     history_size: -1\n",
            "14:42:32 |     image_cropsize: 224\n",
            "14:42:32 |     image_mode: raw\n",
            "14:42:32 |     image_size: 256\n",
            "14:42:32 |     inference: topk\n",
            "14:42:32 |     init_model: zoo:blender/blender_90M/model\n",
            "14:42:32 |     init_opt: None\n",
            "14:42:32 |     interactive_mode: False\n",
            "14:42:32 |     invsqrt_lr_decay_gamma: -1\n",
            "14:42:32 |     is_debug: False\n",
            "14:42:32 |     label_truncate: 128\n",
            "14:42:32 |     learn_positional_embeddings: False\n",
            "14:42:32 |     learningrate: 1e-05\n",
            "14:42:32 |     log_every_n_secs: 60.0\n",
            "14:42:32 |     log_every_n_steps: 50\n",
            "14:42:32 |     log_keep_fields: all\n",
            "14:42:32 |     loglevel: info\n",
            "14:42:32 |     lr_scheduler: reduceonplateau\n",
            "14:42:32 |     lr_scheduler_decay: 0.5\n",
            "14:42:32 |     lr_scheduler_patience: 3\n",
            "14:42:32 |     max_train_steps: -1\n",
            "14:42:32 |     max_train_time: -1\n",
            "14:42:32 |     metrics: default\n",
            "14:42:32 |     model: transformer/generator\n",
            "14:42:32 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m/model\n",
            "14:42:32 |     model_parallel: False\n",
            "14:42:32 |     momentum: 0\n",
            "14:42:32 |     multitask_weights: '[1.0, 3.0, 3.0, 3.0]'\n",
            "14:42:32 |     mutators: None\n",
            "14:42:32 |     n_decoder_layers: -1\n",
            "14:42:32 |     n_encoder_layers: -1\n",
            "14:42:32 |     n_heads: 16\n",
            "14:42:32 |     n_layers: 8\n",
            "14:42:32 |     n_positions: 512\n",
            "14:42:32 |     n_segments: 0\n",
            "14:42:32 |     nesterov: True\n",
            "14:42:32 |     no_cuda: False\n",
            "14:42:32 |     num_epochs: 5.0\n",
            "14:42:32 |     num_examples: 20\n",
            "14:42:32 |     num_workers: 0\n",
            "14:42:32 |     nus: [0.7]\n",
            "14:42:32 |     optimizer: mem_eff_adam\n",
            "14:42:32 |     output_scaling: 1.0\n",
            "14:42:32 |     override: \"{'task': 'french_blended_skill_talk', 'datatype': 'test', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m/model', 'num_examples': '20', 'skip_generation': False, 'beam_block_ngram': 2, 'beam_context_block_ngram': 3, 'beam_length_penalty': 1.0, 'beam_min_length': 10, 'beam_size': 20, 'inference': 'topk', 'temperature': 0.5, 'topk': 20, 'topp': 0.9}\"\n",
            "14:42:32 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "14:42:32 |     person_tokens: False\n",
            "14:42:32 |     rank_candidates: False\n",
            "14:42:32 |     relu_dropout: 0.0\n",
            "14:42:32 |     save_after_valid: False\n",
            "14:42:32 |     save_every_n_secs: -1\n",
            "14:42:32 |     save_format: conversations\n",
            "14:42:32 |     share_word_embeddings: True\n",
            "14:42:32 |     short_final_eval: False\n",
            "14:42:32 |     skip_generation: False\n",
            "14:42:32 |     special_tok_lst: None\n",
            "14:42:32 |     split_lines: False\n",
            "14:42:32 |     starttime: May24_08-21\n",
            "14:42:32 |     task: french_blended_skill_talk\n",
            "14:42:32 |     temperature: 0.5\n",
            "14:42:32 |     tensorboard_log: False\n",
            "14:42:32 |     tensorboard_logdir: None\n",
            "14:42:32 |     text_truncate: 512\n",
            "14:42:32 |     topk: 20\n",
            "14:42:32 |     topp: 0.9\n",
            "14:42:32 |     truncate: -1\n",
            "14:42:32 |     update_freq: 1\n",
            "14:42:32 |     use_reply: label\n",
            "14:42:32 |     validation_cutoff: 1.0\n",
            "14:42:32 |     validation_every_n_epochs: 0.25\n",
            "14:42:32 |     validation_every_n_secs: -1\n",
            "14:42:32 |     validation_every_n_steps: -1\n",
            "14:42:32 |     validation_max_exs: -1\n",
            "14:42:32 |     validation_metric: ppl\n",
            "14:42:32 |     validation_metric_mode: min\n",
            "14:42:32 |     validation_patience: 10\n",
            "14:42:32 |     validation_share_agent: False\n",
            "14:42:32 |     variant: xlm\n",
            "14:42:32 |     verbose: False\n",
            "14:42:32 |     wandb_entity: None\n",
            "14:42:32 |     wandb_log: False\n",
            "14:42:32 |     wandb_name: None\n",
            "14:42:32 |     wandb_project: None\n",
            "14:42:32 |     warmup_rate: 0.0001\n",
            "14:42:32 |     warmup_updates: 100\n",
            "14:42:32 |     weight_decay: None\n",
            "14:42:32 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk- - -\u001b[0;0m\n",
            "\u001b[0mWow, je ne suis jamais timide. Vous tes anxieux ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Oui. Je finis par transpirer et rougir et j'ai l'impression que je vais vomir.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui , je suis un peu ennuyeux .\u001b[0;0m\n",
            "\u001b[0met pourquoi a ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je pense que c'est parce que dans ma tte, je pense que tout le monde me juge. Je commence  transpirer et j'ai mal au ventre.\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est un peu effra__unk__ mais je suis content que tout soit bien .\u001b[0;0m\n",
            "\u001b[0mintressant mais je sais ce que tu ressens, surtout le fait que les gens te disent que c'est dans ta tte. \u001b[0;0m\n",
            "\u001b[1;94m    labels: Je n'ai pas vraiment de gens qui me le disent dans ma tte, plutt dans mon dos.\u001b[0;0m\n",
            "\u001b[0;95m     model: merci . je suis d ' accord .\u001b[0;0m\n",
            "\u001b[0mDang, c'est vrai. Mais je comprends aussi a. J'ai des gens qui parlent dans mon dos  cause de certaines choses auxquelles je crois. \u001b[0;0m\n",
            "\u001b[1;94m    labels: Moi aussi ! En quoi crois-tu ? Je crois aux dragons... Je viens de finir de regarder Game of Thrones. Mec, ces choses sont gniales\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis dsol d ' entendre a .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk- - -\u001b[0;0m\n",
            "\u001b[0mOui. Je finis par transpirer et rougir et j'ai l'impression que je vais vomir.\u001b[0;0m\n",
            "\u001b[1;94m    labels: et pourquoi a ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis dsol d ' entendre a .\u001b[0;0m\n",
            "\u001b[0mJe pense que c'est parce que dans ma tte, je pense que tout le monde me juge. Je commence  transpirer et j'ai mal au ventre.\u001b[0;0m\n",
            "\u001b[1;94m    labels: intressant mais je sais ce que tu ressens, surtout le fait que les gens te disent que c'est dans ta tte. \u001b[0;0m\n",
            "\u001b[0;95m     model: je suis dsol d ' entendre a .\u001b[0;0m\n",
            "\u001b[0mJe n'ai pas vraiment de gens qui me le disent dans ma tte, plutt dans mon dos.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Dang, c'est vrai. Mais je comprends aussi a. J'ai des gens qui parlent dans mon dos  cause de certaines choses auxquelles je crois. \u001b[0;0m\n",
            "\u001b[0;95m     model: je suis dsol que tu aies d faire a .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk- - -\u001b[0;0m\n",
            "\u001b[0mMa tortue s'est enfuie aujourd'hui.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Oh mon dieu. Vous vous tes disputs ?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh non ! je suis dsol d ' entendre a .\u001b[0;0m\n",
            "\u001b[0mC'est drle. Non. Je le laisse errer dans la maison.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Que mange ta tortue ?  Est-il difficile de s'occuper d'une tortue ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis dsol d ' entendre a .\u001b[0;0m\n",
            "\u001b[0mIl mange des insectes, des feuilles et des graines de tournesol. C'est facile. Il n'a pas besoin de marcher et le nettoyage est simple. Avez-vous des animaux domestiques ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Non, pas pour le moment.  J'ai 3 filles et elles ont assez d'ennuis ! LOL\u001b[0;0m\n",
            "\u001b[0;95m     model: non , je ne le fais pas . je suis un homme de compagnie .\u001b[0;0m\n",
            "\u001b[0mFlicitations. Venez-vous d'une grande famille ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je n'en ai pas, juste 2 frres et surs.  Mais ma femme voulait beaucoup d'enfants. Donc, voil.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui , je l ' ai fait . et vous ?\u001b[0;0m\n",
            "\u001b[0mWow. cool. Combien de fois utilisez-vous des ordinateurs ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'utilise un Mac pour mon travail - je suis professeur de musique.  Je le trouve trs utile.  Et vous ?  Utilisez-vous beaucoup l'ordinateur ?\u001b[0;0m\n",
            "\u001b[0;95m     model: 3 et 4 . ils sont trs amusants .\u001b[0;0m\n",
            "\u001b[0mSouvent. Je les utilise au travail pour le montage. Je suis  la recherche d'un moyen de diffuser des vidos ou des films depuis la bibliothque locale.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Stream de la bibliothque - maintenant, cela semble intressant. Ma femme et moi faisons de la plonge sous-marine et nous voulons commencer  filmer sous l'eau. Vous avez dj essay ?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh , c ' tait sympa de te parler . je ne suis pas un enfant de compagnie .\u001b[0;0m\n",
            "\u001b[0mNon, mais j'ai fait du snorkeling.\u001b[0;0m\n",
            "\u001b[1;94m    labels: C'est cool aussi. C'est comme a qu'on a commenc.  J'ai vu beaucoup de Tortues !  LOL\u001b[0;0m\n",
            "\u001b[0;95m     model: oh , a a l ' air trs amusant !\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk- - -\u001b[0;0m\n",
            "\u001b[0mOh mon dieu. Vous vous tes disputs ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: C'est drle. Non. Je le laisse errer dans la maison.\u001b[0;0m\n",
            "\u001b[0;95m     model: non , je suis en train de trouver un nouveau travail .\u001b[0;0m\n",
            "\u001b[0mQue mange ta tortue ?  Est-il difficile de s'occuper d'une tortue ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Il mange des insectes, des feuilles et des graines de tournesol. C'est facile. Il n'a pas besoin de marcher et le nettoyage est simple. Avez-vous des animaux domestiques ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne sais pas . je suis dsol .\u001b[0;0m\n",
            "\u001b[0mNon, pas pour le moment.  J'ai 3 filles et elles ont assez d'ennuis ! LOL\u001b[0;0m\n",
            "\u001b[1;94m    labels: Flicitations. Venez-vous d'une grande famille ?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh , je suis dsol d ' entendre a .\u001b[0;0m\n",
            "\u001b[0mJe n'en ai pas, juste 2 frres et surs.  Mais ma femme voulait beaucoup d'enfants. Donc, voil.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Wow. cool. Combien de fois utilisez-vous des ordinateurs ?\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis dsol . vous avez un enfant ?\u001b[0;0m\n",
            "\u001b[0mJ'utilise un Mac pour mon travail - je suis professeur de musique.  Je le trouve trs utile.  Et vous ?  Utilisez-vous beaucoup l'ordinateur ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Souvent. Je les utilise au travail pour le montage. Je suis  la recherche d'un moyen de diffuser des vidos ou des films depuis la bibliothque locale.\u001b[0;0m\n",
            "\u001b[0;95m     model: je travaille dans un magasin de compagnie .\u001b[0;0m\n",
            "\u001b[0mStream de la bibliothque - maintenant, cela semble intressant. Ma femme et moi faisons de la plonge sous-marine et nous voulons commencer  filmer sous l'eau. Vous avez dj essay ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Non, mais j'ai fait du snorkeling.\u001b[0;0m\n",
            "\u001b[0;95m     model: non , je ne l ' ai pas fait .\u001b[0;0m\n"
          ]
        }
      ],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='french_blended_skill_talk',\n",
        "    # task='fromfile:parlaiformat', \n",
        "    # fromfile_datapath= f'{data_path}data',\n",
        "    # fromfile_datatype_extension=True,\n",
        "    datatype= \"test\",\n",
        "\n",
        "    model_file= f'{finetuned_model_path}model',\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    # the result of grid search on 400M model and BST dataset when inference=topk\n",
        "    beam_block_ngram= 2,\n",
        "\tbeam_context_block_ngram= 3,\n",
        "\tbeam_length_penalty= 1,\n",
        "\tbeam_min_length= 10,\n",
        "\tbeam_size= 20,\n",
        "\tinference= \"topk\",\n",
        "\ttemperature= 0.5,\n",
        "\ttopk= 20,\n",
        "\ttopp= 0.9\n",
        "\n",
        "    # # Farnaz sent me\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_min_length= 20, \n",
        "    # beam_size= 10,\n",
        "    # inference =  'topk',  \n",
        "    # topk=20, \n",
        "    # temperature = 0.5, \n",
        "    # beam_length_penalty=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Display Data"
      ],
      "metadata": {
        "id": "qtVH017ArR6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(\n",
        "    task='blended_skill_talk',\n",
        "    num_examples=50\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yswHy-pbrUTn",
        "outputId": "a4b882a9-e5bc-428d-ce51-f8094e5cb55c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09:51:55 | Opt:\n",
            "09:51:55 |     allow_missing_init_opts: False\n",
            "09:51:55 |     batchsize: 1\n",
            "09:51:55 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "09:51:55 |     datatype: train:ordered\n",
            "09:51:55 |     dict_class: None\n",
            "09:51:55 |     display_add_fields: \n",
            "09:51:55 |     download_path: None\n",
            "09:51:55 |     dynamic_batching: None\n",
            "09:51:55 |     hide_labels: False\n",
            "09:51:55 |     ignore_agent_reply: True\n",
            "09:51:55 |     image_cropsize: 224\n",
            "09:51:55 |     image_mode: raw\n",
            "09:51:55 |     image_size: 256\n",
            "09:51:55 |     init_model: None\n",
            "09:51:55 |     init_opt: None\n",
            "09:51:55 |     is_debug: False\n",
            "09:51:55 |     loglevel: info\n",
            "09:51:55 |     max_display_len: 1000\n",
            "09:51:55 |     model: None\n",
            "09:51:55 |     model_file: None\n",
            "09:51:55 |     multitask_weights: [1]\n",
            "09:51:55 |     mutators: None\n",
            "09:51:55 |     num_examples: 50\n",
            "09:51:55 |     override: \"{'task': 'blended_skill_talk', 'num_examples': 50}\"\n",
            "09:51:55 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "09:51:55 |     starttime: Jun15_09-51\n",
            "09:51:55 |     task: blended_skill_talk\n",
            "09:51:55 |     verbose: False\n",
            "09:51:55 | creating task(s): blended_skill_talk\n",
            "09:51:55 | Loading ParlAI text data: /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/train.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: blended_skill_talk - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i've 2 kids.\n",
            "your persona: i love flowers.\n",
            "I love live music, that's why I try to go to concerts\n",
            "I do too. Wat do you like?\n",
            "I like acting, I hope to be an actor, what about you?\u001b[0;0m\n",
            "   \u001b[1;94mthat is ok.  have any kids?\u001b[0;0m\n",
            "\u001b[0mNo, but someday.\u001b[0;0m\n",
            "   \u001b[1;94mthat is good. I have 2\u001b[0;0m\n",
            "\u001b[0mAfter I am done with school I plan to have a family.\u001b[0;0m\n",
            "   \u001b[1;94mthat is great! you will be ready\u001b[0;0m\n",
            "\u001b[0mI hope so, how old are your kids?\u001b[0;0m\n",
            "   \u001b[1;94m5 & 7.  they take up a lot of my time\u001b[0;0m\n",
            "\u001b[0mI would imagine. I am sure they a great kids.\u001b[0;0m\n",
            "   \u001b[1;94mluckily, they love flowers just as much as I do.  we spend a lot of time in the garden\u001b[0;0m\n",
            "\u001b[0mI wish I had more time to do stuff like that. Medical school is exhausting. \u001b[0;0m\n",
            "   \u001b[1;94msounds like it. have you gotten any acting jobs, though?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: blended_skill_talk - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i just bought a new house with my partner.\n",
            "your persona: i like to make my own coffee.\n",
            "Lasagne\n",
            "Oh, I love lasagne. I make my own noodles as well as the sauce. \n",
            "Wow.  That's amazing.  I read where lasagne originated in Italy during the Middle Ages.  \n",
            "Oh really!? That is interesting. I am actually italian myself.\u001b[0;0m\n",
            "   \u001b[1;94mAwesome. Me and my partner just bought a house. I can't wait to cook in my kitchen.\u001b[0;0m\n",
            "\u001b[0mMoving in a new place can be a lot of fun. Are you a good cook?\u001b[0;0m\n",
            "   \u001b[1;94mI like to think so. I love to make coffee for an after dinner treat too.\u001b[0;0m\n",
            "\u001b[0mMmm That sounds delicious right now.\u001b[0;0m\n",
            "   \u001b[1;94mWhat do you like to do?\u001b[0;0m\n",
            "\u001b[0mWell I like tattoos and piercings, I am working on my next one right now.\u001b[0;0m\n",
            "   \u001b[1;94mpiercings are cool . i do not have any tattoos though. Too scared. I want some\u001b[0;0m\n",
            "\u001b[0mWhat would you get?\u001b[0;0m\n",
            "   \u001b[1;94mMaybe something for my kids. I've always wanted an anarchy symbol.\u001b[0;0m\n",
            "\u001b[0mHaha that is a cool idea.\u001b[0;0m\n",
            "   \u001b[1;94mI like to think I'm cool too. Hopefully one day.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: blended_skill_talk - - -\u001b[0;0m\n",
            "\u001b[0myour persona: my parents were divorced.\n",
            "your persona: i'm a widow.\n",
            "My neighbors dog won't stop barking at me. Ugh!\n",
            "Thats the worst, is it a big dog or little dog? \n",
            "It's a little dog. Why is it the little ones always bark the most?\u001b[0;0m\n",
            "   \u001b[1;94mI believe it is due to their behavior and internal and external stimuli.\u001b[0;0m\n",
            "\u001b[0mI'd like to introduce that dog to my pet snakes. I think they'd eat him though!\u001b[0;0m\n",
            "   \u001b[1;94mOh no! But I think black snakes are good? They eat nasty bugs and rodents!\u001b[0;0m\n",
            "\u001b[0mMy snakes are both pythons. I feed them mice. Do you have any pets?\u001b[0;0m\n",
            "   \u001b[1;94mNo, but I would love to have a cat for it to hunt mice in my house lol\u001b[0;0m\n",
            "\u001b[0mYou have mice running around your house? Yikes! I think I'd rather have the barking dogs than that.\u001b[0;0m\n",
            "   \u001b[1;94mLol, but they do make great companions at times.I give them bread and they leave me alone for the most part. \u001b[0;0m\n",
            "\u001b[0mHave you given them names? If not, maybe we can come up with some.\u001b[0;0m\n",
            "   \u001b[1;94mi just called them all Jerry, Big Jerry, Always hungry Jerry and then there's the Tiny Jerry. \u001b[0;0m\n",
            "\u001b[0mNow you really do need to get a cat and call him Tom.\u001b[0;0m\n",
            "   \u001b[1;94mYep that would completely the collection! I am widow so I need all the companions I can get haha\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: blended_skill_talk - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i have blue eyes and curly brown hair.\n",
            "your persona: i love to snack between meals.\n",
            "I took the train to work the other day and it was so crowded. I was feeling really claustrophobic.\n",
            "I would have been too! Do you always take it to work? \n",
            "yes i do because i do not have a car\u001b[0;0m\n",
            "   \u001b[1;94mif you could have a car what would it be?\u001b[0;0m\n",
            "\u001b[0ma honda because they are affordable and reliable\u001b[0;0m\n",
            "   \u001b[1;94mi love the oscar mier wiener-mobile. it reminds me of snacking when i'm not having a meal. would you drive the oscar meir weiner mobile?\u001b[0;0m\n",
            "\u001b[0mi do not think so, what about you?\u001b[0;0m\n",
            "   \u001b[1;94mi lost my driver's liscense, ufortunately, but i would if i could\u001b[0;0m\n",
            "\u001b[0mhow did you lose it?\u001b[0;0m\n",
            "   \u001b[1;94mi did some bad stuff, but that's not important\u001b[0;0m\n",
            "\u001b[0mim saving up to buy a new camera to take pictures of people who lost their licenses actually\u001b[0;0m\n",
            "   \u001b[1;94mdo you take pictures of people with curly hair like mine?\u001b[0;0m\n",
            "\u001b[0mno, only people with long hair like mine\u001b[0;0m\n",
            "   \u001b[1;94mwell, you got to do you, brother. i'm proud of you anyway!\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: blended_skill_talk - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i always answer my cellphone.\n",
            "your persona: i work in sales.\n",
            "that is good . i'm nursing a cold and vitamin c does nothing lol\n",
            "oh sorry about that . taking lots of fluids ?\n",
            "Yes, thank you. I think I just need some time to make it pass.\u001b[0;0m\n",
            "   \u001b[1;94mShould always be drinking water anyways, your body needs it!\u001b[0;0m\n",
            "\u001b[0mYes it does, especially during summer.\u001b[0;0m\n",
            "   \u001b[1;94mWhat do you do for a living?  I try my best in sales.\u001b[0;0m\n",
            "\u001b[0mI work at a ski resort. I love it!\u001b[0;0m\n",
            "   \u001b[1;94mI heard Mount Tom Ski Area in Holyoke, Mass is a great resort to visit\u001b[0;0m\n",
            "\u001b[0mI have never been but I have heard the same. I love snow and the mountains.\u001b[0;0m\n",
            "   \u001b[1;94mI wish I could enjoy the snow and mountains, I would be afraid I could not answer my cellphone!\u001b[0;0m\n",
            "\u001b[0mHaha, these days it is amazing where you can get cell service.\u001b[0;0m\n",
            "   \u001b[1;94mi agree ! that is the main reason I have a phone !\u001b[0;0m\n",
            "\u001b[0mDo you have any plans this weekend?\u001b[0;0m\n",
            "   \u001b[1;94mNo plans yet, I'm just happy to sleep in for once!\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: blended_skill_talk - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i love to go out to eat with my family.\n",
            "your persona: i like to go to the movies.\n",
            "He's a good worker but not as reliable as I am. He is always calling in or not putting much effort into his work. I feel I should have gotten that position.\n",
            "That's a shame, if that's the case then I agree.\n",
            "Yea, but to be perfectly honest, I actually don't work hard either.\u001b[0;0m\n",
            "   \u001b[1;94moh i see . i am really reliable in my job on other things i am not\u001b[0;0m\n",
            "\u001b[0mWell my problem is that I just never had to really work. I always had money. What things are you not reliable in?\u001b[0;0m\n",
            "   \u001b[1;94mMostly jobs and children.\u001b[0;0m\n",
            "\u001b[0mWhat do you mean children?\u001b[0;0m\n",
            "   \u001b[1;94mThey are food, shelter, child care, health care, transportation, and utilities.\u001b[0;0m\n",
            "\u001b[0mOk, Im pretty confused as to what you mean, but that sounds like a lot.\u001b[0;0m\n",
            "   \u001b[1;94mit is a lot to take care of because kids are a lot of responsibility\u001b[0;0m\n",
            "\u001b[0mI see. Well good luck with that, I wouldn't know because I don't have kids yet.\u001b[0;0m\n",
            "   \u001b[1;94moh well thank you . what else should i know about you ?\u001b[0;0m\n",
            "\u001b[0mWell, Im going through some drama right now with my sister in law, she hates me. \u001b[0;0m\n",
            "   \u001b[1;94mFamily conflict is always hard.  Maybe you should be the bigger man and work for peace \u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: blended_skill_talk - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i work as a teacher and love my job.\n",
            "your persona: i've two dogs who are like my babies.\n",
            "My boss earns way more money than me for less work.\n",
            "That must be so annoying. You should ask for a pay raise.\n",
            "yeah, i think i will!\u001b[0;0m\n",
            "   \u001b[1;94mI love my job as a teacher but would also like to assk for a raise\u001b[0;0m\n",
            "\u001b[0mwho do you ask? aren't teacher salaries decided by some sort of... thing, or whatever? i love my family.\u001b[0;0m\n",
            "   \u001b[1;94mI would ask the school board they base it on how long you've been there and test scores\u001b[0;0m\n",
            "\u001b[0mhow would the principal feel about you going behind his back?\u001b[0;0m\n",
            "   \u001b[1;94mNot sure, I suppose I could sit down and discuss it with him. What kind of work do you do?\u001b[0;0m\n",
            "\u001b[0mi'm a garbage man, although i'm a woman. my boss drives the truck, but i'm more qualified, really.\u001b[0;0m\n",
            "   \u001b[1;94mI've never driven a truck, only small vehicles! How's that working out for you? My two dogs would love to ride along if I did that\u001b[0;0m\n",
            "\u001b[0mdogs would for sure love a big truck full of garbage. i thik i'm the top 20 or so best garbage men in the area\u001b[0;0m\n",
            "   \u001b[1;94moh wow that is impressive . must be an extremely difficult job .\u001b[0;0m\n",
            "\u001b[0mno.literally anyone could do it. you just pick up garbage. it's not as easy as teaching, im sure, buts ittakes no skill\u001b[0;0m\n",
            "   \u001b[1;94mIf you have patience teaching is great but they test your nerves every day\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: blended_skill_talk - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i have woman calling me all the time.\n",
            "your persona: i was once offered to play basketball professionally.\n",
            "I got really joyful when my dad got me my first vehicle. It was just a happy feeling\n",
            "Yeah. I know that feeling., although I had to buy my own.\n",
            "The vehicle was exactly what I wanted, a newer model sports car.\u001b[0;0m\n",
            "   \u001b[1;94mThat was a good choice, What kind of car was it?\u001b[0;0m\n",
            "\u001b[0mA Mustang GT, convertible\u001b[0;0m\n",
            "   \u001b[1;94mNice! Too bad you couldn't get  your hands on a 1963 Mustang II. That's my favorite car.\u001b[0;0m\n",
            "\u001b[0mA few years ago I helped with a restoration on a 67. It was absolutely gorgeous.\u001b[0;0m\n",
            "   \u001b[1;94mThat sounds awesome. Did you restore it with your dad?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: blended_skill_talk - - -\u001b[0;0m\n",
            "\u001b[0myour persona: dancing is my passion.\n",
            "your persona: i'm terrible at math.\n",
            "Dance\n",
            "I enjoy dance a lot as well, my daughter does tap dancing. What kind of dancing do you like most?\n",
            "I don't have a particular favorite style, but I've always thought tap was the odd one, even though you see it all the time on Broadway.\n",
            "Tap is pretty odd compared to the others. My daughter loves ballet.\u001b[0;0m\n",
            "   \u001b[1;94mi love broadway shows , so this was the closest thing i had .\u001b[0;0m\n",
            "\u001b[0mYea, well keep it up, maybe one day you'll be on broadway.\u001b[0;0m\n",
            "   \u001b[1;94mi hope so. i'm so bad at math. they count 5,6,7,8! at the beginning, and i wish they would stick to smaller numbers\u001b[0;0m\n",
            "\u001b[0mhahah that's funny. I believe in you though, you can overcome that. \u001b[0;0m\n",
            "   \u001b[1;94mit's like a chorus line. the musical with actor michael douglass. have you seen it/\u001b[0;0m\n",
            "\u001b[0mNo I haven't, I don't watch movies very often. Im usually out at clubs people watching.\u001b[0;0m\n",
            "   \u001b[1;94mwhen do you think you'll get a chance to catch up on some film? I love dancing, but when I'm not treading the boards, I love watching tv\u001b[0;0m\n",
            "\u001b[0mProbably not for a while. I just bought a new jaguar and I try to get out and just drive around and show it off as much as possible. \u001b[0;0m\n",
            "   \u001b[1;94mwow, i love animals. What does it eat?\u001b[0;0m\n",
            "09:51:56 | loaded 4819 episodes with a total of 27018 examples\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "blender-90m-400m-multitask-double-sided.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOYyUTzJ9a9okKUpRLX6yR9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "128fd40b58b1410aaef82c012c5c9ed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c38cefd4fea487aba661ac2f6bb16f1",
              "IPY_MODEL_52448a45dde44b8896446c69043ede4d"
            ],
            "layout": "IPY_MODEL_56226e1618e8444ab41b43c2b7cac556"
          }
        },
        "4c38cefd4fea487aba661ac2f6bb16f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5703fb09f21a4931aa2067a4d62502fa",
            "placeholder": "",
            "style": "IPY_MODEL_c4a694833b694a9199182e47ec6da791",
            "value": "0.541 MB of 0.541 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "52448a45dde44b8896446c69043ede4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad9d10e70e034784bff2250d38b60060",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_664750a79215441ea7b43aca361c3e7c",
            "value": 1
          }
        },
        "56226e1618e8444ab41b43c2b7cac556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5703fb09f21a4931aa2067a4d62502fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4a694833b694a9199182e47ec6da791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad9d10e70e034784bff2250d38b60060": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "664750a79215441ea7b43aca361c3e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}