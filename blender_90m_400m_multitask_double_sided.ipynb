{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirpaia/blenderbot/blob/main/blender_90m_400m_multitask_double_sided.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SONwSWMp6qPv"
      },
      "source": [
        "# 0.Installing prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d-SZ_On6Kxg"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BegaSUz6iUz",
        "outputId": "b3dbf6d8-e9db-409f-8dae-c604e3b47f98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SzGRXHQ6kDQ",
        "outputId": "e6918aa6-5fea-40fd-ed07-e0b68e523cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWDakYmy6mIQ"
      },
      "outputs": [],
      "source": [
        "mydrive_path = '/content/drive/MyDrive/colabs/blender-models/'\n",
        "# !pip uninstall -q parlai\n",
        "!pip install -q parlai\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIJEq9_r63hi"
      },
      "source": [
        "# 1.Preparing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R-fgBUdcPX5"
      },
      "source": [
        "## Genreal Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47u8M3RK65m_",
        "outputId": "c4b6d06c-fc29-4d75-aa05-7445754bbfab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n",
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def transfer_list_of_turns_to_dialog(d):\n",
        "    if len(d)%2 !=0: d = d[:-1]\n",
        "    t = \"\"\n",
        "    for i in range(0,len(d),2):\n",
        "        u1 = d[i]\n",
        "        u2 = d[i+1]\n",
        "\n",
        "        if (i+2) != len(d):\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "        else:\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "    return t\n",
        "\n",
        "def transfer_list_of_pairs_to_dialog(d):\n",
        "  t = \"\"\n",
        "  for i, text_label_pair in enumerate(d):\n",
        "    u1 = text_label_pair[0]\n",
        "    u2 = text_label_pair[1]\n",
        "\n",
        "    if i != (len(d) - 1):\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "    else:\n",
        "      t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "  return t\n",
        "\n",
        "def convert_parlai_format_to_list_of_turns(lines):\n",
        "    result = []\n",
        "    for line in lines:\n",
        "        text_label = line.split(\"\\t\")\n",
        "        result.append(text_label[0].replace(\"text:\", \"\"))\n",
        "        result.append(text_label[1].replace(\"labels:\", \"\").replace(\"\\n\",\"\"))\n",
        "    return result\n",
        "\n",
        "t = ['hello','how are you','good','bye','test']\n",
        "print(transfer_list_of_turns_to_dialog(t))\n",
        "\n",
        "t = [['hello','how are you'],['good','bye']]\n",
        "print(transfer_list_of_pairs_to_dialog(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VIROa77CJtG9"
      },
      "outputs": [],
      "source": [
        "def make_dataset_double_sided(lines, path, filename): \n",
        "    # train[:10]\n",
        "    all_dialogs_parlai_format = []\n",
        "    dialog = []\n",
        "    for line in lines:\n",
        "        dialog.append(line)\n",
        "        if 'episode_done:True' in line:\n",
        "            turns = convert_parlai_format_to_list_of_turns(dialog)\n",
        "            first_parlai_dialog = transfer_list_of_turns_to_dialog(turns)\n",
        "            second_parlai_dialog = transfer_list_of_turns_to_dialog(turns[1:])\n",
        "\n",
        "            all_dialogs_parlai_format.append(first_parlai_dialog)\n",
        "            all_dialogs_parlai_format.append(second_parlai_dialog)\n",
        "\n",
        "            dialog = []\n",
        "            # break\n",
        "    print(sum([1 for a in lines if 'episode_done:True' in a]), len(all_dialogs_parlai_format))\n",
        "\n",
        "    with open(f\"{path}{filename}\", \"w\") as f:\n",
        "        f.writelines(all_dialogs_parlai_format)\n",
        "\n",
        "# !mkdir /content/dataset_french_bst/\n",
        "# make_dataset_double_sided(lines,\"/content/dataset_french_bst/\", \"train.txt\")\n",
        "\n",
        "\n",
        "\n",
        "# # import os.path\n",
        "# # from os import path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE45ZyKC8WjC"
      },
      "source": [
        "## XPersona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZmSJtQG8Y1o",
        "outputId": "cbfdd43e-b003-4469-e7e0-2d7e7aba611b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Xpersona'...\n",
            "remote: Enumerating objects: 285, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 285 (delta 6), reused 6 (delta 4), pack-reused 275\u001b[K\n",
            "Receiving objects: 100% (285/285), 45.01 MiB | 21.61 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n",
            "Checking out files: 100% (218/218), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HLTCHKUST/Xpersona.git\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_train_corrected.json','r') as f:\n",
        "   train_data = json.load(f)\n",
        "\n",
        "dialogs_train = pd.DataFrame(train_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_valid_human_annotated.json','r') as f:\n",
        "   valid_data = json.load(f)\n",
        "\n",
        "dialogs_valid = pd.DataFrame(valid_data)['dialogue'].tolist()\n",
        "\n",
        "with open('Xpersona/dataset/Fr_persona_split_test_human_annotated.json','r') as f:\n",
        "   test_data = json.load(f)\n",
        "\n",
        "dialogs_test = pd.DataFrame(test_data)['dialogue'].tolist()\n",
        "\n",
        "# dialogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plw61KMBJU4I",
        "outputId": "dacec375-cb1d-4b4b-e098-c2ee93a204de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/dataset_french_xpersona/': No such file or directory\n",
            "33756\n",
            "496\n",
            "498\n"
          ]
        }
      ],
      "source": [
        "data_path = \"/content/dataset_french_xpersona/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "\n",
        "def convert_xpersona_dialogs_to_parlai_format_file_and_double_sided(dialogs, filename):\n",
        "    all_dialogs_parlai_format = []\n",
        "    import numpy as np\n",
        "    for d in dialogs:\n",
        "        turns = np.reshape(d, (-1)).tolist()\n",
        "        all_dialogs_parlai_format.append(transfer_list_of_turns_to_dialog(turns))\n",
        "        all_dialogs_parlai_format.append(transfer_list_of_turns_to_dialog(turns[1:]))\n",
        "\n",
        "    print(len(all_dialogs_parlai_format))\n",
        "\n",
        "    with open(f\"{data_path}{filename}\",\"w\") as f:\n",
        "        f.writelines(all_dialogs_parlai_format)\n",
        "\n",
        "convert_xpersona_dialogs_to_parlai_format_file_and_double_sided(dialogs_train, \"train.txt\")\n",
        "convert_xpersona_dialogs_to_parlai_format_file_and_double_sided(dialogs_valid, \"valid.txt\")\n",
        "convert_xpersona_dialogs_to_parlai_format_file_and_double_sided(dialogs_test, \"test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKlGi0f08zQc"
      },
      "outputs": [],
      "source": [
        "# data_path = \"/content/dataset_french_xpersona/\"\n",
        "# !rm -R $data_path\n",
        "# !mkdir $data_path\n",
        "\n",
        "# #region Training\n",
        "# data_train = \"\"\n",
        "# for d in dialogs_train:\n",
        "#   data_train += transfer_list_of_pairs_to_dialog(d)\n",
        "\n",
        "# file_train = open(f\"{data_path}train.txt\",\"w\")\n",
        "# print(\"Training Set:\", file_train.write(data_train))\n",
        "# #endregion \n",
        "    \n",
        "# #region Validation\n",
        "# data_valid = \"\"\n",
        "# for d in dialogs_valid:\n",
        "#   data_valid += transfer_list_of_pairs_to_dialog(d)\n",
        "\n",
        "# file_valid = open(f\"{data_path}valid.txt\",\"w\")\n",
        "# print(\"Validation Set:\", file_valid.write(data_valid))\n",
        "# #endregion\n",
        "\n",
        "# #region Test\n",
        "# data_test = \"\"\n",
        "# for d in dialogs_test:\n",
        "#   data_test += transfer_list_of_pairs_to_dialog(d)\n",
        "\n",
        "# file_test = open(f\"{data_path}test.txt\",\"w\")\n",
        "# print(\"Test Set:\", file_test.write(data_test))\n",
        "# #endregion \n",
        "\n",
        "# # print(len(data_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoSv5zhs8ZIv"
      },
      "source": [
        "## ED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mISEjZbP8dN-",
        "outputId": "793eacc9-aa59-4a3c-b7ae-b72da6e0dfde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/dataset_french_ed/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/test.txt' -> '/content/dataset_french_ed/test.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/train.txt' -> '/content/dataset_french_ed/train.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/valid.txt' -> '/content/dataset_french_ed/valid.txt'\n"
          ]
        }
      ],
      "source": [
        "googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_empathetic_dialogues/\"\n",
        "data_path = \"/content/dataset_french_ed/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "!cp -rv $googledrive_data_path* $data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNbq22L-Khmg",
        "outputId": "7183e24f-74a6-4386-c421-51b755e362c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64636\n",
            "39057 5537 5093 49687\n"
          ]
        }
      ],
      "source": [
        "# with open(f\"{googledrive_data_path}train.txt\") as f:\n",
        "#     train = f.readlines()\n",
        "\n",
        "# with open(f\"{googledrive_data_path}valid.txt\") as f:\n",
        "#     valid = f.readlines()\n",
        "\n",
        "# with open(f\"{googledrive_data_path}test.txt\") as f:\n",
        "#     test = f.readlines()\n",
        "\n",
        "\n",
        "# print(len(train))\n",
        "# a, b, c = sum([1 for a in train if 'episode_done:True' in a]), sum([1 for a in valid if 'episode_done:True' in a]), sum([1 for a in test if 'episode_done:True' in a])\n",
        "# print(a,b,c,a+b+c)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUxXs_Oe8dda"
      },
      "source": [
        "## BST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIRVhsq88e7A",
        "outputId": "b691f946-d218-42c7-bbcc-3f2ca44205cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/dataset_french_bst/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/test.txt' -> '/content/dataset_french_bst/test.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/train.txt' -> '/content/dataset_french_bst/train.txt'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/valid.txt' -> '/content/dataset_french_bst/valid.txt'\n"
          ]
        }
      ],
      "source": [
        "googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/\"\n",
        "data_path = \"/content/dataset_french_bst/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "!cp -rv $googledrive_data_path* $data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuQo9hiTRT0Y",
        "outputId": "7da8be18-fc80-408d-9da3-5534e8e19e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59700\n",
            "9638 2018 1958 13614\n"
          ]
        }
      ],
      "source": [
        "googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_blended_skill_talk/\"\n",
        "with open(f\"{googledrive_data_path}train.txt\") as f:\n",
        "    train = f.readlines()\n",
        "\n",
        "with open(f\"{googledrive_data_path}valid.txt\") as f:\n",
        "    valid = f.readlines()\n",
        "\n",
        "with open(f\"{googledrive_data_path}test.txt\") as f:\n",
        "    test = f.readlines()\n",
        "\n",
        "\n",
        "print(len(train))\n",
        "a, b, c = sum([1 for a in train if 'episode_done:True' in a]), sum([1 for a in valid if 'episode_done:True' in a]), sum([1 for a in test if 'episode_done:True' in a])\n",
        "print(a, b, c, a+b+c)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WoW"
      ],
      "metadata": {
        "id": "1l6gr-_0Ohlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_wizard_of_wikipedia/\"\n",
        "# data_path = \"/content/dataset_french_wow/\"\n",
        "# !rm -R $data_path\n",
        "# !mkdir $data_path\n",
        "# !cp -rv $googledrive_data_path* $data_path"
      ],
      "metadata": {
        "id": "yv1tbBZOOnW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_xpersona_dialogs_to_parlai_format_file_and_double_sided(dialogs_train, \"train.txt\")\n",
        "googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_wizard_of_wikipedia/\"\n",
        "data_path = \"/content/dataset_french_wow/\"\n",
        "!rm -R $data_path\n",
        "!mkdir $data_path\n",
        "\n",
        "with open(f\"{googledrive_data_path}train.txt\") as f:\n",
        "    train_lines = f.readlines()\n",
        "with open(f\"{googledrive_data_path}valid.txt\") as f:\n",
        "    valid_lines = f.readlines()\n",
        "with open(f\"{googledrive_data_path}test.txt\") as f:\n",
        "    test_lines = f.readlines()\n",
        "\n",
        "\n",
        "make_dataset_double_sided(train_lines, data_path, \"train.txt\")\n",
        "make_dataset_double_sided(valid_lines, data_path, \"valid.txt\")\n",
        "make_dataset_double_sided(test_lines, data_path, \"test.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vj3Ii8TAknd",
        "outputId": "fdda97db-e37e-4bed-ed83-ece063c946b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/dataset_french_wow/': No such file or directory\n",
            "17848 35696\n",
            "2231 4462\n",
            "2231 4462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# googledrive_data_path = \"/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_wizard_of_wikipedia/\"\n",
        "# with open(f\"{googledrive_data_path}train.txt\") as f:\n",
        "#     train = f.readlines()\n",
        "# with open(f\"{googledrive_data_path}valid.txt\") as f:\n",
        "#     valid = f.readlines()\n",
        "# with open(f\"{googledrive_data_path}test.txt\") as f:\n",
        "#     test = f.readlines()\n",
        "# print(len(train))\n",
        "# a, b, c = sum([1 for a in train if 'episode_done:True' in a]), sum([1 for a in valid if 'episode_done:True' in a]), sum([1 for a in test if 'episode_done:True' in a])\n",
        "# print(a, b, c, a+b+c)"
      ],
      "metadata": {
        "id": "9jcuyswOBWrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PQfT57Q8I2u"
      },
      "source": [
        "# 2.Creating new Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oVPKrhO8ISe",
        "outputId": "67ccaba9-5659-470f-d8fb-ed3bbc316fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/agents.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/agents.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/build.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/build.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/__init__.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/__init__.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/LICENSE_DOCUMENTATION' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/LICENSE_DOCUMENTATION'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/README.md' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/README.md'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test/empathetic_dialogues_test.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test/empathetic_dialogues_test.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test/empathetic_dialogues_train.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test/empathetic_dialogues_train.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test/empathetic_dialogues_valid.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test/empathetic_dialogues_valid.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/test.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/test.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/worlds.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_xpersona/worlds.py'\n",
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/agents.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/agents.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/build.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/build.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/__init__.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/__init__.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/LICENSE_DOCUMENTATION' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/LICENSE_DOCUMENTATION'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/README.md' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/README.md'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test/empathetic_dialogues_test.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test/empathetic_dialogues_test.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test/empathetic_dialogues_train.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test/empathetic_dialogues_train.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test/empathetic_dialogues_valid.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test/empathetic_dialogues_valid.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/test.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/test.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/worlds.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_empathetic_dialogues/worlds.py'\n",
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/agents.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/agents.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/build.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/build.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/__init__.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/__init__.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/LICENSE_DOCUMENTATION' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/LICENSE_DOCUMENTATION'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/README.md' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/README.md'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test/empathetic_dialogues_test.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test/empathetic_dialogues_test.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test/empathetic_dialogues_train.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test/empathetic_dialogues_train.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test/empathetic_dialogues_valid.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test/empathetic_dialogues_valid.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/test.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/test.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/worlds.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_blended_skill_talk/worlds.py'\n",
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/': No such file or directory\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/agents.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/agents.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/build.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/build.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/__init__.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/__init__.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/LICENSE_DOCUMENTATION' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/LICENSE_DOCUMENTATION'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/README.md' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/README.md'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/test' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/test'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/test/empathetic_dialogues_test.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/test/empathetic_dialogues_test.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/test/empathetic_dialogues_train.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/test/empathetic_dialogues_train.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/test/empathetic_dialogues_valid.yml' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/test/empathetic_dialogues_valid.yml'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/test.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/test.py'\n",
            "'/content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/worlds.py' -> '/usr/local/lib/python3.7/dist-packages/parlai/tasks/french_wizard_of_wikipedia/worlds.py'\n"
          ]
        }
      ],
      "source": [
        "#region XPersona\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_xpersona/'\n",
        "!mkdir $task_path'french_xpersona'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_xpersona/* $task_path'french_xpersona/'\n",
        "#endregion\n",
        "\n",
        "\n",
        "#region ED\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_empathetic_dialogues/'\n",
        "!mkdir $task_path'french_empathetic_dialogues'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_empathetic_dialogues/* $task_path'french_empathetic_dialogues/'\n",
        "#endregion\n",
        "\n",
        "\n",
        "#region BST\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_blended_skill_talk/'\n",
        "!mkdir $task_path'french_blended_skill_talk'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_blended_skill_talk/* $task_path'french_blended_skill_talk/'\n",
        "#endregion\n",
        "\n",
        "#region WoW\n",
        "task_path = \"/usr/local/lib/python3.7/dist-packages/parlai/tasks/\"\n",
        "\n",
        "!rm -R $task_path'french_wizard_of_wikipedia/'\n",
        "!mkdir $task_path'french_wizard_of_wikipedia'\n",
        "!cp -ruv /content/drive/MyDrive/colabs/aliae-workspace/parlai-tasks/french_wizard_of_wikipedia/* $task_path'french_wizard_of_wikipedia/'\n",
        "#endregion "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !parlai display_data --task french_empathetic_dialogues\n",
        "# !parlai display_data --task french_xpersona \n",
        "# !parlai display_data --task french_blended_skill_talk \n",
        "# !parlai display_data --task french_wizard_of_wikipedia"
      ],
      "metadata": {
        "id": "uDe0BBqqChqF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGoTqdFD7x_3"
      },
      "source": [
        "# 3.Finetuning + Multitasking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "65rK6pfj70Px",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9e048268-1306-4697-934d-45bd41472752"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/colabs/blender-models/finetuned-400m-4task-2sided/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "finetuned_model_path = f'{mydrive_path}finetuned-400m-4task-2sided/'\n",
        "init_model = 'zoo:blender/blender_400Mdistill/model'\n",
        "dict_file  = 'zoo:blender/blender_400Mdistill/model.dict'\n",
        "# init_model = 'zoo:blender/blender_90M/model'\n",
        "# dict_file  = 'zoo:blender/blender_90M/model.dict'\n",
        "finetuned_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zArppuSCGS7"
      },
      "outputs": [],
      "source": [
        "# !mkdir copied_dataset_french_bst\n",
        "# !cp dataset_french_bst/* copied_dataset_french_bst\n",
        "# !mv copied_dataset_french_bst/test.txt copied_dataset_french_bst/_test.txt\n",
        "# !mv copied_dataset_french_bst/train.txt copied_dataset_french_bst/_train.txt\n",
        "# !mv copied_dataset_french_bst/valid.txt copied_dataset_french_bst/_valid.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "X7vLiYpF8HbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8770774-8e47-4a4f-9c44-86d23ce60f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15:19:55 | building dictionary first...\n",
            "15:19:55 | No model with opt yet at: /content/drive/MyDrive/colabs/blender-models/blender-models/finetuned-90m-4task-2sided/model(.opt)\n",
            "15:19:55 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,verbose: True,is_debug: False,datapath: /usr/local/lib/python3.7/dist-packages/data,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,beam_block_full_context: True,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,interactive_mode: False,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None\u001b[0m\n",
            "15:19:55 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues --numthreads 1 --batchsize 16 --num-epochs -1 --save-every-n-secs 60.0 --save-after-valid True --validation-every-n-epochs 0.25 --validation-max-exs 20000 --validation-patience 15 --log-every-n-secs 2 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --num-topics 5 --train-experiencer-only False --dropout 0.1 --learn-positional-embeddings True --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --skip-generation False --inference beam --fp16-impl apex --optimizer adamax --learningrate 7.5e-06 --max-lr-steps -1 --warmup-updates -1 --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
            "15:19:55 | Using CUDA\n",
            "15:19:55 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict\n",
            "15:19:55 | num words = 54944\n",
            "15:19:57 | Total parameters: 87,508,992 (86,984,704 trainable)\n",
            "15:19:57 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model\n",
            "15:19:57 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
            "15:19:57 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
            "15:19:57 | Opt:\n",
            "15:19:57 |     activation: gelu\n",
            "15:19:57 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "15:19:57 |     adam_eps: 1e-08\n",
            "15:19:57 |     add_p1_after_newln: False\n",
            "15:19:57 |     aggregate_micro: False\n",
            "15:19:57 |     allow_missing_init_opts: False\n",
            "15:19:57 |     attention_dropout: 0.0\n",
            "15:19:57 |     batchsize: 8\n",
            "15:19:57 |     beam_block_full_context: True\n",
            "15:19:57 |     beam_block_list_filename: None\n",
            "15:19:57 |     beam_block_ngram: -1\n",
            "15:19:57 |     beam_context_block_ngram: -1\n",
            "15:19:57 |     beam_delay: 30\n",
            "15:19:57 |     beam_length_penalty: 0.65\n",
            "15:19:57 |     beam_min_length: 1\n",
            "15:19:57 |     beam_size: 1\n",
            "15:19:57 |     betas: '(0.9, 0.999)'\n",
            "15:19:57 |     bpe_add_prefix_space: None\n",
            "15:19:57 |     bpe_debug: False\n",
            "15:19:57 |     bpe_dropout: None\n",
            "15:19:57 |     bpe_merge: None\n",
            "15:19:57 |     bpe_vocab: None\n",
            "15:19:57 |     checkpoint_activations: False\n",
            "15:19:57 |     compute_tokenized_bleu: False\n",
            "15:19:57 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "15:19:57 |     datatype: train\n",
            "15:19:57 |     delimiter: '\\n'\n",
            "15:19:57 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "15:19:57 |     dict_endtoken: __end__\n",
            "15:19:57 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict\n",
            "15:19:57 |     dict_include_test: False\n",
            "15:19:57 |     dict_include_valid: False\n",
            "15:19:57 |     dict_initpath: None\n",
            "15:19:57 |     dict_language: english\n",
            "15:19:57 |     dict_loaded: True\n",
            "15:19:57 |     dict_lower: True\n",
            "15:19:57 |     dict_max_ngram_size: -1\n",
            "15:19:57 |     dict_maxexs: -1\n",
            "15:19:57 |     dict_maxtokens: -1\n",
            "15:19:57 |     dict_minfreq: 0\n",
            "15:19:57 |     dict_nulltoken: __null__\n",
            "15:19:57 |     dict_starttoken: __start__\n",
            "15:19:57 |     dict_textfields: text,labels\n",
            "15:19:57 |     dict_tokenizer: bpe\n",
            "15:19:57 |     dict_unktoken: __unk__\n",
            "15:19:57 |     display_examples: False\n",
            "15:19:57 |     download_path: None\n",
            "15:19:57 |     dropout: 0.0\n",
            "15:19:57 |     dynamic_batching: full\n",
            "15:19:57 |     embedding_projection: random\n",
            "15:19:57 |     embedding_size: 512\n",
            "15:19:57 |     embedding_type: random\n",
            "15:19:57 |     embeddings_scale: True\n",
            "15:19:57 |     eval_batchsize: None\n",
            "15:19:57 |     eval_dynamic_batching: None\n",
            "15:19:57 |     evaltask: None\n",
            "15:19:57 |     ffn_size: 2048\n",
            "15:19:57 |     final_extra_opt: \n",
            "15:19:57 |     force_fp16_tokens: False\n",
            "15:19:57 |     fp16: True\n",
            "15:19:57 |     fp16_impl: mem_efficient\n",
            "15:19:57 |     gpu: -1\n",
            "15:19:57 |     gradient_clip: 0.1\n",
            "15:19:57 |     hide_labels: False\n",
            "15:19:57 |     history_add_global_end_token: None\n",
            "15:19:57 |     history_reversed: False\n",
            "15:19:57 |     history_size: -1\n",
            "15:19:57 |     image_cropsize: 224\n",
            "15:19:57 |     image_mode: raw\n",
            "15:19:57 |     image_size: 256\n",
            "15:19:57 |     inference: greedy\n",
            "15:19:57 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model\n",
            "15:19:57 |     init_opt: None\n",
            "15:19:57 |     interactive_mode: False\n",
            "15:19:57 |     invsqrt_lr_decay_gamma: -1\n",
            "15:19:57 |     is_debug: False\n",
            "15:19:57 |     label_truncate: 128\n",
            "15:19:57 |     learn_positional_embeddings: False\n",
            "15:19:57 |     learningrate: 1e-05\n",
            "15:19:57 |     load_from_checkpoint: True\n",
            "15:19:57 |     log_every_n_secs: 60.0\n",
            "15:19:57 |     log_every_n_steps: 50\n",
            "15:19:57 |     log_keep_fields: all\n",
            "15:19:57 |     loglevel: info\n",
            "15:19:57 |     lr_scheduler: reduceonplateau\n",
            "15:19:57 |     lr_scheduler_decay: 0.5\n",
            "15:19:57 |     lr_scheduler_patience: 3\n",
            "15:19:57 |     max_train_steps: -1\n",
            "15:19:57 |     max_train_time: -1\n",
            "15:19:57 |     metrics: default\n",
            "15:19:57 |     model: transformer/generator\n",
            "15:19:57 |     model_file: /content/drive/MyDrive/colabs/blender-models/blender-models/finetuned-90m-4task-2sided/model\n",
            "15:19:57 |     model_parallel: False\n",
            "15:19:57 |     momentum: 0\n",
            "15:19:57 |     multitask_weights: '(1.0, 3.0, 3.0, 3.0)'\n",
            "15:19:57 |     mutators: None\n",
            "15:19:57 |     n_decoder_layers: -1\n",
            "15:19:57 |     n_encoder_layers: -1\n",
            "15:19:57 |     n_heads: 16\n",
            "15:19:57 |     n_layers: 8\n",
            "15:19:57 |     n_positions: 512\n",
            "15:19:57 |     n_segments: 0\n",
            "15:19:57 |     nesterov: True\n",
            "15:19:57 |     no_cuda: False\n",
            "15:19:57 |     num_epochs: 0.1\n",
            "15:19:57 |     num_workers: 0\n",
            "15:19:57 |     nus: (0.7,)\n",
            "15:19:57 |     optimizer: mem_eff_adam\n",
            "15:19:57 |     output_scaling: 1.0\n",
            "15:19:57 |     override: \"{'task': 'french_blended_skill_talk,french_xpersona,french_empathetic_dialogues,french_wizard_of_wikipedia', 'multitask_weights': (1.0, 3.0, 3.0, 3.0), 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/colabs/blender-models/blender-models/finetuned-90m-4task-2sided/model', 'init_model': 'zoo:blender/blender_90M/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model.dict', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'validation_every_n_epochs': 0.1, 'num_epochs': 0.1, 'log_every_n_secs': 60.0, 'verbose': True, 'batchsize': 8, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'validation_patience': 10, 'validation_metric': 'ppl', 'validation_metric_mode': 'min', 'dynamic_batching': 'full', 'learningrate': 1e-05, 'optimizer': 'adam', 'attention_dropout': 0.0, 'model_parallel': False, 'warmup_updates': 100}\"\n",
            "15:19:57 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "15:19:57 |     person_tokens: False\n",
            "15:19:57 |     rank_candidates: False\n",
            "15:19:57 |     relu_dropout: 0.0\n",
            "15:19:57 |     save_after_valid: False\n",
            "15:19:57 |     save_every_n_secs: -1\n",
            "15:19:57 |     save_format: conversations\n",
            "15:19:57 |     share_word_embeddings: True\n",
            "15:19:57 |     short_final_eval: False\n",
            "15:19:57 |     skip_generation: True\n",
            "15:19:57 |     special_tok_lst: None\n",
            "15:19:57 |     split_lines: False\n",
            "15:19:57 |     starttime: Jun09_15-19\n",
            "15:19:57 |     task: french_blended_skill_talk,french_xpersona,french_empathetic_dialogues,french_wizard_of_wikipedia\n",
            "15:19:57 |     temperature: 1.0\n",
            "15:19:57 |     tensorboard_log: False\n",
            "15:19:57 |     tensorboard_logdir: None\n",
            "15:19:57 |     text_truncate: 512\n",
            "15:19:57 |     topk: 10\n",
            "15:19:57 |     topp: 0.9\n",
            "15:19:57 |     truncate: -1\n",
            "15:19:57 |     update_freq: 1\n",
            "15:19:57 |     use_reply: label\n",
            "15:19:57 |     validation_cutoff: 1.0\n",
            "15:19:57 |     validation_every_n_epochs: 0.1\n",
            "15:19:57 |     validation_every_n_secs: -1\n",
            "15:19:57 |     validation_every_n_steps: -1\n",
            "15:19:57 |     validation_max_exs: -1\n",
            "15:19:57 |     validation_metric: ppl\n",
            "15:19:57 |     validation_metric_mode: min\n",
            "15:19:57 |     validation_patience: 10\n",
            "15:19:57 |     validation_share_agent: False\n",
            "15:19:57 |     variant: xlm\n",
            "15:19:57 |     verbose: True\n",
            "15:19:57 |     wandb_entity: None\n",
            "15:19:57 |     wandb_log: False\n",
            "15:19:57 |     wandb_name: None\n",
            "15:19:57 |     wandb_project: None\n",
            "15:19:57 |     warmup_rate: 0.0001\n",
            "15:19:57 |     warmup_updates: 100\n",
            "15:19:57 |     weight_decay: None\n",
            "15:19:57 |     world_logs: \n",
            "15:19:58 | creating task(s): french_blended_skill_talk,french_xpersona,french_empathetic_dialogues,french_wizard_of_wikipedia\n",
            "15:19:58 | Loading ParlAI text data: /content/dataset_french_bst/train.txt\n",
            "15:19:58 | Loading ParlAI text data: /content/dataset_french_xpersona/train.txt\n",
            "15:20:00 | Loading ParlAI text data: /content/dataset_french_ed/train.txt\n",
            "15:20:01 | Loading ParlAI text data: /content/dataset_french_wow/train.txt\n",
            "15:20:23 | training...\n",
            "15:20:23 | Overflow: setting loss scale to 65536.0\n",
            "15:20:24 | Overflow: setting loss scale to 32768.0\n",
            "15:20:25 | Overflow: setting loss scale to 16384.0\n",
            "15:20:38 | time:15s total_exs:2176 total_steps:50 epochs:0.00 time_left:334s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         54.84 .9400  2238  7507       0          0   146 2176             19988  9.565    .3901 28.28   \n",
            "   french_blended_skill_talk   62.59                         0          0        171                                   29.39   \n",
            "   french_empathetic_dialogues 39.96                         0          0        728                                   25.65   \n",
            "   french_wizard_of_wikipedia  69.16                         0          0        522                                   35.93   \n",
            "   french_xpersona             47.66                         0          0        755                                   22.15   \n",
            "                                loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         3.853 5.001e-06  1183  3970       0          0 47.59      .3512         0                   50   \n",
            "   french_blended_skill_talk   3.773                             0          0 43.53      .3643         0                        \n",
            "   french_empathetic_dialogues  3.69                             0          0 40.03      .3690         0                        \n",
            "   french_wizard_of_wikipedia  3.902                             0          0 49.53      .3428         0                        \n",
            "   french_xpersona             4.048                             0          0 57.26      .3286         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3421 11477 3.355  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:20:53 | time:30s total_exs:3680 total_steps:100 epochs:0.01 time_left:372s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         109.7     1  2751  9599       0          0   105 1504             16384  9.095    .3748 30.72   \n",
            "   french_blended_skill_talk   134.8                         0          0        104                                   34.66   \n",
            "   french_empathetic_dialogues 52.23                         0          0        716                                   28.52   \n",
            "   french_wizard_of_wikipedia  127.1                         0          0        327                                   36.72   \n",
            "   french_xpersona             124.8                         0          0        357                                      23   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         3.471 9.9e-06 884.6  3087 .0007645    .007645 32.43      .3849         0                  100   \n",
            "   french_blended_skill_talk   3.316                            0          0 27.55      .4061         0                        \n",
            "   french_empathetic_dialogues 3.383                            0          0 29.45      .3878         0                        \n",
            "   french_wizard_of_wikipedia   3.62                      .003058     .03058 37.33      .3738         0                        \n",
            "   french_xpersona             3.567                            0          0 35.41      .3721         0                        \n",
            "                                tpb   tps  ups  \n",
            "   all                         3635 12686 3.49  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "15:21:07 | time:44s total_exs:4912 total_steps:150 epochs:0.01 time_left:400s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         138.7     1  2998 10715       0          0 88.08 1232             16384  8.791    .3829 31.11   \n",
            "   french_blended_skill_talk   181.7                         0          0         80                                   34.06   \n",
            "   french_empathetic_dialogues 58.96                         0          0        472                                   28.15   \n",
            "   french_wizard_of_wikipedia  149.1                         0          0        299                                   39.05   \n",
            "   french_xpersona             165.2                         0          0        381                                   23.17   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         3.179 9.9e-06 730.2  2610 .001672    .001672 24.21      .4200         0                  150   \n",
            "   french_blended_skill_talk   3.037                           0          0 20.84      .4378         0                        \n",
            "   french_empathetic_dialogues 3.072                           0          0 21.58      .4289         0                        \n",
            "   french_wizard_of_wikipedia  3.321                     .006689    .006689  27.7      .4045         0                        \n",
            "   french_xpersona             3.286                           0          0 26.73      .4089         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3728 13325 3.575  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:21:21 | time:58s total_exs:6024 total_steps:200 epochs:0.01 time_left:421s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         155.4     1  3148 11175       0          0 78.95 1112             16384  8.545    .3816 30.89   \n",
            "   french_blended_skill_talk     226                         0          0        103                                   33.96   \n",
            "   french_empathetic_dialogues 52.37                         0          0        340                                   29.51   \n",
            "   french_wizard_of_wikipedia  153.6                         0          0        293                                   37.51   \n",
            "   french_xpersona             189.7                         0          0        376                                   22.59   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         3.056 9.9e-06 660.1  2343 .0007353    .008088 21.38      .4324         0                  200   \n",
            "   french_blended_skill_talk   2.956                            0          0 19.23      .4465         0                        \n",
            "   french_empathetic_dialogues 2.942                      .002941     .03235 18.94      .4405         0                        \n",
            "   french_wizard_of_wikipedia  3.192                            0          0 24.34      .4194         0                        \n",
            "   french_xpersona             3.135                            0          0    23      .4231         0                        \n",
            "                                tpb   tps  ups  \n",
            "   all                         3808 13519 3.55  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "15:21:35 | time:72s total_exs:7296 total_steps:250 epochs:0.01 time_left:420s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         132.2     1  2616  9254       0          0 90.01 1272             16384  7.954    .3907  31.3   \n",
            "   french_blended_skill_talk   206.1                         0          0         70                                   33.81   \n",
            "   french_empathetic_dialogues 50.23                         0          0        551                                   29.78   \n",
            "   french_wizard_of_wikipedia  127.9                         0          0        325                                   38.42   \n",
            "   french_xpersona             144.5                         0          0        326                                   23.19   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.995 9.9e-06 774.9  2742 .0009074     .03403 20.12      .4377         0                  250   \n",
            "   french_blended_skill_talk    2.91                            0          0 18.37      .4474         0                        \n",
            "   french_empathetic_dialogues 2.852                       .00363      .1361 17.32      .4485         0                        \n",
            "   french_wizard_of_wikipedia  3.127                            0          0 22.81      .4209         0                        \n",
            "   french_xpersona              3.09                            0          0 21.98      .4340         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3390 11996 3.539  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:21:50 | time:88s total_exs:8668 total_steps:300 epochs:0.02 time_left:416s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         130.8     1  2630  8506  .01268      .7500 88.75 1372             16384  7.935    .3959 29.83   \n",
            "   french_blended_skill_talk   204.9                    .04688      2.594         64                                   32.53   \n",
            "   french_empathetic_dialogues 51.03                         0          0        671                                   26.89   \n",
            "   french_wizard_of_wikipedia  139.4                   .003831      .4061        261                                   37.69   \n",
            "   french_xpersona             127.7                         0          0        376                                   22.22   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.927 9.9e-06 765.6  2476 .0007452     .01379 18.79      .4439         0                  300   \n",
            "   french_blended_skill_talk   2.857                            0          0 17.41      .4524         0                        \n",
            "   french_empathetic_dialogues 2.789                      .002981     .05514 16.26      .4578         0                        \n",
            "   french_wizard_of_wikipedia  3.087                            0          0 21.91      .4261         0                        \n",
            "   french_xpersona             2.975                            0          0 19.58      .4392         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3395 10982 3.235  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:22:06 | time:103s total_exs:9936 total_steps:350 epochs:0.02 time_left:412s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         138.7     1  2834  9453       0          0 84.57 1268             16384  7.516    .3815  30.2   \n",
            "   french_blended_skill_talk   198.6                         0          0         60                                    32.3   \n",
            "   french_empathetic_dialogues 57.24                         0          0        541                                   30.65   \n",
            "   french_wizard_of_wikipedia  157.8                         0          0        286                                   35.91   \n",
            "   french_xpersona             140.9                         0          0        381                                   21.93   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.885 9.9e-06 741.6  2473 .001386     .03142 18.09      .4551         0                  350   \n",
            "   french_blended_skill_talk    2.71                           0          0 15.02      .4856         0                        \n",
            "   french_empathetic_dialogues 2.783                     .005545      .1257 16.16      .4568         0                        \n",
            "   french_wizard_of_wikipedia  3.071                           0          0 21.57      .4331         0                        \n",
            "   french_xpersona             2.976                           0          0 19.61      .4448         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3576 11926 3.335  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:22:20 | time:117s total_exs:11268 total_steps:400 epochs:0.02 time_left:400s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                           134     1  2813  9828       0          0 93.08 1332             16384  7.688    .3861 29.76   \n",
            "   french_blended_skill_talk   196.9                         0          0         67                                   30.69   \n",
            "   french_empathetic_dialogues 56.98                         0          0        608                                   29.82   \n",
            "   french_wizard_of_wikipedia  134.6                         0          0        318                                   35.17   \n",
            "   french_xpersona             147.5                         0          0        339                                   23.38   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.845 9.9e-06   785  2743 .0004112      .0185 17.46      .4568         0                  400   \n",
            "   french_blended_skill_talk   2.672                            0          0 14.47      .4771         0                        \n",
            "   french_empathetic_dialogues 2.721                      .001645     .07401  15.2      .4695         0                        \n",
            "   french_wizard_of_wikipedia  3.104                            0          0 22.28      .4267         0                        \n",
            "   french_xpersona             2.884                            0          0 17.89      .4538         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3598 12571 3.494  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:22:34 | time:131s total_exs:12588 total_steps:450 epochs:0.03 time_left:389s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         148.2     1  2882 10066       0          0  92.2 1320             16384   7.84    .3705 29.08   \n",
            "   french_blended_skill_talk   222.3                         0          0         45                                    28.4   \n",
            "   french_empathetic_dialogues 54.48                         0          0        651                                   27.77   \n",
            "   french_wizard_of_wikipedia  157.1                         0          0        265                                   37.38   \n",
            "   french_xpersona             158.8                         0          0        359                                   22.77   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.802 9.9e-06 748.7  2615       0          0 16.69      .4673         0                  450   \n",
            "   french_blended_skill_talk   2.636                           0          0 13.96      .4937         0                        \n",
            "   french_empathetic_dialogues 2.678                           0          0 14.56      .4695         0                        \n",
            "   french_wizard_of_wikipedia  3.044                           0          0 20.98      .4390         0                        \n",
            "   french_xpersona             2.849                           0          0 17.27      .4670         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3631 12680 3.493  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:22:49 | time:146s total_exs:13884 total_steps:500 epochs:0.03 time_left:379s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         138.7     1  2775  9406       0          0 87.86 1296             16384   7.96    .3960 29.18   \n",
            "   french_blended_skill_talk   204.5                         0          0         57                                   30.35   \n",
            "   french_empathetic_dialogues 51.19                         0          0        590                                   27.54   \n",
            "   french_wizard_of_wikipedia  150.9                         0          0        268                                   37.69   \n",
            "   french_xpersona             148.2                         0          0        381                                   21.14   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.814 9.9e-06 721.1  2444 .0008475     .03347 16.81      .4628         0                  500   \n",
            "   french_blended_skill_talk   2.717                            0          0 15.13      .4832         0                        \n",
            "   french_empathetic_dialogues 2.664                       .00339      .1339 14.35      .4729         0                        \n",
            "   french_wizard_of_wikipedia  2.984                            0          0 19.76      .4380         0                        \n",
            "   french_xpersona              2.89                            0          0    18      .4570         0                        \n",
            "                                tpb   tps  ups  \n",
            "   all                         3496 11851 3.39  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "15:23:03 | time:160s total_exs:15020 total_steps:550 epochs:0.03 time_left:371s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         152.2     1  2913 10587 .008238      .2156 82.57 1136             16384  8.071    .3882 29.03   \n",
            "   french_blended_skill_talk   211.3                    .01408      .3380         71                                   28.79   \n",
            "   french_empathetic_dialogues 64.51                         0          0        451                                   28.06   \n",
            "   french_wizard_of_wikipedia  172.4                    .01887      .5245        265                                   36.34   \n",
            "   french_xpersona             160.6                         0          0        349                                   22.92   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.758 9.9e-06 646.6  2350       0          0 15.97      .4727         0                  550   \n",
            "   french_blended_skill_talk   2.613                           0          0 13.64      .4961         0                        \n",
            "   french_empathetic_dialogues 2.597                           0          0 13.43      .4848         0                        \n",
            "   french_wizard_of_wikipedia  2.988                           0          0 19.85      .4468         0                        \n",
            "   french_xpersona             2.832                           0          0 16.97      .4630         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3560 12937 3.634  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:23:17 | time:174s total_exs:16260 total_steps:600 epochs:0.03 time_left:360s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         138.9     1  2763  9537 .008399      .2454 85.59 1240             16384  7.689    .3829 30.37   \n",
            "   french_blended_skill_talk     214                     .0300      .6400        100                                      32   \n",
            "   french_empathetic_dialogues 55.76                         0          0        526                                   29.34   \n",
            "   french_wizard_of_wikipedia  145.9                   .003597      .3417        278                                   39.21   \n",
            "   french_xpersona               140                         0          0        336                                   20.92   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.741 9.9e-06 730.7  2522 .0009506     .01378 15.62      .4668         0                  600   \n",
            "   french_blended_skill_talk   2.657                            0          0 14.25      .4778         0                        \n",
            "   french_empathetic_dialogues  2.58                      .003802     .05513  13.2      .4858         0                        \n",
            "   french_wizard_of_wikipedia  2.898                            0          0 18.14      .4476         0                        \n",
            "   french_xpersona             2.828                            0          0 16.91      .4559         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3494 12059 3.452  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:23:29 | Overflow: setting loss scale to 8192.0\n",
            "15:23:32 | time:189s total_exs:17540 total_steps:650 epochs:0.04 time_left:348s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         129.5 .9800  2797  9690       0          0  88.7 1280             14582  7.642    .3862  30.5   \n",
            "   french_blended_skill_talk   174.8                         0          0         65                                   32.29   \n",
            "   french_empathetic_dialogues 52.81                         0          0        514                                   28.29   \n",
            "   french_wizard_of_wikipedia  148.8                         0          0        279                                   38.99   \n",
            "   french_xpersona             141.7                         0          0        422                                   22.42   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.753 9.9e-06 739.2  2561 .0009728      .0107  15.8      .4684         0                  650   \n",
            "   french_blended_skill_talk   2.679                            0          0 14.57      .4826         0                        \n",
            "   french_empathetic_dialogues 2.612                      .003891      .0428 13.63      .4803         0                        \n",
            "   french_wizard_of_wikipedia  2.911                            0          0 18.37      .4451         0                        \n",
            "   french_xpersona             2.811                            0          0 16.63      .4657         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3536 12251 3.465  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:23:46 | time:203s total_exs:18812 total_steps:700 epochs:0.04 time_left:335s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         144.6     1  2758  9711       0          0 89.59 1272              8192  7.937    .3932 30.13   \n",
            "   french_blended_skill_talk   239.6                         0          0         64                                   31.52   \n",
            "   french_empathetic_dialogues  56.2                         0          0        565                                   28.57   \n",
            "   french_wizard_of_wikipedia  144.6                         0          0        321                                   37.88   \n",
            "   french_xpersona             137.8                         0          0        322                                   22.55   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.719 9.9e-06 751.2  2646 .000885    .008407 15.32      .4705         0                  700   \n",
            "   french_blended_skill_talk   2.607                           0          0 13.56      .4849         0                        \n",
            "   french_empathetic_dialogues 2.567                      .00354     .03363 13.02      .4809         0                        \n",
            "   french_wizard_of_wikipedia  2.928                           0          0  18.7      .4514         0                        \n",
            "   french_xpersona             2.772                           0          0    16      .4647         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3509 12357 3.522  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:24:00 | time:217s total_exs:19976 total_steps:750 epochs:0.04 time_left:325s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         150.7     1  2951 10749       0          0  84.8 1164              8192  8.018    .3761 29.87   \n",
            "   french_blended_skill_talk   217.3                         0          0         70                                   30.41   \n",
            "   french_empathetic_dialogues 58.21                         0          0        448                                   28.37   \n",
            "   french_wizard_of_wikipedia  155.2                         0          0        290                                   38.26   \n",
            "   french_xpersona               172                         0          0        356                                   22.45   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.686 9.9e-06 678.3  2471 .000558    .004464 14.89      .4764         0                  750   \n",
            "   french_blended_skill_talk    2.53                           0          0 12.55      .4955         0                        \n",
            "   french_empathetic_dialogues 2.541                     .002232     .01786 12.69      .4891         0                        \n",
            "   french_wizard_of_wikipedia  2.945                           0          0 19.01      .4470         0                        \n",
            "   french_xpersona             2.729                           0          0 15.31      .4740         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3629 13220 3.643  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:24:15 | time:232s total_exs:21156 total_steps:800 epochs:0.04 time_left:314s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         155.6     1  2932 10047  .01985      .6404 80.87 1180              8192  7.626    .3934 30.43   \n",
            "   french_blended_skill_talk   230.8                    .07576        2.5         66                                   29.18   \n",
            "   french_empathetic_dialogues 64.74                         0          0        507                                   30.23   \n",
            "   french_wizard_of_wikipedia  154.5                         0          0        331                                   40.19   \n",
            "   french_xpersona             172.4                   .003623     .06159        276                                   22.11   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.723 9.9e-06 732.8  2511 .0009862     .01085 15.33      .4748         0                  800   \n",
            "   french_blended_skill_talk   2.667                            0          0 14.39      .4787         0                        \n",
            "   french_empathetic_dialogues 2.603                      .003945     .04339  13.5      .4820         0                        \n",
            "   french_wizard_of_wikipedia  2.915                            0          0 18.44      .4547         0                        \n",
            "   french_xpersona             2.708                            0          0    15      .4836         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3665 12558 3.427  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:24:29 | time:246s total_exs:22524 total_steps:850 epochs:0.05 time_left:299s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         142.6     1  2840  9562  .01203      .4398 92.13 1368              8192  8.154    .3883 29.59   \n",
            "   french_blended_skill_talk   238.6                    .02632      .5921         76                                   32.61   \n",
            "   french_empathetic_dialogues 46.72                         0          0        615                                   27.53   \n",
            "   french_wizard_of_wikipedia  151.3                    .02182      1.167        275                                   35.95   \n",
            "   french_xpersona               134                         0          0        402                                   22.28   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.716 9.9e-06 765.1  2576       0          0 15.23      .4742         0                  850   \n",
            "   french_blended_skill_talk   2.674                           0          0  14.5      .4782         0                        \n",
            "   french_empathetic_dialogues 2.544                           0          0 12.73      .4879         0                        \n",
            "   french_wizard_of_wikipedia   2.89                           0          0    18      .4603         0                        \n",
            "   french_xpersona             2.754                           0          0  15.7      .4705         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3605 12138 3.367  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:24:44 | time:261s total_exs:23892 total_steps:900 epochs:0.05 time_left:284s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         136.7     1  2817  9705  .01874      .4304 94.27 1368              8192   7.64    .3862 28.76   \n",
            "   french_blended_skill_talk   207.4                    .06849      1.397         73                                   32.48   \n",
            "   french_empathetic_dialogues 55.32                         0          0        672                                   25.99   \n",
            "   french_wizard_of_wikipedia  136.8                   .003521      .1092        284                                   35.35   \n",
            "   french_xpersona             147.1                    .00295      .2153        339                                   21.22   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.717 9.9e-06 741.4  2554 .000372     .00186 15.33      .4731         0                  900   \n",
            "   french_blended_skill_talk   2.644                           0          0 14.06      .4854         0                        \n",
            "   french_empathetic_dialogues 2.487                     .001488     .00744 12.03      .4941         0                        \n",
            "   french_wizard_of_wikipedia  2.872                           0          0 17.67      .4520         0                        \n",
            "   french_xpersona             2.865                           0          0 17.54      .4609         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3558 12260 3.446  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:24:59 | time:276s total_exs:25140 total_steps:950 epochs:0.05 time_left:271s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         144.1     1  2970 10255  .02547      2.883 86.16 1248              8192  7.524    .3705 29.88   \n",
            "   french_blended_skill_talk   214.9                    .08654      10.18        104                                   30.74   \n",
            "   french_empathetic_dialogues 56.42                         0          0        487                                   27.35   \n",
            "   french_wizard_of_wikipedia  160.2                    .01235      .9722        324                                   38.83   \n",
            "   french_xpersona               145                   .003003      .3754        333                                   22.58   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.698 9.9e-06 731.8  2526 .0005133     .01386 14.98      .4749         0                  950   \n",
            "   french_blended_skill_talk    2.67                            0          0 14.43      .4817         0                        \n",
            "   french_empathetic_dialogues 2.498                      .002053     .05544 12.15      .4929         0                        \n",
            "   french_wizard_of_wikipedia  2.848                            0          0 17.25      .4558         0                        \n",
            "   french_xpersona             2.779                            0          0  16.1      .4693         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3702 12781 3.453  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:25:13 | time:290s total_exs:26440 total_steps:1000 epochs:0.05 time_left:257s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         124.9     1  2821  9895       0          0  91.2 1300              8192  7.271    .3862 30.68   \n",
            "   french_blended_skill_talk   153.9                         0          0         76                                   32.24   \n",
            "   french_empathetic_dialogues 60.83                         0          0        556                                   31.29   \n",
            "   french_wizard_of_wikipedia  132.5                         0          0        311                                   37.24   \n",
            "   french_xpersona             152.1                         0          0        357                                   21.93   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.648 9.9e-06 784.4  2752 .0004496     .01754 14.21      .4801         0                 1000   \n",
            "   french_blended_skill_talk   2.567                            0          0 13.02      .4894         0                        \n",
            "   french_empathetic_dialogues 2.567                      .001799     .07014 13.03      .4842         0                        \n",
            "   french_wizard_of_wikipedia  2.824                            0          0 16.85      .4597         0                        \n",
            "   french_xpersona             2.634                            0          0 13.93      .4870         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3605 12647 3.508  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:25:28 | time:305s total_exs:27672 total_steps:1050 epochs:0.06 time_left:244s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         141.8     1  2808  9562       0          0  83.9 1232              8192  7.557    .3862 31.37   \n",
            "   french_blended_skill_talk   206.1                         0          0         92                                   32.33   \n",
            "   french_empathetic_dialogues 49.26                         0          0        521                                   31.59   \n",
            "   french_wizard_of_wikipedia  166.4                         0          0        273                                   37.86   \n",
            "   french_xpersona             145.5                         0          0        346                                   23.71   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.656 9.9e-06 759.4  2586       0          0 14.37      .4797  .0007225                 1050   \n",
            "   french_blended_skill_talk   2.499                           0          0 12.17      .5024         0                        \n",
            "   french_empathetic_dialogues 2.547                           0          0 12.76      .4847         0                        \n",
            "   french_wizard_of_wikipedia  2.808                           0          0 16.58      .4629         0                        \n",
            "   french_xpersona             2.771                           0          0 15.97      .4687    .00289                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3567 12148 3.405  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:25:42 | time:319s total_exs:28944 total_steps:1100 epochs:0.06 time_left:230s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         136.2     1  2838 10118       0          0  90.7 1272              8192  7.571    .3932 29.79   \n",
            "   french_blended_skill_talk   188.8                         0          0         79                                    30.2   \n",
            "   french_empathetic_dialogues 56.33                         0          0        556                                   29.38   \n",
            "   french_wizard_of_wikipedia    147                         0          0        288                                   37.12   \n",
            "   french_xpersona             152.8                         0          0        349                                   22.47   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.625 9.9e-06 744.9  2656 .0004496    .003597 13.94      .4821         0                 1100   \n",
            "   french_blended_skill_talk    2.52                            0          0 12.43      .4983         0                        \n",
            "   french_empathetic_dialogues 2.489                      .001799     .01439 12.05      .4885         0                        \n",
            "   french_wizard_of_wikipedia  2.835                            0          0 17.03      .4601         0                        \n",
            "   french_xpersona             2.656                            0          0 14.24      .4814         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3583 12774 3.566  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:25:56 | time:333s total_exs:30280 total_steps:1150 epochs:0.06 time_left:215s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         143.6     1  2816  9858  .01012      .4418 93.54 1336              8192  7.638    .3908 29.75   \n",
            "   french_blended_skill_talk   224.7                     .0303      1.409         66                                   32.21   \n",
            "   french_empathetic_dialogues 52.68                         0          0        650                                    27.9   \n",
            "   french_wizard_of_wikipedia  150.9                   .007273      .2655        275                                   37.43   \n",
            "   french_xpersona             146.1                   .002899     .09275        345                                   21.48   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.627 9.9e-06 758.7  2656 .0003846     .01154 13.98      .4811         0                 1150   \n",
            "   french_blended_skill_talk   2.518                            0          0  12.4      .4929         0                        \n",
            "   french_empathetic_dialogues 2.462                      .001538     .04615 11.72      .4934         0                        \n",
            "   french_wizard_of_wikipedia  2.812                            0          0 16.64      .4617         0                        \n",
            "   french_xpersona             2.717                            0          0 15.14      .4763         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3574 12514 3.501  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:26:11 | time:348s total_exs:31524 total_steps:1200 epochs:0.06 time_left:202s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         132.6     1  2727  9375       0          0 85.52 1244              8192  7.753    .3977 30.89   \n",
            "   french_blended_skill_talk   173.6                         0          0         78                                   31.37   \n",
            "   french_empathetic_dialogues 56.01                         0          0        555                                   30.48   \n",
            "   french_wizard_of_wikipedia  151.6                         0          0        268                                   39.53   \n",
            "   french_xpersona               149                         0          0        343                                   22.17   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.623 9.9e-06 750.9  2581 .0004505    .006306 13.95      .4834         0                 1200   \n",
            "   french_blended_skill_talk   2.492                            0          0 12.09      .5002         0                        \n",
            "   french_empathetic_dialogues 2.512                      .001802     .02523 12.33      .4878         0                        \n",
            "   french_wizard_of_wikipedia  2.875                            0          0 17.73      .4505         0                        \n",
            "   french_xpersona             2.614                            0          0 13.66      .4952         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3478 11956 3.438  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:26:25 | time:362s total_exs:32884 total_steps:1250 epochs:0.07 time_left:187s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         135.2     1  2768  9426       0          0 92.62 1360              8192  7.351    .3961 29.94   \n",
            "   french_blended_skill_talk   201.4                         0          0         63                                   32.68   \n",
            "   french_empathetic_dialogues 54.89                         0          0        673                                   28.42   \n",
            "   french_wizard_of_wikipedia  142.6                         0          0        258                                   37.31   \n",
            "   french_xpersona               142                         0          0        366                                   21.33   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.638 9.9e-06 770.6  2624 .0007429     .03343 14.11      .4828         0                 1250   \n",
            "   french_blended_skill_talk   2.592                            0          0 13.36      .4832         0                        \n",
            "   french_empathetic_dialogues 2.492                      .002972      .1337 12.08      .4972         0                        \n",
            "   french_wizard_of_wikipedia   2.85                            0          0 17.29      .4590         0                        \n",
            "   french_xpersona             2.619                            0          0 13.72      .4919         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3539 12050 3.405  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:26:39 | time:376s total_exs:34100 total_steps:1300 epochs:0.07 time_left:174s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         143.2     1  2888 10352       0          0 87.17 1216              8192  7.572    .3802 29.46   \n",
            "   french_blended_skill_talk   199.4                         0          0         79                                   30.51   \n",
            "   french_empathetic_dialogues 59.32                         0          0        504                                   28.32   \n",
            "   french_wizard_of_wikipedia  162.9                         0          0        263                                   36.52   \n",
            "   french_xpersona             151.1                         0          0        370                                    22.5   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                           2.6 9.9e-06 691.4  2478 .0009921     .02083 13.63      .4847         0                 1300   \n",
            "   french_blended_skill_talk   2.417                            0          0 11.21      .4979         0                        \n",
            "   french_empathetic_dialogues 2.479                      .003968     .08333 11.93      .4981         0                        \n",
            "   french_wizard_of_wikipedia  2.812                            0          0 16.65      .4628         0                        \n",
            "   french_xpersona             2.691                            0          0 14.75      .4799         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3580 12831 3.585  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:26:54 | time:391s total_exs:35280 total_steps:1350 epochs:0.07 time_left:161s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         143.5     1  2853 10055       0          0 83.17 1180              8192  7.687    .3783 29.67   \n",
            "   french_blended_skill_talk   204.7                         0          0         93                                   30.08   \n",
            "   french_empathetic_dialogues 63.85                         0          0        477                                   30.21   \n",
            "   french_wizard_of_wikipedia  151.8                         0          0        302                                   35.84   \n",
            "   french_xpersona             153.6                         0          0        308                                   22.55   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.557 9.9e-06 699.4  2465       0          0 12.99      .4933  .0008117                 1350   \n",
            "   french_blended_skill_talk   2.434                           0          0 11.41      .5156         0                        \n",
            "   french_empathetic_dialogues 2.456                           0          0 11.66      .4960         0                        \n",
            "   french_wizard_of_wikipedia  2.729                           0          0 15.32      .4687         0                        \n",
            "   french_xpersona             2.609                           0          0 13.59      .4929   .003247                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3552 12520 3.525  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:27:08 | time:405s total_exs:36552 total_steps:1400 epochs:0.07 time_left:148s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         146.5     1  2952 10114  .01402      .3366 87.16 1272              8192  7.481    .3840 29.79   \n",
            "   french_blended_skill_talk   217.1                    .03896      .6364         77                                   29.94   \n",
            "   french_empathetic_dialogues 54.34                         0          0        556                                   29.93   \n",
            "   french_wizard_of_wikipedia  152.6                    .01434      .5376        279                                   37.66   \n",
            "   french_xpersona             162.1                   .002778      .1722        360                                   21.61   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                          2.59 9.9e-06 744.7  2552       0          0 13.45      .4873         0                 1400   \n",
            "   french_blended_skill_talk   2.494                           0          0 12.11      .4976         0                        \n",
            "   french_empathetic_dialogues 2.452                           0          0 11.61      .4987         0                        \n",
            "   french_wizard_of_wikipedia  2.783                           0          0 16.17      .4673         0                        \n",
            "   french_xpersona             2.633                           0          0 13.91      .4855         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3697 12666 3.426  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:27:22 | time:419s total_exs:37708 total_steps:1450 epochs:0.08 time_left:135s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         144.5     1  3016 10680       0          0 81.86 1156              8192  7.822    .3921 30.44   \n",
            "   french_blended_skill_talk   189.3                         0          0        100                                   31.52   \n",
            "   french_empathetic_dialogues 62.16                         0          0        399                                   28.42   \n",
            "   french_wizard_of_wikipedia  166.2                         0          0        292                                   39.05   \n",
            "   french_xpersona             160.4                         0          0        365                                   22.78   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.626 9.9e-06 683.7  2421 .0006266     .01253 13.91      .4849         0                 1450   \n",
            "   french_blended_skill_talk   2.602                            0          0 13.49      .4892         0                        \n",
            "   french_empathetic_dialogues 2.457                      .002506     .05013 11.67      .4969         0                        \n",
            "   french_wizard_of_wikipedia  2.763                            0          0 15.84      .4668         0                        \n",
            "   french_xpersona             2.683                            0          0 14.63      .4866         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3700 13101 3.541  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:27:37 | time:434s total_exs:39112 total_steps:1500 epochs:0.08 time_left:119s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         123.6     1  2750  9374       0          0 95.71 1404              8192   7.65    .3803 28.63   \n",
            "   french_blended_skill_talk   177.2                         0          0         76                                   28.79   \n",
            "   french_empathetic_dialogues 49.26                         0          0        642                                    27.9   \n",
            "   french_wizard_of_wikipedia  127.5                         0          0        304                                   36.61   \n",
            "   french_xpersona             140.4                         0          0        382                                   21.23   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.558 9.9e-06 785.7  2678 .001168     .02298 13.05      .4946         0                 1500   \n",
            "   french_blended_skill_talk   2.413                           0          0 11.16      .5114         0                        \n",
            "   french_empathetic_dialogues 2.463                     .004673      .0919 11.74      .4973         0                        \n",
            "   french_wizard_of_wikipedia  2.785                           0          0  16.2      .4677         0                        \n",
            "   french_xpersona             2.571                           0          0 13.08      .5020         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3536 12052 3.409  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:27:51 | time:448s total_exs:40412 total_steps:1550 epochs:0.08 time_left:105s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         131.7     1  2724  9585       0          0 91.47 1300              8192   7.49    .3803 29.81   \n",
            "   french_blended_skill_talk   192.1                         0          0         80                                   29.46   \n",
            "   french_empathetic_dialogues  50.7                         0          0        564                                   28.23   \n",
            "   french_wizard_of_wikipedia  148.6                         0          0        260                                   39.31   \n",
            "   french_xpersona             135.4                         0          0        396                                   22.25   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.552 9.9e-06 744.7  2620  .00133     .03059 12.94      .4933         0                 1550   \n",
            "   french_blended_skill_talk   2.456                           0          0 11.66      .5121         0                        \n",
            "   french_empathetic_dialogues 2.405                     .005319      .1223 11.08      .5054         0                        \n",
            "   french_wizard_of_wikipedia  2.738                           0          0 15.45      .4685         0                        \n",
            "   french_xpersona             2.607                           0          0 13.56      .4873         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3469 12205 3.519  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:28:06 | time:463s total_exs:41676 total_steps:1600 epochs:0.08 time_left:91s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         143.8     1  2976 10361       0          0 87.99 1264              8192  8.084    .3817 28.86   \n",
            "   french_blended_skill_talk   214.3                         0          0         83                                   27.64   \n",
            "   french_empathetic_dialogues 56.28                         0          0        516                                   27.97   \n",
            "   french_wizard_of_wikipedia  147.6                         0          0        251                                   37.12   \n",
            "   french_xpersona             156.9                         0          0        414                                   22.71   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.575 9.9e-06 708.9  2467 .000996    .001992 13.28      .4921         0                 1600   \n",
            "   french_blended_skill_talk    2.46                           0          0  11.7      .5139         0                        \n",
            "   french_empathetic_dialogues 2.409                           0          0 11.12      .5032         0                        \n",
            "   french_wizard_of_wikipedia  2.781                     .003984    .007968 16.13      .4705         0                        \n",
            "   french_xpersona             2.651                           0          0 14.17      .4806         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3685 12828 3.481  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:28:20 | time:477s total_exs:42916 total_steps:1650 epochs:0.09 time_left:77s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         140.8     1  2746  9540       0          0 86.14 1240              8192  7.534    .3933 30.84   \n",
            "   french_blended_skill_talk     223                         0          0         77                                   32.08   \n",
            "   french_empathetic_dialogues 59.64                         0          0        529                                   29.83   \n",
            "   french_wizard_of_wikipedia  146.2                         0          0        284                                   38.95   \n",
            "   french_xpersona             134.5                         0          0        350                                   22.52   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.541 9.9e-06 743.2  2582 .0004726     .01654 12.81      .4922         0                 1650   \n",
            "   french_blended_skill_talk   2.426                            0          0 11.31      .5061         0                        \n",
            "   french_empathetic_dialogues 2.408                       .00189     .06616 11.11      .5023         0                        \n",
            "   french_wizard_of_wikipedia  2.723                            0          0 15.23      .4717         0                        \n",
            "   french_xpersona             2.608                            0          0 13.57      .4888         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3490 12122 3.474  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:28:35 | time:492s total_exs:44308 total_steps:1700 epochs:0.09 time_left:62s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         128.2     1  2901  9908       0          0 95.09 1392              8192  7.312    .3717 29.26   \n",
            "   french_blended_skill_talk   167.4                         0          0         81                                   31.11   \n",
            "   french_empathetic_dialogues 57.73                         0          0        667                                   26.21   \n",
            "   french_wizard_of_wikipedia  140.7                         0          0        268                                   36.88   \n",
            "   french_xpersona               147                         0          0        376                                   22.82   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                          2.57 9.9e-06 768.4  2624 .001308      .0243 13.18      .4867         0                 1700   \n",
            "   french_blended_skill_talk    2.48                           0          0 11.94      .4893         0                        \n",
            "   french_empathetic_dialogues  2.42                     .001499     .04498 11.25      .5040         0                        \n",
            "   french_wizard_of_wikipedia  2.776                     .003731     .05224 16.06      .4614         0                        \n",
            "   french_xpersona             2.602                           0          0 13.49      .4921         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3669 12533 3.416  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:28:50 | time:507s total_exs:45604 total_steps:1750 epochs:0.09 time_left:47s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         143.9     1  2834  9550  .01038      .4900 87.34 1296              8192  7.806    .3841 31.08   \n",
            "   french_blended_skill_talk   224.9                    .02817      .8732         71                                   33.73   \n",
            "   french_empathetic_dialogues 47.15                         0          0        568                                   28.45   \n",
            "   french_wizard_of_wikipedia  157.9                    .01333      1.087        300                                   39.64   \n",
            "   french_xpersona             145.6                         0          0        357                                   22.49   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.551 9.9e-06 769.4  2593 .0008333      .0050 12.96      .4967         0                 1750   \n",
            "   french_blended_skill_talk   2.464                            0          0 11.75      .5161         0                        \n",
            "   french_empathetic_dialogues 2.403                            0          0 11.06      .5067         0                        \n",
            "   french_wizard_of_wikipedia   2.79                      .003333      .0200 16.29      .4656         0                        \n",
            "   french_xpersona             2.545                            0          0 12.74      .4985         0                        \n",
            "                                tpb   tps  ups  \n",
            "   all                         3604 12143 3.37  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "15:29:04 | time:521s total_exs:46836 total_steps:1800 epochs:0.09 time_left:34s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         138.9     1  2930 10230       0          0 86.03 1232              8192  7.474    .3818 30.12   \n",
            "   french_blended_skill_talk   181.2                         0          0         90                                   30.68   \n",
            "   french_empathetic_dialogues 58.45                         0          0        504                                   27.84   \n",
            "   french_wizard_of_wikipedia  157.7                         0          0        282                                   40.09   \n",
            "   french_xpersona               158                         0          0        356                                   21.88   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                          2.56 9.9e-06 716.6  2502 .0009921     .02778 13.03      .4952         0                 1800   \n",
            "   french_blended_skill_talk   2.524                            0          0 12.48      .5071         0                        \n",
            "   french_empathetic_dialogues 2.419                      .003968      .1111 11.24      .5005         0                        \n",
            "   french_wizard_of_wikipedia  2.741                            0          0 15.51      .4742         0                        \n",
            "   french_xpersona             2.556                            0          0 12.88      .4992         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3647 12732 3.492  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:29:19 | time:536s total_exs:48096 total_steps:1850 epochs:0.10 time_left:20s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         148.4     1  2957 10215       0          0 87.06 1260              8192  7.601    .3803 31.64   \n",
            "   french_blended_skill_talk     225                         0          0         78                                   35.54   \n",
            "   french_empathetic_dialogues 61.33                         0          0        555                                    28.3   \n",
            "   french_wizard_of_wikipedia  154.6                         0          0        286                                   39.53   \n",
            "   french_xpersona             152.6                         0          0        341                                    23.2   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.539 9.9e-06 752.8  2601 .001775     .02901 12.79      .4931         0                 1850   \n",
            "   french_blended_skill_talk   2.442                           0          0  11.5      .5087         0                        \n",
            "   french_empathetic_dialogues 2.399                     .003604     .08108 11.02      .5007         0                        \n",
            "   french_wizard_of_wikipedia  2.752                     .003497     .03497 15.67      .4709         0                        \n",
            "   french_xpersona             2.564                           0          0 12.99      .4920         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3709 12816 3.455  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:29:33 | time:550s total_exs:49368 total_steps:1900 epochs:0.10 time_left:5s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         136.5     1  2758  9845  .01516      .8322  90.8 1272              8192  7.627    .3767 30.08   \n",
            "   french_blended_skill_talk   191.1                    .04762      2.635         63                                   31.79   \n",
            "   french_empathetic_dialogues 49.09                         0          0        561                                   28.17   \n",
            "   french_wizard_of_wikipedia    162                    .01303      .6938        307                                   37.84   \n",
            "   french_xpersona             143.6                         0          0        341                                    22.5   \n",
            "                                loss      lr  ltpb  ltps   ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                           2.5 9.9e-06 741.3  2646 .0008913     .01337 12.37      .5027         0                 1900   \n",
            "   french_blended_skill_talk    2.25                            0          0 9.488      .5422         0                        \n",
            "   french_empathetic_dialogues 2.451                      .003565     .05348  11.6      .5008         0                        \n",
            "   french_wizard_of_wikipedia  2.728                            0          0 15.31      .4685         0                        \n",
            "   french_xpersona             2.572                            0          0 13.09      .4993         0                        \n",
            "                                tpb   tps  ups  \n",
            "   all                         3500 12491 3.57  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "15:29:40 | time:557s total_exs:49864 total_steps:1924 epochs:0.10 time_left:0s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         166.6     1  3074 10845   .0219      .4674  72.9  496              8192  7.402    .3863 33.55   \n",
            "   french_blended_skill_talk   226.3                    .06061      .8182         33                                   35.21   \n",
            "   french_empathetic_dialogues 82.44                         0          0        170                                   29.27   \n",
            "   french_wizard_of_wikipedia  191.6                     .0200      .9533        150                                   45.57   \n",
            "   french_xpersona             166.1                   .006993      .0979        143                                   24.15   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         2.527 9.9e-06 684.5  2415       0          0 12.68      .4987         0                 1924   \n",
            "   french_blended_skill_talk   2.382                           0          0 10.83      .5224         0                        \n",
            "   french_empathetic_dialogues 2.384                           0          0 10.85      .5052         0                        \n",
            "   french_wizard_of_wikipedia  2.772                           0          0 15.98      .4668         0                        \n",
            "   french_xpersona              2.57                           0          0 13.07      .5003         0                        \n",
            "                                tpb   tps   ups  \n",
            "   all                         3759 13260 3.528  \n",
            "   french_blended_skill_talk                     \n",
            "   french_empathetic_dialogues                   \n",
            "   french_wizard_of_wikipedia                    \n",
            "   french_xpersona\n",
            "\n",
            "15:29:40 | num_epochs completed:0.1 time elapsed:556.583877325058s\n",
            "15:29:40 | Saving dictionary to /content/drive/MyDrive/colabs/blender-models/blender-models/finetuned-90m-4task-2sided/model.dict\n",
            "15:29:44 | \u001b[33mOverriding opt[\"multitask_weights\"] to (1.0, 3.0, 3.0, 3.0) (previously: [1.0, 3.0, 3.0, 3.0])\u001b[0m\n",
            "15:29:44 | \u001b[33mOverriding opt[\"init_model\"] to zoo:blender/blender_90M/model (previously: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model)\u001b[0m\n",
            "15:29:44 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "15:29:44 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: full,is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,beam_block_full_context: True,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,dict_loaded: True,verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "15:29:44 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues --numthreads 1 --batchsize 16 --num-epochs -1 --save-every-n-secs 60.0 --save-after-valid True --validation-every-n-epochs 0.25 --validation-max-exs 20000 --validation-patience 15 --log-every-n-secs 2 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --num-topics 5 --train-experiencer-only False --dropout 0.1 --learn-positional-embeddings True --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --skip-generation False --inference beam --fp16-impl apex --force-fp16-tokens False --optimizer adamax --learningrate 7.5e-06 --max-lr-steps -1 --warmup-updates -1 --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
            "15:29:44 | Using CUDA\n",
            "15:29:44 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/blender-models/finetuned-90m-4task-2sided/model.dict\n",
            "15:29:44 | num words = 54944\n",
            "15:29:46 | Total parameters: 87,508,992 (86,984,704 trainable)\n",
            "15:29:46 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/blender-models/finetuned-90m-4task-2sided/model\n",
            "15:29:50 | creating task(s): french_blended_skill_talk\n",
            "15:29:50 | Loading ParlAI text data: /content/dataset_french_bst/valid.txt\n",
            "15:29:52 | creating task(s): french_xpersona\n",
            "15:29:52 | Loading ParlAI text data: /content/dataset_french_xpersona/valid.txt\n",
            "15:29:54 | creating task(s): french_empathetic_dialogues\n",
            "15:29:54 | Loading ParlAI text data: /content/dataset_french_ed/valid.txt\n",
            "15:29:57 | creating task(s): french_wizard_of_wikipedia\n",
            "15:29:57 | Loading ParlAI text data: /content/dataset_french_wow/valid.txt\n",
            "15:30:00 | running eval: valid\n",
            "15:30:00 | \u001b[33m--skip-generation true produces limited metrics\u001b[0m\n",
            "15:33:08 | eval completed in 188.71s\n",
            "15:33:08 | \u001b[1mvalid:\n",
            "                                clen  ctpb  ctps   ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   all                         144.9  3446 38927  .004689      .2258 195.7 43257    .1541 30.78  2.55 9.9e-06 536.8  6064   \n",
            "   french_blended_skill_talk   199.5               .01395      .6120       12476          30.98 2.486                       \n",
            "   french_empathetic_dialogues 60.69                    0          0        9308             31 2.419                       \n",
            "   french_wizard_of_wikipedia  150.9              .004257      .2811       17853          38.01 2.741                       \n",
            "   french_xpersona             168.6             .0005525     .01022        3620          23.13 2.556                       \n",
            "                                  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  \n",
            "   all                           .000955     .02212 12.91      .4927 9.592e-05                 1924 3983 44991  \n",
            "   french_blended_skill_talk   8.015e-05   .0005611 12.01      .5000         0                                  \n",
            "   french_empathetic_dialogues   .001612     .04738 11.24      .5009  .0001074                                  \n",
            "   french_wizard_of_wikipedia    .002128     .04055  15.5      .4705         0                                  \n",
            "   french_xpersona                     0          0 12.88      .4991  .0002762\n",
            "\u001b[0m\n",
            "15:33:08 | creating task(s): french_blended_skill_talk\n",
            "15:33:08 | Loading ParlAI text data: /content/dataset_french_bst/test.txt\n",
            "15:33:11 | creating task(s): french_xpersona\n",
            "15:33:11 | Loading ParlAI text data: /content/dataset_french_xpersona/test.txt\n",
            "15:33:13 | creating task(s): french_empathetic_dialogues\n",
            "15:33:13 | Loading ParlAI text data: /content/dataset_french_ed/test.txt\n",
            "15:33:15 | creating task(s): french_wizard_of_wikipedia\n",
            "15:33:15 | Loading ParlAI text data: /content/dataset_french_wow/test.txt\n",
            "15:33:17 | running eval: test\n",
            "15:36:28 | eval completed in 190.73s\n",
            "15:36:28 | \u001b[1mtest:\n",
            "                                clen  ctpb  ctps   ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   all                         147.5  3439 38405  .005961      .3297 190.1 42087    .1540  31.5 2.564 9.9e-06 541.6  6049   \n",
            "   french_blended_skill_talk   203.1               .01939      1.094       12121          31.82 2.479                       \n",
            "   french_empathetic_dialogues 64.16             .0001187    .001187        8426          32.52 2.463                       \n",
            "   french_wizard_of_wikipedia  151.7              .003242      .2063       17889          38.25  2.74                       \n",
            "   french_xpersona               171              .001096      .0178        3651          23.41 2.575                       \n",
            "                                 ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  \n",
            "   all                         .0008269     .02161 13.07      .4904         0                 1924 3980 44453  \n",
            "   french_blended_skill_talk    .000165    .006023 11.93      .5025         0                                  \n",
            "   french_empathetic_dialogues  .002136     .05578 11.73      .4955         0                                  \n",
            "   french_wizard_of_wikipedia   .001006     .02465 15.48      .4698         0                                  \n",
            "   french_xpersona                    0          0 13.13      .4938         0\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'clen': MacroAverageMetric(144.9),\n",
              "  'ctpb': GlobalAverageMetric(3446),\n",
              "  'ctps': GlobalTimerMetric(3.893e+04),\n",
              "  'ctrunc': MacroAverageMetric(0.004689),\n",
              "  'ctrunclen': MacroAverageMetric(0.2258),\n",
              "  'exps': GlobalTimerMetric(195.7),\n",
              "  'exs': SumMetric(4.326e+04),\n",
              "  'french_blended_skill_talk/clen': AverageMetric(199.5),\n",
              "  'french_blended_skill_talk/ctrunc': AverageMetric(0.01395),\n",
              "  'french_blended_skill_talk/ctrunclen': AverageMetric(0.612),\n",
              "  'french_blended_skill_talk/exs': SumMetric(1.248e+04),\n",
              "  'french_blended_skill_talk/llen': AverageMetric(30.98),\n",
              "  'french_blended_skill_talk/loss': AverageMetric(2.486),\n",
              "  'french_blended_skill_talk/ltrunc': AverageMetric(8.015e-05),\n",
              "  'french_blended_skill_talk/ltrunclen': AverageMetric(0.0005611),\n",
              "  'french_blended_skill_talk/ppl': PPLMetric(12.01),\n",
              "  'french_blended_skill_talk/token_acc': AverageMetric(0.5),\n",
              "  'french_blended_skill_talk/token_em': AverageMetric(0),\n",
              "  'french_empathetic_dialogues/clen': AverageMetric(60.69),\n",
              "  'french_empathetic_dialogues/ctrunc': AverageMetric(0),\n",
              "  'french_empathetic_dialogues/ctrunclen': AverageMetric(0),\n",
              "  'french_empathetic_dialogues/exs': SumMetric(9308),\n",
              "  'french_empathetic_dialogues/llen': AverageMetric(31),\n",
              "  'french_empathetic_dialogues/loss': AverageMetric(2.419),\n",
              "  'french_empathetic_dialogues/ltrunc': AverageMetric(0.001612),\n",
              "  'french_empathetic_dialogues/ltrunclen': AverageMetric(0.04738),\n",
              "  'french_empathetic_dialogues/ppl': PPLMetric(11.24),\n",
              "  'french_empathetic_dialogues/token_acc': AverageMetric(0.5009),\n",
              "  'french_empathetic_dialogues/token_em': AverageMetric(0.0001074),\n",
              "  'french_wizard_of_wikipedia/clen': AverageMetric(150.9),\n",
              "  'french_wizard_of_wikipedia/ctrunc': AverageMetric(0.004257),\n",
              "  'french_wizard_of_wikipedia/ctrunclen': AverageMetric(0.2811),\n",
              "  'french_wizard_of_wikipedia/exs': SumMetric(1.785e+04),\n",
              "  'french_wizard_of_wikipedia/llen': AverageMetric(38.01),\n",
              "  'french_wizard_of_wikipedia/loss': AverageMetric(2.741),\n",
              "  'french_wizard_of_wikipedia/ltrunc': AverageMetric(0.002128),\n",
              "  'french_wizard_of_wikipedia/ltrunclen': AverageMetric(0.04055),\n",
              "  'french_wizard_of_wikipedia/ppl': PPLMetric(15.5),\n",
              "  'french_wizard_of_wikipedia/token_acc': AverageMetric(0.4705),\n",
              "  'french_wizard_of_wikipedia/token_em': AverageMetric(0),\n",
              "  'french_xpersona/clen': AverageMetric(168.6),\n",
              "  'french_xpersona/ctrunc': AverageMetric(0.0005525),\n",
              "  'french_xpersona/ctrunclen': AverageMetric(0.01022),\n",
              "  'french_xpersona/exs': SumMetric(3620),\n",
              "  'french_xpersona/llen': AverageMetric(23.13),\n",
              "  'french_xpersona/loss': AverageMetric(2.556),\n",
              "  'french_xpersona/ltrunc': AverageMetric(0),\n",
              "  'french_xpersona/ltrunclen': AverageMetric(0),\n",
              "  'french_xpersona/ppl': PPLMetric(12.88),\n",
              "  'french_xpersona/token_acc': AverageMetric(0.4991),\n",
              "  'french_xpersona/token_em': AverageMetric(0.0002762),\n",
              "  'gpu_mem': GlobalAverageMetric(0.1541),\n",
              "  'llen': MacroAverageMetric(30.78),\n",
              "  'loss': MacroAverageMetric(2.55),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'ltpb': GlobalAverageMetric(536.8),\n",
              "  'ltps': GlobalTimerMetric(6064),\n",
              "  'ltrunc': MacroAverageMetric(0.000955),\n",
              "  'ltrunclen': MacroAverageMetric(0.02212),\n",
              "  'ppl': MacroAverageMetric(12.91),\n",
              "  'token_acc': MacroAverageMetric(0.4927),\n",
              "  'token_em': MacroAverageMetric(9.592e-05),\n",
              "  'total_train_updates': GlobalFixedMetric(1924),\n",
              "  'tpb': GlobalAverageMetric(3983),\n",
              "  'tps': GlobalTimerMetric(4.499e+04)},\n",
              " {'clen': MacroAverageMetric(147.5),\n",
              "  'ctpb': GlobalAverageMetric(3439),\n",
              "  'ctps': GlobalTimerMetric(3.84e+04),\n",
              "  'ctrunc': MacroAverageMetric(0.005961),\n",
              "  'ctrunclen': MacroAverageMetric(0.3297),\n",
              "  'exps': GlobalTimerMetric(190.1),\n",
              "  'exs': SumMetric(4.209e+04),\n",
              "  'french_blended_skill_talk/clen': AverageMetric(203.1),\n",
              "  'french_blended_skill_talk/ctrunc': AverageMetric(0.01939),\n",
              "  'french_blended_skill_talk/ctrunclen': AverageMetric(1.094),\n",
              "  'french_blended_skill_talk/exs': SumMetric(1.212e+04),\n",
              "  'french_blended_skill_talk/llen': AverageMetric(31.82),\n",
              "  'french_blended_skill_talk/loss': AverageMetric(2.479),\n",
              "  'french_blended_skill_talk/ltrunc': AverageMetric(0.000165),\n",
              "  'french_blended_skill_talk/ltrunclen': AverageMetric(0.006023),\n",
              "  'french_blended_skill_talk/ppl': PPLMetric(11.93),\n",
              "  'french_blended_skill_talk/token_acc': AverageMetric(0.5025),\n",
              "  'french_blended_skill_talk/token_em': AverageMetric(0),\n",
              "  'french_empathetic_dialogues/clen': AverageMetric(64.16),\n",
              "  'french_empathetic_dialogues/ctrunc': AverageMetric(0.0001187),\n",
              "  'french_empathetic_dialogues/ctrunclen': AverageMetric(0.001187),\n",
              "  'french_empathetic_dialogues/exs': SumMetric(8426),\n",
              "  'french_empathetic_dialogues/llen': AverageMetric(32.52),\n",
              "  'french_empathetic_dialogues/loss': AverageMetric(2.463),\n",
              "  'french_empathetic_dialogues/ltrunc': AverageMetric(0.002136),\n",
              "  'french_empathetic_dialogues/ltrunclen': AverageMetric(0.05578),\n",
              "  'french_empathetic_dialogues/ppl': PPLMetric(11.73),\n",
              "  'french_empathetic_dialogues/token_acc': AverageMetric(0.4955),\n",
              "  'french_empathetic_dialogues/token_em': AverageMetric(0),\n",
              "  'french_wizard_of_wikipedia/clen': AverageMetric(151.7),\n",
              "  'french_wizard_of_wikipedia/ctrunc': AverageMetric(0.003242),\n",
              "  'french_wizard_of_wikipedia/ctrunclen': AverageMetric(0.2063),\n",
              "  'french_wizard_of_wikipedia/exs': SumMetric(1.789e+04),\n",
              "  'french_wizard_of_wikipedia/llen': AverageMetric(38.25),\n",
              "  'french_wizard_of_wikipedia/loss': AverageMetric(2.74),\n",
              "  'french_wizard_of_wikipedia/ltrunc': AverageMetric(0.001006),\n",
              "  'french_wizard_of_wikipedia/ltrunclen': AverageMetric(0.02465),\n",
              "  'french_wizard_of_wikipedia/ppl': PPLMetric(15.48),\n",
              "  'french_wizard_of_wikipedia/token_acc': AverageMetric(0.4698),\n",
              "  'french_wizard_of_wikipedia/token_em': AverageMetric(0),\n",
              "  'french_xpersona/clen': AverageMetric(171),\n",
              "  'french_xpersona/ctrunc': AverageMetric(0.001096),\n",
              "  'french_xpersona/ctrunclen': AverageMetric(0.0178),\n",
              "  'french_xpersona/exs': SumMetric(3651),\n",
              "  'french_xpersona/llen': AverageMetric(23.41),\n",
              "  'french_xpersona/loss': AverageMetric(2.575),\n",
              "  'french_xpersona/ltrunc': AverageMetric(0),\n",
              "  'french_xpersona/ltrunclen': AverageMetric(0),\n",
              "  'french_xpersona/ppl': PPLMetric(13.13),\n",
              "  'french_xpersona/token_acc': AverageMetric(0.4938),\n",
              "  'french_xpersona/token_em': AverageMetric(0),\n",
              "  'gpu_mem': GlobalAverageMetric(0.154),\n",
              "  'llen': MacroAverageMetric(31.5),\n",
              "  'loss': MacroAverageMetric(2.564),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'ltpb': GlobalAverageMetric(541.6),\n",
              "  'ltps': GlobalTimerMetric(6049),\n",
              "  'ltrunc': MacroAverageMetric(0.0008269),\n",
              "  'ltrunclen': MacroAverageMetric(0.02161),\n",
              "  'ppl': MacroAverageMetric(13.07),\n",
              "  'token_acc': MacroAverageMetric(0.4904),\n",
              "  'token_em': MacroAverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(1924),\n",
              "  'tpb': GlobalAverageMetric(3980),\n",
              "  'tps': GlobalTimerMetric(4.445e+04)})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# 90M settings\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues,french_wizard_of_wikipedia\",\n",
        "    multitask_weights= \"1,3,3,3\",\n",
        "\n",
        "    # task='fromfile:parlaiformat', \n",
        "    # fromfile_datapath= f'{data_path}data',\n",
        "    # fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    \n",
        "    # depend on your gpu. \n",
        "    validation_every_n_epochs=0.25,\n",
        "    num_epochs = 5,\n",
        "    log_every_n_secs= 60,\n",
        "    verbose = True,\n",
        "    batchsize= 8, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        "\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    attention_dropout= 0.0, \n",
        "    model_parallel= False,\n",
        "    warmup_updates=100,\n",
        "\n",
        "    # customized parameters\n",
        "    # inference= \"beam\"\n",
        "    # beam_min_length= 20,\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_size= 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mydrive_path = '/content/finetuned-multitask-400m-double-sided-2epochs'\n",
        "# mydrive_path = '/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-double-sided/'"
      ],
      "metadata": {
        "id": "mxL0-1x1YsQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNZE5ta2pO-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc0f916c-ab12-4c68-85ec-20f01f263f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15:44:17 | building dictionary first...\n",
            "15:44:17 | No model with opt yet at: /content/drive/MyDrive/colabs/blender-models/finetuned-400m-4task-2sided//model(.opt)\n",
            "15:44:17 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: download_path: None,verbose: True,is_debug: False,datapath: /usr/local/lib/python3.7/dist-packages/data,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,world_logs: ,save_format: conversations,log_keep_fields: all,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,checkpoint_activations: False,interactive_mode: False\u001b[0m\n",
            "15:44:17 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--init-opt /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt --allow-missing-init-opts True --task blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --batchsize 8 --dynamic-batching full --eval-batchsize 8 --num-epochs -1 --validation-every-n-secs 900.0 --save-after-valid True --validation-every-n-epochs -1.0 --validation-patience 20 --log-every-n-secs 10.0 --distributed-world-size 8 --port 61337 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --chosen-topic-delimiter \n",
            " --num-topics 5 --train-experiencer-only False --remove-political-convos False --teacher-model /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model --task-loss-coeff 1.0 --encoder-loss-coeff 24.0 --hidden-loss-coeff 5.0 --pred-loss-coeff 8.0 --embedding-loss-coeff 0.35 --self-attn-loss-coeff 0.6 --enc-dec-attn-loss-coeff 3.0 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --inference beam --learningrate 0.0004 --gpu 0 --bpe-vocab /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json --bpe-merge /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt --bpe-add-prefix-space True --max-lr-steps -1 --parlai-home /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI --show-advanced-args False --numthreads 1 --rank 0 --dict-loaded True\u001b[0m\n",
            "15:44:17 | Using CUDA\n",
            "15:44:17 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict\n",
            "15:44:17 | num words = 8008\n",
            "15:44:23 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "15:44:23 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model\n",
            "15:44:25 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
            "15:44:25 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
            "15:44:26 | Opt:\n",
            "15:44:26 |     activation: gelu\n",
            "15:44:26 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "15:44:26 |     adam_eps: 1e-08\n",
            "15:44:26 |     add_p1_after_newln: False\n",
            "15:44:26 |     aggregate_micro: False\n",
            "15:44:26 |     allow_missing_init_opts: False\n",
            "15:44:26 |     attention_dropout: 0.0\n",
            "15:44:26 |     batchsize: 16\n",
            "15:44:26 |     beam_block_full_context: True\n",
            "15:44:26 |     beam_block_list_filename: None\n",
            "15:44:26 |     beam_block_ngram: -1\n",
            "15:44:26 |     beam_context_block_ngram: -1\n",
            "15:44:26 |     beam_delay: 30\n",
            "15:44:26 |     beam_length_penalty: 0.65\n",
            "15:44:26 |     beam_min_length: 1\n",
            "15:44:26 |     beam_size: 1\n",
            "15:44:26 |     betas: '(0.9, 0.999)'\n",
            "15:44:26 |     bpe_add_prefix_space: None\n",
            "15:44:26 |     bpe_debug: False\n",
            "15:44:26 |     bpe_dropout: None\n",
            "15:44:26 |     bpe_merge: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict-merges.txt\n",
            "15:44:26 |     bpe_vocab: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict-vocab.json\n",
            "15:44:26 |     checkpoint_activations: False\n",
            "15:44:26 |     compute_tokenized_bleu: False\n",
            "15:44:26 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "15:44:26 |     datatype: train\n",
            "15:44:26 |     delimiter: '  '\n",
            "15:44:26 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "15:44:26 |     dict_endtoken: __end__\n",
            "15:44:26 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict\n",
            "15:44:26 |     dict_include_test: False\n",
            "15:44:26 |     dict_include_valid: False\n",
            "15:44:26 |     dict_initpath: None\n",
            "15:44:26 |     dict_language: english\n",
            "15:44:26 |     dict_loaded: True\n",
            "15:44:26 |     dict_lower: False\n",
            "15:44:26 |     dict_max_ngram_size: -1\n",
            "15:44:26 |     dict_maxexs: -1\n",
            "15:44:26 |     dict_maxtokens: -1\n",
            "15:44:26 |     dict_minfreq: 0\n",
            "15:44:26 |     dict_nulltoken: __null__\n",
            "15:44:26 |     dict_starttoken: __start__\n",
            "15:44:26 |     dict_textfields: text,labels\n",
            "15:44:26 |     dict_tokenizer: bytelevelbpe\n",
            "15:44:26 |     dict_unktoken: __unk__\n",
            "15:44:26 |     display_examples: False\n",
            "15:44:26 |     download_path: None\n",
            "15:44:26 |     dropout: 0.1\n",
            "15:44:26 |     dynamic_batching: None\n",
            "15:44:26 |     embedding_projection: random\n",
            "15:44:26 |     embedding_size: 1280\n",
            "15:44:26 |     embedding_type: random\n",
            "15:44:26 |     embeddings_scale: True\n",
            "15:44:26 |     eval_batchsize: None\n",
            "15:44:26 |     eval_dynamic_batching: None\n",
            "15:44:26 |     evaltask: None\n",
            "15:44:26 |     ffn_size: 5120\n",
            "15:44:26 |     final_extra_opt: \n",
            "15:44:26 |     force_fp16_tokens: False\n",
            "15:44:26 |     fp16: True\n",
            "15:44:26 |     fp16_impl: mem_efficient\n",
            "15:44:26 |     gpu: -1\n",
            "15:44:26 |     gradient_clip: 0.1\n",
            "15:44:26 |     hide_labels: False\n",
            "15:44:26 |     history_add_global_end_token: end\n",
            "15:44:26 |     history_reversed: False\n",
            "15:44:26 |     history_size: -1\n",
            "15:44:26 |     image_cropsize: 224\n",
            "15:44:26 |     image_mode: raw\n",
            "15:44:26 |     image_size: 256\n",
            "15:44:26 |     inference: greedy\n",
            "15:44:26 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model\n",
            "15:44:26 |     init_opt: None\n",
            "15:44:26 |     interactive_mode: False\n",
            "15:44:26 |     invsqrt_lr_decay_gamma: -1\n",
            "15:44:26 |     is_debug: False\n",
            "15:44:26 |     label_truncate: 128\n",
            "15:44:26 |     learn_positional_embeddings: False\n",
            "15:44:26 |     learningrate: 7e-06\n",
            "15:44:26 |     load_from_checkpoint: True\n",
            "15:44:26 |     log_every_n_secs: 300.0\n",
            "15:44:26 |     log_every_n_steps: 50\n",
            "15:44:26 |     log_keep_fields: all\n",
            "15:44:26 |     loglevel: info\n",
            "15:44:26 |     lr_scheduler: reduceonplateau\n",
            "15:44:26 |     lr_scheduler_decay: 0.5\n",
            "15:44:26 |     lr_scheduler_patience: 3\n",
            "15:44:26 |     max_train_steps: -1\n",
            "15:44:26 |     max_train_time: -1\n",
            "15:44:26 |     metrics: default\n",
            "15:44:26 |     model: transformer/generator\n",
            "15:44:26 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-400m-4task-2sided//model\n",
            "15:44:26 |     model_parallel: False\n",
            "15:44:26 |     momentum: 0\n",
            "15:44:26 |     multitask_weights: '(1.0, 3.0, 3.0, 3.0)'\n",
            "15:44:26 |     mutators: None\n",
            "15:44:26 |     n_decoder_layers: 12\n",
            "15:44:26 |     n_encoder_layers: 2\n",
            "15:44:26 |     n_heads: 32\n",
            "15:44:26 |     n_layers: 2\n",
            "15:44:26 |     n_positions: 128\n",
            "15:44:26 |     n_segments: 0\n",
            "15:44:26 |     nesterov: True\n",
            "15:44:26 |     no_cuda: False\n",
            "15:44:26 |     num_epochs: 0.01\n",
            "15:44:26 |     num_workers: 0\n",
            "15:44:26 |     nus: (0.7,)\n",
            "15:44:26 |     optimizer: mem_eff_adam\n",
            "15:44:26 |     output_scaling: 1.0\n",
            "15:44:26 |     override: \"{'task': 'french_blended_skill_talk,french_xpersona,french_empathetic_dialogues,french_wizard_of_wikipedia', 'multitask_weights': (1.0, 3.0, 3.0, 3.0), 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-400m-4task-2sided//model', 'init_model': 'zoo:blender/blender_400Mdistill/model', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model.dict', 'validation_every_n_epochs': 0.01, 'num_epochs': 0.01, 'log_every_n_secs': 300.0, 'verbose': True, 'attention_dropout': 0.0, 'batchsize': 16, 'fp16': True, 'fp16_impl': 'mem_efficient', 'embedding_size': 1280, 'ffn_size': 5120, 'variant': 'prelayernorm', 'n_heads': 32, 'n_positions': 128, 'n_encoder_layers': 2, 'n_decoder_layers': 12, 'label_truncate': 128, 'text_truncate': 128, 'truncate': 128, 'activation': 'gelu', 'history_add_global_end_token': 'end', 'delimiter': '  ', 'dict_tokenizer': 'bytelevelbpe', 'dropout': 0.1, 'learningrate': 7e-06, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'optimizer': 'mem_eff_adam', 'relu_dropout': 0.0, 'model_parallel': False, 'warmup_updates': 100, 'update_freq': 2, 'gradient_clip': 0.1, 'skip_generation': True, 'validation_patience': 10, 'validation_metric': 'ppl', 'validation_metric_mode': 'min'}\"\n",
            "15:44:26 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "15:44:26 |     person_tokens: False\n",
            "15:44:26 |     rank_candidates: False\n",
            "15:44:26 |     relu_dropout: 0.0\n",
            "15:44:26 |     save_after_valid: False\n",
            "15:44:26 |     save_every_n_secs: -1\n",
            "15:44:26 |     save_format: conversations\n",
            "15:44:26 |     share_word_embeddings: True\n",
            "15:44:26 |     short_final_eval: False\n",
            "15:44:26 |     skip_generation: True\n",
            "15:44:26 |     special_tok_lst: None\n",
            "15:44:26 |     split_lines: False\n",
            "15:44:26 |     starttime: Jun09_15-44\n",
            "15:44:26 |     task: french_blended_skill_talk,french_xpersona,french_empathetic_dialogues,french_wizard_of_wikipedia\n",
            "15:44:26 |     temperature: 1.0\n",
            "15:44:26 |     tensorboard_log: False\n",
            "15:44:26 |     tensorboard_logdir: None\n",
            "15:44:26 |     text_truncate: 128\n",
            "15:44:26 |     topk: 10\n",
            "15:44:26 |     topp: 0.9\n",
            "15:44:26 |     truncate: 128\n",
            "15:44:26 |     update_freq: 2\n",
            "15:44:26 |     use_reply: label\n",
            "15:44:26 |     validation_cutoff: 1.0\n",
            "15:44:26 |     validation_every_n_epochs: 0.01\n",
            "15:44:26 |     validation_every_n_secs: -1\n",
            "15:44:26 |     validation_every_n_steps: -1\n",
            "15:44:26 |     validation_max_exs: -1\n",
            "15:44:26 |     validation_metric: ppl\n",
            "15:44:26 |     validation_metric_mode: min\n",
            "15:44:26 |     validation_patience: 10\n",
            "15:44:26 |     validation_share_agent: False\n",
            "15:44:26 |     variant: prelayernorm\n",
            "15:44:26 |     verbose: True\n",
            "15:44:26 |     wandb_entity: None\n",
            "15:44:26 |     wandb_log: False\n",
            "15:44:26 |     wandb_name: None\n",
            "15:44:26 |     wandb_project: None\n",
            "15:44:26 |     warmup_rate: 0.0001\n",
            "15:44:26 |     warmup_updates: 100\n",
            "15:44:26 |     weight_decay: None\n",
            "15:44:26 |     world_logs: \n",
            "15:44:27 | creating task(s): french_blended_skill_talk,french_xpersona,french_empathetic_dialogues,french_wizard_of_wikipedia\n",
            "15:44:27 | Loading ParlAI text data: /content/dataset_french_bst/train.txt\n",
            "15:44:30 | Loading ParlAI text data: /content/dataset_french_xpersona/train.txt\n",
            "15:44:32 | Loading ParlAI text data: /content/dataset_french_ed/train.txt\n",
            "15:44:32 | Loading ParlAI text data: /content/dataset_french_wow/train.txt\n",
            "15:44:34 | training...\n",
            "15:44:35 | Overflow: setting loss scale to 65536.0\n",
            "15:44:39 | Overflow: setting loss scale to 32768.0\n",
            "15:45:28 | time:54s total_exs:1600 total_steps:50 epochs:0.00 time_left:114s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         167.3 .9600  1560  2887   .5087      74.82 29.62 1600             34734   15.8    .5320 35.53   \n",
            "   french_blended_skill_talk   247.4                     .6972      139.2        251                                   36.28   \n",
            "   french_empathetic_dialogues 66.84                     .1317      6.246        167                                   33.42   \n",
            "   french_wizard_of_wikipedia  181.8                     .6055      79.93        469                                   46.68   \n",
            "   french_xpersona             173.4                     .6003      73.87        713                                   25.73   \n",
            "                                loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         5.817 3.5e-06 548.5  1015 .001599     .03838 338.3      .1087         0                   50   \n",
            "   french_blended_skill_talk   5.679                           0          0 292.6      .1115         0                        \n",
            "   french_empathetic_dialogues 5.997                           0          0 402.1     .09389         0                        \n",
            "   french_wizard_of_wikipedia  5.837                     .006397      .1535 342.6      .1156         0                        \n",
            "   french_xpersona             5.755                           0          0 315.8      .1138         0                        \n",
            "                                tpb  tps   ups  \n",
            "   all                         2108 3903 .9256  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "15:46:25 | time:110s total_exs:3200 total_steps:100 epochs:0.01 time_left:61s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         172.1     1  1568  2799   .5131      78.73 28.56 1600             32768  8.255    .4502 36.82   \n",
            "   french_blended_skill_talk   244.3                     .6923      137.4        221                                   37.45   \n",
            "   french_empathetic_dialogues 67.84                     .1327      5.918        196                                   36.03   \n",
            "   french_wizard_of_wikipedia  190.1                     .6000      87.83        415                                   47.56   \n",
            "   french_xpersona               186                     .6276      83.74        768                                   26.26   \n",
            "                                loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         5.281 6.93e-06 551.1 983.8 .005031      .1568 197.2      .1294         0                  100   \n",
            "   french_blended_skill_talk   5.177                            0          0 177.1      .1308         0                        \n",
            "   french_empathetic_dialogues 5.379                       .01531      .5765 216.9      .1169         0                        \n",
            "   french_wizard_of_wikipedia   5.33                      .004819      .0506 206.4      .1318         0                        \n",
            "   french_xpersona             5.239                            0          0 188.4      .1382         0                        \n",
            "                                tpb  tps   ups  \n",
            "   all                         2119 3782 .8926  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "15:47:18 | time:163s total_exs:4800 total_steps:150 epochs:0.01 time_left:6s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         170.5     1  1583  2987   .5188      76.93 30.19 1600             32768  4.764    .4502 35.32   \n",
            "   french_blended_skill_talk   234.8                     .6936      126.1        173                                   38.94   \n",
            "   french_empathetic_dialogues  66.8                     .1143       7.12        175                                   33.86   \n",
            "   french_wizard_of_wikipedia  182.4                     .6141       80.2        425                                   41.32   \n",
            "   french_xpersona             197.8                     .6530      94.29        827                                   27.15   \n",
            "                                loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         4.702 6.93e-06 526.2 992.9 .004286     .08571 110.5      .1795         0                  150   \n",
            "   french_blended_skill_talk   4.643                            0          0 103.9      .1844         0                        \n",
            "   french_empathetic_dialogues 4.757                       .01714      .3429 116.4      .1669         0                        \n",
            "   french_wizard_of_wikipedia  4.808                            0          0 122.5      .1771         0                        \n",
            "   french_xpersona             4.598                            0          0  99.3      .1898         0                        \n",
            "                                tpb  tps   ups  \n",
            "   all                         2109 3980 .9435  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "15:47:24 | time:170s total_exs:4992 total_steps:156 epochs:0.01 time_left:0s\n",
            "                                clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  \\\n",
            "   all                         187.1     1  1554  2705   .5051      92.37 27.84  192             32768  4.299    .4502 37.46   \n",
            "   french_blended_skill_talk   328.6                     .8378      209.8         37                                   40.51   \n",
            "   french_empathetic_dialogues 71.12                    .08333      4.125         24                                   32.33   \n",
            "   french_wizard_of_wikipedia  182.2                     .5538      84.45         65                                   48.54   \n",
            "   french_xpersona             166.5                     .5455      71.14         66                                   28.45   \n",
            "                                loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  \\\n",
            "   all                         4.464 6.93e-06 608.5  1059 .003846     .02308 87.18      .2004         0                  156   \n",
            "   french_blended_skill_talk   4.381                            0          0 79.92      .2135         0                        \n",
            "   french_empathetic_dialogues 4.396                            0          0 81.15      .2075         0                        \n",
            "   french_wizard_of_wikipedia  4.617                       .01538     .09231 101.2      .1813         0                        \n",
            "   french_xpersona              4.46                            0          0 86.49      .1991         0                        \n",
            "                                tpb  tps   ups  \n",
            "   all                         2163 3764 .8702  \n",
            "   french_blended_skill_talk                    \n",
            "   french_empathetic_dialogues                  \n",
            "   french_wizard_of_wikipedia                   \n",
            "   french_xpersona\n",
            "\n",
            "15:47:24 | num_epochs completed:0.01 time elapsed:170.04231786727905s\n",
            "15:47:24 | Saving dictionary to /content/drive/MyDrive/colabs/blender-models/finetuned-400m-4task-2sided//model.dict\n",
            "15:47:39 | \u001b[33mOverriding opt[\"multitask_weights\"] to (1.0, 3.0, 3.0, 3.0) (previously: [1.0, 3.0, 3.0, 3.0])\u001b[0m\n",
            "15:47:39 | \u001b[33mOverriding opt[\"init_model\"] to zoo:blender/blender_400Mdistill/model (previously: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_400Mdistill/model)\u001b[0m\n",
            "15:47:39 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,world_logs: ,save_format: conversations,log_keep_fields: all,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,checkpoint_activations: False,verbose: True,download_path: None,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "15:47:39 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--init-opt /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model.opt --allow-missing-init-opts True --task blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --batchsize 8 --dynamic-batching full --eval-batchsize 8 --num-epochs -1 --validation-every-n-secs 900.0 --save-after-valid True --validation-every-n-epochs -1.0 --validation-patience 20 --log-every-n-secs 10.0 --distributed-world-size 8 --port 61337 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --chosen-topic-delimiter \n",
            " --num-topics 5 --train-experiencer-only False --remove-political-convos False --teacher-model /private/home/ems/GitHub/facebookresearch/ParlAI/data/models/blender/blender_3B/model --task-loss-coeff 1.0 --encoder-loss-coeff 24.0 --hidden-loss-coeff 5.0 --pred-loss-coeff 8.0 --embedding-loss-coeff 0.35 --self-attn-loss-coeff 0.6 --enc-dec-attn-loss-coeff 3.0 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --inference beam --force-fp16-tokens False --learningrate 0.0004 --gpu 0 --bpe-vocab /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json --bpe-merge /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt --bpe-add-prefix-space True --max-lr-steps -1 --parlai-home /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI --show-advanced-args False --numthreads 1 --rank 0\u001b[0m\n",
            "15:47:39 | Using CUDA\n",
            "15:47:39 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-400m-4task-2sided//model.dict\n",
            "15:47:39 | num words = 8008\n",
            "15:47:45 | Total parameters: 364,802,560 (364,474,880 trainable)\n",
            "15:47:45 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-400m-4task-2sided//model\n"
          ]
        }
      ],
      "source": [
        "# 400M settings\n",
        "# !rm -rf $finetuned_model_path\n",
        "!mkdir -p $finetuned_model_path\n",
        "\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    # task\n",
        "    task= \"french_blended_skill_talk,french_xpersona,french_empathetic_dialogues,french_wizard_of_wikipedia\",\n",
        "    multitask_weights= \"1,3,3,3\",\n",
        "\n",
        "    # task='fromfile:parlaiformat', \n",
        "    # fromfile_datapath= f'{data_path}data',\n",
        "    # fromfile_datatype_extension=True,\n",
        "\n",
        "    model='transformer/generator',\n",
        "    model_file= f'{finetuned_model_path}model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model= init_model,\n",
        "    dict_file=dict_file,\n",
        "    \n",
        "    # depend on your gpu\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 5,\n",
        "    log_every_n_secs= 300,\n",
        "    verbose = True,\n",
        "    attention_dropout= 0.0, \n",
        "    batchsize= 16, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    # save_after_valid= True,\n",
        "\n",
        "    # arguments we get from the pretrained model. \"from recipes page for 2.7B model\" \n",
        "    embedding_size= 1280, ffn_size= 5120,\n",
        "    variant= \"prelayernorm\",\n",
        "    n_heads= 32, n_positions= 128, \n",
        "    n_encoder_layers= 2, n_decoder_layers= 12,\n",
        "\n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    dict_tokenizer= \"bytelevelbpe\",\n",
        "    dropout= 0.1,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    optimizer= \"mem_eff_adam\",\n",
        "    relu_dropout= 0.0, \n",
        "    model_parallel= False,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    # inference = 'topk', \n",
        "    # temperature = 0.7, \n",
        "    # topk=30, \n",
        "    # beam_length_penalty=1.03\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -rv /content/finetuned-multitask-400m-double-sided-2epochs/* /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-400m-double-sided/\n",
        "# !ls -lah /content/finetuned-multitask-400m-double-sided-2epochs/\n",
        "# !ls -lah /content/finetuned-multitask-400m-double-sided/"
      ],
      "metadata": {
        "id": "K1S0kRk3W63i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVPS6p4XzPh4"
      },
      "source": [
        "# 4.Display Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9FBtnZZzPPg",
        "outputId": "c2343265-c6b1-4870-8cc4-6c58863b76de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15:06:36 | \u001b[33mOverriding opt[\"task\"] to french_blended_skill_talk (previously: french_blended_skill_talk,french_xpersona,french_empathetic_dialogues)\u001b[0m\n",
            "15:06:36 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "15:06:36 | Using CUDA\n",
            "15:06:36 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m-double-sided/model.dict\n",
            "15:06:36 | num words = 54944\n",
            "15:06:38 | Total parameters: 87,508,992 (86,984,704 trainable)\n",
            "15:06:38 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m-double-sided/model\n",
            "15:06:42 | creating task(s): french_blended_skill_talk\n",
            "15:06:42 | Loading ParlAI text data: /content/dataset_french_bst/valid.txt\n",
            "15:06:42 | Opt:\n",
            "15:06:42 |     activation: gelu\n",
            "15:06:42 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "15:06:42 |     adam_eps: 1e-08\n",
            "15:06:42 |     add_p1_after_newln: False\n",
            "15:06:42 |     aggregate_micro: False\n",
            "15:06:42 |     allow_missing_init_opts: False\n",
            "15:06:42 |     attention_dropout: 0.0\n",
            "15:06:42 |     batchsize: 8\n",
            "15:06:42 |     beam_block_full_context: True\n",
            "15:06:42 |     beam_block_list_filename: None\n",
            "15:06:42 |     beam_block_ngram: -1\n",
            "15:06:42 |     beam_context_block_ngram: -1\n",
            "15:06:42 |     beam_delay: 30\n",
            "15:06:42 |     beam_length_penalty: 0.65\n",
            "15:06:42 |     beam_min_length: 1\n",
            "15:06:42 |     beam_size: 1\n",
            "15:06:42 |     betas: '[0.9, 0.999]'\n",
            "15:06:42 |     bpe_add_prefix_space: None\n",
            "15:06:42 |     bpe_debug: False\n",
            "15:06:42 |     bpe_dropout: None\n",
            "15:06:42 |     bpe_merge: None\n",
            "15:06:42 |     bpe_vocab: None\n",
            "15:06:42 |     checkpoint_activations: False\n",
            "15:06:42 |     compute_tokenized_bleu: False\n",
            "15:06:42 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "15:06:42 |     datatype: train\n",
            "15:06:42 |     delimiter: '\\n'\n",
            "15:06:42 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "15:06:42 |     dict_endtoken: __end__\n",
            "15:06:42 |     dict_file: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m-double-sided/model.dict\n",
            "15:06:42 |     dict_include_test: False\n",
            "15:06:42 |     dict_include_valid: False\n",
            "15:06:42 |     dict_initpath: None\n",
            "15:06:42 |     dict_language: english\n",
            "15:06:42 |     dict_loaded: True\n",
            "15:06:42 |     dict_lower: True\n",
            "15:06:42 |     dict_max_ngram_size: -1\n",
            "15:06:42 |     dict_maxexs: -1\n",
            "15:06:42 |     dict_maxtokens: -1\n",
            "15:06:42 |     dict_minfreq: 0\n",
            "15:06:42 |     dict_nulltoken: __null__\n",
            "15:06:42 |     dict_starttoken: __start__\n",
            "15:06:42 |     dict_textfields: text,labels\n",
            "15:06:42 |     dict_tokenizer: bpe\n",
            "15:06:42 |     dict_unktoken: __unk__\n",
            "15:06:42 |     display_add_fields: \n",
            "15:06:42 |     display_examples: False\n",
            "15:06:42 |     download_path: None\n",
            "15:06:42 |     dropout: 0.0\n",
            "15:06:42 |     dynamic_batching: full\n",
            "15:06:42 |     embedding_projection: random\n",
            "15:06:42 |     embedding_size: 512\n",
            "15:06:42 |     embedding_type: random\n",
            "15:06:42 |     embeddings_scale: True\n",
            "15:06:42 |     eval_batchsize: None\n",
            "15:06:42 |     eval_dynamic_batching: None\n",
            "15:06:42 |     evaltask: None\n",
            "15:06:42 |     ffn_size: 2048\n",
            "15:06:42 |     final_extra_opt: \n",
            "15:06:42 |     force_fp16_tokens: True\n",
            "15:06:42 |     fp16: True\n",
            "15:06:42 |     fp16_impl: mem_efficient\n",
            "15:06:42 |     gpu: -1\n",
            "15:06:42 |     gradient_clip: 0.1\n",
            "15:06:42 |     hide_labels: False\n",
            "15:06:42 |     history_add_global_end_token: None\n",
            "15:06:42 |     history_reversed: False\n",
            "15:06:42 |     history_size: -1\n",
            "15:06:42 |     image_cropsize: 224\n",
            "15:06:42 |     image_mode: raw\n",
            "15:06:42 |     image_size: 256\n",
            "15:06:42 |     inference: greedy\n",
            "15:06:42 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_90M/model\n",
            "15:06:42 |     init_opt: None\n",
            "15:06:42 |     interactive_mode: False\n",
            "15:06:42 |     invsqrt_lr_decay_gamma: -1\n",
            "15:06:42 |     is_debug: False\n",
            "15:06:42 |     label_truncate: 128\n",
            "15:06:42 |     learn_positional_embeddings: False\n",
            "15:06:42 |     learningrate: 1e-05\n",
            "15:06:42 |     log_every_n_secs: 60.0\n",
            "15:06:42 |     log_every_n_steps: 50\n",
            "15:06:42 |     log_keep_fields: all\n",
            "15:06:42 |     loglevel: info\n",
            "15:06:42 |     lr_scheduler: reduceonplateau\n",
            "15:06:42 |     lr_scheduler_decay: 0.5\n",
            "15:06:43 |     lr_scheduler_patience: 3\n",
            "15:06:43 |     max_train_steps: -1\n",
            "15:06:43 |     max_train_time: -1\n",
            "15:06:43 |     metrics: default\n",
            "15:06:43 |     model: transformer/generator\n",
            "15:06:43 |     model_file: /content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m-double-sided/model\n",
            "15:06:43 |     model_parallel: False\n",
            "15:06:43 |     momentum: 0\n",
            "15:06:43 |     multitask_weights: '[1.0, 3.0, 3.0]'\n",
            "15:06:43 |     mutators: None\n",
            "15:06:43 |     n_decoder_layers: -1\n",
            "15:06:43 |     n_encoder_layers: -1\n",
            "15:06:43 |     n_heads: 16\n",
            "15:06:43 |     n_layers: 8\n",
            "15:06:43 |     n_positions: 512\n",
            "15:06:43 |     n_segments: 0\n",
            "15:06:43 |     nesterov: True\n",
            "15:06:43 |     no_cuda: False\n",
            "15:06:43 |     num_epochs: 5.0\n",
            "15:06:43 |     num_examples: 20\n",
            "15:06:43 |     num_workers: 0\n",
            "15:06:43 |     nus: [0.7]\n",
            "15:06:43 |     optimizer: mem_eff_adam\n",
            "15:06:43 |     output_scaling: 1.0\n",
            "15:06:43 |     override: \"{'task': 'french_blended_skill_talk', 'model_file': '/content/drive/MyDrive/colabs/blender-models/finetuned-multitask-90m-double-sided/model', 'num_examples': '20', 'skip_generation': False}\"\n",
            "15:06:43 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "15:06:43 |     person_tokens: False\n",
            "15:06:43 |     rank_candidates: False\n",
            "15:06:43 |     relu_dropout: 0.0\n",
            "15:06:43 |     save_after_valid: False\n",
            "15:06:43 |     save_every_n_secs: -1\n",
            "15:06:43 |     save_format: conversations\n",
            "15:06:43 |     share_word_embeddings: True\n",
            "15:06:43 |     short_final_eval: False\n",
            "15:06:43 |     skip_generation: False\n",
            "15:06:43 |     special_tok_lst: None\n",
            "15:06:43 |     split_lines: False\n",
            "15:06:43 |     starttime: May25_08-36\n",
            "15:06:43 |     task: french_blended_skill_talk\n",
            "15:06:43 |     temperature: 1.0\n",
            "15:06:43 |     tensorboard_log: False\n",
            "15:06:43 |     tensorboard_logdir: None\n",
            "15:06:43 |     text_truncate: 512\n",
            "15:06:43 |     topk: 10\n",
            "15:06:43 |     topp: 0.9\n",
            "15:06:43 |     truncate: -1\n",
            "15:06:43 |     update_freq: 1\n",
            "15:06:43 |     use_reply: label\n",
            "15:06:43 |     validation_cutoff: 1.0\n",
            "15:06:43 |     validation_every_n_epochs: 0.25\n",
            "15:06:43 |     validation_every_n_secs: -1\n",
            "15:06:43 |     validation_every_n_steps: -1\n",
            "15:06:43 |     validation_max_exs: -1\n",
            "15:06:43 |     validation_metric: ppl\n",
            "15:06:43 |     validation_metric_mode: min\n",
            "15:06:43 |     validation_patience: 10\n",
            "15:06:43 |     validation_share_agent: False\n",
            "15:06:43 |     variant: xlm\n",
            "15:06:43 |     verbose: False\n",
            "15:06:43 |     wandb_entity: None\n",
            "15:06:43 |     wandb_log: False\n",
            "15:06:43 |     wandb_name: None\n",
            "15:06:43 |     wandb_project: None\n",
            "15:06:43 |     warmup_rate: 0.0001\n",
            "15:06:43 |     warmup_updates: 100\n",
            "15:06:43 |     weight_decay: None\n",
            "15:06:43 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk- - -\u001b[0;0m\n",
            "\u001b[0mCela semble dangereux. Cela vaut-il la peine de faire un travail aussi dangereux ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Wekk, c'est bon si vous êtes bien formé.  Il y a trois niveaux \u001b[0;0m\n",
            "\u001b[0;95m     model: oui , c ' est vrai . je ne sais pas si je peux m ' en empêcher .\u001b[0;0m\n",
            "\u001b[0mA quel niveau êtes-vous ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'ai reçu une formation sur le tas lorsque j'ai commencé à travailler.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un grand fan de la musique country .\u001b[0;0m\n",
            "\u001b[0mC'est génial ! Depuis combien de temps faites-vous ce travail ? \u001b[0;0m\n",
            "\u001b[1;94m    labels: Depuis un bon nombre d'années maintenant.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un étudiant de la marine .\u001b[0;0m\n",
            "\u001b[0mOn dirait que ça peut être un travail dangereux parfois.\u001b[0;0m\n",
            "\u001b[1;94m    labels: C'est vrai, surtout si vous ne prenez pas les mesures appropriées.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui , c ' est vrai .\u001b[0;0m\n",
            "\u001b[0mEh bien, vous êtes entré dans un métier qui vous garantira toujours du travail.\u001b[0;0m\n",
            "\u001b[1;94m    labels: C'est vrai, surtout dans cette économie\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un peu déçu , mais je suis un peu plus âgé que moi .\u001b[0;0m\n",
            "\u001b[0mJ'ai moi-même un emploi assez stable, mais surtout du travail de bureau.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Est-ce que ça vous plaît ?\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est bien , je suis content que tu aies pu être stable .\u001b[0;0m\n",
            "\u001b[0mJe le fais vraiment, mais parfois j'aimerais être plus actif et debout.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je vous comprends. S'étirer le long des couloirs\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est bien , je suis sûr que vous vous êtes amusés .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk- - -\u001b[0;0m\n",
            "\u001b[0mWekk, c'est bon si vous êtes bien formé.  Il y a trois niveaux \u001b[0;0m\n",
            "\u001b[1;94m    labels: A quel niveau êtes-vous ?\u001b[0;0m\n",
            "\u001b[0;95m     model: oui , je suis un grand fan de la musique .\u001b[0;0m\n",
            "\u001b[0mJ'ai reçu une formation sur le tas lorsque j'ai commencé à travailler.\u001b[0;0m\n",
            "\u001b[1;94m    labels: C'est génial ! Depuis combien de temps faites-vous ce travail ? \u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est génial . je suis sûr que vous avez __unk__ is à faire ça .\u001b[0;0m\n",
            "\u001b[0mDepuis un bon nombre d'années maintenant.\u001b[0;0m\n",
            "\u001b[1;94m    labels: On dirait que ça peut être un travail dangereux parfois.\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est génial ! je suis sûr que vous avez beaucoup de temps pour travailler .\u001b[0;0m\n",
            "\u001b[0mC'est vrai, surtout si vous ne prenez pas les mesures appropriées.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Eh bien, vous êtes entré dans un métier qui vous garantira toujours du travail.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis d ' accord . je suis un peu déçu .\u001b[0;0m\n",
            "\u001b[0mC'est vrai, surtout dans cette économie\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'ai moi-même un emploi assez stable, mais surtout du travail de bureau.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis sûr que vous avez beaucoup de chance .\u001b[0;0m\n",
            "\u001b[0mEst-ce que ça vous plaît ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je le fais vraiment, mais parfois j'aimerais être plus actif et debout.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui , je suis un peu déçu .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_blended_skill_talk- - -\u001b[0;0m\n",
            "\u001b[0mJe me sens très bien dans ma relation avec mon partenaire. Mon partenaire apprécie mon opinion sur les choses.\u001b[0;0m\n",
            "\u001b[1;94m    labels: ok, si doux\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est bien . je suis sûr que tu es très fier de lui .\u001b[0;0m\n",
            "\u001b[0mEt vous, quelqu'un vers qui vous pouvez vous tourner ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Bien sûr, j'ai des amis avec qui je suis toujours libre de partager.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne suis pas sûr .\u001b[0;0m\n",
            "\u001b[0mJ'aime faire du vélo tout terrain avec mes amis.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Je n'ai jamais fait ça.  Je ne suis pas vraiment du genre à faire de l'activité physique, mais je serais prêt à essayer, j'imagine.\u001b[0;0m\n",
            "\u001b[0;95m     model: j ' aime aussi faire du vélo .\u001b[0;0m\n",
            "\u001b[0mJ'aime aussi aller prendre un thé à bulles, c'est tellement amusant.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Ça, c'est plus mon truc. Ça a un goût un peu étrange, mais j'aime ça.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne suis pas sûr de ce que vous aimez faire .\u001b[0;0m\n",
            "\u001b[0mExactement, c'est bizarre mais je continue à en redemander. Tellement étrange.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Oui, c'est juste une de ces choses. Une fois que vous commencez, vous ne pouvez plus vous arrêter.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis sûr que vous avez __unk__ is à faire ça .\u001b[0;0m\n",
            "\u001b[0mY a-t-il quelque chose que vous aimez vraiment faire ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'aime me détendre avec des amis, boire un petit verre, ce genre de choses.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne suis pas sûr , mais je suis sûr que je vais faire ça .\u001b[0;0m\n",
            "\u001b[0mÇa semble être une chose assez cool à faire, je suis assez décontracté aussi.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Yea. Je veux dire, le monde bouge si vite, parfois vous devez juste prendre du recul et laisser le passé s'accomplir.\u001b[0;0m\n",
            "\u001b[0;95m     model: oui , c ' est vrai .\u001b[0;0m\n"
          ]
        }
      ],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='french_blended_skill_talk',\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap-cP0uzFF4y",
        "outputId": "d35ec469-4e4a-40ef-f059-ecad54e1ee19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12:46:14 | Opt:\n",
            "12:46:14 |     allow_missing_init_opts: False\n",
            "12:46:14 |     batchsize: 1\n",
            "12:46:14 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "12:46:14 |     datatype: train:ordered\n",
            "12:46:14 |     dict_class: None\n",
            "12:46:14 |     display_add_fields: \n",
            "12:46:14 |     download_path: None\n",
            "12:46:14 |     dynamic_batching: None\n",
            "12:46:14 |     fromfile_datapath: copied_dataset_french_bst/\n",
            "12:46:14 |     fromfile_datatype_extension: True\n",
            "12:46:14 |     hide_labels: False\n",
            "12:46:14 |     ignore_agent_reply: True\n",
            "12:46:14 |     image_cropsize: 224\n",
            "12:46:14 |     image_mode: raw\n",
            "12:46:14 |     image_size: 256\n",
            "12:46:14 |     init_model: None\n",
            "12:46:14 |     init_opt: None\n",
            "12:46:14 |     is_debug: False\n",
            "12:46:14 |     loglevel: info\n",
            "12:46:14 |     max_display_len: 1000\n",
            "12:46:14 |     model: None\n",
            "12:46:14 |     model_file: None\n",
            "12:46:14 |     multitask_weights: [1]\n",
            "12:46:14 |     mutators: None\n",
            "12:46:14 |     num_examples: 20\n",
            "12:46:14 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'copied_dataset_french_bst/', 'fromfile_datatype_extension': True, 'num_examples': 20}\"\n",
            "12:46:14 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "12:46:14 |     starttime: May24_12-46\n",
            "12:46:14 |     task: fromfile:parlaiformat\n",
            "12:46:14 |     verbose: False\n",
            "12:46:14 | creating task(s): fromfile:parlaiformat\n",
            "12:46:14 | Loading ParlAI text data: copied_dataset_french_bst/_train.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: copied_dataset_french_bst/_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mJ'aime la musique live, c'est pourquoi j'essaie d'aller aux concerts.\u001b[0;0m\n",
            "   \u001b[1;94mMoi aussi. Qu'est-ce que tu aimes ?\u001b[0;0m\n",
            "\u001b[0mJ'aime jouer la comédie, j'espère être un acteur, et vous ?\u001b[0;0m\n",
            "   \u001b[1;94mC'est bon. Vous avez des enfants ?\u001b[0;0m\n",
            "\u001b[0mNon, mais un jour.\u001b[0;0m\n",
            "   \u001b[1;94mc'est bien. J'ai 2\u001b[0;0m\n",
            "\u001b[0mLorsque j'aurai terminé mes études, je compte fonder une famille.\u001b[0;0m\n",
            "   \u001b[1;94mc'est génial ! tu seras prête\u001b[0;0m\n",
            "\u001b[0mJe l'espère, quel âge ont vos enfants ?\u001b[0;0m\n",
            "   \u001b[1;94m5 & 7. Ils me prennent beaucoup de temps.\u001b[0;0m\n",
            "\u001b[0mJ'imagine. Je suis sûr qu'ils sont de grands enfants.\u001b[0;0m\n",
            "   \u001b[1;94mheureusement, ils aiment les fleurs tout autant que moi. Nous passons beaucoup de temps dans le jardin.\u001b[0;0m\n",
            "\u001b[0mJ'aimerais avoir plus de temps pour faire ce genre de choses. L'école de médecine est épuisante. \u001b[0;0m\n",
            "   \u001b[1;94mOn dirait bien. As-tu trouvé un travail d'actrice, cependant ?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: copied_dataset_french_bst/_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mMoi aussi. Qu'est-ce que tu aimes ?\u001b[0;0m\n",
            "   \u001b[1;94mJ'aime jouer la comédie, j'espère être un acteur, et vous ?\u001b[0;0m\n",
            "\u001b[0mC'est bon. Vous avez des enfants ?\u001b[0;0m\n",
            "   \u001b[1;94mNon, mais un jour.\u001b[0;0m\n",
            "\u001b[0mc'est bien. J'ai 2\u001b[0;0m\n",
            "   \u001b[1;94mLorsque j'aurai terminé mes études, je compte fonder une famille.\u001b[0;0m\n",
            "\u001b[0mc'est génial ! tu seras prête\u001b[0;0m\n",
            "   \u001b[1;94mJe l'espère, quel âge ont vos enfants ?\u001b[0;0m\n",
            "\u001b[0m5 & 7. Ils me prennent beaucoup de temps.\u001b[0;0m\n",
            "   \u001b[1;94mJ'imagine. Je suis sûr qu'ils sont de grands enfants.\u001b[0;0m\n",
            "\u001b[0mheureusement, ils aiment les fleurs tout autant que moi. Nous passons beaucoup de temps dans le jardin.\u001b[0;0m\n",
            "   \u001b[1;94mJ'aimerais avoir plus de temps pour faire ce genre de choses. L'école de médecine est épuisante. \u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: copied_dataset_french_bst/_train.txt - - -\u001b[0;0m\n",
            "\u001b[0mOh, j'adore les lasagnes. Je fais mes propres nouilles ainsi que la sauce. \u001b[0;0m\n",
            "   \u001b[1;94mWow.  C'est incroyable.  J'ai lu que les lasagnes sont nées en Italie au Moyen-âge.  \u001b[0;0m\n",
            "\u001b[0mOh vraiment ? C'est intéressant. En fait, je suis moi-même italien.\u001b[0;0m\n",
            "   \u001b[1;94mGénial. Moi et mon partenaire venons d'acheter une maison. Je suis impatient de cuisiner dans ma cuisine.\u001b[0;0m\n",
            "\u001b[0mDéménager dans un nouvel endroit peut être très amusant. Êtes-vous un bon cuisinier ?\u001b[0;0m\n",
            "   \u001b[1;94mJ'aime à le penser. J'aime aussi faire du café pour le plaisir après le repas.\u001b[0;0m\n",
            "\u001b[0mMmm. Ça a l'air délicieux en ce moment.\u001b[0;0m\n",
            "   \u001b[1;94mQu'est-ce que vous aimez faire ?\u001b[0;0m\n",
            "\u001b[0mEh bien j'aime les tatouages et les piercings, je travaille sur mon prochain en ce moment.\u001b[0;0m\n",
            "   \u001b[1;94mLes piercings sont cool. Mais je n'ai pas de tatouages. J'ai trop peur. Je veux en avoir\u001b[0;0m\n",
            "\u001b[0mQue prendriez-vous ?\u001b[0;0m\n",
            "   \u001b[1;94mPeut-être quelque chose pour mes enfants. J'ai toujours voulu un symbole d'anarchie.\u001b[0;0m\n",
            "\u001b[0mHaha c'est une idée cool.\u001b[0;0m\n",
            "   \u001b[1;94mJ'aime penser que je suis cool aussi. Avec un peu de chance, un jour.\u001b[0;0m\n",
            "12:46:15 | loaded 9638 episodes with a total of 59700 examples\n"
          ]
        }
      ],
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    # task='french_blended_skill_talk',\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "    datatype= \"test\",\n",
        "\n",
        "    model_file= f'{finetuned_model_path}/model',\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    # # the result of grid search on 400M model and BST dataset when inference=topk\n",
        "    # beam_block_ngram= 2,\n",
        "\t# beam_context_block_ngram= 3,\n",
        "\t# beam_length_penalty= 1,\n",
        "\t# beam_min_length= 10,\n",
        "\t# beam_size= 20,\n",
        "\t# inference= \"topk\",\n",
        "\t# temperature= 0.5,\n",
        "\t# topk= 20,\n",
        "\t# topp= 0.9\n",
        "\n",
        "    # # Farnaz sent me\n",
        "    # beam_block_ngram= 3,\n",
        "    # beam_context_block_ngram= 3,\n",
        "    # beam_min_length= 20, \n",
        "    # beam_size= 10,\n",
        "    # inference =  'topk',  \n",
        "    # topk=20, \n",
        "    # temperature = 0.5, \n",
        "    # beam_length_penalty=0.8\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "blender-90m-400m-multitask-double-sided.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPNGICSWefqYuR5tjkw2ktM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}