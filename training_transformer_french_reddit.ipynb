{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirpaia/blenderbot/blob/main/training_transformer_french_reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installating kaggle and download the dataset\n",
        "\n",
        "please upload kaggle.json"
      ],
      "metadata": {
        "id": "h89rB0oWNEBN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9h93bjSvpgu",
        "outputId": "3f6bc3b6-5243-40ea-911a-518890037665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ],
      "source": [
        "# download from kaggle\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjEeFwwc3gT9",
        "outputId": "a612af64-19fc-4859-b3e0-42597ed52a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading french-reddit-discussion.zip to /content\n",
            " 96% 409M/426M [00:03<00:00, 144MB/s]\n",
            "100% 426M/426M [00:03<00:00, 145MB/s]\n",
            "Archive:  french-reddit-discussion.zip\n",
            "  inflating: final_SPF_2.xml         \n",
            "  inflating: spf.tar.gz              \n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets download breandan/french-reddit-discussion\n",
        "! unzip french-reddit-discussion.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "vlBcjN_zNYdv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "BCabG4Dx3ibs",
        "outputId": "a52be405-7336-4541-8d6b-8185f0c34be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of conversations:  556622\n",
            "number of comments:  1583083\n",
            "All done in : 25.800025463104248  seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  link_id subreddit_id      uid comment_id score parent_id  create_utc  \\\n",
              "0   8r1kz        2qhjz  1688932    c0a62uj     3     8r1kz  1244576002   \n",
              "1   8r1kz        2qhjz   786883    c0a6lmb     1   c0a62uj  1244621120   \n",
              "2   8sncs        2qhjz   390497    c0aawpk     1     8sncs  1245076061   \n",
              "3   8sncs        2qhjz    32884    c0aaxba     3   c0aawpk  1245077396   \n",
              "4   8v13c        2qhjz   796919    c0aj3ov     2     8v13c  1245830384   \n",
              "\n",
              "                                                text  \n",
              "0  Ironie : l'article disant qu'on est plus capab...  \n",
              "1  Moi-même, j'ai dû me forcer pour arriver jusqu...  \n",
              "2  Service qui sera rendu au contribuable pour la...  \n",
              "3  Eeeeh oui ! 70 millions pour une loi qui aura ...  \n",
              "4  Est-ce qu'elle a vraiment commis des crimes qu...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ddec0fcd-75aa-46dd-9a19-49545167c5f4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link_id</th>\n",
              "      <th>subreddit_id</th>\n",
              "      <th>uid</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>score</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>create_utc</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8r1kz</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1688932</td>\n",
              "      <td>c0a62uj</td>\n",
              "      <td>3</td>\n",
              "      <td>8r1kz</td>\n",
              "      <td>1244576002</td>\n",
              "      <td>Ironie : l'article disant qu'on est plus capab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8r1kz</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>786883</td>\n",
              "      <td>c0a6lmb</td>\n",
              "      <td>1</td>\n",
              "      <td>c0a62uj</td>\n",
              "      <td>1244621120</td>\n",
              "      <td>Moi-même, j'ai dû me forcer pour arriver jusqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8sncs</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>390497</td>\n",
              "      <td>c0aawpk</td>\n",
              "      <td>1</td>\n",
              "      <td>8sncs</td>\n",
              "      <td>1245076061</td>\n",
              "      <td>Service qui sera rendu au contribuable pour la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8sncs</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>32884</td>\n",
              "      <td>c0aaxba</td>\n",
              "      <td>3</td>\n",
              "      <td>c0aawpk</td>\n",
              "      <td>1245077396</td>\n",
              "      <td>Eeeeh oui ! 70 millions pour une loi qui aura ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8v13c</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>796919</td>\n",
              "      <td>c0aj3ov</td>\n",
              "      <td>2</td>\n",
              "      <td>8v13c</td>\n",
              "      <td>1245830384</td>\n",
              "      <td>Est-ce qu'elle a vraiment commis des crimes qu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ddec0fcd-75aa-46dd-9a19-49545167c5f4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ddec0fcd-75aa-46dd-9a19-49545167c5f4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ddec0fcd-75aa-46dd-9a19-49545167c5f4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import time \n",
        "import lxml.etree as ET\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "file_path = 'final_SPF_2.xml'\n",
        "start=time.time()\n",
        "#Initializes the parser\n",
        "parser = ET.XMLParser(recover=True)\n",
        "#Parses the file\n",
        "tree = ET.parse(file_path, parser=parser)\n",
        "xroot = tree.getroot()\n",
        "\n",
        "#One conversation -> one line in the data array\n",
        "dfcols = ['link_id', 'subreddit_id', 'uid',\"comment_id\",'score', 'parent_id', 'create_utc', 'text']\n",
        "data =np.array(\n",
        "    (\n",
        "        [\n",
        "            [\n",
        "                [\n",
        "                    node.attrib.get('link_id'),\n",
        "                    node.attrib.get('subreddit_id'), \n",
        "                    node.getchildren()[j].get('uid'), \n",
        "                    node.getchildren()[j].get('comment_id'), \n",
        "                    node.getchildren()[j].get('score'), \n",
        "                    node.getchildren()[j].get('parent_id'), \n",
        "                    node.getchildren()[j].get('create_utc'),\n",
        "                    node.getchildren()[j].text\n",
        "                ] \n",
        "                for j in range(len(node.getchildren()))\n",
        "            ] \n",
        "            for node in xroot\n",
        "        ]\n",
        "     ), \n",
        "    dtype=object)\n",
        "\n",
        "print('number of conversations: ', data.shape[0])\n",
        "\n",
        "#one comments -> one line in the data array\n",
        "data=np.array([liste for conversation in data for liste in conversation], dtype=object)\n",
        "print('number of comments: ',data.shape[0])\n",
        "\n",
        "df_xml = pd.DataFrame(data=data, columns=dfcols)\n",
        "print('All done in :',time.time()-start,' seconds')\n",
        "\n",
        "df_xml.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# df_xml[df_xml['comment_id'] == \"c4ojtph\"]\n",
        "df_xml[df_xml['link_id'] == \"tpa2b\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qdMvmfb-Skfq",
        "outputId": "a591c377-8791-4848-e86e-f1d5b2a2c518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        link_id subreddit_id      uid comment_id score parent_id  create_utc  \\\n",
              "6390      tpa2b        2qhjz  1191098    c4ojtph     3     tpa2b  1337135281   \n",
              "6391      tpa2b        2qhjz  1160192    c4ok1v0     7   c4ojtph  1337136321   \n",
              "6392      tpa2b        2qhjz  1304107    c4onq32     2   c4ok1v0  1337159142   \n",
              "6393      tpa2b        2qhjz  1327014    c4opt7o     2   c4onq32  1337178071   \n",
              "6394      tpa2b        2qhjz  1139155    c4oyopd     2   c4opt7o  1337219286   \n",
              "6395      tpa2b        2qhjz  1528670    c4p0jla     1   c4oyopd  1337228203   \n",
              "6396      tpa2b        2qhjz   335827    c4okz7t     9     tpa2b  1337140473   \n",
              "6397      tpa2b        2qhjz  2021690    c4oqn3r     2   c4okz7t  1337182168   \n",
              "6398      tpa2b        2qhjz    88967    c4oljp9     5     tpa2b  1337143061   \n",
              "6399      tpa2b        2qhjz  1756539    c4oloey     2   c4oljp9  1337143682   \n",
              "6400      tpa2b        2qhjz  1624247    c4olqh9     5   c4oloey  1337143958   \n",
              "6401      tpa2b        2qhjz  1010844    c4omkka     5     tpa2b  1337148535   \n",
              "6402      tpa2b        2qhjz  1116781    c4opa36     5   c4omkka  1337175035   \n",
              "6405      tpa2b        2qhjz  1503874    c4on4hk     2     tpa2b  1337152670   \n",
              "6406      tpa2b        2qhjz  1830219    c4opbvh     3   c4on4hk  1337175342   \n",
              "6420      tpa2b        2qhjz  1170390    c4ondh6    -3     tpa2b  1337155078   \n",
              "6421      tpa2b        2qhjz    27130    c4opodb     1   c4ondh6  1337177348   \n",
              "6422      tpa2b        2qhjz  1424603    c4opun7     2   c4opodb  1337178277   \n",
              "6423      tpa2b        2qhjz   453916    c4oqb3f     2   c4opun7  1337180580   \n",
              "6424      tpa2b        2qhjz   722098    c4otv5r     2   c4oqb3f  1337196653   \n",
              "6427      tpa2b        2qhjz  1571383    c4onmf9     4     tpa2b  1337157874   \n",
              "6428      tpa2b        2qhjz  1701699    c4uehf8     1   c4onmf9  1338439163   \n",
              "6429      tpa2b        2qhjz   673756    c4uigss     1   c4uehf8  1338472437   \n",
              "6430      tpa2b        2qhjz  1440945    c4uiqdk     1   c4uigss  1338473916   \n",
              "6437      tpa2b        2qhjz   639517    c4onscx     4     tpa2b  1337159930   \n",
              "6438      tpa2b        2qhjz   736476    c4op8l4     1   c4onscx  1337174777   \n",
              "6439      tpa2b        2qhjz   623007    c4oo370     0     tpa2b  1337164071   \n",
              "6440      tpa2b        2qhjz   827166    c4opf33     6   c4oo370  1337175872   \n",
              "6449      tpa2b        2qhjz  1068367    c4oofq6     2   c4ooeu2  1337168208   \n",
              "6450      tpa2b        2qhjz   927512    c4ooguk    -1   c4oofq6  1337168532   \n",
              "6451      tpa2b        2qhjz  1245167    c4oofsh     1   c4oo4vb  1337168230   \n",
              "6452      tpa2b        2qhjz   372557    c4ooi4i     2   c4oofsh  1337168885   \n",
              "6453      tpa2b        2qhjz  1622271    c4ooj5y     2   c4ooi4i  1337169185   \n",
              "6454      tpa2b        2qhjz  2023409    c4ookct     5   c4ooj5y  1337169516   \n",
              "6455      tpa2b        2qhjz  1432895    c4oofwf     2     tpa2b  1337168261   \n",
              "6456      tpa2b        2qhjz   306622    c4opfpb     2   c4oofwf  1337175967   \n",
              "6457      tpa2b        2qhjz  1143420    c4opy15     2   c4opfpb  1337178761   \n",
              "6458      tpa2b        2qhjz  1195142    c4osgsh     2   c4opy15  1337190469   \n",
              "6459      tpa2b        2qhjz    12449    c4oojca    13     tpa2b  1337169235   \n",
              "6460      tpa2b        2qhjz  1459212    c4op75p     5   c4oojca  1337174518   \n",
              "6461      tpa2b        2qhjz   994562    c4opnls     2   c4oorbx  1337177229   \n",
              "6462      tpa2b        2qhjz   216789    c4otqs0     4   c4opnls  1337196124   \n",
              "6470      tpa2b        2qhjz  1033553    c4ore4f     6   c4opwqk  1337185631   \n",
              "6471      tpa2b        2qhjz   208543    c4osdll     5   c4ore4f  1337190079   \n",
              "792652    tpa2b        2qhjz  1850872    c4omrzr     0   c4ok1v0  1337149892   \n",
              "792653    tpa2b        2qhjz   352162    c4onan0     6   c4omrzr  1337154248   \n",
              "792654    tpa2b        2qhjz  1818038    c4oofjh     3   c4onan0  1337168159   \n",
              "792655    tpa2b        2qhjz  1247297    c4ordnu     2   c4opt7o  1337185575   \n",
              "792656    tpa2b        2qhjz  1795394    c4ormsz     2   c4ordnu  1337186737   \n",
              "792657    tpa2b        2qhjz  1630381    c4osccy     2   c4ormsz  1337189927   \n",
              "792658    tpa2b        2qhjz  1457203    c4oyv4q     2   c4oljp9  1337220032   \n",
              "792659    tpa2b        2qhjz  1783294    c4oz1mm     2   c4oyv4q  1337220843   \n",
              "792660    tpa2b        2qhjz   972241    c4oznwz     2   c4oz1mm  1337223761   \n",
              "792669    tpa2b        2qhjz  1176822    c4opda6     4   c4onmf9  1337175577   \n",
              "792670    tpa2b        2qhjz  2034198    c4orar6     3   c4opda6  1337185193   \n",
              "792673    tpa2b        2qhjz  1477540    c4oolcj     9   c4oo370  1337169791   \n",
              "792674    tpa2b        2qhjz  1023086    c4oqq6j     3   c4oolcj  1337182565   \n",
              "792675    tpa2b        2qhjz  1041834    c4osvqy     2   c4oqq6j  1337192294   \n",
              "792676    tpa2b        2qhjz  1314138    c4opeh1     1   c4oo370  1337175770   \n",
              "792677    tpa2b        2qhjz  1249212    c4oqpni     3   c4opeh1  1337182499   \n",
              "792678    tpa2b        2qhjz  1668791    c4opmng     3   c4oofwf  1337177075   \n",
              "792679    tpa2b        2qhjz   704157    c4opurx     2   c4opmng  1337178297   \n",
              "792680    tpa2b        2qhjz   421715    c4oq0hi     3   c4opurx  1337179115   \n",
              "1250551   tpa2b        2qhjz   206574    c4omtlf     3   c4omrzr  1337150217   \n",
              "1250552   tpa2b        2qhjz  2025831    c4oo7dx     9   c4omtlf  1337165543   \n",
              "1250553   tpa2b        2qhjz   685441    c4otn3c     2   c4oo7dx  1337195686   \n",
              "1250554   tpa2b        2qhjz   484474    c4oo0m1     2   c4omrzr  1337163111   \n",
              "1250555   tpa2b        2qhjz  1934325    c4orj08     1   c4oo0m1  1337186242   \n",
              "1250556   tpa2b        2qhjz  1012874    c4ortx9     2   c4orj08  1337187635   \n",
              "1250559   tpa2b        2qhjz  2032046    c4ope3z     5   c4opda6  1337175711   \n",
              "1250560   tpa2b        2qhjz   633289    c4oppu1     2   c4ope3z  1337177562   \n",
              "1250561   tpa2b        2qhjz   797523    c4or7kz     4   c4oppu1  1337184779   \n",
              "1250562   tpa2b        2qhjz   156120    c4orb3m     2   c4or7kz  1337185238   \n",
              "1250563   tpa2b        2qhjz  1357288    c4oyj1a     2   c4orb3m  1337218637   \n",
              "1250564   tpa2b        2qhjz   740853    c4opiaa     5   c4opda6  1337176388   \n",
              "1250565   tpa2b        2qhjz   499072    c4opq4y     2   c4opiaa  1337177608   \n",
              "1447554   tpa2b        2qhjz   627544    c4oqp0a     1   c4omtlf  1337182410   \n",
              "1447555   tpa2b        2qhjz    65631    c4otmba     2   c4oqp0a  1337195591   \n",
              "1447556   tpa2b        2qhjz  1526535    c4owpgd     0   c4otmba  1337209528   \n",
              "1447557   tpa2b        2qhjz  1151518    c4owszz     3   c4owpgd  1337210025   \n",
              "1447558   tpa2b        2qhjz   898276    c4owzch     1   c4owszz  1337210915   \n",
              "1447559   tpa2b        2qhjz   511050    c4oynrl     1   c4owzch  1337219178   \n",
              "1447560   tpa2b        2qhjz  1897779    c4p31fv     1   c4orj08  1337251166   \n",
              "1447561   tpa2b        2qhjz  1410618    c4p7kla     1   c4p31fv  1337278658   \n",
              "1447564   tpa2b        2qhjz   223138    c4osjok     2   c4orb3m  1337190817   \n",
              "1447565   tpa2b        2qhjz  1357373    c4oyjns     2   c4osjok  1337218715   \n",
              "\n",
              "                                                      text  \n",
              "6390     Francais vivant dans le WI depuis 4 ans. Ou vi...  \n",
              "6391     Ce que j'ai trouvé surtout marrant, c'est que ...  \n",
              "6392     Tu/vous pouvez développer sur l'humour?\\n\\nSi ...  \n",
              "6393     En comédie : y'a de tout, et pour tout le mond...  \n",
              "6394     Moi je me suis toujours demande comment un ric...  \n",
              "6395     C'est vrai. Ceci dit j'ai converti ma femme à ...  \n",
              "6396     Disons qu'on prend souvent au serieux ce que l...  \n",
              "6397     Et le nacho / queso cheese. Putain, la premièr...  \n",
              "6398     Français vivant dans le CT depuis 2005. J'ai b...  \n",
              "6399     Hmmm. Oui je crois qu'il est un peu difficile ...  \n",
              "6400     Ma femme en a sa claque des US (oui oui elle e...  \n",
              "6401     Français vivant en Californie, j'ai remarqué q...  \n",
              "6402     Oui. Ils me demandent de parler français, quel...  \n",
              "6405     As-tu trouvé un boulot facilement là bas? T'as...  \n",
              "6406     Je suis dévelopeur et designer web, je n'ai pa...  \n",
              "6420     Ils sont pas un peu relou avec leur Jésus? Tou...  \n",
              "6421     Seulement si tu y paies attention. La religion...  \n",
              "6422     Rien à voir mais \"payer attention\" ça se dit v...  \n",
              "6423     Euh, je crois que ça se dit, non ? C'est bizar...  \n",
              "6424     Non, je suis quasiment certain que ça se dit p...  \n",
              "6427     Je déménage le mois prochain dans le New Jerse...  \n",
              "6428     Salut, je profite rapidement du thread pour te...  \n",
              "6429     Pas de piston du tout! http://www.civiweb.com ...  \n",
              "6430     Merci pour ta réponse. J'aimerais bien faire u...  \n",
              "6437     Finale de conférence OC - Spurs tu soutiendras...  \n",
              "6438     Je ne suis pas du tout le basket. Ceci dit, le...  \n",
              "6439     J'habite depuis 1992 aux Etats-unis. Yep. It's...  \n",
              "6440     Moi je m'appelle expatrié parce que je compte ...  \n",
              "6449                               Non, sérieux? /facepalm  \n",
              "6450     Haha ça méritait le facepalm ouais :D\\n\\nC'est...  \n",
              "6451                Et tu crois que je ne le sais pas?\\n\\n  \n",
              "6452     Ben justment...comment des américains juifs pe...  \n",
              "6453     Les américains que je connais ne sont pas relo...  \n",
              "6454     Je dois avouer que j'ai aussi mis du temps à c...  \n",
              "6455     Je suis francais, née en Martinique et vivant ...  \n",
              "6456     Il y a un paquet de choses qui font que je n'a...  \n",
              "6457     As-tu pensé au Canada ? Et pas nécessairement ...  \n",
              "6458     Oui, ma copine est américaine et on envisage l...  \n",
              "6459     AMA francais: répond a 2 questions, sort prend...  \n",
              "6460     Hahaha, désolé, je ne m'en suis rendu compte q...  \n",
              "6461     On trouve ici pas mal de fromage français, imp...  \n",
              "6462     Solution à tous les problèmes d'empoisonnement...  \n",
              "6470     Et Coluche !! N'oublie pas Coluche et son pote...  \n",
              "6471     Bien sûr mais je parlais des vivants :) Sinon ...  \n",
              "792652   En tant que ricain, oui, le sense de l'humour ...  \n",
              "792653   Euh plaît-il?\\n\\nExcuse moi mais en prenant un...  \n",
              "792654   As-tu regardé plus que quelques épisodes de la...  \n",
              "792655   C'est marrant, on reconnait que tu es aux US d...  \n",
              "792656   C'est clair, je me JC Van Damne-ise jour après...  \n",
              "792657   Je me doute bien :)\\n\\nJ'ai passé quelques sem...  \n",
              "792658   Je pourrais m'etendre la dessus mais je ne sui...  \n",
              "792659   Bah ça fait presque 7 ans que je suis ici et j...  \n",
              "792660   Ba ouais, je suis exactement dans le meme cas....  \n",
              "792669   Etablie un contact en France pour t'envoyer de...  \n",
              "792670   Je me fais envoyer de la crème de marron de te...  \n",
              "792673   Faut arrêter de voir du racisme dans tous les ...  \n",
              "792674   Je le sais bien. Mais utiliser le terme \"expat...  \n",
              "792675   Je suis d'accord avec toi. Je n'y avais pas vr...  \n",
              "792676   J'utilise parfois immigré, parfois expatrié po...  \n",
              "792677   Rocky point, NY. Depuis 5 ans. 10 ans tout ron...  \n",
              "792678   Le travail : ça dépend de ton domaine. Si t'es...  \n",
              "792679   Merci pour tes réponses ; je suis moi-même dév...  \n",
              "792680   Bouger aux \"USA\" est trop vague. C'est un gran...  \n",
              "1250551  Dis le mec qui vient d'un pays qui produit des...  \n",
              "1250552  Toi t'as peur. Descendre Louis CK sur reddit, ...  \n",
              "1250553  Haha, ouais j'ai un problème avec tous les sta...  \n",
              "1250554  En effet, l'humour français est à son plus bas...  \n",
              "1250555  existe-t'il un équivalent actuel de reiser, co...  \n",
              "1250556  Je dirais que ce qui s'en rapproche le plus c'...  \n",
              "1250559     C'est plus le pain qui va me manquer je pense!  \n",
              "1250560  Si tu veux une baguette, c'est clair, tu vas p...  \n",
              "1250561  Alors je dois reconnaître un truc, je ne suis ...  \n",
              "1250562  T'as essayé Whole Foods ? Ils ont un camembert...  \n",
              "1250563  Faut que j'essai Whole Food. Moi j'ai esayer u...  \n",
              "1250564  Astuce : on peut aussi apprendre à faire la ma...  \n",
              "1250565  Je sais, je sais... Je cuisine tous les jours ...  \n",
              "1447554  Je suis vraiment curieux : pourquoi dis-tu que...  \n",
              "1447555  Je dirais que j'ai un problème avec presque to...  \n",
              "1447556  Personnellement, j'aime encore bien Colbert et...  \n",
              "1447557  La différence restant que personne ne dit vrai...  \n",
              "1447558  Tout à fait d'accord. C'est d'ailleurs les seu...  \n",
              "1447559  C\"est vrai que les late shows ont tous la meme...  \n",
              "1447560  non c'est fini tout ça, et Internet n'apporte ...  \n",
              "1447561  c'est dommage. en tout cas il ne faut pas sure...  \n",
              "1447564  Putin j'en parlais à des potes, la charcuterie...  \n",
              "1447565  Oh pitain oui charcuterie. La maintenant tout ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-db97ad6b-44df-4d04-9af1-826a600d4ef6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link_id</th>\n",
              "      <th>subreddit_id</th>\n",
              "      <th>uid</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>score</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>create_utc</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6390</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1191098</td>\n",
              "      <td>c4ojtph</td>\n",
              "      <td>3</td>\n",
              "      <td>tpa2b</td>\n",
              "      <td>1337135281</td>\n",
              "      <td>Francais vivant dans le WI depuis 4 ans. Ou vi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6391</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1160192</td>\n",
              "      <td>c4ok1v0</td>\n",
              "      <td>7</td>\n",
              "      <td>c4ojtph</td>\n",
              "      <td>1337136321</td>\n",
              "      <td>Ce que j'ai trouvé surtout marrant, c'est que ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6392</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1304107</td>\n",
              "      <td>c4onq32</td>\n",
              "      <td>2</td>\n",
              "      <td>c4ok1v0</td>\n",
              "      <td>1337159142</td>\n",
              "      <td>Tu/vous pouvez développer sur l'humour?\\n\\nSi ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6393</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1327014</td>\n",
              "      <td>c4opt7o</td>\n",
              "      <td>2</td>\n",
              "      <td>c4onq32</td>\n",
              "      <td>1337178071</td>\n",
              "      <td>En comédie : y'a de tout, et pour tout le mond...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6394</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1139155</td>\n",
              "      <td>c4oyopd</td>\n",
              "      <td>2</td>\n",
              "      <td>c4opt7o</td>\n",
              "      <td>1337219286</td>\n",
              "      <td>Moi je me suis toujours demande comment un ric...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6395</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1528670</td>\n",
              "      <td>c4p0jla</td>\n",
              "      <td>1</td>\n",
              "      <td>c4oyopd</td>\n",
              "      <td>1337228203</td>\n",
              "      <td>C'est vrai. Ceci dit j'ai converti ma femme à ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6396</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>335827</td>\n",
              "      <td>c4okz7t</td>\n",
              "      <td>9</td>\n",
              "      <td>tpa2b</td>\n",
              "      <td>1337140473</td>\n",
              "      <td>Disons qu'on prend souvent au serieux ce que l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6397</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>2021690</td>\n",
              "      <td>c4oqn3r</td>\n",
              "      <td>2</td>\n",
              "      <td>c4okz7t</td>\n",
              "      <td>1337182168</td>\n",
              "      <td>Et le nacho / queso cheese. Putain, la premièr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6398</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>88967</td>\n",
              "      <td>c4oljp9</td>\n",
              "      <td>5</td>\n",
              "      <td>tpa2b</td>\n",
              "      <td>1337143061</td>\n",
              "      <td>Français vivant dans le CT depuis 2005. J'ai b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6399</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1756539</td>\n",
              "      <td>c4oloey</td>\n",
              "      <td>2</td>\n",
              "      <td>c4oljp9</td>\n",
              "      <td>1337143682</td>\n",
              "      <td>Hmmm. Oui je crois qu'il est un peu difficile ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6400</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1624247</td>\n",
              "      <td>c4olqh9</td>\n",
              "      <td>5</td>\n",
              "      <td>c4oloey</td>\n",
              "      <td>1337143958</td>\n",
              "      <td>Ma femme en a sa claque des US (oui oui elle e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6401</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1010844</td>\n",
              "      <td>c4omkka</td>\n",
              "      <td>5</td>\n",
              "      <td>tpa2b</td>\n",
              "      <td>1337148535</td>\n",
              "      <td>Français vivant en Californie, j'ai remarqué q...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6402</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1116781</td>\n",
              "      <td>c4opa36</td>\n",
              "      <td>5</td>\n",
              "      <td>c4omkka</td>\n",
              "      <td>1337175035</td>\n",
              "      <td>Oui. Ils me demandent de parler français, quel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6405</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1503874</td>\n",
              "      <td>c4on4hk</td>\n",
              "      <td>2</td>\n",
              "      <td>tpa2b</td>\n",
              "      <td>1337152670</td>\n",
              "      <td>As-tu trouvé un boulot facilement là bas? T'as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6406</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1830219</td>\n",
              "      <td>c4opbvh</td>\n",
              "      <td>3</td>\n",
              "      <td>c4on4hk</td>\n",
              "      <td>1337175342</td>\n",
              "      <td>Je suis dévelopeur et designer web, je n'ai pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6420</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1170390</td>\n",
              "      <td>c4ondh6</td>\n",
              "      <td>-3</td>\n",
              "      <td>tpa2b</td>\n",
              "      <td>1337155078</td>\n",
              "      <td>Ils sont pas un peu relou avec leur Jésus? Tou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6421</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>27130</td>\n",
              "      <td>c4opodb</td>\n",
              "      <td>1</td>\n",
              "      <td>c4ondh6</td>\n",
              "      <td>1337177348</td>\n",
              "      <td>Seulement si tu y paies attention. La religion...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6422</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1424603</td>\n",
              "      <td>c4opun7</td>\n",
              "      <td>2</td>\n",
              "      <td>c4opodb</td>\n",
              "      <td>1337178277</td>\n",
              "      <td>Rien à voir mais \"payer attention\" ça se dit v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6423</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>453916</td>\n",
              "      <td>c4oqb3f</td>\n",
              "      <td>2</td>\n",
              "      <td>c4opun7</td>\n",
              "      <td>1337180580</td>\n",
              "      <td>Euh, je crois que ça se dit, non ? C'est bizar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6424</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>722098</td>\n",
              "      <td>c4otv5r</td>\n",
              "      <td>2</td>\n",
              "      <td>c4oqb3f</td>\n",
              "      <td>1337196653</td>\n",
              "      <td>Non, je suis quasiment certain que ça se dit p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6427</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1571383</td>\n",
              "      <td>c4onmf9</td>\n",
              "      <td>4</td>\n",
              "      <td>tpa2b</td>\n",
              "      <td>1337157874</td>\n",
              "      <td>Je déménage le mois prochain dans le New Jerse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6428</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1701699</td>\n",
              "      <td>c4uehf8</td>\n",
              "      <td>1</td>\n",
              "      <td>c4onmf9</td>\n",
              "      <td>1338439163</td>\n",
              "      <td>Salut, je profite rapidement du thread pour te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6429</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>673756</td>\n",
              "      <td>c4uigss</td>\n",
              "      <td>1</td>\n",
              "      <td>c4uehf8</td>\n",
              "      <td>1338472437</td>\n",
              "      <td>Pas de piston du tout! http://www.civiweb.com ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6430</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1440945</td>\n",
              "      <td>c4uiqdk</td>\n",
              "      <td>1</td>\n",
              "      <td>c4uigss</td>\n",
              "      <td>1338473916</td>\n",
              "      <td>Merci pour ta réponse. J'aimerais bien faire u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6437</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>639517</td>\n",
              "      <td>c4onscx</td>\n",
              "      <td>4</td>\n",
              "      <td>tpa2b</td>\n",
              "      <td>1337159930</td>\n",
              "      <td>Finale de conférence OC - Spurs tu soutiendras...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6438</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>736476</td>\n",
              "      <td>c4op8l4</td>\n",
              "      <td>1</td>\n",
              "      <td>c4onscx</td>\n",
              "      <td>1337174777</td>\n",
              "      <td>Je ne suis pas du tout le basket. Ceci dit, le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6439</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>623007</td>\n",
              "      <td>c4oo370</td>\n",
              "      <td>0</td>\n",
              "      <td>tpa2b</td>\n",
              "      <td>1337164071</td>\n",
              "      <td>J'habite depuis 1992 aux Etats-unis. Yep. It's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6440</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>827166</td>\n",
              "      <td>c4opf33</td>\n",
              "      <td>6</td>\n",
              "      <td>c4oo370</td>\n",
              "      <td>1337175872</td>\n",
              "      <td>Moi je m'appelle expatrié parce que je compte ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1068367</td>\n",
              "      <td>c4oofq6</td>\n",
              "      <td>2</td>\n",
              "      <td>c4ooeu2</td>\n",
              "      <td>1337168208</td>\n",
              "      <td>Non, sérieux? /facepalm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6450</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>927512</td>\n",
              "      <td>c4ooguk</td>\n",
              "      <td>-1</td>\n",
              "      <td>c4oofq6</td>\n",
              "      <td>1337168532</td>\n",
              "      <td>Haha ça méritait le facepalm ouais :D\\n\\nC'est...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6451</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1245167</td>\n",
              "      <td>c4oofsh</td>\n",
              "      <td>1</td>\n",
              "      <td>c4oo4vb</td>\n",
              "      <td>1337168230</td>\n",
              "      <td>Et tu crois que je ne le sais pas?\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6452</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>372557</td>\n",
              "      <td>c4ooi4i</td>\n",
              "      <td>2</td>\n",
              "      <td>c4oofsh</td>\n",
              "      <td>1337168885</td>\n",
              "      <td>Ben justment...comment des américains juifs pe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6453</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1622271</td>\n",
              "      <td>c4ooj5y</td>\n",
              "      <td>2</td>\n",
              "      <td>c4ooi4i</td>\n",
              "      <td>1337169185</td>\n",
              "      <td>Les américains que je connais ne sont pas relo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6454</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>2023409</td>\n",
              "      <td>c4ookct</td>\n",
              "      <td>5</td>\n",
              "      <td>c4ooj5y</td>\n",
              "      <td>1337169516</td>\n",
              "      <td>Je dois avouer que j'ai aussi mis du temps à c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6455</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1432895</td>\n",
              "      <td>c4oofwf</td>\n",
              "      <td>2</td>\n",
              "      <td>tpa2b</td>\n",
              "      <td>1337168261</td>\n",
              "      <td>Je suis francais, née en Martinique et vivant ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6456</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>306622</td>\n",
              "      <td>c4opfpb</td>\n",
              "      <td>2</td>\n",
              "      <td>c4oofwf</td>\n",
              "      <td>1337175967</td>\n",
              "      <td>Il y a un paquet de choses qui font que je n'a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6457</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1143420</td>\n",
              "      <td>c4opy15</td>\n",
              "      <td>2</td>\n",
              "      <td>c4opfpb</td>\n",
              "      <td>1337178761</td>\n",
              "      <td>As-tu pensé au Canada ? Et pas nécessairement ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6458</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1195142</td>\n",
              "      <td>c4osgsh</td>\n",
              "      <td>2</td>\n",
              "      <td>c4opy15</td>\n",
              "      <td>1337190469</td>\n",
              "      <td>Oui, ma copine est américaine et on envisage l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6459</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>12449</td>\n",
              "      <td>c4oojca</td>\n",
              "      <td>13</td>\n",
              "      <td>tpa2b</td>\n",
              "      <td>1337169235</td>\n",
              "      <td>AMA francais: répond a 2 questions, sort prend...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6460</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1459212</td>\n",
              "      <td>c4op75p</td>\n",
              "      <td>5</td>\n",
              "      <td>c4oojca</td>\n",
              "      <td>1337174518</td>\n",
              "      <td>Hahaha, désolé, je ne m'en suis rendu compte q...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6461</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>994562</td>\n",
              "      <td>c4opnls</td>\n",
              "      <td>2</td>\n",
              "      <td>c4oorbx</td>\n",
              "      <td>1337177229</td>\n",
              "      <td>On trouve ici pas mal de fromage français, imp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6462</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>216789</td>\n",
              "      <td>c4otqs0</td>\n",
              "      <td>4</td>\n",
              "      <td>c4opnls</td>\n",
              "      <td>1337196124</td>\n",
              "      <td>Solution à tous les problèmes d'empoisonnement...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6470</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1033553</td>\n",
              "      <td>c4ore4f</td>\n",
              "      <td>6</td>\n",
              "      <td>c4opwqk</td>\n",
              "      <td>1337185631</td>\n",
              "      <td>Et Coluche !! N'oublie pas Coluche et son pote...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6471</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>208543</td>\n",
              "      <td>c4osdll</td>\n",
              "      <td>5</td>\n",
              "      <td>c4ore4f</td>\n",
              "      <td>1337190079</td>\n",
              "      <td>Bien sûr mais je parlais des vivants :) Sinon ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792652</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1850872</td>\n",
              "      <td>c4omrzr</td>\n",
              "      <td>0</td>\n",
              "      <td>c4ok1v0</td>\n",
              "      <td>1337149892</td>\n",
              "      <td>En tant que ricain, oui, le sense de l'humour ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792653</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>352162</td>\n",
              "      <td>c4onan0</td>\n",
              "      <td>6</td>\n",
              "      <td>c4omrzr</td>\n",
              "      <td>1337154248</td>\n",
              "      <td>Euh plaît-il?\\n\\nExcuse moi mais en prenant un...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792654</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1818038</td>\n",
              "      <td>c4oofjh</td>\n",
              "      <td>3</td>\n",
              "      <td>c4onan0</td>\n",
              "      <td>1337168159</td>\n",
              "      <td>As-tu regardé plus que quelques épisodes de la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792655</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1247297</td>\n",
              "      <td>c4ordnu</td>\n",
              "      <td>2</td>\n",
              "      <td>c4opt7o</td>\n",
              "      <td>1337185575</td>\n",
              "      <td>C'est marrant, on reconnait que tu es aux US d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792656</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1795394</td>\n",
              "      <td>c4ormsz</td>\n",
              "      <td>2</td>\n",
              "      <td>c4ordnu</td>\n",
              "      <td>1337186737</td>\n",
              "      <td>C'est clair, je me JC Van Damne-ise jour après...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792657</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1630381</td>\n",
              "      <td>c4osccy</td>\n",
              "      <td>2</td>\n",
              "      <td>c4ormsz</td>\n",
              "      <td>1337189927</td>\n",
              "      <td>Je me doute bien :)\\n\\nJ'ai passé quelques sem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792658</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1457203</td>\n",
              "      <td>c4oyv4q</td>\n",
              "      <td>2</td>\n",
              "      <td>c4oljp9</td>\n",
              "      <td>1337220032</td>\n",
              "      <td>Je pourrais m'etendre la dessus mais je ne sui...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792659</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1783294</td>\n",
              "      <td>c4oz1mm</td>\n",
              "      <td>2</td>\n",
              "      <td>c4oyv4q</td>\n",
              "      <td>1337220843</td>\n",
              "      <td>Bah ça fait presque 7 ans que je suis ici et j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792660</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>972241</td>\n",
              "      <td>c4oznwz</td>\n",
              "      <td>2</td>\n",
              "      <td>c4oz1mm</td>\n",
              "      <td>1337223761</td>\n",
              "      <td>Ba ouais, je suis exactement dans le meme cas....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792669</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1176822</td>\n",
              "      <td>c4opda6</td>\n",
              "      <td>4</td>\n",
              "      <td>c4onmf9</td>\n",
              "      <td>1337175577</td>\n",
              "      <td>Etablie un contact en France pour t'envoyer de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792670</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>2034198</td>\n",
              "      <td>c4orar6</td>\n",
              "      <td>3</td>\n",
              "      <td>c4opda6</td>\n",
              "      <td>1337185193</td>\n",
              "      <td>Je me fais envoyer de la crème de marron de te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792673</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1477540</td>\n",
              "      <td>c4oolcj</td>\n",
              "      <td>9</td>\n",
              "      <td>c4oo370</td>\n",
              "      <td>1337169791</td>\n",
              "      <td>Faut arrêter de voir du racisme dans tous les ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792674</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1023086</td>\n",
              "      <td>c4oqq6j</td>\n",
              "      <td>3</td>\n",
              "      <td>c4oolcj</td>\n",
              "      <td>1337182565</td>\n",
              "      <td>Je le sais bien. Mais utiliser le terme \"expat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792675</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1041834</td>\n",
              "      <td>c4osvqy</td>\n",
              "      <td>2</td>\n",
              "      <td>c4oqq6j</td>\n",
              "      <td>1337192294</td>\n",
              "      <td>Je suis d'accord avec toi. Je n'y avais pas vr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792676</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1314138</td>\n",
              "      <td>c4opeh1</td>\n",
              "      <td>1</td>\n",
              "      <td>c4oo370</td>\n",
              "      <td>1337175770</td>\n",
              "      <td>J'utilise parfois immigré, parfois expatrié po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792677</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1249212</td>\n",
              "      <td>c4oqpni</td>\n",
              "      <td>3</td>\n",
              "      <td>c4opeh1</td>\n",
              "      <td>1337182499</td>\n",
              "      <td>Rocky point, NY. Depuis 5 ans. 10 ans tout ron...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792678</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1668791</td>\n",
              "      <td>c4opmng</td>\n",
              "      <td>3</td>\n",
              "      <td>c4oofwf</td>\n",
              "      <td>1337177075</td>\n",
              "      <td>Le travail : ça dépend de ton domaine. Si t'es...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792679</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>704157</td>\n",
              "      <td>c4opurx</td>\n",
              "      <td>2</td>\n",
              "      <td>c4opmng</td>\n",
              "      <td>1337178297</td>\n",
              "      <td>Merci pour tes réponses ; je suis moi-même dév...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792680</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>421715</td>\n",
              "      <td>c4oq0hi</td>\n",
              "      <td>3</td>\n",
              "      <td>c4opurx</td>\n",
              "      <td>1337179115</td>\n",
              "      <td>Bouger aux \"USA\" est trop vague. C'est un gran...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250551</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>206574</td>\n",
              "      <td>c4omtlf</td>\n",
              "      <td>3</td>\n",
              "      <td>c4omrzr</td>\n",
              "      <td>1337150217</td>\n",
              "      <td>Dis le mec qui vient d'un pays qui produit des...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250552</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>2025831</td>\n",
              "      <td>c4oo7dx</td>\n",
              "      <td>9</td>\n",
              "      <td>c4omtlf</td>\n",
              "      <td>1337165543</td>\n",
              "      <td>Toi t'as peur. Descendre Louis CK sur reddit, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250553</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>685441</td>\n",
              "      <td>c4otn3c</td>\n",
              "      <td>2</td>\n",
              "      <td>c4oo7dx</td>\n",
              "      <td>1337195686</td>\n",
              "      <td>Haha, ouais j'ai un problème avec tous les sta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250554</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>484474</td>\n",
              "      <td>c4oo0m1</td>\n",
              "      <td>2</td>\n",
              "      <td>c4omrzr</td>\n",
              "      <td>1337163111</td>\n",
              "      <td>En effet, l'humour français est à son plus bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250555</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1934325</td>\n",
              "      <td>c4orj08</td>\n",
              "      <td>1</td>\n",
              "      <td>c4oo0m1</td>\n",
              "      <td>1337186242</td>\n",
              "      <td>existe-t'il un équivalent actuel de reiser, co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250556</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1012874</td>\n",
              "      <td>c4ortx9</td>\n",
              "      <td>2</td>\n",
              "      <td>c4orj08</td>\n",
              "      <td>1337187635</td>\n",
              "      <td>Je dirais que ce qui s'en rapproche le plus c'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250559</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>2032046</td>\n",
              "      <td>c4ope3z</td>\n",
              "      <td>5</td>\n",
              "      <td>c4opda6</td>\n",
              "      <td>1337175711</td>\n",
              "      <td>C'est plus le pain qui va me manquer je pense!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250560</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>633289</td>\n",
              "      <td>c4oppu1</td>\n",
              "      <td>2</td>\n",
              "      <td>c4ope3z</td>\n",
              "      <td>1337177562</td>\n",
              "      <td>Si tu veux une baguette, c'est clair, tu vas p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250561</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>797523</td>\n",
              "      <td>c4or7kz</td>\n",
              "      <td>4</td>\n",
              "      <td>c4oppu1</td>\n",
              "      <td>1337184779</td>\n",
              "      <td>Alors je dois reconnaître un truc, je ne suis ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250562</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>156120</td>\n",
              "      <td>c4orb3m</td>\n",
              "      <td>2</td>\n",
              "      <td>c4or7kz</td>\n",
              "      <td>1337185238</td>\n",
              "      <td>T'as essayé Whole Foods ? Ils ont un camembert...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250563</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1357288</td>\n",
              "      <td>c4oyj1a</td>\n",
              "      <td>2</td>\n",
              "      <td>c4orb3m</td>\n",
              "      <td>1337218637</td>\n",
              "      <td>Faut que j'essai Whole Food. Moi j'ai esayer u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250564</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>740853</td>\n",
              "      <td>c4opiaa</td>\n",
              "      <td>5</td>\n",
              "      <td>c4opda6</td>\n",
              "      <td>1337176388</td>\n",
              "      <td>Astuce : on peut aussi apprendre à faire la ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250565</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>499072</td>\n",
              "      <td>c4opq4y</td>\n",
              "      <td>2</td>\n",
              "      <td>c4opiaa</td>\n",
              "      <td>1337177608</td>\n",
              "      <td>Je sais, je sais... Je cuisine tous les jours ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447554</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>627544</td>\n",
              "      <td>c4oqp0a</td>\n",
              "      <td>1</td>\n",
              "      <td>c4omtlf</td>\n",
              "      <td>1337182410</td>\n",
              "      <td>Je suis vraiment curieux : pourquoi dis-tu que...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447555</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>65631</td>\n",
              "      <td>c4otmba</td>\n",
              "      <td>2</td>\n",
              "      <td>c4oqp0a</td>\n",
              "      <td>1337195591</td>\n",
              "      <td>Je dirais que j'ai un problème avec presque to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447556</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1526535</td>\n",
              "      <td>c4owpgd</td>\n",
              "      <td>0</td>\n",
              "      <td>c4otmba</td>\n",
              "      <td>1337209528</td>\n",
              "      <td>Personnellement, j'aime encore bien Colbert et...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447557</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1151518</td>\n",
              "      <td>c4owszz</td>\n",
              "      <td>3</td>\n",
              "      <td>c4owpgd</td>\n",
              "      <td>1337210025</td>\n",
              "      <td>La différence restant que personne ne dit vrai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447558</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>898276</td>\n",
              "      <td>c4owzch</td>\n",
              "      <td>1</td>\n",
              "      <td>c4owszz</td>\n",
              "      <td>1337210915</td>\n",
              "      <td>Tout à fait d'accord. C'est d'ailleurs les seu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447559</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>511050</td>\n",
              "      <td>c4oynrl</td>\n",
              "      <td>1</td>\n",
              "      <td>c4owzch</td>\n",
              "      <td>1337219178</td>\n",
              "      <td>C\"est vrai que les late shows ont tous la meme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447560</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1897779</td>\n",
              "      <td>c4p31fv</td>\n",
              "      <td>1</td>\n",
              "      <td>c4orj08</td>\n",
              "      <td>1337251166</td>\n",
              "      <td>non c'est fini tout ça, et Internet n'apporte ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447561</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1410618</td>\n",
              "      <td>c4p7kla</td>\n",
              "      <td>1</td>\n",
              "      <td>c4p31fv</td>\n",
              "      <td>1337278658</td>\n",
              "      <td>c'est dommage. en tout cas il ne faut pas sure...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447564</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>223138</td>\n",
              "      <td>c4osjok</td>\n",
              "      <td>2</td>\n",
              "      <td>c4orb3m</td>\n",
              "      <td>1337190817</td>\n",
              "      <td>Putin j'en parlais à des potes, la charcuterie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447565</th>\n",
              "      <td>tpa2b</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1357373</td>\n",
              "      <td>c4oyjns</td>\n",
              "      <td>2</td>\n",
              "      <td>c4osjok</td>\n",
              "      <td>1337218715</td>\n",
              "      <td>Oh pitain oui charcuterie. La maintenant tout ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db97ad6b-44df-4d04-9af1-826a600d4ef6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-db97ad6b-44df-4d04-9af1-826a600d4ef6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-db97ad6b-44df-4d04-9af1-826a600d4ef6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naOQiCsL3pLa",
        "outputId": "c3d777bc-00c1-44a7-e9d9-807dc4fcdae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def transfer_dialog(d):\n",
        "\n",
        "    if len(d)%2 !=0: d = d[:-1]\n",
        "        \n",
        "    t = \"\"\n",
        "    for i in range(0,len(d),2):\n",
        "        u1 = d[i]\n",
        "        u2 = d[i+1]\n",
        "\n",
        "        if (i+2) != len(d):\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "        else:\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "\n",
        "    return t\n",
        "\n",
        "t = ['hello','how are you','good','bye']\n",
        "print(transfer_dialog(t))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## previous data preparation"
      ],
      "metadata": {
        "id": "iS4WNAkWkq4p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9OT-zt03sFN",
        "outputId": "1a539c10-b85b-498f-982d-e26cff420ed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index: 0 number of dialogs: 0\n",
            "index: 10000 number of dialogs: 1551\n",
            "index: 20000 number of dialogs: 2765\n",
            "index: 30000 number of dialogs: 3954\n",
            "index: 40000 number of dialogs: 5205\n",
            "index: 50000 number of dialogs: 6527\n",
            "index: 60000 number of dialogs: 7850\n",
            "index: 66538 number of dialogs: 8850\n",
            "train set: 7080, validation set: 885,test set: 885\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "train_dialogs_in_parlai_format = \"\"\n",
        "valid_dialogs_in_parlai_format = \"\"\n",
        "test_dialogs_in_parlai_format = \"\"\n",
        "number_of_dialogs = len(df_xml.groupby('link_id'))\n",
        "\n",
        "for index, (key, dialog) in enumerate(df_xml.groupby('link_id')):\n",
        "#     print(key, dialog)\n",
        "    sorted_dialog = dialog.values[dialog.values[:, 6].argsort()]\n",
        "    list_of_turns = [turn[7].replace(\"\\n\\n\",\" \").replace(\"\\n\", \"\") for turn in sorted_dialog]\n",
        "    parlai_format = transfer_dialog(list_of_turns)\n",
        "#     print(parlai_format)\n",
        "    if index < number_of_dialogs * 0.8:\n",
        "      train_dialogs_in_parlai_format +=parlai_format\n",
        "    elif index < number_of_dialogs * 0.9:\n",
        "      valid_dialogs_in_parlai_format +=parlai_format\n",
        "    else:\n",
        "      test_dialogs_in_parlai_format +=parlai_format\n",
        "\n",
        "    if index % 10000 == 0: print(index)\n",
        "print(index)\n",
        "\n",
        "# !rm -R fr_reddit_dataset\n",
        "!mkdir fr_reddit_dataset\n",
        "with open(\"fr_reddit_dataset/data_train.txt\",\"w\") as f:\n",
        "    f.write(train_dialogs_in_parlai_format)\n",
        "\n",
        "with open(\"fr_reddit_dataset/data_valid.txt\",\"w\") as f:\n",
        "    f.write(valid_dialogs_in_parlai_format)\n",
        "\n",
        "with open(\"fr_reddit_dataset/data_test.txt\",\"w\") as f:\n",
        "    f.write(test_dialogs_in_parlai_format)\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## new data preparation"
      ],
      "metadata": {
        "id": "Qm7b-GZHkuRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_dialogs_in_parlai_format = []\n",
        "\n",
        "for index, (key, dialog) in enumerate(df_xml.groupby('link_id')):\n",
        "    # if the dialog has two speakers\n",
        "    if (len(set([a[2] for a in dialog.values])) == 2) & \\\n",
        "        (sum([1 for a in dialog.values if (len(a[7]) > 300)]) == 0):\n",
        "        # print(key, dialog)\n",
        "        sorted_dialog = dialog.values[dialog.values[:, 6].argsort()]\n",
        "        list_of_turns = [turn[7].replace(\"\\n\\n\",\" \").replace(\"\\n\", \"\") for turn in sorted_dialog]\n",
        "\n",
        "        parlai_format = transfer_dialog(list_of_turns)\n",
        "        all_dialogs_in_parlai_format.append(parlai_format)\n",
        "        # print(parlai_format)\n",
        "\n",
        "    number_of_dialogs = len(all_dialogs_in_parlai_format)\n",
        "    if index % 10000 == 0: print(f\"index: {index}\", f\"number of dialogs: {number_of_dialogs}\" )\n",
        "print(f\"index: {index}\", f\"number of dialogs: {number_of_dialogs}\" )\n",
        "\n",
        "df = pd.DataFrame (all_dialogs_in_parlai_format, columns = ['dialog'])\n",
        "\n",
        "train, valid, test = np.split(df.sample(frac=1, random_state=42), \n",
        "                                 [int(.8*len(df)), int(.9*len(df))])\n",
        "\n",
        "print(f\"train set: {len(train)}, validation set: {len(valid)},test set: {len(test)}\")\n",
        "\n",
        "data_folder = \"fr_reddit_dataset_small\"\n",
        "!rm -R $data_folder\n",
        "!mkdir $data_folder\n",
        "with open(f\"{data_folder}/data_train.txt\",\"w\") as f:\n",
        "    f.write(''.join(a[0] for a in train.values))\n",
        "\n",
        "with open(f\"{data_folder}/data_valid.txt\",\"w\") as f:\n",
        "    f.write(''.join(a[0] for a in valid.values))\n",
        "\n",
        "with open(\"fr_reddit_dataset_small/data_test.txt\",\"w\") as f:\n",
        "    f.write(''.join(a[0] for a in test.values))\n",
        "print('done!')"
      ],
      "metadata": {
        "id": "ONTuD9pykx7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxSxo-jstgOO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b1595c3d-3997-40f5-945a-28ee784087ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"text:Oh ouiii ! Je sais pas pourquoi, mais en voyant le bordel du débat à 11 je m'étais dit qu'il y avait un très fort potentiel de YTP.  Hâte de voir le résultat final, merci du partage.\\tlabels:Je suis assez impressionné par la qualité de son travail. Je n'ai pas trouvé d'autres poopeurs qui soient aussi bon.\\tepisode_done:True\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "train.values[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd2Wv9B08OI5"
      },
      "source": [
        "# Installatin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX_qJ6Xd8Kl3",
        "outputId": "8b7280b2-a5fe-4330-9205-08fa2710589e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon May  9 22:47:10 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWbSl5cl8Z56",
        "outputId": "2440df04-9f03-42c4-bb0b-5da1b24ccc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kinY5uq18b-F",
        "outputId": "529138f4-31f2-4964-a1b9-17bf4b644663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 15.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 208 kB 94.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 175 kB 83.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 92.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 248 kB 78.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 65.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 74 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 83.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 125 kB 105.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 547 kB 92.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 74.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 61.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48 kB 6.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 95 kB 6.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 235 kB 113.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 749 kB 81.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 88.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 89.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 81.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 84.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 72.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 6.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 68.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 79.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 85.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 107.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 82.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 110 kB 85.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 100 kB 12.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 121 kB 99.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 13.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 73.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 79.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25h  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 79.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.4.24)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=3b0aaa6488102e85f0480040575129f5660e294c25ec48204ce0b333c65672a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "# mydrive_path = '/content/drive/MyDrive/colabs/blender-models/'\n",
        "# !pip uninstall -q parlai\n",
        "!pip install -q parlai\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZAiRNxe8LNz"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3LwA_gA7RPl"
      },
      "outputs": [],
      "source": [
        "model_path = \"trained_fr_reddit/\"\n",
        "# !rm -R $model_path\n",
        "!mkdir $model_path\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    task= 'fromfile:parlaiformat', \n",
        "    fromfile_datapath='fr_reddit.txt',\n",
        "    # fromfile_datatype_extension=True,\n",
        "\n",
        "    model= \"transformer/generator\",\n",
        "    model_file= f\"{model_path}model\",\n",
        "    \n",
        "    # depend on your gpu\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 3,\n",
        "    log_every_n_secs= 60,\n",
        "    verbose = True,\n",
        "    attention_dropout= 0.0, \n",
        "    batchsize= 8, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    \n",
        "    # arguments we get from the pretrained model. \"from recipes page for 2.7B model\" \n",
        "    embedding_size= 2560, ffn_size= 10240,\n",
        "    variant= \"prelayernorm\",\n",
        "    n_heads= 32, n_positions= 128, \n",
        "    n_encoder_layers= 2, n_decoder_layers= 24,\n",
        "    \n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    # dict_tokenizer= \"bytelevelbpe\",\n",
        "    dropout= 0.1,\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    optimizer= \"adam\",\n",
        "    relu_dropout= 0.0, \n",
        "    model_parallel= True,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "    # save_after_valid= True,\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        "    # validation_metric= \"ppl\", #vmt = \"ppl\"\n",
        "    # validation_metric_mode= \"min\", # vmm= \"min\"\n",
        "\n",
        "    # customized parameters\n",
        "    # inference = 'topk', \n",
        "    # temperature = 0.7, \n",
        "    # topk=30, \n",
        "    # beam_length_penalty=1.03\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmBTzNJsSw9I",
        "outputId": "8540315c-eabf-411d-c3d8-882a59870ba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘trained_fr_reddit-small/’: File exists\n",
            "15:50:28 | building dictionary first...\n",
            "15:50:28 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "15:50:28 | \u001b[33mOverriding opt[\"validation_every_n_epochs\"] to 2.0 (previously: 0.25)\u001b[0m\n",
            "15:50:28 | Using CUDA\n",
            "15:50:28 | loading dictionary from trained_fr_reddit-small/model.dict\n",
            "15:50:28 | num words = 9986\n",
            "15:50:29 | Total parameters: 64,493,568 (64,493,568 trainable)\n",
            "15:50:29 | Loading existing model params from trained_fr_reddit-small/model\n",
            "15:50:31 | Opt:\n",
            "15:50:31 |     activation: gelu\n",
            "15:50:31 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "15:50:31 |     adam_eps: 1e-08\n",
            "15:50:31 |     add_p1_after_newln: False\n",
            "15:50:31 |     aggregate_micro: False\n",
            "15:50:31 |     allow_missing_init_opts: False\n",
            "15:50:31 |     attention_dropout: 0.0\n",
            "15:50:31 |     batchsize: 12\n",
            "15:50:31 |     beam_block_full_context: True\n",
            "15:50:31 |     beam_block_list_filename: None\n",
            "15:50:31 |     beam_block_ngram: -1\n",
            "15:50:31 |     beam_context_block_ngram: -1\n",
            "15:50:31 |     beam_delay: 30\n",
            "15:50:31 |     beam_length_penalty: 0.65\n",
            "15:50:31 |     beam_min_length: 1\n",
            "15:50:31 |     beam_size: 1\n",
            "15:50:31 |     betas: '[0.9, 0.999]'\n",
            "15:50:31 |     bpe_add_prefix_space: None\n",
            "15:50:31 |     bpe_debug: False\n",
            "15:50:31 |     bpe_dropout: None\n",
            "15:50:31 |     bpe_merge: None\n",
            "15:50:31 |     bpe_vocab: None\n",
            "15:50:31 |     checkpoint_activations: False\n",
            "15:50:31 |     compute_tokenized_bleu: False\n",
            "15:50:31 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "15:50:31 |     datatype: train\n",
            "15:50:31 |     delimiter: '\\n'\n",
            "15:50:31 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "15:50:31 |     dict_endtoken: __end__\n",
            "15:50:31 |     dict_file: trained_fr_reddit-small/model.dict\n",
            "15:50:31 |     dict_include_test: False\n",
            "15:50:31 |     dict_include_valid: False\n",
            "15:50:31 |     dict_initpath: None\n",
            "15:50:31 |     dict_language: english\n",
            "15:50:31 |     dict_loaded: True\n",
            "15:50:31 |     dict_lower: True\n",
            "15:50:31 |     dict_max_ngram_size: -1\n",
            "15:50:31 |     dict_maxexs: -1\n",
            "15:50:31 |     dict_maxtokens: -1\n",
            "15:50:31 |     dict_minfreq: 0\n",
            "15:50:31 |     dict_nulltoken: __null__\n",
            "15:50:31 |     dict_starttoken: __start__\n",
            "15:50:31 |     dict_textfields: text,labels\n",
            "15:50:31 |     dict_tokenizer: bpe\n",
            "15:50:31 |     dict_unktoken: __unk__\n",
            "15:50:31 |     display_examples: False\n",
            "15:50:31 |     download_path: None\n",
            "15:50:31 |     dropout: 0.0\n",
            "15:50:31 |     dynamic_batching: full\n",
            "15:50:31 |     embedding_projection: random\n",
            "15:50:31 |     embedding_size: 512\n",
            "15:50:31 |     embedding_type: random\n",
            "15:50:31 |     embeddings_scale: True\n",
            "15:50:31 |     eval_batchsize: None\n",
            "15:50:31 |     eval_dynamic_batching: None\n",
            "15:50:31 |     evaltask: None\n",
            "15:50:31 |     ffn_size: 2048\n",
            "15:50:31 |     final_extra_opt: \n",
            "15:50:31 |     force_fp16_tokens: True\n",
            "15:50:31 |     fp16: True\n",
            "15:50:31 |     fp16_impl: mem_efficient\n",
            "15:50:31 |     fromfile_datapath: fr_reddit_dataset/data\n",
            "15:50:31 |     fromfile_datatype_extension: True\n",
            "15:50:31 |     gpu: -1\n",
            "15:50:31 |     gradient_clip: 0.1\n",
            "15:50:31 |     hide_labels: False\n",
            "15:50:31 |     history_add_global_end_token: None\n",
            "15:50:31 |     history_reversed: False\n",
            "15:50:31 |     history_size: -1\n",
            "15:50:31 |     image_cropsize: 224\n",
            "15:50:31 |     image_mode: raw\n",
            "15:50:31 |     image_size: 256\n",
            "15:50:31 |     inference: greedy\n",
            "15:50:31 |     init_model: None\n",
            "15:50:31 |     init_opt: None\n",
            "15:50:31 |     interactive_mode: False\n",
            "15:50:31 |     invsqrt_lr_decay_gamma: -1\n",
            "15:50:31 |     is_debug: False\n",
            "15:50:31 |     label_truncate: 128\n",
            "15:50:31 |     learn_positional_embeddings: True\n",
            "15:50:31 |     learningrate: 1e-05\n",
            "15:50:31 |     load_from_checkpoint: True\n",
            "15:50:31 |     log_every_n_secs: -1\n",
            "15:50:31 |     log_every_n_steps: 50\n",
            "15:50:31 |     log_keep_fields: all\n",
            "15:50:31 |     loglevel: info\n",
            "15:50:31 |     lr_scheduler: reduceonplateau\n",
            "15:50:31 |     lr_scheduler_decay: 0.5\n",
            "15:50:31 |     lr_scheduler_patience: 3\n",
            "15:50:31 |     max_train_steps: -1\n",
            "15:50:31 |     max_train_time: -1\n",
            "15:50:31 |     metrics: default\n",
            "15:50:31 |     model: transformer/generator\n",
            "15:50:31 |     model_file: trained_fr_reddit-small/model\n",
            "15:50:31 |     model_parallel: False\n",
            "15:50:31 |     momentum: 0\n",
            "15:50:31 |     multitask_weights: [1]\n",
            "15:50:31 |     mutators: None\n",
            "15:50:31 |     n_decoder_layers: -1\n",
            "15:50:31 |     n_encoder_layers: -1\n",
            "15:50:31 |     n_heads: 16\n",
            "15:50:31 |     n_layers: 8\n",
            "15:50:31 |     n_positions: 512\n",
            "15:50:31 |     n_segments: 0\n",
            "15:50:31 |     nesterov: True\n",
            "15:50:31 |     no_cuda: False\n",
            "15:50:31 |     num_epochs: 10.0\n",
            "15:50:31 |     num_workers: 0\n",
            "15:50:31 |     nus: [0.7]\n",
            "15:50:31 |     optimizer: mem_eff_adam\n",
            "15:50:31 |     output_scaling: 1.0\n",
            "15:50:31 |     override: \"{'model_file': 'trained_fr_reddit-small/model', 'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'fr_reddit_dataset/data', 'fromfile_datatype_extension': True, 'batchsize': 12, 'model': 'transformer/generator', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 100, 'validation_metric': 'ppl', 'num_epochs': 10.0, 'validation_every_n_epochs': 2.0, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
            "15:50:31 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "15:50:31 |     person_tokens: False\n",
            "15:50:31 |     rank_candidates: False\n",
            "15:50:31 |     relu_dropout: 0.0\n",
            "15:50:31 |     save_after_valid: False\n",
            "15:50:31 |     save_every_n_secs: -1\n",
            "15:50:31 |     save_format: conversations\n",
            "15:50:31 |     share_word_embeddings: True\n",
            "15:50:31 |     short_final_eval: False\n",
            "15:50:31 |     skip_generation: True\n",
            "15:50:31 |     special_tok_lst: None\n",
            "15:50:31 |     split_lines: False\n",
            "15:50:31 |     starttime: May10_15-43\n",
            "15:50:31 |     task: fromfile:parlaiformat\n",
            "15:50:31 |     temperature: 1.0\n",
            "15:50:31 |     tensorboard_log: False\n",
            "15:50:31 |     tensorboard_logdir: None\n",
            "15:50:31 |     text_truncate: 512\n",
            "15:50:31 |     topk: 10\n",
            "15:50:31 |     topp: 0.9\n",
            "15:50:31 |     truncate: -1\n",
            "15:50:31 |     update_freq: 1\n",
            "15:50:31 |     use_reply: label\n",
            "15:50:31 |     validation_cutoff: 1.0\n",
            "15:50:31 |     validation_every_n_epochs: 2.0\n",
            "15:50:31 |     validation_every_n_secs: -1\n",
            "15:50:31 |     validation_every_n_steps: -1\n",
            "15:50:31 |     validation_max_exs: -1\n",
            "15:50:31 |     validation_metric: ppl\n",
            "15:50:31 |     validation_metric_mode: None\n",
            "15:50:31 |     validation_patience: 10\n",
            "15:50:31 |     validation_share_agent: False\n",
            "15:50:31 |     variant: xlm\n",
            "15:50:31 |     verbose: False\n",
            "15:50:31 |     wandb_entity: None\n",
            "15:50:31 |     wandb_log: False\n",
            "15:50:31 |     wandb_name: None\n",
            "15:50:31 |     wandb_project: None\n",
            "15:50:31 |     warmup_rate: 0.0001\n",
            "15:50:31 |     warmup_updates: 100\n",
            "15:50:31 |     weight_decay: None\n",
            "15:50:31 |     world_logs: \n",
            "15:50:32 | creating task(s): fromfile:parlaiformat\n",
            "15:50:32 | Loading ParlAI text data: fr_reddit_dataset/data_train.txt\n",
            "15:50:34 | training...\n",
            "15:50:52 | time:299s total_exs:45428 total_steps:571 epochs:6.42 time_left:167s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   27.95     1  2151  6019       0          0 215.3 3848            131072  4.062    .3442 26.71 6.897 9.9e-06  2055  5750   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0005198    .005198 988.9     .09034         0                  554 4206 11769 2.798\n",
            "\n",
            "15:51:10 | time:316s total_exs:49352 total_steps:621 epochs:6.97 time_left:137s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.14     1  2366  6736       0          0 223.5 3924            131072   4.74    .3439  27.2 6.794 9.9e-06  2135  6079   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 892.7      .1092         0                  604 4500 12815 2.848\n",
            "\n",
            "15:51:28 | time:334s total_exs:53260 total_steps:671 epochs:7.52 time_left:110s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.29     1  2445  6990       0          0 223.4 3908            131072  5.286    .3393 26.36 6.697 9.9e-06  2060  5890   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002559   .0002559 809.7      .1266         0                  654 4506 12880 2.859\n",
            "\n",
            "15:51:40 | time:346s total_exs:55784 total_steps:704 epochs:7.88 time_left:93s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.25     1  2390  6547       0          0 209.5 2524            131072  5.284    .3415 25.49 6.653 9.9e-06  1950  5341   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "         0          0 775.3      .1332         0                  687 4340 11888 2.74\n",
            "\n",
            "15:51:40 | creating task(s): fromfile:parlaiformat\n",
            "15:51:40 | Loading ParlAI text data: fr_reddit_dataset/data_valid.txt\n",
            "15:51:41 | running eval: valid\n",
            "15:51:43 | eval completed in 1.79s\n",
            "15:51:43 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.13  1837 18671       0          0 599.5  885    .1751 28.03 6.681 9.9e-06  1654 16813       0          0 797.1   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .1299         0                  687 3491 35485\n",
            "\u001b[0m\n",
            "15:51:43 | \u001b[1;32mnew best ppl: 797.1 (previous best was 1069)\u001b[0m\n",
            "15:51:43 | saving best valid model: trained_fr_reddit-small/model\n",
            "15:52:03 | time:369s total_exs:59740 total_steps:754 epochs:8.44 time_left:68s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   28.84     1  2282  6544       0          0 226.9 3956            131072  4.353    .3442 28.59 6.601 9.9e-06  2262  6486   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0007583    .005308 735.5      .1354         0                  737 4544 13030 2.868\n",
            "\n",
            "15:52:20 | time:386s total_exs:63760 total_steps:804 epochs:9.01 time_left:43s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.21     1  2509  7142       0          0 228.9 4020            131072   4.28    .3543 27.58 6.559 9.9e-06  2217  6310   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0004975    .002736 705.5      .1372         0                  787 4726 13452 2.847\n",
            "\n",
            "15:52:38 | time:404s total_exs:67588 total_steps:854 epochs:9.55 time_left:19s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   29.85     1  2285  6385       0          0 213.9 3828            131072   4.41    .3416 27.69 6.521 9.9e-06  2120  5923   \n",
            "     ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002612   .0002612  679      .1408         0                  837 4405 12308 2.795\n",
            "\n",
            "15:52:49 | time:415s total_exs:70040 total_steps:884 epochs:9.89 time_left:4s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   27.98     1  2287  6589       0          0 235.5 2452            131072  4.195    .3441 26.32 6.482 9.9e-06  2151  6196   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0004078    .004078 653.2      .1421         0                  867 4437 12785 2.882\n",
            "\n",
            "15:52:49 | running eval: valid\n",
            "15:52:50 | eval completed in 1.75s\n",
            "15:52:50 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.13  1968 17836       0          0 572.8  885    .1751 28.03 6.531 9.9e-06  1772 16062       0          0 686.4   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .1414         0                  867 3740 33898\n",
            "\u001b[0m\n",
            "15:52:50 | \u001b[1;32mnew best ppl: 686.4 (previous best was 797.1)\u001b[0m\n",
            "15:52:50 | saving best valid model: trained_fr_reddit-small/model\n",
            "15:52:56 | time:422s total_exs:70844 total_steps:895 epochs:10.01 time_left:0s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   38.23     1  2794  7998       0          0 209.2  804            131072  3.827    .3374 30.87 6.497 9.9e-06  2257  6459   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0  663      .1367         0                  878 5051 14458 2.864\n",
            "\n",
            "15:52:56 | num_epochs completed:10.0 time elapsed:421.8857989311218s\n",
            "15:52:56 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "15:52:56 | Using CUDA\n",
            "15:52:56 | loading dictionary from trained_fr_reddit-small/model.dict\n",
            "15:52:56 | num words = 9986\n",
            "15:52:57 | Total parameters: 64,493,568 (64,493,568 trainable)\n",
            "15:52:57 | Loading existing model params from trained_fr_reddit-small/model\n",
            "15:52:58 | creating task(s): fromfile:parlaiformat\n",
            "15:52:58 | Loading ParlAI text data: fr_reddit_dataset/data_valid.txt\n",
            "15:53:00 | running eval: valid\n",
            "15:53:02 | eval completed in 1.94s\n",
            "15:53:02 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.13  1968 17982       0          0 577.6  885    .1672 28.03 6.531 9.9e-06  1772 16193       0          0 686.4   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .1414         0                  867 3740 34175\n",
            "\u001b[0m\n",
            "15:53:02 | creating task(s): fromfile:parlaiformat\n",
            "15:53:02 | Loading ParlAI text data: fr_reddit_dataset/data_test.txt\n",
            "15:53:04 | running eval: test\n",
            "15:53:05 | eval completed in 1.74s\n",
            "15:53:05 | \u001b[1mtest:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.24  2058 19024       0          0 629.1  885    .1672 27.51 6.526 9.9e-06  1873 17310       0          0 682.4   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .1415         0                  867 3931 36334\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'clen': AverageMetric(31.13),\n",
              "  'ctpb': GlobalAverageMetric(1968),\n",
              "  'ctps': GlobalTimerMetric(1.798e+04),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'exps': GlobalTimerMetric(577.6),\n",
              "  'exs': SumMetric(885),\n",
              "  'gpu_mem': GlobalAverageMetric(0.1672),\n",
              "  'llen': AverageMetric(28.03),\n",
              "  'loss': AverageMetric(6.531),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'ltpb': GlobalAverageMetric(1772),\n",
              "  'ltps': GlobalTimerMetric(1.619e+04),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'ppl': PPLMetric(686.4),\n",
              "  'token_acc': AverageMetric(0.1414),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(867),\n",
              "  'tpb': GlobalAverageMetric(3740),\n",
              "  'tps': GlobalTimerMetric(3.418e+04)},\n",
              " {'clen': AverageMetric(30.24),\n",
              "  'ctpb': GlobalAverageMetric(2058),\n",
              "  'ctps': GlobalTimerMetric(1.902e+04),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'exps': GlobalTimerMetric(629.1),\n",
              "  'exs': SumMetric(885),\n",
              "  'gpu_mem': GlobalAverageMetric(0.1672),\n",
              "  'llen': AverageMetric(27.51),\n",
              "  'loss': AverageMetric(6.526),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'ltpb': GlobalAverageMetric(1873),\n",
              "  'ltps': GlobalTimerMetric(1.731e+04),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'ppl': PPLMetric(682.4),\n",
              "  'token_acc': AverageMetric(0.1415),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(867),\n",
              "  'tpb': GlobalAverageMetric(3931),\n",
              "  'tps': GlobalTimerMetric(3.633e+04)})"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "model_path = \"trained_fr_reddit-small/\"\n",
        "data_path = \"fr_reddit_dataset/\"\n",
        "# !rm -R $model_path\n",
        "!mkdir $model_path\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    model_file= f\"{model_path}model\",\n",
        "    task= 'fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    batchsize=12,\n",
        "    \n",
        "    model= \"transformer/generator\",\n",
        "\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    learn_positional_embeddings=True,\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    # use a small learning rate with ADAM optimizer\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    # early stopping on perplexity\n",
        "    validation_metric='ppl',\n",
        "\n",
        "    num_epochs = 10,\n",
        "    validation_every_n_epochs=2,\n",
        "\n",
        "    # depend on your gpu. If you have a V100, this is good\n",
        "    fp16=True, fp16_impl='mem_efficient',\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "\n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"trained_fr_reddit-small/\"\n",
        "data_path = \"fr_reddit_dataset/\"\n",
        "print(f'{model_path}/model')\n",
        "\n",
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f\"{data_path}data\",\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    # model= \"transformer/generator\",\n",
        "    model_file= f\"{model_path}model\",\n",
        "    # init_model= \"zoo:blender/blender_400Mdistill/model\",\n",
        "    # dict_file= f\"zoo:blender/blender_400Mdistill/model.dict\",\n",
        "\n",
        "    datatype= \"test\",\n",
        "    # fromfile_datatype_extension= True,\n",
        "\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    # inference = 'topk', # Generation algorithm. Choices: beam, topk, greedy, delayedbeam, nucleus\n",
        "    # temperature = 0.7, # Temperature to add during decoding. Default 1.0\n",
        "    # topk=30, # K used in Top K sampling\n",
        "    # beam_length_penalty=1.03 # Applies a length penalty. Set to 0 for no penalty. Default: 0.65.\n",
        "\n",
        "    beam_context_block_ngram= 3,\n",
        "    beam_block_ngram= 3, beam_min_length= 20, beam_size= 10,\n",
        "    inference =  'topk',  topk=20, temperature = 0.5, beam_length_penalty=0.8\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHfzqgYp38vi",
        "outputId": "05462bd6-ce35-4a8c-cf90-a3377df414f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trained_fr_reddit-small//model\n",
            "15:58:38 | \u001b[33mOverriding opt[\"datatype\"] to test (previously: train)\u001b[0m\n",
            "15:58:38 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "15:58:38 | \u001b[33mOverriding opt[\"beam_context_block_ngram\"] to 3 (previously: -1)\u001b[0m\n",
            "15:58:38 | \u001b[33mOverriding opt[\"beam_block_ngram\"] to 3 (previously: -1)\u001b[0m\n",
            "15:58:38 | \u001b[33mOverriding opt[\"beam_min_length\"] to 20 (previously: 1)\u001b[0m\n",
            "15:58:38 | \u001b[33mOverriding opt[\"beam_size\"] to 10 (previously: 1)\u001b[0m\n",
            "15:58:38 | \u001b[33mOverriding opt[\"inference\"] to topk (previously: greedy)\u001b[0m\n",
            "15:58:38 | \u001b[33mOverriding opt[\"topk\"] to 20 (previously: 10)\u001b[0m\n",
            "15:58:38 | \u001b[33mOverriding opt[\"temperature\"] to 0.5 (previously: 1.0)\u001b[0m\n",
            "15:58:38 | \u001b[33mOverriding opt[\"beam_length_penalty\"] to 0.8 (previously: 0.65)\u001b[0m\n",
            "15:58:38 | Using CUDA\n",
            "15:58:38 | loading dictionary from trained_fr_reddit-small/model.dict\n",
            "15:58:38 | num words = 9986\n",
            "15:58:39 | Total parameters: 64,493,568 (64,493,568 trainable)\n",
            "15:58:39 | Loading existing model params from trained_fr_reddit-small/model\n",
            "15:58:39 | creating task(s): fromfile:parlaiformat\n",
            "15:58:39 | Loading ParlAI text data: fr_reddit_dataset/data_test.txt\n",
            "15:58:39 | Opt:\n",
            "15:58:39 |     activation: gelu\n",
            "15:58:39 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "15:58:39 |     adam_eps: 1e-08\n",
            "15:58:39 |     add_p1_after_newln: False\n",
            "15:58:39 |     aggregate_micro: False\n",
            "15:58:39 |     allow_missing_init_opts: False\n",
            "15:58:39 |     attention_dropout: 0.0\n",
            "15:58:39 |     batchsize: 12\n",
            "15:58:39 |     beam_block_full_context: True\n",
            "15:58:39 |     beam_block_list_filename: None\n",
            "15:58:39 |     beam_block_ngram: 3\n",
            "15:58:39 |     beam_context_block_ngram: 3\n",
            "15:58:39 |     beam_delay: 30\n",
            "15:58:39 |     beam_length_penalty: 0.8\n",
            "15:58:39 |     beam_min_length: 20\n",
            "15:58:39 |     beam_size: 10\n",
            "15:58:39 |     betas: '[0.9, 0.999]'\n",
            "15:58:39 |     bpe_add_prefix_space: None\n",
            "15:58:39 |     bpe_debug: False\n",
            "15:58:39 |     bpe_dropout: None\n",
            "15:58:39 |     bpe_merge: None\n",
            "15:58:39 |     bpe_vocab: None\n",
            "15:58:39 |     checkpoint_activations: False\n",
            "15:58:39 |     compute_tokenized_bleu: False\n",
            "15:58:39 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "15:58:39 |     datatype: test\n",
            "15:58:39 |     delimiter: '\\n'\n",
            "15:58:39 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "15:58:39 |     dict_endtoken: __end__\n",
            "15:58:39 |     dict_file: trained_fr_reddit-small/model.dict\n",
            "15:58:39 |     dict_include_test: False\n",
            "15:58:39 |     dict_include_valid: False\n",
            "15:58:39 |     dict_initpath: None\n",
            "15:58:39 |     dict_language: english\n",
            "15:58:39 |     dict_loaded: True\n",
            "15:58:39 |     dict_lower: True\n",
            "15:58:39 |     dict_max_ngram_size: -1\n",
            "15:58:39 |     dict_maxexs: -1\n",
            "15:58:39 |     dict_maxtokens: -1\n",
            "15:58:39 |     dict_minfreq: 0\n",
            "15:58:39 |     dict_nulltoken: __null__\n",
            "15:58:39 |     dict_starttoken: __start__\n",
            "15:58:39 |     dict_textfields: text,labels\n",
            "15:58:39 |     dict_tokenizer: bpe\n",
            "15:58:39 |     dict_unktoken: __unk__\n",
            "15:58:39 |     display_add_fields: \n",
            "15:58:39 |     display_examples: False\n",
            "15:58:39 |     download_path: None\n",
            "15:58:39 |     dropout: 0.0\n",
            "15:58:39 |     dynamic_batching: full\n",
            "15:58:39 |     embedding_projection: random\n",
            "15:58:39 |     embedding_size: 512\n",
            "15:58:39 |     embedding_type: random\n",
            "15:58:39 |     embeddings_scale: True\n",
            "15:58:39 |     eval_batchsize: None\n",
            "15:58:39 |     eval_dynamic_batching: None\n",
            "15:58:39 |     evaltask: None\n",
            "15:58:39 |     ffn_size: 2048\n",
            "15:58:39 |     final_extra_opt: \n",
            "15:58:39 |     force_fp16_tokens: True\n",
            "15:58:39 |     fp16: True\n",
            "15:58:39 |     fp16_impl: mem_efficient\n",
            "15:58:39 |     fromfile_datapath: fr_reddit_dataset/data\n",
            "15:58:39 |     fromfile_datatype_extension: True\n",
            "15:58:39 |     gpu: -1\n",
            "15:58:39 |     gradient_clip: 0.1\n",
            "15:58:39 |     hide_labels: False\n",
            "15:58:39 |     history_add_global_end_token: None\n",
            "15:58:39 |     history_reversed: False\n",
            "15:58:39 |     history_size: -1\n",
            "15:58:39 |     image_cropsize: 224\n",
            "15:58:39 |     image_mode: raw\n",
            "15:58:39 |     image_size: 256\n",
            "15:58:39 |     inference: topk\n",
            "15:58:39 |     init_model: None\n",
            "15:58:39 |     init_opt: None\n",
            "15:58:39 |     interactive_mode: False\n",
            "15:58:39 |     invsqrt_lr_decay_gamma: -1\n",
            "15:58:39 |     is_debug: False\n",
            "15:58:39 |     label_truncate: 128\n",
            "15:58:39 |     learn_positional_embeddings: True\n",
            "15:58:39 |     learningrate: 1e-05\n",
            "15:58:39 |     log_every_n_secs: -1\n",
            "15:58:39 |     log_every_n_steps: 50\n",
            "15:58:39 |     log_keep_fields: all\n",
            "15:58:39 |     loglevel: info\n",
            "15:58:39 |     lr_scheduler: reduceonplateau\n",
            "15:58:39 |     lr_scheduler_decay: 0.5\n",
            "15:58:39 |     lr_scheduler_patience: 3\n",
            "15:58:39 |     max_train_steps: -1\n",
            "15:58:39 |     max_train_time: -1\n",
            "15:58:39 |     metrics: default\n",
            "15:58:39 |     model: transformer/generator\n",
            "15:58:39 |     model_file: trained_fr_reddit-small/model\n",
            "15:58:39 |     model_parallel: False\n",
            "15:58:39 |     momentum: 0\n",
            "15:58:39 |     multitask_weights: [1]\n",
            "15:58:39 |     mutators: None\n",
            "15:58:39 |     n_decoder_layers: -1\n",
            "15:58:39 |     n_encoder_layers: -1\n",
            "15:58:39 |     n_heads: 16\n",
            "15:58:39 |     n_layers: 8\n",
            "15:58:39 |     n_positions: 512\n",
            "15:58:39 |     n_segments: 0\n",
            "15:58:39 |     nesterov: True\n",
            "15:58:39 |     no_cuda: False\n",
            "15:58:39 |     num_epochs: 10.0\n",
            "15:58:39 |     num_examples: 20\n",
            "15:58:39 |     num_workers: 0\n",
            "15:58:39 |     nus: [0.7]\n",
            "15:58:39 |     optimizer: mem_eff_adam\n",
            "15:58:39 |     output_scaling: 1.0\n",
            "15:58:39 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'fr_reddit_dataset/data', 'fromfile_datatype_extension': True, 'model_file': 'trained_fr_reddit-small/model', 'datatype': 'test', 'num_examples': '20', 'skip_generation': False, 'beam_context_block_ngram': 3, 'beam_block_ngram': 3, 'beam_min_length': 20, 'beam_size': 10, 'inference': 'topk', 'topk': 20, 'temperature': 0.5, 'beam_length_penalty': 0.8}\"\n",
            "15:58:39 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "15:58:39 |     person_tokens: False\n",
            "15:58:39 |     rank_candidates: False\n",
            "15:58:39 |     relu_dropout: 0.0\n",
            "15:58:39 |     save_after_valid: False\n",
            "15:58:39 |     save_every_n_secs: -1\n",
            "15:58:39 |     save_format: conversations\n",
            "15:58:39 |     share_word_embeddings: True\n",
            "15:58:39 |     short_final_eval: False\n",
            "15:58:39 |     skip_generation: False\n",
            "15:58:39 |     special_tok_lst: None\n",
            "15:58:39 |     split_lines: False\n",
            "15:58:39 |     starttime: May10_15-43\n",
            "15:58:39 |     task: fromfile:parlaiformat\n",
            "15:58:39 |     temperature: 0.5\n",
            "15:58:39 |     tensorboard_log: False\n",
            "15:58:39 |     tensorboard_logdir: None\n",
            "15:58:39 |     text_truncate: 512\n",
            "15:58:39 |     topk: 20\n",
            "15:58:39 |     topp: 0.9\n",
            "15:58:39 |     truncate: -1\n",
            "15:58:39 |     update_freq: 1\n",
            "15:58:39 |     use_reply: label\n",
            "15:58:39 |     validation_cutoff: 1.0\n",
            "15:58:39 |     validation_every_n_epochs: 2.0\n",
            "15:58:39 |     validation_every_n_secs: -1\n",
            "15:58:39 |     validation_every_n_steps: -1\n",
            "15:58:39 |     validation_max_exs: -1\n",
            "15:58:39 |     validation_metric: ppl\n",
            "15:58:39 |     validation_metric_mode: None\n",
            "15:58:39 |     validation_patience: 10\n",
            "15:58:39 |     validation_share_agent: False\n",
            "15:58:39 |     variant: xlm\n",
            "15:58:39 |     verbose: False\n",
            "15:58:39 |     wandb_entity: None\n",
            "15:58:39 |     wandb_log: False\n",
            "15:58:39 |     wandb_name: None\n",
            "15:58:39 |     wandb_project: None\n",
            "15:58:39 |     warmup_rate: 0.0001\n",
            "15:58:39 |     warmup_updates: 100\n",
            "15:58:39 |     weight_decay: None\n",
            "15:58:39 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mJe suppose que c'est au sujet du groupe Indochine ... edit: mot manquant\u001b[0;0m\n",
            "\u001b[1;94m    labels: Oui c’est ça, confirmation de ma copine ce matin.\u001b[0;0m\n",
            "\u001b[0;95m     model: j ' ai ai , de de de , le la pas le . . et de : / / / . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mJe trouve la musique trop répétitive mais le clip est une merveille\u001b[0;0m\n",
            "\u001b[1;94m    labels: Ouep, dommage pour la musique car le clip est très réussi.\u001b[0;0m\n",
            "\u001b[0;95m     model: pas , , j ' est la est pas , de de pas et , , . . . le . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mMerci ! Je conseille ces courts-métrages à tous les amateurs de mystère.  Je les ai vus il y a 2 ou 3 ans donc mes souvenirs sont flous (et j'hésite à les revoir tout de suite vu l'heure tardive) mais il me semble que *Le Cas Ferguson* et *La Sorcière* étaient particulièrement marquants.\u001b[0;0m\n",
            "\u001b[1;94m    labels: J'aime le fait que tout soit dans la suggestion, l'ambiance, et surtout la voix off monotone donne un coté particulièrement \"creepy\" à l'ensemble.  \u001b[0;0m\n",
            "\u001b[0;95m     model: j ' ai je le le . . . c ' est de de de pas un en . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mprend* et du Languedoc Roussillon pour Poitiers Toulouse, non ? Sinon, que ces villes prennent de l'importance est dangereux pour la chocolatine, car ça va attirer des immigrés parisiens qui vont dire pain au chocolat.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Corrigé. C'est sur qu'il faudra les assimiler à ça et à la pratique du Rugby.\u001b[0;0m\n",
            "\u001b[0;95m     model: , je c ' est , c ' a pas pas le le le , le le . . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0m Quelque 31 mineurs et ayants droit pourraient bénéficier de cette mesure d'indemnisation, selon la Chancellerie. De la fumée quoi.  Les ayants droits, c'est pour les épouses encore en vie non (sur le mode de la retraite) ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Oui, ça doit être les parts de réversion pour épouses, enfants handicapé ou encore à charge de moins de 21 ans et scolarisés, mais là... \u001b[0;0m\n",
            "\u001b[0;95m     model: je la je ' est la que c ' , , un de l ' est de de . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mSi Assange met ne serait-ce qu'un pied sur le sol Français il se fera livrer aux américains pieds et poings liés par un François irradié d'un sourire benêt.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Il n'y a rien à attendre d'un président Young Leader de la French American Foundation.\u001b[0;0m\n",
            "\u001b[0;95m     model: je je je le . . . c ' est pas un . . j ' est de le .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mPour ceux qui se disent comme moi : \"ouaih on a compris avec cette affaire\" : C'est une nouvelle affaire.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Une de plus ! Une de plus !\u001b[0;0m\n",
            "\u001b[0;95m     model: la la la , , est . . . le . . c ' ' est , , c ' .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mCes bourgeois, ils savent plus quoi inventer ! A force de lire et d'écrire, ils deviennent plus cons les uns qu'les autres ! Sacré ghetenoc !\u001b[0;0m\n",
            "\u001b[1;94m    labels: Oui, la confédération paysanne c'est clairement des bourgeois parisiens.\u001b[0;0m\n",
            "\u001b[0;95m     model: je je , mais , , de de de . . . et . . c ' est . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mon a vu mieux, dommage etant donné le potentiel infini\u001b[0;0m\n",
            "\u001b[1;94m    labels:  le potentiel infini c'est fou comme on peut désarçonner les gens simplement en reformulant leurs affirmations, avec un point d'interrogation au bout.\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est le . . . c ' ' est la le le le la la et . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mAlexis Kohler m'a donné l'impression d'être sur le point de faire un malaise ou d'éclater un sanglot\u001b[0;0m\n",
            "\u001b[1;94m    labels: A sa décharge, en costard par ce temps il doit avoir chaud ! \u001b[0;0m\n",
            "\u001b[0;95m     model: je c ' est le . . . j ' est a la la ' est . . ? . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0m[Ok mais je te met la photo quand meme.](http://twitter.com/greub1/status/760695725797863425/photo/1)\u001b[0;0m\n",
            "\u001b[1;94m    labels: C'est particulier comme maillot de bain. Sa propre création en plus : http://www.afidaturner.com/product/miami-vice/ On voit qu'elle a bien compris que son visage n'était pas son point fort, elle met en avant le reste.\u001b[0;0m\n",
            "\u001b[0;95m     model: j ' ai je la la je pas pas de de de . . . et , , je , je de , , , de de la .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mSorry but :\"un truc rançais\",br\"c'est urc ?\"br\"lbanais ?\"La vychissoise de grammaire est à la ue\u001b[0;0m\n",
            "\u001b[1;94m    labels:  à la ue Tu l'aimes bien ton Bonjour Chatounet en fait, hein?\u001b[0;0m\n",
            "\u001b[0;95m     model: , c ' ' est de pas de de la la de de , en en , je . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mEn terme d'inefficacité, Jean-Luc sait de quoi il parle. \u001b[0;0m\n",
            "\u001b[1;94m    labels: J'allais dire un peu comme lui mais tu m'as devancé…\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est la la la mais de de . . . a . . l ' est . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mDu vrai journalisme d'investigation.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Ce n'est pas le but de Closer.\u001b[0;0m\n",
            "\u001b[0;95m     model: la , je c ' est pas en . . . c ' ' est l ' est le en .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0m le volcan François Hollande lolwut\u001b[0;0m\n",
            "\u001b[1;94m    labels: T'as jamais visité l'auvergne ? :)\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est pas , de la le pas , la la le . . . c ' un la de de .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mC'est quoi cette petite poussée d'astroturfing en faveur de l'homéopathie ces derniers jours ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: C'est par vague, une study est boom, ils sortent du bois. Je suis pour l'utilisation du thé plutôt, car le thé à au moins une propriété chimique alors que l'homéopathie en a aucune.\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' ' est un un de la , la la , je je , en la les l ' est , en .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mLe fromage. \u001b[0;0m\n",
            "\u001b[1;94m    labels: Mais non voyons ! Si on le partage il y en aura moins pour nous...\u001b[0;0m\n",
            "\u001b[0;95m     model: j ' ai ai les , de . . . le le de de , , , ça . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mLes commentaires de l'article sont édifiants.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Le plus piquant, c’est celui-ci:   «La gauche a instrumentalisé les immigrés pour gagner des electeurs en les naturalisant ou en leur donnant le droit de vote. » Ici, c’est la droite qui donne la nationalité pour s’en servir contre les québécois…\u001b[0;0m\n",
            "\u001b[0;95m     model: les , je c ' est pas la de de la je le . . . et , , je . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mNon, t'en fais pas, c'est assez consternant.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Ce sont loin d'être les seuls à faire cette analogie, on lit cela régulièrement dès qu'il s'agit d'expulsion de sans papiers mais dans ce cas ça vient plutot de militants.\u001b[0;0m\n",
            "\u001b[0;95m     model: j ' est le . . . c ' ' est de a le l ' est . . je , le le .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: fr_reddit_dataset/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mJe crois qu'ils n'aiment pas la gauche chez Capital…\u001b[0;0m\n",
            "\u001b[1;94m    labels: C'est ce qu'un ennemi des édriseurs dirait !\u001b[0;0m\n",
            "\u001b[0;95m     model: mais c ' est le de de la , , c ' ' est pas pas l ' est ce .\u001b[0;0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "h89rB0oWNEBN",
        "Kd2Wv9B08OI5"
      ],
      "machine_shape": "hm",
      "name": "training-transformer-french_reddit.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQetxX/ju+602ll5ffg+Cz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}