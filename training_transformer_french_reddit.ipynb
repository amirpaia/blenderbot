{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirpaia/blenderbot/blob/main/training_transformer_french_reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h89rB0oWNEBN"
      },
      "source": [
        "# Installating kaggle and download the dataset\n",
        "\n",
        "please upload kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9h93bjSvpgu",
        "outputId": "20f2ed64-bc15-4e4a-c490-8d7bcd3096d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.26.9)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "# download from kaggle\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjEeFwwc3gT9",
        "outputId": "1d5b941a-66e6-455a-fb94-343750af8403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading french-reddit-discussion.zip to /content\n",
            " 99% 421M/426M [00:18<00:00, 24.1MB/s]\n",
            "100% 426M/426M [00:18<00:00, 24.4MB/s]\n",
            "Archive:  french-reddit-discussion.zip\n",
            "  inflating: final_SPF_2.xml         \n",
            "  inflating: spf.tar.gz              \n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets download breandan/french-reddit-discussion\n",
        "! unzip french-reddit-discussion.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlBcjN_zNYdv"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "BCabG4Dx3ibs",
        "outputId": "5a9a95e4-6061-4a59-eb68-03993f0560e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of conversations:  556622\n",
            "number of comments:  1583083\n",
            "All done in : 23.115362644195557  seconds\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6015662c-fcee-499e-9aa1-0cd1476f2a11\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link_id</th>\n",
              "      <th>subreddit_id</th>\n",
              "      <th>uid</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>score</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>create_utc</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8r1kz</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>1688932</td>\n",
              "      <td>c0a62uj</td>\n",
              "      <td>3</td>\n",
              "      <td>8r1kz</td>\n",
              "      <td>1244576002</td>\n",
              "      <td>Ironie : l'article disant qu'on est plus capab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8r1kz</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>786883</td>\n",
              "      <td>c0a6lmb</td>\n",
              "      <td>1</td>\n",
              "      <td>c0a62uj</td>\n",
              "      <td>1244621120</td>\n",
              "      <td>Moi-même, j'ai dû me forcer pour arriver jusqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8sncs</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>390497</td>\n",
              "      <td>c0aawpk</td>\n",
              "      <td>1</td>\n",
              "      <td>8sncs</td>\n",
              "      <td>1245076061</td>\n",
              "      <td>Service qui sera rendu au contribuable pour la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8sncs</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>32884</td>\n",
              "      <td>c0aaxba</td>\n",
              "      <td>3</td>\n",
              "      <td>c0aawpk</td>\n",
              "      <td>1245077396</td>\n",
              "      <td>Eeeeh oui ! 70 millions pour une loi qui aura ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8v13c</td>\n",
              "      <td>2qhjz</td>\n",
              "      <td>796919</td>\n",
              "      <td>c0aj3ov</td>\n",
              "      <td>2</td>\n",
              "      <td>8v13c</td>\n",
              "      <td>1245830384</td>\n",
              "      <td>Est-ce qu'elle a vraiment commis des crimes qu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6015662c-fcee-499e-9aa1-0cd1476f2a11')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6015662c-fcee-499e-9aa1-0cd1476f2a11 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6015662c-fcee-499e-9aa1-0cd1476f2a11');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "  link_id subreddit_id      uid comment_id score parent_id  create_utc  \\\n",
              "0   8r1kz        2qhjz  1688932    c0a62uj     3     8r1kz  1244576002   \n",
              "1   8r1kz        2qhjz   786883    c0a6lmb     1   c0a62uj  1244621120   \n",
              "2   8sncs        2qhjz   390497    c0aawpk     1     8sncs  1245076061   \n",
              "3   8sncs        2qhjz    32884    c0aaxba     3   c0aawpk  1245077396   \n",
              "4   8v13c        2qhjz   796919    c0aj3ov     2     8v13c  1245830384   \n",
              "\n",
              "                                                text  \n",
              "0  Ironie : l'article disant qu'on est plus capab...  \n",
              "1  Moi-même, j'ai dû me forcer pour arriver jusqu...  \n",
              "2  Service qui sera rendu au contribuable pour la...  \n",
              "3  Eeeeh oui ! 70 millions pour une loi qui aura ...  \n",
              "4  Est-ce qu'elle a vraiment commis des crimes qu...  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "import lxml.etree as ET\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "file_path = 'final_SPF_2.xml'\n",
        "start=time.time()\n",
        "#Initializes the parser\n",
        "parser = ET.XMLParser(recover=True)\n",
        "#Parses the file\n",
        "tree = ET.parse(file_path, parser=parser)\n",
        "xroot = tree.getroot()\n",
        "\n",
        "#One conversation -> one line in the data array\n",
        "dfcols = ['link_id', 'subreddit_id', 'uid',\"comment_id\",'score', 'parent_id', 'create_utc', 'text']\n",
        "data =np.array(\n",
        "    (\n",
        "        [\n",
        "            [\n",
        "                [\n",
        "                    node.attrib.get('link_id'),\n",
        "                    node.attrib.get('subreddit_id'), \n",
        "                    node.getchildren()[j].get('uid'), \n",
        "                    node.getchildren()[j].get('comment_id'), \n",
        "                    node.getchildren()[j].get('score'), \n",
        "                    node.getchildren()[j].get('parent_id'), \n",
        "                    node.getchildren()[j].get('create_utc'),\n",
        "                    node.getchildren()[j].text\n",
        "                ] \n",
        "                for j in range(len(node.getchildren()))\n",
        "            ] \n",
        "            for node in xroot\n",
        "        ]\n",
        "     ), \n",
        "    dtype=object)\n",
        "\n",
        "print('number of conversations: ', data.shape[0])\n",
        "\n",
        "#one comments -> one line in the data array\n",
        "data=np.array([liste for conversation in data for liste in conversation], dtype=object)\n",
        "print('number of comments: ',data.shape[0])\n",
        "\n",
        "df_xml = pd.DataFrame(data=data, columns=dfcols)\n",
        "print('All done in :',time.time()-start,' seconds')\n",
        "\n",
        "df_xml.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdMvmfb-Skfq"
      },
      "outputs": [],
      "source": [
        "# pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# df_xml[df_xml['comment_id'] == \"c4ojtph\"].text.to_string(index=False)\n",
        "# df_xml[df_xml['link_id'] == \"tpa2b\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naOQiCsL3pLa",
        "outputId": "acb6f606-2024-4e63-c088-f65a3b085645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text:hello\tlabels:how are you\n",
            "text:good\tlabels:bye\tepisode_done:True\n",
            "\n"
          ]
        }
      ],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS4WNAkWkq4p"
      },
      "source": [
        "## previous data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9OT-zt03sFN",
        "outputId": "1a539c10-b85b-498f-982d-e26cff420ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "index: 0 number of dialogs: 0\n",
            "index: 10000 number of dialogs: 1551\n",
            "index: 20000 number of dialogs: 2765\n",
            "index: 30000 number of dialogs: 3954\n",
            "index: 40000 number of dialogs: 5205\n",
            "index: 50000 number of dialogs: 6527\n",
            "index: 60000 number of dialogs: 7850\n",
            "index: 66538 number of dialogs: 8850\n",
            "train set: 7080, validation set: 885,test set: 885\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "train_dialogs_in_parlai_format = \"\"\n",
        "valid_dialogs_in_parlai_format = \"\"\n",
        "test_dialogs_in_parlai_format = \"\"\n",
        "number_of_dialogs = len(df_xml.groupby('link_id'))\n",
        "\n",
        "for index, (key, dialog) in enumerate(df_xml.groupby('link_id')):\n",
        "#     print(key, dialog)\n",
        "    sorted_dialog = dialog.values[dialog.values[:, 6].argsort()]\n",
        "    list_of_turns = [turn[7].replace(\"\\n\\n\",\" \").replace(\"\\n\", \"\") for turn in sorted_dialog]\n",
        "    parlai_format = transfer_dialog(list_of_turns)\n",
        "#     print(parlai_format)\n",
        "    if index < number_of_dialogs * 0.8:\n",
        "      train_dialogs_in_parlai_format +=parlai_format\n",
        "    elif index < number_of_dialogs * 0.9:\n",
        "      valid_dialogs_in_parlai_format +=parlai_format\n",
        "    else:\n",
        "      test_dialogs_in_parlai_format +=parlai_format\n",
        "\n",
        "    if index % 10000 == 0: print(index)\n",
        "print(index)\n",
        "\n",
        "# !rm -R fr_reddit_dataset\n",
        "!mkdir fr_reddit_dataset\n",
        "with open(\"fr_reddit_dataset/data_train.txt\",\"w\") as f:\n",
        "    f.write(train_dialogs_in_parlai_format)\n",
        "\n",
        "with open(\"fr_reddit_dataset/data_valid.txt\",\"w\") as f:\n",
        "    f.write(valid_dialogs_in_parlai_format)\n",
        "\n",
        "with open(\"fr_reddit_dataset/data_test.txt\",\"w\") as f:\n",
        "    f.write(test_dialogs_in_parlai_format)\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm7b-GZHkuRg"
      },
      "source": [
        "## new data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONTuD9pykx7K"
      },
      "outputs": [],
      "source": [
        "all_dialogs_in_parlai_format = []\n",
        "\n",
        "for index, (key, dialog) in enumerate(df_xml.groupby('link_id')):\n",
        "    # if the dialog has two speakers\n",
        "    if (len(set([a[2] for a in dialog.values])) == 2) & \\\n",
        "        (sum([1 for a in dialog.values if (len(a[7]) > 300)]) == 0):\n",
        "        # print(key, dialog)\n",
        "        sorted_dialog = dialog.values[dialog.values[:, 6].argsort()]\n",
        "        list_of_turns = [turn[7].replace(\"\\n\\n\",\" \").replace(\"\\n\", \"\") for turn in sorted_dialog]\n",
        "\n",
        "        parlai_format = transfer_dialog(list_of_turns)\n",
        "        all_dialogs_in_parlai_format.append(parlai_format)\n",
        "        # print(parlai_format)\n",
        "\n",
        "    number_of_dialogs = len(all_dialogs_in_parlai_format)\n",
        "    if index % 10000 == 0: print(f\"index: {index}\", f\"number of dialogs: {number_of_dialogs}\" )\n",
        "print(f\"index: {index}\", f\"number of dialogs: {number_of_dialogs}\" )\n",
        "\n",
        "df = pd.DataFrame (all_dialogs_in_parlai_format, columns = ['dialog'])\n",
        "\n",
        "train, valid, test = np.split(df.sample(frac=1, random_state=42), \n",
        "                                 [int(.8*len(df)), int(.9*len(df))])\n",
        "\n",
        "print(f\"train set: {len(train)}, validation set: {len(valid)},test set: {len(test)}\")\n",
        "\n",
        "data_folder = \"fr_reddit_dataset_small\"\n",
        "!rm -R $data_folder\n",
        "!mkdir $data_folder\n",
        "with open(f\"{data_folder}/data_train.txt\",\"w\") as f:\n",
        "    f.write(''.join(a[0] for a in train.values))\n",
        "\n",
        "with open(f\"{data_folder}/data_valid.txt\",\"w\") as f:\n",
        "    f.write(''.join(a[0] for a in valid.values))\n",
        "\n",
        "with open(\"fr_reddit_dataset_small/data_test.txt\",\"w\") as f:\n",
        "    f.write(''.join(a[0] for a in test.values))\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KxSxo-jstgOO",
        "outputId": "b1595c3d-3997-40f5-945a-28ee784087ba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"text:Oh ouiii ! Je sais pas pourquoi, mais en voyant le bordel du débat à 11 je m'étais dit qu'il y avait un très fort potentiel de YTP.  Hâte de voir le résultat final, merci du partage.\\tlabels:Je suis assez impressionné par la qualité de son travail. Je n'ai pas trouvé d'autres poopeurs qui soient aussi bon.\\tepisode_done:True\\n\""
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.values[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd2Wv9B08OI5"
      },
      "source": [
        "# Installatin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX_qJ6Xd8Kl3",
        "outputId": "48b58a20-9065-4b85-c33e-47ef35063b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 18 07:14:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWbSl5cl8Z56",
        "outputId": "e977aa23-91cc-41df-bc2b-3b6a7823beac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuMyYYRz6o92",
        "outputId": "6f33b169-8470-4c17-f8a8-2bf04e629eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kinY5uq18b-F"
      },
      "outputs": [],
      "source": [
        "# mydrive_path = '/content/drive/MyDrive/colabs/blender-models/'\n",
        "# !pip uninstall -q parlai\n",
        "!pip install -q parlai\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZAiRNxe8LNz"
      },
      "source": [
        "# Reddit Ids > Dialog > ParlAI format > Train/Valid/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7XrNetYcG9i"
      },
      "outputs": [],
      "source": [
        "def transfer_dialog(d):\n",
        "    if len(d)%2 !=0: d = d[:-1]\n",
        "    t = \"\"\n",
        "    for i in range(0,len(d),2):\n",
        "        u1 = d[i]\n",
        "        u2 = d[i+1]\n",
        "\n",
        "        if (i+2) != len(d):\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\n\"\n",
        "        else:\n",
        "            t += \"text:\"+u1+\"\\t\"+\"labels:\"+u2+\"\\t\"+\"episode_done:True\"+\"\\n\"\n",
        "    return t\n",
        "    \n",
        "t = ['hello','how are you','good','bye']\n",
        "print(transfer_dialog(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8fiMxDacJka"
      },
      "outputs": [],
      "source": [
        "def convert_line_to_list_of_ids(line):\n",
        "    return line.split(\"\\t\")[1].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\n\",\"\").replace(\" \",\"\").replace(\"'\",\"\").split(\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxT8CN8oZAlL",
        "outputId": "3ed594e5-415f-46b1-906a-8c1e0a8f1d6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this is a test  lets see this too  end\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def remove_urls (vTEXT):\n",
        "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    return(vTEXT)\n",
        "    \n",
        "print( remove_urls(\"this is a test https://sdfs.sdfsdf.com/sdfsdf/sdfsdf/sd/sdfsdfs?bob=%20tree&jef=man lets see this too https://sdfsdf.fdf.com/sdf/f end\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdjqGcp5cOHk"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/colabs/aliae-workspace/datasets/'\n",
        "with open(f\"{data_path}french_reddit_all_dialog_turns_ids.txt\") as f:\n",
        "    lines = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaPXLeGn6Lzy",
        "outputId": "dbb1b09c-8b82-4ff4-ec89-e264ad9970dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "274561\n",
            "index: 0\n",
            "index: 50000\n",
            "index: 100000\n",
            "index: 150000\n",
            "index: 200000\n",
            "index: 250000\n"
          ]
        }
      ],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "# print(len(lines))\n",
        "# print(sum([1 for l in lines if len(convert_line_to_list_of_ids(l)) == 10]))\n",
        "print(len(lines))\n",
        "\n",
        "dict_xml = df_xml.set_index('comment_id').to_dict()['text']\n",
        "dialogs_in_parlai_format = []\n",
        "for index, line in enumerate(lines):\n",
        "    ids = convert_line_to_list_of_ids(line)\n",
        "    # print(ids)\n",
        "    turns = []\n",
        "    for id in ids:\n",
        "        text = dict_xml[id].replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
        "        text = remove_urls(text)\n",
        "        # print(text)\n",
        "        turns.append(text)\n",
        "    dialogs_in_parlai_format.append(transfer_dialog(turns))\n",
        "    # print(f\"index: {index}\", f\"number of turns: {len(turns)}\")\n",
        "    if index % 50000 == 0: print(f\"index: {index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88g6mTne49J_",
        "outputId": "46462e2c-8f24-4849-e135-3c32c5f5ee64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train set: 219648, validation set: 27456,test set: 27457\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "data_path = '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/'\n",
        "# with open(data_path + \"train.txt\", \"w\") as f:\n",
        "#     f.writelines(dialogs_in_parlai_format)\n",
        "\n",
        "df = pd.DataFrame (dialogs_in_parlai_format, columns = ['dialog'])\n",
        "train, valid, test = np.split(df.sample(frac=1, random_state=42), \n",
        "                                 [int(.8*len(df)), \n",
        "                                  int(.9*len(df))])\n",
        "print(f\"train set: {len(train)}, validation set: {len(valid)},test set: {len(test)}\")\n",
        "\n",
        "with open(f\"{data_path}/data_train.txt\",\"w\") as f:\n",
        "    f.write('\\n'.join(a[0] for a in train.values))\n",
        "\n",
        "with open(f\"{data_path}/data_valid.txt\",\"w\") as f:\n",
        "    f.write('\\n'.join(a[0] for a in valid.values))\n",
        "\n",
        "with open(f\"{data_path}/data_test.txt\",\"w\") as f:\n",
        "    f.write('\\n'.join(a[0] for a in test.values))\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37KVqEWA5vhE"
      },
      "source": [
        "# Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3LwA_gA7RPl"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/'\n",
        "model_path = \"/content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/\"\n",
        "# !rm -R $model_path\n",
        "# !mkdir $model_path\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    task= 'fromfile:parlaiformat', \n",
        "    fromfile_datapath='data',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    model= \"transformer/generator\",\n",
        "    model_file= f\"{model_path}model\",\n",
        "    \n",
        "    # depend on your gpu\n",
        "    validation_every_n_epochs=0.25, # veps= 0.25, \n",
        "    num_epochs = 3,\n",
        "    # log_every_n_secs= 60,\n",
        "    verbose = True,\n",
        "    attention_dropout= 0.0, \n",
        "    batchsize= 8, \n",
        "    fp16= True, fp16_impl= \"mem_efficient\",\n",
        "    \n",
        "    # arguments we get from the pretrained model. \"from recipes page for 2.7B model\" \n",
        "    embedding_size= 2560, ffn_size= 10240,\n",
        "    variant= \"prelayernorm\",\n",
        "    n_heads= 32, n_positions= 128, \n",
        "    n_encoder_layers= 2, n_decoder_layers= 24,\n",
        "    \n",
        "    label_truncate= 128, text_truncate= 128, truncate= 128,\n",
        "    activation= \"gelu\",\n",
        "    history_add_global_end_token= \"end\", \n",
        "    delimiter= '  ', \n",
        "    # dict_tokenizer= \"bytelevelbpe\",\n",
        "    dropout= 0.1,\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    lr= 7e-06, lr_scheduler= \"reduceonplateau\", lr_scheduler_patience= 3,\n",
        "    optimizer= \"adam\",\n",
        "    relu_dropout= 0.0, \n",
        "    model_parallel= True,\n",
        "    warmup_updates= 100,\n",
        "    update_freq= 2,\n",
        "    gradient_clip= 0.1, \n",
        "    # save_after_valid= True,\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation= True,\n",
        "    vp= 10,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esbkrv0Pb2rg"
      },
      "source": [
        "## TrainModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmBTzNJsSw9I",
        "outputId": "ec287b00-74a8-472f-a395-9655f81bf1c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21:23:09 | building dictionary first...\n",
            "21:23:09 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "21:23:09 | \u001b[33mOverriding opt[\"num_epochs\"] to 5.0 (previously: 10.0)\u001b[0m\n",
            "21:23:09 | Using CUDA\n",
            "21:23:09 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/model.dict\n",
            "21:23:09 | num words = 30051\n",
            "21:23:10 | Total parameters: 74,766,336 (74,766,336 trainable)\n",
            "21:23:10 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/model\n",
            "21:23:13 | Opt:\n",
            "21:23:13 |     activation: gelu\n",
            "21:23:13 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "21:23:13 |     adam_eps: 1e-08\n",
            "21:23:13 |     add_p1_after_newln: False\n",
            "21:23:13 |     aggregate_micro: False\n",
            "21:23:13 |     allow_missing_init_opts: False\n",
            "21:23:13 |     attention_dropout: 0.0\n",
            "21:23:13 |     batchsize: 12\n",
            "21:23:13 |     beam_block_full_context: True\n",
            "21:23:13 |     beam_block_list_filename: None\n",
            "21:23:13 |     beam_block_ngram: -1\n",
            "21:23:13 |     beam_context_block_ngram: -1\n",
            "21:23:13 |     beam_delay: 30\n",
            "21:23:13 |     beam_length_penalty: 0.65\n",
            "21:23:13 |     beam_min_length: 1\n",
            "21:23:13 |     beam_size: 1\n",
            "21:23:13 |     betas: '[0.9, 0.999]'\n",
            "21:23:13 |     bpe_add_prefix_space: None\n",
            "21:23:13 |     bpe_debug: False\n",
            "21:23:13 |     bpe_dropout: None\n",
            "21:23:13 |     bpe_merge: None\n",
            "21:23:13 |     bpe_vocab: None\n",
            "21:23:13 |     checkpoint_activations: False\n",
            "21:23:13 |     compute_tokenized_bleu: False\n",
            "21:23:13 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "21:23:13 |     datatype: train\n",
            "21:23:13 |     delimiter: '\\n'\n",
            "21:23:13 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "21:23:13 |     dict_endtoken: __end__\n",
            "21:23:13 |     dict_file: /content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/model.dict\n",
            "21:23:13 |     dict_include_test: False\n",
            "21:23:13 |     dict_include_valid: False\n",
            "21:23:13 |     dict_initpath: None\n",
            "21:23:13 |     dict_language: english\n",
            "21:23:13 |     dict_loaded: True\n",
            "21:23:13 |     dict_lower: True\n",
            "21:23:13 |     dict_max_ngram_size: -1\n",
            "21:23:13 |     dict_maxexs: -1\n",
            "21:23:13 |     dict_maxtokens: -1\n",
            "21:23:13 |     dict_minfreq: 0\n",
            "21:23:13 |     dict_nulltoken: __null__\n",
            "21:23:13 |     dict_starttoken: __start__\n",
            "21:23:13 |     dict_textfields: text,labels\n",
            "21:23:13 |     dict_tokenizer: bpe\n",
            "21:23:13 |     dict_unktoken: __unk__\n",
            "21:23:13 |     display_examples: False\n",
            "21:23:13 |     download_path: None\n",
            "21:23:13 |     dropout: 0.0\n",
            "21:23:13 |     dynamic_batching: full\n",
            "21:23:13 |     embedding_projection: random\n",
            "21:23:13 |     embedding_size: 512\n",
            "21:23:13 |     embedding_type: random\n",
            "21:23:13 |     embeddings_scale: True\n",
            "21:23:13 |     eval_batchsize: None\n",
            "21:23:13 |     eval_dynamic_batching: None\n",
            "21:23:13 |     evaltask: None\n",
            "21:23:13 |     ffn_size: 2048\n",
            "21:23:13 |     final_extra_opt: \n",
            "21:23:13 |     force_fp16_tokens: True\n",
            "21:23:13 |     fp16: True\n",
            "21:23:13 |     fp16_impl: mem_efficient\n",
            "21:23:13 |     fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data\n",
            "21:23:13 |     fromfile_datatype_extension: True\n",
            "21:23:13 |     gpu: -1\n",
            "21:23:13 |     gradient_clip: 0.1\n",
            "21:23:13 |     hide_labels: False\n",
            "21:23:13 |     history_add_global_end_token: None\n",
            "21:23:13 |     history_reversed: False\n",
            "21:23:13 |     history_size: -1\n",
            "21:23:13 |     image_cropsize: 224\n",
            "21:23:13 |     image_mode: raw\n",
            "21:23:13 |     image_size: 256\n",
            "21:23:13 |     inference: greedy\n",
            "21:23:13 |     init_model: None\n",
            "21:23:13 |     init_opt: None\n",
            "21:23:13 |     interactive_mode: False\n",
            "21:23:13 |     invsqrt_lr_decay_gamma: -1\n",
            "21:23:13 |     is_debug: False\n",
            "21:23:13 |     label_truncate: 128\n",
            "21:23:13 |     learn_positional_embeddings: True\n",
            "21:23:13 |     learningrate: 1e-05\n",
            "21:23:13 |     load_from_checkpoint: True\n",
            "21:23:13 |     log_every_n_secs: -1\n",
            "21:23:13 |     log_every_n_steps: 50\n",
            "21:23:13 |     log_keep_fields: all\n",
            "21:23:13 |     loglevel: info\n",
            "21:23:13 |     lr_scheduler: reduceonplateau\n",
            "21:23:13 |     lr_scheduler_decay: 0.5\n",
            "21:23:13 |     lr_scheduler_patience: 3\n",
            "21:23:13 |     max_train_steps: -1\n",
            "21:23:13 |     max_train_time: -1\n",
            "21:23:13 |     metrics: default\n",
            "21:23:13 |     model: transformer/generator\n",
            "21:23:13 |     model_file: /content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/model\n",
            "21:23:13 |     model_parallel: False\n",
            "21:23:13 |     momentum: 0\n",
            "21:23:13 |     multitask_weights: [1]\n",
            "21:23:13 |     mutators: None\n",
            "21:23:13 |     n_decoder_layers: -1\n",
            "21:23:13 |     n_encoder_layers: -1\n",
            "21:23:13 |     n_heads: 16\n",
            "21:23:13 |     n_layers: 8\n",
            "21:23:13 |     n_positions: 512\n",
            "21:23:13 |     n_segments: 0\n",
            "21:23:13 |     nesterov: True\n",
            "21:23:13 |     no_cuda: False\n",
            "21:23:13 |     num_epochs: 5.0\n",
            "21:23:13 |     num_workers: 0\n",
            "21:23:13 |     nus: [0.7]\n",
            "21:23:13 |     optimizer: mem_eff_adam\n",
            "21:23:13 |     output_scaling: 1.0\n",
            "21:23:13 |     override: \"{'model_file': '/content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/model', 'task': 'fromfile:parlaiformat', 'fromfile_datapath': '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data', 'fromfile_datatype_extension': True, 'batchsize': 12, 'model': 'transformer/generator', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 100, 'validation_metric': 'ppl', 'num_epochs': 5.0, 'validation_every_n_epochs': 2.0, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
            "21:23:13 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "21:23:13 |     person_tokens: False\n",
            "21:23:13 |     rank_candidates: False\n",
            "21:23:13 |     relu_dropout: 0.0\n",
            "21:23:13 |     save_after_valid: False\n",
            "21:23:13 |     save_every_n_secs: -1\n",
            "21:23:13 |     save_format: conversations\n",
            "21:23:13 |     share_word_embeddings: True\n",
            "21:23:13 |     short_final_eval: False\n",
            "21:23:13 |     skip_generation: True\n",
            "21:23:13 |     special_tok_lst: None\n",
            "21:23:13 |     split_lines: False\n",
            "21:23:13 |     starttime: May17_14-32\n",
            "21:23:13 |     task: fromfile:parlaiformat\n",
            "21:23:13 |     temperature: 1.0\n",
            "21:23:13 |     tensorboard_log: False\n",
            "21:23:13 |     tensorboard_logdir: None\n",
            "21:23:13 |     text_truncate: 512\n",
            "21:23:13 |     topk: 10\n",
            "21:23:13 |     topp: 0.9\n",
            "21:23:13 |     truncate: -1\n",
            "21:23:13 |     update_freq: 1\n",
            "21:23:13 |     use_reply: label\n",
            "21:23:13 |     validation_cutoff: 1.0\n",
            "21:23:13 |     validation_every_n_epochs: 2.0\n",
            "21:23:13 |     validation_every_n_secs: -1\n",
            "21:23:13 |     validation_every_n_steps: -1\n",
            "21:23:13 |     validation_max_exs: -1\n",
            "21:23:13 |     validation_metric: ppl\n",
            "21:23:13 |     validation_metric_mode: None\n",
            "21:23:13 |     validation_patience: 10\n",
            "21:23:13 |     validation_share_agent: False\n",
            "21:23:13 |     variant: xlm\n",
            "21:23:13 |     verbose: False\n",
            "21:23:13 |     wandb_entity: None\n",
            "21:23:13 |     wandb_log: False\n",
            "21:23:13 |     wandb_name: None\n",
            "21:23:13 |     wandb_project: None\n",
            "21:23:13 |     warmup_rate: 0.0001\n",
            "21:23:13 |     warmup_updates: 100\n",
            "21:23:13 |     weight_decay: None\n",
            "21:23:13 |     world_logs: \n",
            "21:23:14 | creating task(s): fromfile:parlaiformat\n",
            "21:23:14 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data_train.txt\n",
            "21:23:44 | training...\n",
            "21:23:59 | time:18818s total_exs:1521652 total_steps:64677 epochs:4.00 time_left:4679s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   110.1     1  3337 12223  .02217      7.331   119 1624            131072  5.549    .4593 67.28 5.303 9.9e-06  1652  6050   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1293      16.42  201      .2283         0                64677 4989 18273 3.663\n",
            "\n",
            "21:24:13 | time:18832s total_exs:1523080 total_steps:64727 epochs:4.01 time_left:4661s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   139.4     1  3696 13189  .03501      9.962 101.9 1428            131072   5.88    .4593 60.54 5.295 9.9e-06  1414  5045   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1001      11.04 199.4      .2290         0                64727 5109 18234 3.569\n",
            "\n",
            "21:24:27 | time:18846s total_exs:1524392 total_steps:64777 epochs:4.01 time_left:4644s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     160     1  3966 13875  .03201      8.893 91.81 1312            131072  5.745    .4593 70.28  5.32 9.9e-06  1487  5204   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1372       13.6 204.4      .2273         0                64777 5453 19078 3.499\n",
            "\n",
            "21:24:41 | time:18861s total_exs:1525612 total_steps:64827 epochs:4.01 time_left:4629s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   206.7     1  4212 14683   .1230      34.03 85.05 1220            131072  6.293    .4593 67.06 5.299 9.9e-06  1327  4624   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1164      12.69 200.1      .2270         0                64827 5539 19307 3.486\n",
            "\n",
            "21:24:56 | time:18875s total_exs:1526752 total_steps:64877 epochs:4.02 time_left:4615s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   207.8     1  4228 15035  .06053      22.32 81.07 1140            131072  6.365    .4593 78.28 5.277 9.9e-06  1319  4691   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1754      20.42 195.8      .2277         0                64877 5547 19726 3.556\n",
            "\n",
            "21:25:10 | time:18889s total_exs:1527896 total_steps:64927 epochs:4.02 time_left:4601s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   240.7     1  4248 14727   .1434      55.03 79.31 1144            131072  6.274    .4594 81.18 5.301 9.9e-06  1343  4655   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1670      22.49 200.6      .2253         0                64927 5591 19382 3.467\n",
            "\n",
            "21:25:24 | time:18903s total_exs:1529144 total_steps:64977 epochs:4.02 time_left:4585s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   186.2     1  3979 14408   .0649      26.78 90.39 1248            131072  6.317    .4594 71.91 5.306 9.9e-06  1376  4982   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1410      16.79 201.6      .2249         0                64977 5354 19390 3.622\n",
            "\n",
            "21:25:38 | time:18918s total_exs:1530280 total_steps:65027 epochs:4.03 time_left:4571s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   258.3     1  4410 15222   .1426      64.14 78.42 1136            131072  6.113    .4594 72.68 5.311 9.9e-06  1226  4230   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1382      18.74 202.5      .2254         0                65027 5636 19453 3.452\n",
            "\n",
            "21:25:53 | time:18932s total_exs:1531408 total_steps:65077 epochs:4.03 time_left:4557s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   235.5     1  4338 15058   .1002      43.24 78.31 1128            131072  6.004    .4594 78.15 5.294 9.9e-06  1286  4466   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1507      21.12 199.1      .2276         0                65077 5624 19523 3.471\n",
            "\n",
            "21:26:07 | time:18947s total_exs:1532532 total_steps:65127 epochs:4.03 time_left:4543s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   264.7     1  4293 14736   .1272      73.75 77.17 1124            131072  5.634    .4595 91.96 5.334 9.9e-06  1369  4699   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .2073      31.07 207.3      .2203         0                65127 5661 19435 3.433\n",
            "\n",
            "21:26:22 | time:18961s total_exs:1533596 total_steps:65177 epochs:4.04 time_left:4530s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   268.8     1  4493 15514   .1438      57.67 73.49 1064            131072  6.424    .4595 71.82 5.282 9.9e-06  1204  4158   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1288      15.24 196.7      .2246         0                65177 5697 19672 3.453\n",
            "\n",
            "21:26:37 | time:18976s total_exs:1534872 total_steps:65227 epochs:4.04 time_left:4514s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   236.9     1  4161 14095   .1301       73.9 86.45 1276            131072  6.203    .4595 80.35 5.292 9.9e-06  1380  4674   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1826      26.28 198.7      .2256         0                65227 5541 18769 3.388\n",
            "\n",
            "21:26:40 | Overflow: setting loss scale to 65536.0\n",
            "21:26:51 | time:18990s total_exs:1536032 total_steps:65277 epochs:4.04 time_left:4500s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   288.2 .9800  4353 15189   .1586      100.6 80.95 1160             81265   5.96    .4595 75.04 5.306 9.9e-06  1275  4449   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1388      20.08 201.5      .2244         0                65277 5628 19638 3.489\n",
            "\n",
            "21:27:06 | time:19006s total_exs:1537120 total_steps:65327 epochs:4.05 time_left:4487s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     257     1  4430 14531  .09835      53.41 71.38 1088             65536  6.494    .4595 78.37 5.279 9.9e-06  1200  3935   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1572      23.25 196.1      .2281         0                65327 5629 18466 3.281\n",
            "\n",
            "21:27:20 | time:19020s total_exs:1538372 total_steps:65377 epochs:4.05 time_left:4471s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   199.9     1  3979 14100  .07348      41.02 88.74 1252             65536  6.045    .4593 74.94  5.31 9.9e-06  1385  4910   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1430      19.62 202.4      .2249         0                65377 5364 19009 3.544\n",
            "\n",
            "21:27:35 | time:19034s total_exs:1539536 total_steps:65427 epochs:4.05 time_left:4457s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   246.4     1  4265 15039   .1005      63.18 82.09 1164             65536  6.019    .4593 75.21 5.301 9.9e-06  1306  4607   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1323       19.1 200.6      .2269         0                65427 5571 19645 3.526\n",
            "\n",
            "21:27:49 | time:19048s total_exs:1540612 total_steps:65477 epochs:4.05 time_left:4444s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   258.8     1  4463 15728   .1283      51.41 75.84 1076             65536  6.598    .4594 76.17 5.327 9.9e-06  1186  4181   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1636      21.05 205.9      .2219         0                65477 5649 19909 3.525\n",
            "\n",
            "21:28:03 | time:19063s total_exs:1541776 total_steps:65527 epochs:4.06 time_left:4429s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   260.9     1  4427 15257   .1572      70.72 80.23 1164             65536  6.443    .4593 66.76 5.269 9.9e-06  1267  4367   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1194      12.33 194.3      .2290         0                65527 5694 19624 3.446\n",
            "\n",
            "21:28:18 | time:19077s total_exs:1543016 total_steps:65577 epochs:4.06 time_left:4414s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   223.1     1  4189 14611   .1040      54.22  86.5 1240             65536  6.068    .4594 68.46 5.236 9.9e-06  1324  4619   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1298      15.07 187.9      .2308         0                65577 5513 19230 3.488\n",
            "\n",
            "21:28:32 | time:19091s total_exs:1544184 total_steps:65627 epochs:4.06 time_left:4399s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   234.1     1  4251 14911   .1164      52.11 81.94 1168             65536  6.347    .4594  72.3 5.305 9.9e-06  1229  4309   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1310      19.71 201.3      .2248         0                65627 5479 19220 3.508\n",
            "\n",
            "21:28:46 | time:19106s total_exs:1545404 total_steps:65677 epochs:4.07 time_left:4384s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     242     1  4177 14603   .1246      70.76  85.3 1220             65536  5.632    .4594 78.21 5.306 9.9e-06  1399  4890   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1705      20.89 201.5      .2250         0                65677 5576 19493 3.496\n",
            "\n",
            "21:29:01 | time:19120s total_exs:1546592 total_steps:65727 epochs:4.07 time_left:4369s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   256.8     1  4231 14818   .1406      78.74 83.22 1188             65536  6.352    .4594 77.56 5.254 9.9e-06  1345  4710   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1557      20.96 191.3      .2294         0                65727 5576 19528 3.503\n",
            "\n",
            "21:29:15 | time:19134s total_exs:1547716 total_steps:65777 epochs:4.07 time_left:4355s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   231.9     1  4438 15759  .08808      34.51 79.83 1124             65536  5.936    .4595 77.11 5.317 9.9e-06  1272  4516   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1566      20.53 203.7      .2219         0                65777 5710 20275 3.551\n",
            "\n",
            "21:29:30 | time:19149s total_exs:1548900 total_steps:65827 epochs:4.08 time_left:4341s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   254.4     1  4227 14373   .1191       75.9 80.51 1184             65536  5.794    .4595 83.28 5.317 9.9e-06  1363  4634   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1689      25.73 203.8      .2231         0                65827 5590 19007 3.401\n",
            "\n",
            "21:29:44 | time:19163s total_exs:1550060 total_steps:65877 epochs:4.08 time_left:4326s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     265     1  4352 15505   .1397      77.43 82.65 1160             65536  6.325    .4595 70.75 5.297 9.9e-06  1284  4573   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1190      15.42 199.8      .2224         0                65877 5636 20078 3.563\n",
            "\n",
            "21:29:58 | time:19177s total_exs:1551244 total_steps:65927 epochs:4.08 time_left:4312s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   253.2     1  4261 14830   .1090      73.28 82.41 1184             65536  6.322    .4595 73.46  5.28 9.9e-06  1257  4374   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1343      20.38 196.4      .2297         0                65927 5518 19204 3.48\n",
            "\n",
            "21:30:12 | time:19192s total_exs:1552404 total_steps:65977 epochs:4.09 time_left:4297s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   248.3     1  4343 15223   .1207      61.07 81.31 1160             65536  6.654    .4595 68.65 5.269 9.9e-06  1233  4322   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1293      15.49 194.3      .2282         0                65977 5576 19545 3.505\n",
            "\n",
            "21:30:27 | time:19206s total_exs:1553644 total_steps:66027 epochs:4.09 time_left:4282s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   215.7     1  4133 14524  .09194      49.05 87.15 1240             65536  5.864    .4595 73.48 5.314 9.9e-06  1402  4928   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1524      16.93 203.1      .2262         0                66027 5535 19452 3.514\n",
            "\n",
            "21:30:41 | time:19220s total_exs:1554804 total_steps:66077 epochs:4.09 time_left:4268s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   242.2     1  4344 15113   .1103      54.95 80.71 1160             65536  6.516    .4596 76.41 5.291 9.9e-06  1245  4332   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1414      22.73 198.6      .2273         0                66077 5590 19445 3.479\n",
            "\n",
            "21:30:55 | time:19234s total_exs:1556008 total_steps:66127 epochs:4.09 time_left:4252s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   208.2     1  4127 14970  .08555      36.75 87.34 1204             65536  6.148    .4595 65.27 5.286 9.9e-06  1265  4589   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1138      12.73 197.5      .2300         0                66127 5392 19559 3.627\n",
            "\n",
            "21:31:10 | time:19249s total_exs:1557236 total_steps:66177 epochs:4.10 time_left:4237s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   254.5     1  4211 14314   .1336      83.04 83.47 1228             65536  6.017    .4596 82.26 5.322 9.9e-06  1444  4909   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1792      23.45 204.8      .2245         0                66177 5656 19223 3.399\n",
            "\n",
            "21:31:24 | time:19263s total_exs:1558284 total_steps:66227 epochs:4.10 time_left:4224s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   301.9     1  4550 15986   .1212       84.8 73.63 1048             65536  6.357    .4597 83.57 5.231 9.9e-06  1155  4057   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1517      28.47  187      .2311         0                66227 5705 20043 3.513\n",
            "\n",
            "21:31:38 | time:19277s total_exs:1559512 total_steps:66277 epochs:4.10 time_left:4209s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   211.5     1  4184 14897  .09528      41.14 87.46 1228             65536  5.939    .4596  72.6 5.277 9.9e-06  1315  4682   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1401      19.06 195.8      .2298         0                66277 5498 19580 3.561\n",
            "\n",
            "21:31:52 | time:19292s total_exs:1560656 total_steps:66327 epochs:4.11 time_left:4195s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   277.5     1  4357 15325   .1530      87.03 80.48 1144             65536  6.774    .4596 76.41 5.305 9.9e-06  1272  4474   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1512      20.82 201.3      .2266         0                66327 5629 19799 3.518\n",
            "\n",
            "21:32:07 | time:19306s total_exs:1561840 total_steps:66377 epochs:4.11 time_left:4180s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   246.3     1  4239 14812   .1208      67.31 82.74 1184             65536  6.157    .4596  79.3 5.298 9.9e-06  1370  4788   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1546      21.44 199.9      .2221         0                66377 5609 19599 3.495\n",
            "\n",
            "21:32:21 | time:19320s total_exs:1563120 total_steps:66427 epochs:4.11 time_left:4164s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   191.3     1  4033 14211  .07734      33.76 90.21 1280             65536  5.874    .4596 69.08 5.291 9.9e-06  1367  4817   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1391      15.68 198.5      .2278         0                66427 5400 19028 3.524\n",
            "\n",
            "21:32:35 | time:19335s total_exs:1564284 total_steps:66477 epochs:4.12 time_left:4150s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   258.4     1  4283 14953   .1211      74.38 81.28 1164             65536  6.121    .4597 82.07 5.294 9.9e-06  1347  4704   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1615       24.2  199      .2247         0                66477 5630 19657 3.492\n",
            "\n",
            "21:32:50 | time:19349s total_exs:1565348 total_steps:66527 epochs:4.12 time_left:4137s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   295.6     1  4576 15686   .1654      80.57 72.95 1064             65536  6.366    .4597 81.36 5.309 9.9e-06  1181  4049   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1645      25.86 202.1      .2245         0                66527 5757 19735 3.428\n",
            "\n",
            "21:33:04 | time:19363s total_exs:1566580 total_steps:66577 epochs:4.12 time_left:4121s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   226.9     1  4183 14785  .09334      57.16 87.08 1232             65536  6.103    .4597 81.47 5.293 9.9e-06  1450  5126   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1648       22.6 198.9      .2282         0                66577 5634 19911 3.534\n",
            "\n",
            "21:33:18 | time:19378s total_exs:1567732 total_steps:66627 epochs:4.13 time_left:4107s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   236.2     1  4335 15164   .1111      48.02 80.59 1152             65536  6.184    .4597 73.66 5.305 9.9e-06  1293  4524   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1510      17.52 201.3      .2271         0                66627 5629 19688 3.498\n",
            "\n",
            "21:33:32 | time:19392s total_exs:1569040 total_steps:66677 epochs:4.13 time_left:4091s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     187     1  4006 14187  .07339       33.8 92.63 1308             65536  5.853    .4534 64.51   5.3 9.9e-06  1370  4853   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1124      12.13 200.4      .2276         0                66677 5377 19040 3.541\n",
            "\n",
            "21:33:46 | time:19406s total_exs:1570260 total_steps:66727 epochs:4.13 time_left:4075s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     214     1  4218 15086   .0877      41.19 87.27 1220             65536  5.866    .4597 74.43 5.286 9.9e-06  1348  4822   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1623      19.17 197.6      .2271         0                66727 5566 19908 3.577\n",
            "\n",
            "21:34:01 | time:19421s total_exs:1571348 total_steps:66777 epochs:4.14 time_left:4062s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   290.1     1  4527 15420   .1599      82.05 74.12 1088             65536   6.03    .4598 80.58 5.305 9.9e-06  1296  4413   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1746      21.04 201.3      .2253         0                66777 5823 19833 3.406\n",
            "\n",
            "21:34:16 | time:19435s total_exs:1572564 total_steps:66827 epochs:4.14 time_left:4047s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   255.9     1  4333 14813   .1423       77.7 83.14 1216             65536  5.971    .4598 75.98 5.288 9.9e-06  1377  4708   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1530      19.36  198      .2268         0                66827 5710 19521 3.419\n",
            "\n",
            "21:34:30 | time:19449s total_exs:1573844 total_steps:66877 epochs:4.14 time_left:4031s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   176.6     1  3806 13725  .05469      27.96 92.31 1280             65536  5.906    .4598 72.89 5.333 9.9e-06  1358  4898   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1273      19.83  207      .2243         0                66877 5164 18623 3.606\n",
            "\n",
            "21:34:44 | time:19463s total_exs:1574952 total_steps:66927 epochs:4.14 time_left:4017s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   277.7     1  4436 15726   .1273      77.51 78.56 1108             65536   6.15    .4598 73.86 5.336 9.9e-06  1247  4422   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1661      17.58 207.8      .2233         0                66927 5683 20148 3.545\n",
            "\n",
            "21:34:58 | time:19477s total_exs:1576144 total_steps:66977 epochs:4.15 time_left:4002s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   217.8     1  4169 14877  .09396       42.9 85.07 1192             65536  6.202    .4598 71.17 5.267 9.9e-06  1281  4571   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1468      17.45 193.8      .2311         0                66977 5450 19448 3.569\n",
            "\n",
            "21:35:12 | time:19492s total_exs:1577304 total_steps:67027 epochs:4.15 time_left:3988s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     260     1  4329 15038   .1397      73.43 80.59 1160             65536  6.033    .4598 76.28  5.26 9.9e-06  1333  4631   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1448      18.82 192.5      .2269         0                67027 5662 19669 3.474\n",
            "\n",
            "21:35:26 | time:19506s total_exs:1578584 total_steps:67077 epochs:4.15 time_left:3972s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   166.7     1  3929 14238  .03516      13.19 92.77 1280             65536  5.724    .4473 63.13 5.267 9.9e-06  1318  4778   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1094      11.63 193.8      .2331         0                67077 5247 19015 3.624\n",
            "\n",
            "21:35:41 | time:19520s total_exs:1579660 total_steps:67127 epochs:4.16 time_left:3959s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   279.2     1  4546 15740   .1682      67.99 74.52 1076             65536  6.784    .4599  74.1 5.275 9.9e-06  1172  4059   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1431      19.62 195.3      .2284         0                67127 5718 19800 3.463\n",
            "\n",
            "21:35:55 | time:19535s total_exs:1580756 total_steps:67177 epochs:4.16 time_left:3945s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   282.9     1  4475 15349   .1788      78.74 75.18 1096             65536  6.257    .4599  84.7 5.301 9.9e-06  1309  4491   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1761      24.96 200.6      .2264         0                67177 5785 19840 3.43\n",
            "\n",
            "21:36:09 | time:19549s total_exs:1581976 total_steps:67227 epochs:4.16 time_left:3930s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   220.4     1  4134 14648  .09672      50.94 86.46 1220             65536   6.12    .4599 74.67 5.298 9.9e-06  1396  4945   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1525      17.47 199.9      .2261         0                67227 5529 19593 3.544\n",
            "\n",
            "21:36:25 | time:19564s total_exs:1583100 total_steps:67277 epochs:4.17 time_left:3917s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   274.8     1  4368 14139   .1566      80.52 72.77 1124            115343  6.023    .4593 76.07 5.322 9.9e-06  1284  4157   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1459      18.94 204.9      .2242         0                67277 5652 18296 3.237\n",
            "\n",
            "21:36:39 | time:19579s total_exs:1584344 total_steps:67327 epochs:4.17 time_left:3901s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   200.9     1  4096 14380  .08601       36.3 87.36 1244            131072  6.378    .4593  65.4 5.289 9.9e-06  1290  4529   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1214      13.56 198.1      .2286         0                67327 5385 18909 3.512\n",
            "\n",
            "21:36:53 | time:19593s total_exs:1585504 total_steps:67377 epochs:4.17 time_left:3887s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   255.8     1  4304 15171   .1233      70.32 81.78 1160            131072  6.664    .4593 73.12 5.286 9.9e-06  1292  4553   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1457      17.45 197.5      .2252         0                67377 5595 19724 3.525\n",
            "\n",
            "21:37:08 | time:19607s total_exs:1586728 total_steps:67427 epochs:4.18 time_left:3871s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   227.2     1  4229 15010   .1127      54.43 86.89 1224            131072  6.104    .4594 74.12 5.255 9.9e-06  1336  4740   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1405      19.57 191.5      .2306         0                67427 5564 19750 3.55\n",
            "\n",
            "21:37:22 | time:19621s total_exs:1587804 total_steps:67477 epochs:4.18 time_left:3858s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   286.3     1  4531 15692   .1515      75.76 74.53 1076            131072  6.407    .4594 79.28 5.288 9.9e-06  1228  4254   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1701      22.21  198      .2250         0                67477 5759 19946 3.464\n",
            "\n",
            "21:37:36 | time:19636s total_exs:1589028 total_steps:67527 epochs:4.18 time_left:3843s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   200.9     1  4154 14680  .08578      31.24 86.52 1224            131072  6.164    .4594 68.96  5.26 9.9e-06  1291  4561   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1283      16.24 192.5      .2289         0                67527 5444 19241 3.535\n",
            "\n",
            "21:37:50 | time:19650s total_exs:1590256 total_steps:67577 epochs:4.18 time_left:3828s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   213.7     1  4190 14864   .0855      43.07 87.14 1228            131072  6.544    .4594 71.65 5.244 9.9e-06  1271  4509   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1450      19.91 189.4      .2322         0                67577 5461 19373 3.548\n",
            "\n",
            "21:38:05 | time:19664s total_exs:1591484 total_steps:67627 epochs:4.19 time_left:3812s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     218     1  4121 14410   .1124      50.19 85.88 1228            131072  5.823    .4594 74.49 5.268 9.9e-06  1383  4835   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1458      18.19 193.9      .2273         0                67627 5504 19245 3.497\n",
            "\n",
            "21:38:19 | time:19678s total_exs:1592572 total_steps:67677 epochs:4.19 time_left:3799s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   255.8     1  4397 15699   .1094      53.67 77.68 1088            131072  6.621    .4594 73.08 5.244 9.9e-06  1211  4323   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1268      17.43 189.5      .2336         0                67677 5608 20021 3.57\n",
            "\n",
            "21:38:33 | time:19692s total_exs:1593872 total_steps:67727 epochs:4.19 time_left:3782s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     216     1  4041 14146   .1054       60.6 91.02 1300            131072  5.723    .4594 72.26 5.267 9.9e-06  1428  4999   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1454      17.33 193.9      .2286         0                67727 5469 19145 3.501\n",
            "\n",
            "21:38:47 | time:19707s total_exs:1595032 total_steps:67777 epochs:4.20 time_left:3768s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   246.4     1  4304 14915   .1233      60.94 80.41 1160            131072  6.183    .4594 82.23 5.284 9.9e-06  1330  4609   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1621      24.91 197.2      .2282         0                67777 5633 19524 3.466\n",
            "\n",
            "21:39:02 | time:19721s total_exs:1596276 total_steps:67827 epochs:4.20 time_left:3753s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   205.4     1  4117 14378  .09486      39.96 86.88 1244            131072  5.904    .4594 71.94 5.288 9.9e-06  1372  4790   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1543      16.81  198      .2266         0                67827 5489 19168 3.492\n",
            "\n",
            "21:39:16 | time:19736s total_exs:1597332 total_steps:67877 epochs:4.20 time_left:3740s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   318.2     1  4480 15529   .1316        106  73.2 1056            131072  6.241    .4596 84.58 5.291 9.9e-06  1210  4195   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1733      27.27 198.4      .2265         0                67877 5691 19724 3.466\n",
            "\n",
            "21:39:30 | time:19750s total_exs:1598524 total_steps:67927 epochs:4.21 time_left:3725s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   203.7     1  4129 14677  .08054      30.46 84.74 1192            131072  6.137    .4595  72.6 5.311 9.9e-06  1327  4716   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1292      16.95 202.5      .2247         0                67927 5456 19392 3.555\n",
            "\n",
            "21:39:45 | time:19764s total_exs:1599612 total_steps:67977 epochs:4.21 time_left:3712s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   295.4     1  4504 15557   .1507      88.42 75.16 1088            131072  7.202    .4596 78.11 5.265 9.9e-06  1214  4193   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1553      22.32 193.4      .2292         0                67977 5718 19750 3.454\n",
            "\n",
            "21:39:59 | time:19779s total_exs:1600896 total_steps:68027 epochs:4.21 time_left:3696s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   202.8     1  4113 14308   .1192      42.65 89.34 1284            131072  6.369    .4595 67.88  5.26 9.9e-06  1345  4680   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1347      15.49 192.5      .2283         0                68027 5458 18989 3.479\n",
            "\n",
            "21:40:14 | time:19793s total_exs:1602048 total_steps:68077 epochs:4.22 time_left:3681s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   244.3     1  4254 14898   .1328      59.66 80.69 1152            131072  6.244    .4596 85.01 5.245 9.9e-06  1358  4754   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1580      26.09 189.7      .2316         0                68077 5611 19652 3.502\n",
            "\n",
            "21:40:28 | time:19807s total_exs:1603248 total_steps:68127 epochs:4.22 time_left:3666s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   238.3     1  4173 14811   .1042      64.41 85.17 1200            131072  6.227    .4596 71.86 5.278 9.9e-06  1281  4546   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1383      18.48  196      .2293         0                68127 5454 19357 3.549\n",
            "\n",
            "21:40:42 | time:19822s total_exs:1604288 total_steps:68177 epochs:4.22 time_left:3654s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   305.9     1  4503 15580   .1558      89.43 71.96 1040            131072  6.217    .4596 87.12 5.295 9.9e-06  1292  4469   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .2038      25.02 199.4      .2224         0                68177 5795 20049 3.46\n",
            "\n",
            "21:40:56 | time:19836s total_exs:1605464 total_steps:68227 epochs:4.22 time_left:3639s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   240.7     1  4280 15033  .09949      58.74 82.61 1176            131072  6.331    .4596 69.59 5.264 9.9e-06  1227  4308   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1318      17.44 193.3      .2290         0                68227 5507 19341 3.513\n",
            "\n",
            "21:41:11 | time:19850s total_exs:1606740 total_steps:68277 epochs:4.23 time_left:3623s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   189.1     1  4001 14074   .0721      32.38 89.77 1276            131072   5.91    .4596 70.63 5.308 9.9e-06  1396  4911   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1262      15.93 201.9      .2272         0                68277 5397 18984 3.518\n",
            "\n",
            "21:41:25 | time:19864s total_exs:1607844 total_steps:68327 epochs:4.23 time_left:3610s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   255.3     1  4392 15388   .1458      56.36 77.35 1104            131072  6.427    .4597 74.05 5.267 9.9e-06  1194  4184   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1359      19.95 193.8      .2307         0                68327 5587 19572 3.504\n",
            "\n",
            "21:41:40 | time:19879s total_exs:1608956 total_steps:68377 epochs:4.23 time_left:3596s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   280.2     1  4439 15261   .1691      80.57 76.46 1112            131072  6.693    .4597 82.84 5.347 9.9e-06  1312  4512   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1745      23.83  210      .2220         0                68377 5752 19773 3.438\n",
            "\n",
            "21:41:54 | time:19893s total_exs:1610184 total_steps:68427 epochs:4.24 time_left:3581s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   234.3     1  4371 15356   .1140      56.35 86.29 1228            131072  6.421    .4597  69.1 5.271 9.9e-06  1292  4541   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1287      16.48 194.7      .2316         0                68427 5663 19897 3.514\n",
            "\n",
            "21:42:08 | time:19907s total_exs:1611452 total_steps:68477 epochs:4.24 time_left:3565s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   197.6     1  3990 14202   .0836      40.25 90.27 1268            131072  5.975    .4597 73.64 5.282 9.9e-06  1373  4887   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1475       19.5 196.7      .2292         0                68477 5363 19089 3.56\n",
            "\n",
            "21:42:22 | time:19922s total_exs:1612544 total_steps:68527 epochs:4.24 time_left:3552s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   278.7     1  4466 15499   .1520      74.24  75.8 1092            131072  6.051    .4597  85.9 5.284 9.9e-06  1326  4601   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1859      25.19 197.3      .2251         0                68527 5792 20100 3.471\n",
            "\n",
            "21:42:36 | time:19936s total_exs:1613756 total_steps:68577 epochs:4.25 time_left:3536s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   220.4     1  4142 14711  .09571      49.48 86.09 1212            131072  6.168    .4597 78.26  5.32 9.9e-06  1345  4776   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1576      22.78 204.5      .2258         0                68577 5487 19487 3.552\n",
            "\n",
            "21:42:51 | time:19950s total_exs:1614912 total_steps:68627 epochs:4.25 time_left:3522s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   226.9     1  4203 14851   .0917      45.17  81.7 1156            131072  6.252    .4597 79.33 5.269 9.9e-06  1247  4408   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1384      25.38 194.2      .2258         0                68627 5450 19259 3.534\n",
            "\n",
            "21:43:05 | time:19964s total_exs:1616100 total_steps:68677 epochs:4.25 time_left:3507s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     242     1  4364 15201   .1271      58.37 82.77 1188            131072  6.398    .4597  74.4 5.264 9.9e-06  1297  4519   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1380      19.81 193.2      .2294         0                68677 5661 19720 3.484\n",
            "\n",
            "21:43:19 | time:19978s total_exs:1617284 total_steps:68727 epochs:4.26 time_left:3493s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   237.6     1  4220 15066   .1081      59.41 84.53 1184            131072  6.201    .4597 73.78 5.291 9.9e-06  1298  4635   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1453      18.94 198.5      .2276         0                68727 5519 19701 3.57\n",
            "\n",
            "21:43:33 | time:19993s total_exs:1618404 total_steps:68777 epochs:4.26 time_left:3479s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   289.9     1  4406 15435   .1554      93.26 78.48 1120            131072  6.699    .4597 71.74 5.275 9.9e-06  1225  4293   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1348      17.04 195.3      .2258         0                68777 5631 19727 3.504\n",
            "\n",
            "21:43:48 | time:20007s total_exs:1619580 total_steps:68827 epochs:4.26 time_left:3464s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   219.2     1  4247 14922   .1122      38.63 82.64 1176            131072  5.853    .4598 73.07 5.281 9.9e-06  1308  4597   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1522      17.44 196.6      .2280         0                68827 5556 19519 3.514\n",
            "\n",
            "21:44:02 | time:20021s total_exs:1620716 total_steps:68877 epochs:4.26 time_left:3450s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   256.4     1  4407 15433   .1417      62.46 79.57 1136            131072  6.375    .4598 77.29 5.244 9.9e-06  1267  4438   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1523      21.51 189.5      .2296         0                68877 5674 19871 3.502\n",
            "\n",
            "21:44:16 | time:20035s total_exs:1622028 total_steps:68927 epochs:4.27 time_left:3434s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   180.4     1  3942 14086  .06784      30.14 93.75 1312            131072  5.991    .4598 62.18 5.288 9.9e-06  1332  4760   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1197      11.41  198      .2293         0                68927 5274 18845 3.573\n",
            "\n",
            "21:44:30 | time:20049s total_exs:1623284 total_steps:68977 epochs:4.27 time_left:3418s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   201.3     1  4012 14224  .07484      41.56 89.06 1256            131072  6.027    .4598 70.49 5.276 9.9e-06  1405  4981   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1282      14.57 195.6      .2258         0                68977 5417 19205 3.546\n",
            "\n",
            "21:44:44 | time:20063s total_exs:1624404 total_steps:69027 epochs:4.27 time_left:3404s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   239.5     1  4216 15143  .09464      51.25 80.44 1120            131072  6.439    .4599 80.13 5.289 9.9e-06  1279  4593   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1696      23.04 198.1      .2268         0                69027 5495 19735 3.592\n",
            "\n",
            "21:44:58 | time:20078s total_exs:1625548 total_steps:69077 epochs:4.28 time_left:3390s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   264.7     1  4335 15174   .1031      75.24 80.09 1144            131072  5.733    .4599 85.93 5.257 9.9e-06  1317  4611   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1521      28.36 191.9      .2286         0                69077 5652 19785 3.501\n",
            "\n",
            "21:45:13 | time:20092s total_exs:1626752 total_steps:69127 epochs:4.28 time_left:3375s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   257.1     1  4230 14731   .1271       81.4 83.85 1204            131072  5.982    .4599 85.12 5.295 9.9e-06  1401  4877   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1636      26.95 199.2      .2260         0                69127 5631 19608 3.482\n",
            "\n",
            "21:45:28 | time:20107s total_exs:1627944 total_steps:69177 epochs:4.28 time_left:3361s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   251.3     1  4299 14107   .1359      70.96 78.22 1192            131072  6.424    .4599 74.58 5.301 9.9e-06  1295  4249   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1409      20.26 200.5      .2260         0                69177 5594 18356 3.281\n",
            "\n",
            "21:45:42 | time:20122s total_exs:1629060 total_steps:69227 epochs:4.29 time_left:3347s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   240.8     1  4465 15607  .09857      40.73 78.01 1116            131072  6.828    .4593 68.04 5.244 9.9e-06  1201  4199   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1254      14.22 189.3      .2298         0                69227 5667 19805 3.495\n",
            "\n",
            "21:45:55 | Overflow: setting loss scale to 131072.0\n",
            "21:45:57 | time:20136s total_exs:1630304 total_steps:69277 epochs:4.29 time_left:3331s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   240.4 .9800  4216 14749   .1141      70.98 87.05 1244            214958  5.756    .4594 80.76 5.277 9.9e-06  1373  4803   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1592      25.59 195.8      .2272         0                69277 5588 19552 3.499\n",
            "\n",
            "21:46:11 | time:20150s total_exs:1631420 total_steps:69327 epochs:4.29 time_left:3318s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     227     1  4256 15426  .08065      36.35  80.9 1116            131072  6.315    .4594 71.25 5.264 9.9e-06  1254  4544   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1523      15.08 193.3      .2293         0                69327 5509 19970 3.625\n",
            "\n",
            "21:46:25 | time:20164s total_exs:1632684 total_steps:69377 epochs:4.30 time_left:3302s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   205.2     1  4016 14075   .1179       46.4 88.61 1264            131072  5.833    .4593 73.33 5.274 9.9e-06  1401  4910   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1432      17.92 195.1      .2274         0                69377 5416 18985 3.505\n",
            "\n",
            "21:46:39 | time:20179s total_exs:1633828 total_steps:69427 epochs:4.30 time_left:3288s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   242.3     1  4392 15359   .1215      50.33 80.01 1144            131072  6.826    .4593  72.1 5.293 9.9e-06  1270  4441   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1469       16.6  199      .2267         0                69427 5662 19800 3.497\n",
            "\n",
            "21:46:54 | time:20193s total_exs:1634936 total_steps:69477 epochs:4.30 time_left:3274s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   250.9     1  4373 15263   .1273      53.52 77.34 1108            131072  6.523    .4594 76.02 5.322 9.9e-06  1225  4277   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1480      20.72 204.8      .2240         0                69477 5599 19540 3.49\n",
            "\n",
            "21:47:08 | time:20207s total_exs:1636168 total_steps:69527 epochs:4.31 time_left:3259s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     203     1  4094 14311  .08523      36.83 86.13 1232            131072  5.846    .4594 78.25 5.299 9.9e-06  1391  4862   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1631       21.8 200.2      .2285         0                69527 5485 19173 3.496\n",
            "\n",
            "21:47:22 | time:20222s total_exs:1637364 total_steps:69577 epochs:4.31 time_left:3244s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   220.4     1  4208 14747   .1028      44.45 83.82 1196            131072  5.888    .4594 70.19 5.284 9.9e-06  1304  4571   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1405      15.66 197.2      .2282         0                69577 5513 19317 3.504\n",
            "\n",
            "21:47:37 | time:20236s total_exs:1638492 total_steps:69627 epochs:4.31 time_left:3230s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   268.3     1  4373 15150   .1551       74.4 78.15 1128            131072  6.853    .4594 80.02 5.252 9.9e-06  1265  4383   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1649      23.93  191      .2302         0                69627 5639 19533 3.464\n",
            "\n",
            "21:47:51 | time:20250s total_exs:1639648 total_steps:69677 epochs:4.31 time_left:3216s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   227.1     1  4217 15004  .09516      44.74 82.27 1156            131072  5.974    .4594 76.83 5.259 9.9e-06  1265  4502   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1471      22.11 192.2      .2304         0                69677 5482 19506 3.559\n",
            "\n",
            "21:48:05 | time:20264s total_exs:1640840 total_steps:69727 epochs:4.32 time_left:3201s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     249     1  4270 14923   .1183      69.87 83.32 1192            131072  6.426    .4594 74.07 5.287 9.9e-06  1278  4468   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1426      20.44 197.7      .2261         0                69727 5548 19392 3.495\n",
            "\n",
            "21:48:19 | time:20279s total_exs:1642100 total_steps:69777 epochs:4.32 time_left:3185s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   215.5     1  4156 14505   .1032      50.64 87.96 1260            131072  6.063    .4595    74 5.248 9.9e-06  1372  4789   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1508      19.56 190.2      .2327         0                69777 5528 19294 3.491\n",
            "\n",
            "21:48:34 | time:20294s total_exs:1643068 total_steps:69827 epochs:4.32 time_left:3174s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   420.2     1  4877 16630   .2800      168.3 66.01  968            131072  6.933    .4595 92.92 5.276 9.9e-06  1174  4003   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1860      32.28 195.5      .2268         0                69827 6051 20633 3.41\n",
            "\n",
            "21:48:49 | time:20308s total_exs:1644252 total_steps:69877 epochs:4.33 time_left:3159s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   294.2     1  4311 14897   .1208      112.1 81.83 1184            131072  5.964    .4595 86.01 5.323 9.9e-06  1337  4622   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1630      29.53  205      .2215         0                69877 5648 19519 3.456\n",
            "\n",
            "21:49:03 | time:20322s total_exs:1645436 total_steps:69927 epochs:4.33 time_left:3144s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   220.3     1  4172 14799  .08868      44.08    84 1184            131072  6.185    .4595  73.3 5.294 9.9e-06  1363  4835   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1394      15.74 199.2      .2267         0                69927 5535 19635 3.548\n",
            "\n",
            "21:49:17 | time:20337s total_exs:1646616 total_steps:69977 epochs:4.33 time_left:3130s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     261     1  4260 14807   .1500      80.49 82.02 1180            131072  6.649    .4595  71.3 5.236 9.9e-06  1250  4343   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1356      18.35 187.9      .2353         0                69977 5510 19149 3.476\n",
            "\n",
            "21:49:31 | time:20350s total_exs:1647724 total_steps:70027 epochs:4.34 time_left:3116s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     226     1  4316 15536  .09025      31.24 79.76 1108            131072  6.669    .4595 63.57 5.234 9.9e-06  1129  4065   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1255      12.61 187.6      .2298         0                70027 5445 19600  3.6\n",
            "\n",
            "21:49:45 | time:20365s total_exs:1648936 total_steps:70077 epochs:4.34 time_left:3101s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   206.8     1  4146 14699  .06683       35.7 85.93 1212            131072  5.853    .4595 76.01 5.296 9.9e-06  1393  4938   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1353      18.54 199.5      .2259         0                70077 5539 19636 3.545\n",
            "\n",
            "21:50:00 | time:20379s total_exs:1650088 total_steps:70127 epochs:4.34 time_left:3087s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   260.9     1  4372 14986   .1719       71.1 78.97 1152            131072  6.497    .4595 75.49 5.272 9.9e-06  1317  4515   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1519      18.32 194.8      .2292         0                70127 5689 19501 3.428\n",
            "\n",
            "21:50:14 | time:20393s total_exs:1651348 total_steps:70177 epochs:4.35 time_left:3071s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   197.4     1  4119 14927  .09127      33.96 91.33 1260            131072   6.54    .4595 62.81  5.25 9.9e-06  1304  4725   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1143      11.07 190.6      .2310         0                70177 5423 19652 3.624\n",
            "\n",
            "21:50:29 | time:20408s total_exs:1652448 total_steps:70227 epochs:4.35 time_left:3058s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   300.9     1  4495 15229   .1473      96.61 74.54 1100            131072  6.038    .4596 93.84 5.279 9.9e-06  1368  4634   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .2018      31.68 196.3      .2252         0                70227 5862 19863 3.389\n",
            "\n",
            "21:50:42 | time:20422s total_exs:1653612 total_steps:70277 epochs:4.35 time_left:3043s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     205     1  4055 14702  .07216       30.8  84.4 1164            131072  6.892    .4596 74.23 5.267 9.9e-06  1275  4620   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1478      19.48 193.9      .2293         0                70277 5330 19322 3.626\n",
            "\n",
            "21:50:57 | time:20436s total_exs:1654736 total_steps:70327 epochs:4.35 time_left:3029s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   266.4     1  4406 15469   .1237      70.39 78.93 1124            131072  6.336    .4596 73.96 5.249 9.9e-06  1208  4241   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1379      20.24 190.3      .2314         0                70327 5613 19709 3.511\n",
            "\n",
            "21:51:11 | time:20450s total_exs:1656024 total_steps:70377 epochs:4.36 time_left:3013s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   199.3     1  3985 14103  .09006      44.62 91.16 1288            131072  6.321    .4596 71.85 5.229 9.9e-06  1314  4649   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1328      20.85 186.6      .2356         0                70377 5299 18752 3.539\n",
            "\n",
            "21:51:25 | time:20464s total_exs:1657152 total_steps:70427 epochs:4.36 time_left:2999s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   232.7     1  4299 15302  .08333      42.12 80.31 1128            131072  5.989    .4596 70.49 5.253 9.9e-06  1263  4497   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1294       14.5 191.2      .2271         0                70427 5562 19799 3.56\n",
            "\n",
            "21:51:40 | time:20479s total_exs:1658228 total_steps:70477 epochs:4.36 time_left:2986s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   304.2     1  4523 15441   .1682      93.99 73.47 1076            131072  6.357    .4597 83.24 5.241 9.9e-06  1276  4356   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1747      23.95 188.8      .2309         0                70477 5799 19797 3.414\n",
            "\n",
            "21:51:54 | time:20493s total_exs:1659472 total_steps:70527 epochs:4.37 time_left:2971s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     217     1  4031 14340  .08601      54.97  88.5 1244            131072  6.056    .4597 75.75 5.262 9.9e-06  1386  4929   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1608      20.06 192.8      .2280         0                70527 5417 19269 3.557\n",
            "\n",
            "21:52:08 | time:20507s total_exs:1660560 total_steps:70577 epochs:4.37 time_left:2957s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   279.1     1  4441 15430   .1517      74.97  75.6 1088            131072  6.609    .4597    83 5.301 9.9e-06  1276  4433   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1645      24.37 200.4      .2268         0                70577 5717 19863 3.475\n",
            "\n",
            "21:52:22 | time:20521s total_exs:1661712 total_steps:70627 epochs:4.37 time_left:2943s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   233.1     1  4181 14949   .1137      51.62 82.37 1152            131072  6.582    .4597 71.26 5.219 9.9e-06  1237  4421   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1250      17.59 184.7      .2354         0                70627 5418 19371 3.576\n",
            "\n",
            "21:52:37 | time:20536s total_exs:1662904 total_steps:70677 epochs:4.38 time_left:2928s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   268.9     1  4328 15007   .1384      87.33 82.67 1192            131072   5.94    .4597  84.2 5.277 9.9e-06  1372  4758   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1644      26.64 195.9      .2284         0                70677 5700 19766 3.468\n",
            "\n",
            "21:52:51 | time:20550s total_exs:1664084 total_steps:70727 epochs:4.38 time_left:2914s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     236     1  4206 14943   .1093      57.77 83.85 1180            131072  6.592    .4597 75.89 5.315 9.9e-06  1338  4755   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1661      19.18 203.4      .2237         0                70727 5544 19698 3.553\n",
            "\n",
            "21:53:05 | time:20564s total_exs:1665208 total_steps:70777 epochs:4.38 time_left:2900s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   279.3     1  4423 15442   .1441      82.54 78.49 1124            131072  5.963    .4597 86.09 5.298 9.9e-06  1365  4767   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1735      25.35 199.9      .2245         0                70777 5788 20210 3.492\n",
            "\n",
            "21:53:19 | time:20578s total_exs:1666396 total_steps:70827 epochs:4.39 time_left:2885s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   246.9     1  4244 15146   .1187      68.26  84.8 1188            131072  7.124    .4597 71.92 5.311 9.9e-06  1242  4431   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1465      19.67 202.7      .2255         0                70827 5485 19577 3.569\n",
            "\n",
            "21:53:33 | time:20593s total_exs:1667508 total_steps:70877 epochs:4.39 time_left:2871s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   289.2     1  4415 15415   .1502      90.71 77.65 1112            131072  6.233    .4598 84.32 5.323 9.9e-06  1310  4574   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1790      25.41 204.9      .2196         0                70877 5725 19990 3.492\n",
            "\n",
            "21:53:47 | time:20607s total_exs:1668748 total_steps:70927 epochs:4.39 time_left:2856s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   199.8     1  4033 14485  .07419       37.2 89.07 1240            131072  6.313    .4597 65.57  5.24 9.9e-06  1307  4693   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1065      12.88 188.7      .2300         0                70927 5340 19177 3.592\n",
            "\n",
            "21:54:02 | time:20621s total_exs:1669820 total_steps:70977 epochs:4.39 time_left:2843s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   291.3     1  4565 15884   .1427      78.42  74.6 1072            131072  6.772    .4597 77.75 5.262 9.9e-06  1218  4237   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1586      20.96 192.9      .2288         0                70977 5783 20121 3.48\n",
            "\n",
            "21:54:16 | time:20635s total_exs:1671132 total_steps:71027 epochs:4.40 time_left:2826s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   211.5     1  4135 14639  .09985      53.89 92.89 1312            131072  5.834    .4598 66.23 5.204 9.9e-06  1308  4632   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1220      16.37  182      .2365         0                71027 5444 19271 3.54\n",
            "\n",
            "21:54:30 | time:20649s total_exs:1672388 total_steps:71077 epochs:4.40 time_left:2811s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   187.5     1  4006 14403  .06768      28.05 90.31 1256            131072  6.127    .4598 68.75 5.291 9.9e-06  1334  4795   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1425      15.66 198.5      .2293         0                71077 5340 19198 3.595\n",
            "\n",
            "21:54:44 | time:20663s total_exs:1673524 total_steps:71127 epochs:4.40 time_left:2797s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   244.2     1  4281 15279   .1338      55.82  81.1 1136            131072  6.262    .4598 72.86 5.266 9.9e-06  1263  4507   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1373      17.28 193.7      .2295         0                71127 5543 19787 3.57\n",
            "\n",
            "21:54:59 | time:20679s total_exs:1674704 total_steps:71177 epochs:4.41 time_left:2782s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   290.8     1  4265 13825   .1186      110.1  76.5 1180            131072   5.98    .4598 82.95 5.301 9.9e-06  1317  4271   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1636      27.13 200.6      .2267         0                71177 5582 18095 3.242\n",
            "\n",
            "21:55:14 | time:20693s total_exs:1675832 total_steps:71227 epochs:4.41 time_left:2768s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   233.7     1  4436 15634   .1046      37.11 79.52 1128            131072  6.481    .4593 72.92 5.253 9.9e-06  1256  4426   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1303      17.26 191.1      .2293         0                71227 5691 20060 3.525\n",
            "\n",
            "21:55:28 | time:20707s total_exs:1677036 total_steps:71277 epochs:4.41 time_left:2753s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   207.7     1  4049 14459  .07226      39.54    86 1204            146801  6.035    .4594 71.16  5.28 9.9e-06  1302  4651   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1287      17.08 196.4      .2285         0                71277 5351 19110 3.572\n",
            "\n",
            "21:55:42 | time:20721s total_exs:1678148 total_steps:71327 epochs:4.42 time_left:2740s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   251.5     1  4327 15339   .1241      56.98 78.84 1112            262144  6.324    .4594 74.48 5.297 9.9e-06  1266  4488   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1520      17.55 199.7      .2263         0                71327 5593 19827 3.545\n",
            "\n",
            "21:55:56 | time:20735s total_exs:1679292 total_steps:71377 epochs:4.42 time_left:2725s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   223.2     1  4320 15247  .09091      34.35 80.76 1144            262144  6.618    .4593 78.28  5.32 9.9e-06  1310  4625   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1713      21.02 204.3      .2249         0                71377 5630 19872 3.53\n",
            "\n",
            "21:56:02 | Overflow: setting loss scale to 131072.0\n",
            "21:56:10 | time:20750s total_exs:1680592 total_steps:71427 epochs:4.42 time_left:2709s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   208.1 .9800  4003 14056   .1008       54.1 91.29 1300            188744  5.815    .4594 73.93 5.261 9.9e-06  1379  4841   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1500       20.9 192.7      .2307         0                71427 5382 18897 3.511\n",
            "\n",
            "21:56:24 | time:20764s total_exs:1681800 total_steps:71477 epochs:4.43 time_left:2694s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   193.1     1  4040 14472   .0596      25.86 86.55 1208            131072  6.102    .4594 75.29  5.25 9.9e-06  1366  4893   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1424      18.76 190.7      .2303         0                71477 5405 19364 3.583\n",
            "\n",
            "21:56:39 | time:20778s total_exs:1682912 total_steps:71527 epochs:4.43 time_left:2681s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   245.2     1  4393 15225   .1142      47.71 77.08 1112            131072  6.293    .4594 77.58 5.203 9.9e-06  1279  4433   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1610      20.07 181.8      .2336         0                71527 5672 19658 3.466\n",
            "\n",
            "21:56:53 | time:20792s total_exs:1684132 total_steps:71577 epochs:4.43 time_left:2665s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   244.4     1  4258 15130   .1451      69.85  86.7 1220            131072  7.193    .4594 71.63 5.266 9.9e-06  1295  4601   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1303      18.57 193.6      .2302         0                71577 5553 19731 3.554\n",
            "\n",
            "21:57:07 | time:20806s total_exs:1685256 total_steps:71627 epochs:4.43 time_left:2651s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   254.5     1  4468 15742   .1495      55.74  79.2 1124            131072  6.584    .4594  67.7 5.267 9.9e-06  1212  4269   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1246      13.79 193.9      .2264         0                71627 5680 20012 3.523\n",
            "\n",
            "21:57:21 | time:20820s total_exs:1686556 total_steps:71677 epochs:4.44 time_left:2635s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   209.1     1  4047 14395   .1092      53.47 92.48 1300            131072  6.054    .4595 68.56 5.234 9.9e-06  1403  4990   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1292       14.6 187.6      .2327         0                71677 5450 19384 3.557\n",
            "\n",
            "21:57:35 | time:20835s total_exs:1687712 total_steps:71727 epochs:4.44 time_left:2621s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   231.7     1  4200 14785   .0891      50.07 81.38 1156            131072  5.905    .4594 82.69 5.301 9.9e-06  1419  4996   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1808       21.3 200.5      .2246         0                71727 5620 19781 3.52\n",
            "\n",
            "21:57:50 | time:20849s total_exs:1688836 total_steps:71777 epochs:4.44 time_left:2607s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   255.9     1  4428 15390   .1370      58.96 78.13 1124            131072  6.095    .4594 78.23 5.263 9.9e-06  1327  4613   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1557      19.19 193.1      .2284         0                71777 5755 20003 3.476\n",
            "\n",
            "21:58:04 | time:20863s total_exs:1689996 total_steps:71827 epochs:4.45 time_left:2593s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   227.5     1  4273 14862   .1026      43.29 80.69 1160            131072  6.498    .4595 76.96 5.306 9.9e-06  1275  4433   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1612      22.02 201.6      .2261         0                71827 5548 19295 3.478\n",
            "\n",
            "21:58:19 | time:20878s total_exs:1691204 total_steps:71877 epochs:4.45 time_left:2578s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   240.9     1  4264 14849   .1432      64.37 84.13 1208            131072  6.297    .4595 80.46 5.264 9.9e-06  1388  4835   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1805         23 193.3      .2267         0                71877 5652 19683 3.482\n",
            "\n",
            "21:58:33 | time:20892s total_exs:1692388 total_steps:71927 epochs:4.45 time_left:2563s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   229.4     1  4183 14769   .1064      52.73 83.62 1184            131072  6.131    .4595 81.06 5.272 9.9e-06  1385  4890   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1799      22.58 194.8      .2273         0                71927 5567 19659 3.531\n",
            "\n",
            "21:58:47 | time:20907s total_exs:1693548 total_steps:71977 epochs:4.46 time_left:2549s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   264.2     1  4434 15324   .1509      73.06 80.18 1160            131072  6.369    .4595 75.29 5.271 9.9e-06  1283  4435   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1422      19.98 194.6      .2302         0                71977 5718 19759 3.456\n",
            "\n",
            "21:59:02 | time:20921s total_exs:1694724 total_steps:72027 epochs:4.46 time_left:2534s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     246     1  4411 15436   .1097      58.49  82.3 1176            131072  6.839    .4596 70.56 5.241 9.9e-06  1215  4250   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1369      18.92 188.9      .2304         0                72027 5626 19686 3.499\n",
            "\n",
            "21:59:16 | time:20935s total_exs:1695812 total_steps:72077 epochs:4.46 time_left:2521s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   243.2     1  4390 15446   .1020      41.43 76.56 1088            131072  6.153    .4595 85.44  5.29 9.9e-06  1308  4602   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1700      25.34 198.4      .2270         0                72077 5698 20048 3.519\n",
            "\n",
            "21:59:30 | time:20950s total_exs:1697028 total_steps:72127 epochs:4.47 time_left:2506s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   237.1     1  4239 14783   .1160      62.85 84.82 1216            131072  6.427    .4596 75.44 5.241 9.9e-06  1326  4625   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1587      20.92 188.9      .2319         0                72127 5565 19408 3.488\n",
            "\n",
            "21:59:44 | time:20964s total_exs:1698212 total_steps:72177 epochs:4.47 time_left:2491s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   216.3     1  4205 15045  .09037      38.76 84.73 1184            131072  6.453    .4596 71.74 5.289 9.9e-06  1290  4617   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1258      17.25 198.2      .2271         0                72177 5495 19662 3.579\n",
            "\n",
            "21:59:58 | time:20978s total_exs:1699404 total_steps:72227 epochs:4.47 time_left:2476s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   214.3     1  4207 14965  .09815      37.81  84.8 1192            131072  6.186    .4596 68.44 5.271 9.9e-06  1293  4599   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1317      14.21 194.6      .2304         0                72227 5500 19564 3.557\n",
            "\n",
            "22:00:12 | time:20992s total_exs:1700708 total_steps:72277 epochs:4.48 time_left:2460s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   186.9     1  3966 14050  .06442       34.8 92.39 1304            131072  5.843    .4596 68.05 5.231 9.9e-06  1409  4992   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1242      14.01  187      .2332         0                72277 5375 19042 3.543\n",
            "\n",
            "22:00:27 | time:21006s total_exs:1701780 total_steps:72327 epochs:4.48 time_left:2447s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   267.2     1  4406 15413   .1082      61.65    75 1072            131072  6.393    .4596 82.85 5.267 9.9e-06  1224  4281   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1744      25.77 193.9      .2286         0                72327 5630 19694 3.498\n",
            "\n",
            "22:00:41 | time:21021s total_exs:1702952 total_steps:72377 epochs:4.48 time_left:2433s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   274.5     1  4390 14924   .1792      87.25 79.69 1172            131072  6.348    .4597  85.5 5.249 9.9e-06  1377  4683   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1766      26.73 190.3      .2309         0                72377 5768 19607  3.4\n",
            "\n",
            "22:00:56 | time:21035s total_exs:1704048 total_steps:72427 epochs:4.48 time_left:2419s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   325.3     1  4502 15619   .1825      119.9 76.05 1096            131072  6.496    .4597 90.09 5.274 9.9e-06  1273  4415   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1761      32.03 195.2      .2287         0                72427 5775 20035 3.469\n",
            "\n",
            "22:01:11 | time:21050s total_exs:1705180 total_steps:72477 epochs:4.49 time_left:2405s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   346.9     1  4461 15056   .1846      149.8 76.41 1132            131072  6.037    .4597  96.7 5.289 9.9e-06  1393  4702   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1917      35.17 198.1      .2249         0                72477 5854 19758 3.375\n",
            "\n",
            "22:01:25 | time:21064s total_exs:1706404 total_steps:72527 epochs:4.49 time_left:2390s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   211.8     1  4155 15067  .07435      42.06 88.78 1224            131072  6.789    .4347 60.01   5.2 9.9e-06  1189  4312   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "    .09722      11.44 181.2      .2365         0                72527 5344 19379 3.627\n",
            "\n",
            "22:01:39 | time:21078s total_exs:1707592 total_steps:72577 epochs:4.49 time_left:2375s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   223.9     1  4162 14830  .09848       48.7 84.66 1188            131072  6.319    .4597 77.54  5.26 9.9e-06  1347  4798   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1776      20.86 192.5      .2301         0                72577 5509 19628 3.563\n",
            "\n",
            "22:01:53 | time:21092s total_exs:1708708 total_steps:72627 epochs:4.50 time_left:2362s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   251.7     1  4350 15275   .1317      56.78 78.39 1116            131072   6.28    .4597  74.7 5.266 9.9e-06  1272  4468   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1371       17.7 193.7      .2288         0                72627 5622 19744 3.512\n",
            "\n",
            "22:02:07 | time:21107s total_exs:1709980 total_steps:72677 epochs:4.50 time_left:2346s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   213.7     1  4038 14204   .1014      54.97 89.48 1272            131072  5.636    .4597 75.21 5.231 9.9e-06  1448  5092   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1470      18.31 186.9      .2322         0                72677 5486 19296 3.518\n",
            "\n",
            "22:02:21 | time:21121s total_exs:1711156 total_steps:72727 epochs:4.50 time_left:2331s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   233.4     1  4275 15261   .1012      51.58 83.96 1176            131072  6.474    .4597 74.64 5.258 9.9e-06  1294  4617   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1531      19.64  192      .2307         0                72727 5569 19878 3.57\n",
            "\n",
            "22:02:35 | time:21135s total_exs:1712340 total_steps:72777 epochs:4.51 time_left:2316s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   214.2     1  4212 15007  .07939      36.31 84.36 1184            131072  5.945    .4598 70.27 5.265 9.9e-06  1295  4614   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1233      15.58 193.5      .2289         0                72777 5508 19620 3.563\n",
            "\n",
            "22:02:49 | time:21149s total_exs:1713464 total_steps:72827 epochs:4.51 time_left:2303s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   242.8     1  4400 15724   .1112      47.09 80.33 1124            131072  6.828    .4598 67.41 5.259 9.9e-06  1202  4297   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1370      13.92 192.3      .2321         0                72827 5603 20021 3.574\n",
            "\n",
            "22:03:04 | time:21163s total_exs:1714604 total_steps:72877 epochs:4.51 time_left:2289s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   282.6     1  4518 15653   .1877      84.44 78.99 1140            131072  6.435    .4598 79.54 5.267 9.9e-06  1286  4456   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1640      23.13 193.9      .2276         0                72877 5804 20109 3.465\n",
            "\n",
            "22:03:18 | time:21177s total_exs:1715820 total_steps:72927 epochs:4.52 time_left:2273s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     241     1  4125 14484   .1069       71.4 85.39 1216            131072  5.904    .4599 79.03 5.291 9.9e-06  1404  4931   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1587      21.29 198.5      .2284         0                72927 5529 19414 3.511\n",
            "\n",
            "22:03:32 | time:21191s total_exs:1716988 total_steps:72977 epochs:4.52 time_left:2259s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   221.4     1  4182 14879   .1010      42.42 83.11 1168            131072  6.296    .4598 72.86 5.259 9.9e-06  1286  4576   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1490       17.8 192.3      .2310         0                72977 5468 19455 3.558\n",
            "\n",
            "22:03:46 | time:21206s total_exs:1718160 total_steps:73027 epochs:4.52 time_left:2245s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     291     1  4382 15240   .1732      104.1 81.52 1172            131072  6.389    .4599 81.78 5.266 9.9e-06  1336  4646   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1553       24.8 193.6      .2288         0                73027 5718 19886 3.478\n",
            "\n",
            "22:04:01 | time:21220s total_exs:1719332 total_steps:73077 epochs:4.52 time_left:2230s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   233.3     1  4197 14645  .07935      54.27 81.79 1172            131072  6.046    .4599 78.13 5.312 9.9e-06  1303  4546   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1536      22.55 202.8      .2249         0                73077 5500 19191 3.49\n",
            "\n",
            "22:04:15 | time:21235s total_exs:1720412 total_steps:73127 epochs:4.53 time_left:2217s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   310.7     1  4456 15388   .1491      104.4 74.59 1080            131072   6.39    .4599 83.52 5.277 9.9e-06  1261  4356   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1667      25.12 195.7      .2271         0                73127 5717 19744 3.454\n",
            "\n",
            "22:04:19 | Overflow: setting loss scale to 65536.0\n",
            "22:04:30 | time:21250s total_exs:1721716 total_steps:73177 epochs:4.53 time_left:2201s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   190.5 .9800  3905 13108  .09202       40.8 87.54 1304             82575  6.146    .4599 69.75 5.269 9.9e-06  1349  4529   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1242      18.01 194.3      .2315         0                73177 5255 17637 3.357\n",
            "\n",
            "22:04:44 | time:21264s total_exs:1722820 total_steps:73227 epochs:4.53 time_left:2187s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     264     1  4499 15918   .1132      60.29 78.13 1104             65536  6.645    .4593 70.62 5.222 9.9e-06  1233  4363   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1313      14.77 185.4      .2342         0                73227 5732 20281 3.539\n",
            "\n",
            "22:04:59 | time:21279s total_exs:1723980 total_steps:73277 epochs:4.54 time_left:2173s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     291     1  4423 15055   .1250      100.3 78.97 1160             65536  5.927    .4594 89.28 5.286 9.9e-06  1342  4569   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .2009      31.43 197.5      .2275         0                73277 5765 19624 3.404\n",
            "\n",
            "22:05:13 | time:21293s total_exs:1725196 total_steps:73327 epochs:4.54 time_left:2158s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   250.7     1  4208 14923   .1135      77.66 86.24 1216             65536  6.347    .4595 77.26 5.234 9.9e-06  1367  4848   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1554      21.04 187.5      .2318         0                73327 5576 19772 3.546\n",
            "\n",
            "22:05:28 | time:21307s total_exs:1726336 total_steps:73377 epochs:4.54 time_left:2144s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   288.9     1  4319 15148   .1526      99.44 79.97 1140             65536   6.33    .4595 73.64 5.279 9.9e-06  1228  4309   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1325      19.76 196.1      .2300         0                73377 5547 19456 3.508\n",
            "\n",
            "22:05:42 | time:21321s total_exs:1727700 total_steps:73427 epochs:4.55 time_left:2127s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   169.6     1  3707 13156  .05279      33.77 96.83 1364             65536  5.741    .4595 73.95 5.239 9.9e-06  1512  5367   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1408      18.52 188.4      .2321         0                73427 5219 18523 3.55\n",
            "\n",
            "22:05:56 | time:21335s total_exs:1728800 total_steps:73477 epochs:4.55 time_left:2113s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   231.4     1  4279 15223  .08455      36.91 78.27 1100             65536  6.677    .4596  72.1  5.25 9.9e-06  1207  4292   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1555      17.25 190.6      .2303         0                73477 5486 19516 3.558\n",
            "\n",
            "22:06:10 | time:21349s total_exs:1729964 total_steps:73527 epochs:4.55 time_left:2099s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     229     1  4208 15099  .08935      48.21 83.53 1164             65536  6.558    .4596 70.04 5.281 9.9e-06  1239  4444   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1357      16.83 196.6      .2283         0                73527 5447 19543 3.588\n",
            "\n",
            "22:06:24 | time:21363s total_exs:1731160 total_steps:73577 epochs:4.56 time_left:2084s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   233.1     1  4292 15090   .1279      53.66  84.1 1196             65536   6.23    .4596 82.32 5.225 9.9e-06  1356  4768   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1547      25.63 185.9      .2342         0                73577 5648 19858 3.516\n",
            "\n",
            "22:06:38 | time:21378s total_exs:1732320 total_steps:73627 epochs:4.56 time_left:2069s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   227.5     1  4246 14937   .1112      44.52 81.62 1160             65536  6.325    .4596 69.47 5.276 9.9e-06  1302  4582   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1362      13.33 195.6      .2274         0                73627 5548 19519 3.519\n",
            "\n",
            "22:06:53 | time:21392s total_exs:1733424 total_steps:73677 epochs:4.56 time_left:2056s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   282.2     1  4523 15776   .1277      77.35 77.02 1104             65536  6.569    .4596 76.76 5.265 9.9e-06  1262  4403   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1458      19.59 193.5      .2274         0                73677 5785 20179 3.488\n",
            "\n",
            "22:07:07 | time:21406s total_exs:1734688 total_steps:73727 epochs:4.56 time_left:2040s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   192.3     1  4058 14497  .08703      31.73  90.3 1264             65536  5.926    .4596 73.47 5.269 9.9e-06  1458  5209   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1392      15.78 194.3      .2298         0                73727 5516 19705 3.572\n",
            "\n",
            "22:07:21 | time:21420s total_exs:1735992 total_steps:73777 epochs:4.57 time_left:2024s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   211.8     1  4027 14487   .1058      57.41 93.83 1304             65536  6.347    .4596 71.78 5.247 9.9e-06  1431  5148   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1365      16.91  190      .2317         0                73777 5457 19635 3.598\n",
            "\n",
            "22:07:35 | time:21434s total_exs:1737228 total_steps:73827 epochs:4.57 time_left:2009s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   215.2     1  4124 14442   .0890      48.37 86.56 1236             65536  5.924    .4596 73.15 5.268 9.9e-06  1382  4838   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1432      17.26  194      .2284         0                73827 5506 19279 3.502\n",
            "\n",
            "22:07:49 | time:21449s total_exs:1738300 total_steps:73877 epochs:4.57 time_left:1995s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   260.5     1  4376 15194  .09795       56.4 74.45 1072             65536  6.002    .4596  85.5 5.271 9.9e-06  1298  4507   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1922      24.96 194.6      .2259         0                73877 5673 19701 3.473\n",
            "\n",
            "22:08:04 | time:21463s total_exs:1739420 total_steps:73927 epochs:4.58 time_left:1982s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   267.1     1  4382 15395   .1536       71.5  78.7 1120             65536  6.531    .4597 70.06  5.25 9.9e-06  1230  4320   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1339      15.17 190.6      .2302         0                73927 5612 19716 3.514\n",
            "\n",
            "22:08:18 | time:21477s total_exs:1740616 total_steps:73977 epochs:4.58 time_left:1967s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   246.3     1  4243 14996   .1087      68.88 84.53 1196             65536  6.603    .4597 73.83 5.265 9.9e-06  1319  4663   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1321      18.67 193.4      .2272         0                73977 5563 19659 3.534\n",
            "\n",
            "22:08:32 | time:21491s total_exs:1741900 total_steps:74027 epochs:4.58 time_left:1951s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   179.3     1  3877 13856  .06386      28.36 91.78 1284             65536  5.991    .4596 63.81 5.277 9.9e-06  1303  4656   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1246      13.08 195.8      .2277         0                74027 5180 18512 3.574\n",
            "\n",
            "22:08:46 | time:21505s total_exs:1743036 total_steps:74077 epochs:4.59 time_left:1937s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     217     1  4239 15088  .08363      30.44 80.87 1136             65536  6.587    .4597 73.02 5.243 9.9e-06  1263  4496   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1400      17.42 189.3      .2327         0                74077 5502 19585 3.559\n",
            "\n",
            "22:09:00 | time:21519s total_exs:1744280 total_steps:74127 epochs:4.59 time_left:1921s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   207.5     1  4135 14540   .1101      41.28 87.49 1244             65536  6.166    .4597  72.5 5.235 9.9e-06  1387  4879   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1495      16.74 187.7      .2337         0                74127 5522 19419 3.517\n",
            "\n",
            "22:09:15 | time:21534s total_exs:1745372 total_steps:74177 epochs:4.59 time_left:1908s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     313     1  4642 15965   .2262      100.4 75.11 1092             65536  6.575    .4597 78.84 5.251 9.9e-06  1221  4200   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1456      22.92 190.8      .2284         0                74177 5864 20165 3.439\n",
            "\n",
            "22:09:29 | time:21548s total_exs:1746504 total_steps:74227 epochs:4.60 time_left:1894s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   226.2     1  4214 14990  .09452       40.1 80.53 1132             65536  6.708    .4597 74.05 5.259 9.9e-06  1249  4443   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1422      18.88 192.3      .2319         0                74227 5464 19433 3.557\n",
            "\n",
            "22:09:43 | time:21562s total_exs:1747720 total_steps:74277 epochs:4.60 time_left:1879s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   215.2     1  4106 14459   .1053       46.4 85.64 1216             65536  6.791    .4597 84.02 5.278 9.9e-06  1382  4865   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1826      27.21  196      .2294         0                74277 5488 19324 3.521\n",
            "\n",
            "22:09:58 | time:21577s total_exs:1748868 total_steps:74327 epochs:4.60 time_left:1865s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   258.8     1  4337 14974   .1533      69.95 79.28 1148             65536  6.113    .4597 78.79 5.244 9.9e-06  1261  4353   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1620      23.88 189.3      .2284         0                74327 5598 19327 3.453\n",
            "\n",
            "22:10:12 | time:21591s total_exs:1750024 total_steps:74377 epochs:4.61 time_left:1851s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   262.8     1  4306 15106   .1263      76.55 81.11 1156             65536  6.508    .4598 77.59 5.264 9.9e-06  1262  4426   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1471      23.02 193.3      .2296         0                74377 5568 19533 3.508\n",
            "\n",
            "22:10:26 | time:21605s total_exs:1751228 total_steps:74427 epochs:4.61 time_left:1836s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   228.9     1  4096 14809  .08804       58.8 87.05 1204             65536  6.463    .4598 72.26 5.277 9.9e-06  1288  4657   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1379      18.76 195.7      .2283         0                74427 5385 19466 3.615\n",
            "\n",
            "22:10:40 | time:21619s total_exs:1752332 total_steps:74477 epochs:4.61 time_left:1822s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   282.5     1  4483 15621   .1712      79.43 76.93 1104             65536  6.355    .4598 76.15 5.246 9.9e-06  1273  4435   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1386       18.5 189.8      .2308         0                74477 5756 20056 3.484\n",
            "\n",
            "22:10:54 | time:21633s total_exs:1753476 total_steps:74527 epochs:4.61 time_left:1808s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   233.3     1  4439 15848  .08129      39.34 81.69 1144             65536  6.239    .4598 75.96 5.239 9.9e-06  1296  4626   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1477      19.34 188.5      .2306         0                74527 5734 20474 3.571\n",
            "\n",
            "22:11:09 | time:21648s total_exs:1754600 total_steps:74577 epochs:4.62 time_left:1794s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   307.8     1  4504 15406   .1904      107.5  76.9 1124             65536  7.146    .4598 81.67 5.265 9.9e-06  1253  4286   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1646      25.93 193.4      .2293         0                74577 5757 19692 3.421\n",
            "\n",
            "22:11:23 | time:21662s total_exs:1755860 total_steps:74627 epochs:4.62 time_left:1779s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   220.3     1  4100 14419   .1016      57.63 88.63 1260             65536  5.971    .4599 75.62 5.233 9.9e-06  1420  4996   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1421      19.25 187.4      .2310         0                74627 5520 19415 3.517\n",
            "\n",
            "22:11:37 | time:21677s total_exs:1757072 total_steps:74677 epochs:4.62 time_left:1764s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   240.7     1  4194 14791  .09818      67.65 85.49 1212             65536  6.164    .4599 88.55  5.27 9.9e-06  1407  4962   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1914      30.51 194.3      .2279         0                74677 5601 19753 3.527\n",
            "\n",
            "22:11:51 | time:21691s total_exs:1758244 total_steps:74727 epochs:4.63 time_left:1749s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   278.6     1  4191 14802   .1015      99.86 82.79 1172             65536  6.057    .4600 82.93 5.223 9.9e-06  1360  4805   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1732       24.9 185.5      .2331         0                74727 5551 19607 3.532\n",
            "\n",
            "22:12:06 | time:21705s total_exs:1759412 total_steps:74777 epochs:4.63 time_left:1735s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   243.9     1  4295 15206   .1019      60.05  82.7 1168             65536  6.504    .4600 69.08 5.234 9.9e-06  1235  4372   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1276      16.21 187.5      .2353         0                74777 5530 19578 3.54\n",
            "\n",
            "22:12:19 | time:21719s total_exs:1760552 total_steps:74827 epochs:4.63 time_left:1721s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   248.4     1  4356 15651   .1316      57.38 81.93 1140             65536  7.045    .4600 64.24 5.255 9.9e-06  1207  4338   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1219      11.29 191.6      .2298         0                74827 5563 19989 3.593\n",
            "\n",
            "22:12:34 | time:21733s total_exs:1761616 total_steps:74877 epochs:4.64 time_left:1708s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   291.6     1  4515 15664   .1344       79.4 73.83 1064             65536  6.936    .4600 89.74 5.265 9.9e-06  1275  4424   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .2209      29.82 193.5      .2290         0                74877 5790 20088 3.47\n",
            "\n",
            "22:12:48 | time:21747s total_exs:1762960 total_steps:74927 epochs:4.64 time_left:1691s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   177.9     1  3926 13848  .08705      31.87 94.81 1344             65536  5.819    .4600 69.21 5.268 9.9e-06  1455  5133   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1324      15.06  194      .2315         0                74927 5382 18981 3.527\n",
            "\n",
            "22:13:02 | time:21762s total_exs:1764164 total_steps:74977 epochs:4.64 time_left:1676s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   206.8     1  4227 14989   .0789      31.27 85.39 1204             65536  6.009    .4600 75.43 5.273 9.9e-06  1356  4807   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1495      19.13 194.9      .2286         0                74977 5582 19796 3.547\n",
            "\n",
            "22:13:16 | time:21776s total_exs:1765392 total_steps:75027 epochs:4.65 time_left:1661s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   232.6     1  4214 14842   .1409      61.03  86.5 1228             65536  7.132    .4600 72.45 5.272 9.9e-06  1322  4657   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1547      18.62 194.8      .2291         0                75027 5536 19499 3.522\n",
            "\n",
            "22:13:31 | time:21790s total_exs:1766576 total_steps:75077 epochs:4.65 time_left:1646s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   223.4     1  4269 15062  .08784      43.16 83.55 1184             65536  5.962    .4600  74.5 5.272 9.9e-06  1309  4618   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1495      19.24 194.8      .2285         0                75077 5577 19680 3.529\n",
            "\n",
            "22:13:45 | time:21804s total_exs:1767636 total_steps:75127 epochs:4.65 time_left:1633s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   256.6     1  4428 15807   .1217      47.69 75.68 1060             65536  7.148    .4600 71.96 5.271 9.9e-06  1154  4118   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1358      17.54 194.6      .2305         0                75127 5582 19925 3.57\n",
            "\n",
            "22:13:59 | time:21819s total_exs:1768864 total_steps:75177 epochs:4.65 time_left:1618s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   248.7     1  4309 14919   .1344      73.27 85.04 1228            114033  6.525    .4600 78.81 5.279 9.9e-06  1360  4709   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1629      23.44 196.2      .2290         0                75177 5669 19628 3.463\n",
            "\n",
            "22:14:14 | time:21834s total_exs:1770084 total_steps:75227 epochs:4.66 time_left:1603s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   228.4     1  4162 13630   .1148       57.8 79.92 1220            131072  6.225    .4601 80.05 5.239 9.9e-06  1367  4476   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1582      24.03 188.4      .2330         0                75227 5528 18107 3.275\n",
            "\n",
            "22:14:29 | time:21848s total_exs:1771152 total_steps:75277 epochs:4.66 time_left:1590s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     282     1  4510 15650   .1170      70.88 74.12 1068            131072  6.187    .4593 82.93 5.259 9.9e-06  1302  4519   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1713      21.96 192.2      .2281         0                75277 5813 20169 3.47\n",
            "\n",
            "22:14:32 | Overflow: setting loss scale to 65536.0\n",
            "22:14:43 | time:21862s total_exs:1772280 total_steps:75327 epochs:4.66 time_left:1576s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   256.5 .9800  4348 15422   .1463      63.78 80.02 1128             78643  6.443    .4594 80.78 5.231 9.9e-06  1291  4578   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1560      23.57  187      .2305         0                75327 5638 20000 3.547\n",
            "\n",
            "22:14:57 | time:21877s total_exs:1773556 total_steps:75377 epochs:4.67 time_left:1560s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   234.2     1  4095 14337   .1207      73.73 89.36 1276             65536  6.301    .4594  74.1 5.251 9.9e-06  1320  4623   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1403      22.37 190.8      .2307         0                75377 5415 18959 3.502\n",
            "\n",
            "22:15:11 | time:21891s total_exs:1774676 total_steps:75427 epochs:4.67 time_left:1546s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   246.7     1  4322 15587   .1027      53.72 80.78 1120             65536   6.79    .4593 70.93 5.217 9.9e-06  1242  4478   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1509       15.5 184.4      .2350         0                75427 5564 20065 3.606\n",
            "\n",
            "22:15:25 | time:21905s total_exs:1775888 total_steps:75477 epochs:4.67 time_left:1531s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   213.8     1  4184 14988  .07426       41.2 86.83 1212             65536  6.211    .4594 73.97 5.286 9.9e-06  1327  4753   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1469      19.22 197.6      .2262         0                75477 5511 19742 3.583\n",
            "\n",
            "22:15:39 | time:21918s total_exs:1777164 total_steps:75527 epochs:4.68 time_left:1515s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   179.7     1  3983 14477  .08229      23.63 92.75 1276             65536  6.316    .4594 67.53 5.237 9.9e-06  1326  4820   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1105      15.57 188.1      .2311         0                75527 5309 19296 3.635\n",
            "\n",
            "22:15:53 | time:21933s total_exs:1778264 total_steps:75577 epochs:4.68 time_left:1502s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   240.2     1  4396 15462   .1191      40.36 77.37 1100             65536    6.3    .4594 72.35 5.251 9.9e-06  1229  4322   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1427      16.49 190.8      .2282         0                75577 5625 19785 3.517\n",
            "\n",
            "22:16:08 | time:21947s total_exs:1779408 total_steps:75627 epochs:4.68 time_left:1488s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   246.8     1  4327 15146   .1416      57.69 80.08 1144             65536  6.401    .4594  76.7 5.233 9.9e-06  1273  4456   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1687      21.05 187.4      .2348         0                75627 5601 19602  3.5\n",
            "\n",
            "22:16:22 | time:21961s total_exs:1780560 total_steps:75677 epochs:4.69 time_left:1473s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   213.9     1  4096 14691   .0816       36.1 82.63 1152             65536  6.589    .4594 72.61  5.23 9.9e-06  1278  4583   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1345      17.15 186.7      .2334         0                75677 5374 19274 3.587\n",
            "\n",
            "22:16:36 | time:21975s total_exs:1781744 total_steps:75727 epochs:4.69 time_left:1459s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     231     1  4313 15265   .1174      48.84 83.82 1184             65536  6.556    .4595 75.76 5.235 9.9e-06  1329  4704   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1444      19.64 187.8      .2335         0                75727 5641 19968 3.54\n",
            "\n",
            "22:16:50 | time:21989s total_exs:1782908 total_steps:75777 epochs:4.69 time_left:1444s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   260.5     1  4337 15112   .1289       74.2 81.11 1164             65536  6.265    .4595 76.39 5.274 9.9e-06  1304  4543   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1495      20.38 195.1      .2296         0                75777 5641 19656 3.484\n",
            "\n",
            "22:17:04 | time:22004s total_exs:1784036 total_steps:75827 epochs:4.69 time_left:1431s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   257.4     1  4315 15165   .1144       66.1 79.29 1128             65536  6.156    .4595 86.74 5.247 9.9e-06  1383  4861   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .2039      25.44 190.1      .2305         0                75827 5698 20025 3.515\n",
            "\n",
            "22:17:19 | time:22018s total_exs:1785240 total_steps:75877 epochs:4.70 time_left:1416s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   227.9     1  4166 14783   .1246      54.89 85.46 1204             65536  6.618    .4595 67.41 5.274 9.9e-06  1245  4417   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1254      15.72 195.2      .2292         0                75877 5410 19200 3.549\n",
            "\n",
            "22:17:33 | time:22032s total_exs:1786432 total_steps:75927 epochs:4.70 time_left:1401s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   234.1     1  4275 14738   .1183      54.75 82.19 1192             65536  6.391    .4595 69.27 5.211 9.9e-06  1271  4382   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1292      15.95 183.3      .2342         0                75927 5546 19120 3.448\n",
            "\n",
            "22:17:47 | time:22046s total_exs:1787580 total_steps:75977 epochs:4.70 time_left:1387s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   230.7     1  4150 15045  .08188      49.98 83.24 1148             65536  6.371    .4595 86.67 5.253 9.9e-06  1390  5039   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1707      26.14 191.2      .2327         0                75977 5540 20085 3.626\n",
            "\n",
            "22:18:01 | time:22060s total_exs:1788760 total_steps:76027 epochs:4.71 time_left:1372s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   239.1     1  4231 14891   .1381      59.82 83.06 1180             65536  6.215    .4595 71.46 5.271 9.9e-06  1289  4535   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1534      16.85 194.7      .2305         0                76027 5520 19426 3.52\n",
            "\n",
            "22:18:15 | time:22075s total_exs:1789920 total_steps:76077 epochs:4.71 time_left:1358s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   270.7     1  4294 14993   .1638      85.56    81 1160             65536  6.675    .4596  71.4 5.225 9.9e-06  1261  4402   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1500      17.05 185.8      .2336         0                76077 5555 19395 3.492\n",
            "\n",
            "22:18:30 | time:22089s total_exs:1791192 total_steps:76127 epochs:4.71 time_left:1342s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   220.2     1  4117 14370   .1022      58.34 88.79 1272             65536  6.095    .4596 80.67 5.241 9.9e-06  1433  5002   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1714      24.34 188.8      .2339         0                76127 5551 19372 3.49\n",
            "\n",
            "22:18:44 | time:22103s total_exs:1792396 total_steps:76177 epochs:4.72 time_left:1327s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   254.3     1  4271 15126   .1337      76.93 85.27 1204             65536  6.876    .4596 68.43 5.246 9.9e-06  1233  4368   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1296      17.21 189.8      .2317         0                76177 5505 19494 3.541\n",
            "\n",
            "22:18:58 | time:22117s total_exs:1793596 total_steps:76227 epochs:4.72 time_left:1312s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   246.7     1  4124 14786   .1050      74.81 86.04 1200             65536  6.564    .4596 72.07 5.247 9.9e-06  1293  4636   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1483      18.19 190.1      .2301         0                76227 5417 19422 3.585\n",
            "\n",
            "22:19:12 | time:22131s total_exs:1794808 total_steps:76277 epochs:4.72 time_left:1297s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   210.6     1  4172 14823  .06683      38.48 86.12 1212             65536  6.076    .4596 73.66 5.248 9.9e-06  1326  4709   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1469      18.98 190.3      .2324         0                76277 5498 19532 3.553\n",
            "\n",
            "22:19:26 | time:22146s total_exs:1796020 total_steps:76327 epochs:4.73 time_left:1282s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   250.3     1  4198 14781   .1271      77.08 85.34 1212             65536  6.328    .4596 83.35 5.234 9.9e-06  1395  4910   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1658      25.81 187.6      .2315         0                76327 5593 19691 3.521\n",
            "\n",
            "22:19:40 | time:22160s total_exs:1797180 total_steps:76377 epochs:4.73 time_left:1268s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   219.4     1  4189 15024  .07845      38.78  83.2 1160             65536  6.538    .4596 66.43 5.251 9.9e-06  1202  4311   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1103      14.62 190.8      .2288         0                76377 5391 19335 3.587\n",
            "\n",
            "22:19:55 | time:22174s total_exs:1798264 total_steps:76427 epochs:4.73 time_left:1255s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   265.6     1  4520 15849   .1310      57.08 76.02 1084             65536  6.673    .4597 75.37  5.25 9.9e-06  1217  4268   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1559      19.22 190.6      .2288         0                76427 5737 20117 3.507\n",
            "\n",
            "22:20:09 | time:22188s total_exs:1799420 total_steps:76477 epochs:4.74 time_left:1240s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   220.1     1  4231 15022  .09602      37.12 82.09 1156             65536  6.423    .4597 72.47 5.226 9.9e-06  1314  4665   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1497      15.65 186.1      .2308         0                76477 5544 19687 3.551\n",
            "\n",
            "22:20:23 | time:22203s total_exs:1800584 total_steps:76527 epochs:4.74 time_left:1226s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   243.9     1  4265 14771   .1306      60.72 80.63 1164             65536   6.04    .4597 87.22 5.272 9.9e-06  1398  4840   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1924      27.19 194.9      .2285         0                76527 5662 19611 3.464\n",
            "\n",
            "22:20:37 | time:22217s total_exs:1801812 total_steps:76577 epochs:4.74 time_left:1211s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   207.2     1  4058 14486  .07655      41.94 87.68 1228             65536  6.276    .4597 73.06 5.259 9.9e-06  1329  4745   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1621      18.94 192.3      .2339         0                76577 5387 19231 3.57\n",
            "\n",
            "22:20:51 | time:22231s total_exs:1802936 total_steps:76627 epochs:4.74 time_left:1197s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   242.7     1  4320 15310   .1005      50.52 79.66 1124             65536  6.713    .4597 72.35 5.201 9.9e-06  1188  4211   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1432       19.5 181.5      .2359         0                76627 5509 19521 3.544\n",
            "\n",
            "22:21:06 | time:22245s total_exs:1803988 total_steps:76677 epochs:4.75 time_left:1184s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   301.1     1  4579 15790   .1644      83.46 72.55 1052             65536  6.163    .4598 82.98 5.228 9.9e-06  1247  4300   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1559       23.7 186.5      .2323         0                76677 5826 20091 3.448\n",
            "\n",
            "22:21:20 | time:22259s total_exs:1805204 total_steps:76727 epochs:4.75 time_left:1169s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   214.5     1  4151 14680  .09704       43.8 86.01 1216             65536  6.349    .4598  68.1 5.215 9.9e-06  1269  4490   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1209      15.91  184      .2355         0                76727 5420 19170 3.537\n",
            "\n",
            "22:21:34 | time:22274s total_exs:1806316 total_steps:76777 epochs:4.75 time_left:1155s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   269.4     1  4424 15409   .1709      70.47 77.46 1112             65536  6.477    .4598 78.53 5.238 9.9e-06  1302  4535   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1511      19.98 188.3      .2302         0                76777 5726 19944 3.483\n",
            "\n",
            "22:21:48 | time:22288s total_exs:1807628 total_steps:76827 epochs:4.76 time_left:1139s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   168.5     1  3834 13989   .0503      22.41 95.75 1312             65536  5.993    .4598 67.89 5.264 9.9e-06  1410  5146   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1258      14.15 193.3      .2317         0                76827 5244 19135 3.649\n",
            "\n",
            "22:22:02 | time:22302s total_exs:1808792 total_steps:76877 epochs:4.76 time_left:1125s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   232.1     1  4232 14962   .1005      50.34  82.3 1164             65536  6.108    .4598 70.24 5.255 9.9e-06  1277  4515   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1512      15.38 191.5      .2301         0                76877 5509 19477 3.535\n",
            "\n",
            "22:22:17 | time:22316s total_exs:1809976 total_steps:76927 epochs:4.76 time_left:1110s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   240.4     1  4198 14820   .1191      63.12  83.6 1184             65536  6.459    .4598 76.62  5.22 9.9e-06  1309  4621   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1402      21.35 184.9      .2339         0                76927 5507 19441 3.531\n",
            "\n",
            "22:22:31 | time:22330s total_exs:1811172 total_steps:76977 epochs:4.77 time_left:1095s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   225.8     1  4255 14990  .09615      47.88 84.26 1196             65536  6.286    .4598 64.76 5.279 9.9e-06  1238  4359   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1229      13.02 196.2      .2313         0                76977 5493 19349 3.523\n",
            "\n",
            "22:22:45 | time:22344s total_exs:1812312 total_steps:77027 epochs:4.77 time_left:1081s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   253.9     1  4363 15355   .1325      62.49 80.24 1140             65536    6.6    .4598 74.19 5.257 9.9e-06  1256  4421   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1447      19.09 191.9      .2317         0                77027 5620 19776 3.519\n",
            "\n",
            "22:22:59 | time:22359s total_exs:1813392 total_steps:77077 epochs:4.77 time_left:1068s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   249.9     1  4413 15431  .09907      45.59 75.52 1080             65536  6.345    .4598  80.5 5.235 9.9e-06  1270  4439   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1565      21.72 187.8      .2344         0                77077 5683 19870 3.497\n",
            "\n",
            "22:23:14 | time:22373s total_exs:1814568 total_steps:77127 epochs:4.78 time_left:1054s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     205     1  4269 15134  .06037      23.47 83.37 1176             65536  6.204    .4599 74.83 5.224 9.9e-06  1335  4733   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1488      18.07 185.8      .2335         0                77127 5605 19867 3.545\n",
            "\n",
            "22:23:28 | time:22387s total_exs:1815768 total_steps:77177 epochs:4.78 time_left:1039s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   230.5     1  4212 14706   .1450      55.02 83.79 1200             65536  6.427    .4599 72.64 5.192 9.9e-06  1309  4569   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1450      18.11 179.8      .2385         0                77177 5521 19275 3.492\n",
            "\n",
            "22:23:43 | time:22402s total_exs:1817028 total_steps:77227 epochs:4.78 time_left:1023s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     205     1  4118 13809   .1040      41.59 84.51 1260             65536  6.496    .4599 64.93 5.231 9.9e-06  1323  4436   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1214      12.43  187      .2358         0                77227 5440 18245 3.354\n",
            "\n",
            "22:23:57 | time:22417s total_exs:1818272 total_steps:77277 epochs:4.78 time_left:1008s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   239.5     1  4197 14644   .1294      70.79 86.82 1244             65536  6.348    .4593 78.68 5.274 9.9e-06  1422  4962   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1584      21.52 195.3      .2286         0                77277 5619 19606 3.49\n",
            "\n",
            "22:24:11 | time:22431s total_exs:1819416 total_steps:77327 epochs:4.79 time_left:994s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   228.2     1  4276 15156  .09003      41.35  81.1 1144            117965  6.195    .4593 76.73 5.228 9.9e-06  1311  4647   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1521      19.44 186.3      .2323         0                77327 5587 19803 3.545\n",
            "\n",
            "22:24:26 | time:22445s total_exs:1820668 total_steps:77377 epochs:4.79 time_left:978s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   238.1     1  4240 14517   .1390      68.81 85.74 1252            131072   6.23    .4593 81.17 5.246 9.9e-06  1416  4847   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1526      24.64 189.7      .2306         0                77377 5655 19364 3.424\n",
            "\n",
            "22:24:40 | time:22460s total_exs:1821848 total_steps:77427 epochs:4.79 time_left:964s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   265.7     1  4272 14820   .1195      84.63 81.87 1180            131072  6.103    .4594 78.23 5.236 9.9e-06  1298  4501   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1542      23.24 187.9      .2331         0                77427 5570 19321 3.469\n",
            "\n",
            "22:24:54 | time:22474s total_exs:1822960 total_steps:77477 epochs:4.80 time_left:950s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   268.7     1  4355 15528   .1097      72.91  79.3 1112            131072  6.611    .4594 73.76 5.237 9.9e-06  1260  4494   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1448      17.09 188.2      .2283         0                77477 5615 20022 3.566\n",
            "\n",
            "22:25:09 | time:22488s total_exs:1824096 total_steps:77527 epochs:4.80 time_left:936s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   278.8     1  4386 15218   .1320      85.71 78.83 1136            131072  6.082    .4594 78.26 5.238 9.9e-06  1318  4572   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1549      20.26 188.2      .2347         0                77527 5704 19789 3.47\n",
            "\n",
            "22:25:23 | time:22502s total_exs:1825300 total_steps:77577 epochs:4.80 time_left:921s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     222     1  4230 14985  .09884      46.34  85.3 1204            131072  6.638    .4594 72.99 5.279 9.9e-06  1274  4514   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1528      20.08 196.3      .2293         0                77577 5504 19498 3.543\n",
            "\n",
            "22:25:37 | time:22517s total_exs:1826476 total_steps:77627 epochs:4.81 time_left:907s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   233.3     1  4192 14858   .1114      55.07 83.36 1176            131072  6.827    .4594 74.53 5.218 9.9e-06  1301  4610   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1573      19.22 184.5      .2326         0                77627 5493 19469 3.544\n",
            "\n",
            "22:25:52 | time:22531s total_exs:1827580 total_steps:77677 epochs:4.81 time_left:893s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   309.9     1  4552 15761   .1703      103.8 76.45 1104            131072  6.248    .4595 85.83 5.306 9.9e-06  1330  4605   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1911      25.59 201.4      .2251         0                77677 5882 20366 3.463\n",
            "\n",
            "22:26:06 | time:22545s total_exs:1828820 total_steps:77727 epochs:4.81 time_left:878s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   228.8     1  4197 14709   .1323      59.54 86.92 1240            131072  6.299    .4594 72.04 5.262 9.9e-06  1324  4640   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1331      18.66 192.9      .2306         0                77727 5521 19349 3.505\n",
            "\n",
            "22:26:20 | time:22560s total_exs:1830088 total_steps:77777 epochs:4.82 time_left:862s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   208.2     1  4033 14139   .1009      49.17 88.91 1268            131072  6.428    .4594 72.03 5.202 9.9e-06  1361  4771   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1435      18.36 181.7      .2357         0                77777 5394 18910 3.506\n",
            "\n",
            "22:26:35 | time:22574s total_exs:1831188 total_steps:77827 epochs:4.82 time_left:849s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   268.2     1  4439 15486   .1364      66.44 76.74 1100            131072  6.674    .4595 76.55 5.222 9.9e-06  1303  4547   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1518       17.3 185.4      .2337         0                77827 5743 20032 3.488\n",
            "\n",
            "22:26:49 | time:22588s total_exs:1832256 total_steps:77877 epochs:4.82 time_left:835s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   280.7     1  4471 15748   .1442      71.36 75.23 1068            131072   7.21    .4595 77.87 5.189 9.9e-06  1202  4233   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1573       21.6 179.4      .2363         0                77877 5673 19982 3.522\n",
            "\n",
            "22:27:03 | time:22603s total_exs:1833452 total_steps:77927 epochs:4.82 time_left:821s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   249.7     1  4355 15070   .1279      67.58 82.77 1196            131072  6.182    .4595 77.79 5.238 9.9e-06  1386  4795   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1589      19.87 188.2      .2339         0                77927 5741 19865 3.46\n",
            "\n",
            "22:27:18 | time:22617s total_exs:1834700 total_steps:77977 epochs:4.83 time_left:805s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   195.9     1  4174 14750  .07452      28.64  88.2 1248            131072  6.405    .4595 71.53 5.236 9.9e-06  1327  4689   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1354      18.37  188      .2329         0                77977 5501 19439 3.534\n",
            "\n",
            "22:27:32 | time:22631s total_exs:1835912 total_steps:78027 epochs:4.83 time_left:790s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   222.7     1  4171 14745   .1271      50.64 85.69 1212            131072  6.498    .4596 67.71 5.245 9.9e-06  1302  4604   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1155      13.98 189.7      .2331         0                78027 5473 19349 3.535\n",
            "\n",
            "22:27:46 | time:22645s total_exs:1837116 total_steps:78077 epochs:4.83 time_left:775s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   199.7     1  4116 14815  .06811      28.75 86.67 1204            131072   6.09    .4595 69.18 5.234 9.9e-06  1314  4728   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1287      14.63 187.6      .2302         0                78077 5429 19542  3.6\n",
            "\n",
            "22:28:00 | time:22660s total_exs:1838256 total_steps:78127 epochs:4.84 time_left:761s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   234.5     1  4400 15144   .1070      41.49 78.48 1140            131072  6.196    .4596 72.08 5.276 9.9e-06  1220  4199   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1263      18.57 195.5      .2315         0                78127 5620 19344 3.442\n",
            "\n",
            "22:28:15 | time:22674s total_exs:1839324 total_steps:78177 epochs:4.84 time_left:748s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   279.4     1  4636 16246   .1442      62.36 74.85 1068            131072  7.322    .4596 74.46 5.302 9.9e-06  1155  4046   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1507      20.41 200.8      .2271         0                78177 5791 20292 3.504\n",
            "\n",
            "22:28:29 | time:22688s total_exs:1840440 total_steps:78227 epochs:4.84 time_left:734s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   271.3     1  4378 15332   .1577      75.17 78.16 1116            131072  6.571    .4596 76.17 5.239 9.9e-06  1281  4485   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1461      18.79 188.5      .2307         0                78227 5659 19817 3.502\n",
            "\n",
            "22:28:43 | time:22702s total_exs:1841680 total_steps:78277 epochs:4.85 time_left:719s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   241.6     1  4167 14802  .09516      73.62  88.1 1240            131072   6.25    .4597 77.49  5.24 9.9e-06  1355  4812   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1532      22.86 188.6      .2323         0                78277 5522 19614 3.553\n",
            "\n",
            "22:28:57 | time:22717s total_exs:1842912 total_steps:78327 epochs:4.85 time_left:704s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     235     1  4182 14649  .09984      65.27  86.3 1232            131072  6.019    .4597 77.63 5.191 9.9e-06  1389  4865   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1534      21.25 179.6      .2417         0                78327 5571 19514 3.503\n",
            "\n",
            "22:29:11 | time:22731s total_exs:1844068 total_steps:78377 epochs:4.85 time_left:690s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   239.6     1  4213 14902   .1081      57.42 81.78 1156            131072  6.446    .4596 74.04 5.225 9.9e-06  1266  4480   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1471      19.26 185.8      .2333         0                78377 5479 19382 3.537\n",
            "\n",
            "22:29:25 | time:22744s total_exs:1845384 total_steps:78427 epochs:4.86 time_left:673s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   187.9     1  3961 14509  .08207      37.38 96.41 1316            131072  6.413    .4597 66.62 5.214 9.9e-06  1362  4990   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1231      14.87 183.9      .2350         0                78427 5323 19499 3.663\n",
            "\n",
            "22:29:39 | time:22759s total_exs:1846468 total_steps:78477 epochs:4.86 time_left:660s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   258.6     1  4420 15432  .08395      54.77  75.7 1084            131072  6.475    .4597 82.52 5.242 9.9e-06  1227  4286   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1707      25.91 189.1      .2330         0                78477 5647 19718 3.492\n",
            "\n",
            "22:29:54 | time:22773s total_exs:1847656 total_steps:78527 epochs:4.86 time_left:645s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     293     1  4182 14586   .1380        117 82.87 1188            131072  7.038    .4597 92.08 5.238 9.9e-06  1404  4896   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1827         33 188.3      .2334         0                78527 5586 19481 3.488\n",
            "\n",
            "22:30:08 | time:22788s total_exs:1848856 total_steps:78577 epochs:4.87 time_left:631s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   251.3     1  4292 14948   .1250      72.51 83.59 1200            131072   6.24    .4597 81.89 5.252 9.9e-06  1385  4824   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1625      24.18  191      .2281         0                78577 5677 19772 3.483\n",
            "\n",
            "22:30:22 | time:22802s total_exs:1850048 total_steps:78627 epochs:4.87 time_left:616s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   218.3     1  4189 14889   .1099      42.64 84.74 1192            131072  6.459    .4597 68.63 5.245 9.9e-06  1244  4424   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1191      16.43 189.7      .2329         0                78627 5433 19312 3.555\n",
            "\n",
            "22:30:37 | time:22816s total_exs:1851204 total_steps:78677 epochs:4.87 time_left:602s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   259.1     1  4316 14871   .1237      72.39 79.66 1156            131072  6.369    .4598 81.76 5.199 9.9e-06  1375  4739   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1843      22.27 181.1      .2342         0                78677 5691 19611 3.446\n",
            "\n",
            "22:30:51 | time:22831s total_exs:1852268 total_steps:78727 epochs:4.87 time_left:589s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   274.6     1  4555 15811   .1457      60.58 73.87 1064            131072  6.656    .4598  73.9 5.224 9.9e-06  1215  4219   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1391      16.79 185.8      .2312         0                78727 5770 20030 3.472\n",
            "\n",
            "22:31:06 | time:22845s total_exs:1853396 total_steps:78777 epochs:4.88 time_left:575s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     275     1  4411 15391   .1560      79.45 78.71 1128            131072  6.906    .4598 79.48 5.216 9.9e-06  1291  4505   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1835      22.25 184.1      .2344         0                78777 5703 19896 3.489\n",
            "\n",
            "22:31:20 | time:22859s total_exs:1854516 total_steps:78827 epochs:4.88 time_left:561s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   262.9     1  4514 15820   .1304      61.39  78.5 1120            131072  6.423    .4598 73.13 5.194 9.9e-06  1261  4420   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1339      16.82 180.3      .2372         0                78827 5775 20240 3.505\n",
            "\n",
            "22:31:34 | time:22873s total_exs:1855784 total_steps:78877 epochs:4.88 time_left:545s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   192.5     1  4110 14548  .06861      30.47 89.76 1268            131072  6.285    .4598 66.61 5.242 9.9e-06  1314  4652   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1333      14.78 189.1      .2344         0                78877 5424 19200 3.54\n",
            "\n",
            "22:31:48 | time:22888s total_exs:1856960 total_steps:78927 epochs:4.89 time_left:531s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   257.4     1  4246 14997   .1369      76.88 83.08 1176            131072   6.91    .4599 70.96 5.243 9.9e-06  1250  4417   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1335      17.79 189.2      .2313         0                78927 5496 19414 3.533\n",
            "\n",
            "22:32:03 | time:22902s total_exs:1858136 total_steps:78977 epochs:4.89 time_left:516s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   252.6     1  4219 14775   .1293       73.2 82.36 1176            131072  6.232    .4598 83.04 5.227 9.9e-06  1315  4606   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1726      27.12 186.2      .2320         0                78977 5534 19381 3.502\n",
            "\n",
            "22:32:17 | time:22916s total_exs:1859456 total_steps:79027 epochs:4.89 time_left:500s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   188.9     1  3961 14141  .05227      38.92 94.26 1320            131072  6.024    .4599 66.84  5.24 9.9e-06  1370  4893   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1220      14.94 188.7      .2319         0                79027 5331 19034 3.571\n",
            "\n",
            "22:32:31 | time:22931s total_exs:1860604 total_steps:79077 epochs:4.90 time_left:486s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     277     1  4371 15107   .1289      86.58 79.36 1148            131072  6.084    .4599 77.39 5.233 9.9e-06  1313  4539   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1611      20.19 187.4      .2336         0                79077 5684 19646 3.456\n",
            "\n",
            "22:32:45 | time:22945s total_exs:1861780 total_steps:79127 epochs:4.90 time_left:471s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     226     1  4205 14985  .08929      47.22 83.82 1176            131072  5.934    .4599 80.05 5.288 9.9e-06  1361  4849   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1582       22.2 197.9      .2277         0                79127 5565 19835 3.564\n",
            "\n",
            "22:33:00 | time:22959s total_exs:1862964 total_steps:79177 epochs:4.90 time_left:457s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   253.5     1  4301 14999   .1267      71.87 82.58 1184            131072  6.265    .4599 77.69 5.246 9.9e-06  1316  4591   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1394       22.1 189.7      .2331         0                79177 5617 19589 3.488\n",
            "\n",
            "22:33:14 | time:22973s total_exs:1864108 total_steps:79227 epochs:4.91 time_left:443s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   254.5     1  4221 14916   .1233      70.01 80.85 1144            131072  6.352    .4600 82.92 5.276 9.9e-06  1327  4690   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1862      24.91 195.5      .2298         0                79227 5548 19605 3.534\n",
            "\n",
            "22:33:29 | time:22988s total_exs:1865272 total_steps:79277 epochs:4.91 time_left:428s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   243.5     1  4296 14208   .1211      58.98    77 1164            131072  7.026    .4599 70.81 5.249 9.9e-06  1238  4096   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1246      17.62 190.3      .2325         0                79277 5534 18304 3.308\n",
            "\n",
            "22:33:43 | time:23003s total_exs:1866396 total_steps:79327 epochs:4.91 time_left:414s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   244.8     1  4396 15273   .1299      49.28  78.1 1124            235930  6.779    .4593 66.31 5.211 9.9e-06  1170  4064   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1219      14.28 183.3      .2364         0                79327 5566 19337 3.475\n",
            "\n",
            "22:33:58 | time:23017s total_exs:1867552 total_steps:79377 epochs:4.91 time_left:400s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   242.2     1  4412 15519   .1393      51.35 81.32 1156            262144   6.62    .4594 75.62 5.186 9.9e-06  1270  4466   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1401       20.7 178.7      .2390         0                79377 5682 19985 3.517\n",
            "\n",
            "22:34:11 | time:23031s total_exs:1868776 total_steps:79427 epochs:4.92 time_left:385s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   200.1     1  4123 14981  .07516      31.64 88.94 1224            262144  6.491    .4594 62.52  5.24 9.9e-06  1236  4490   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1029      12.03 188.6      .2356         0                79427 5359 19472 3.634\n",
            "\n",
            "22:34:24 | Overflow: setting loss scale to 131072.0\n",
            "22:34:26 | time:23045s total_exs:1870040 total_steps:79477 epochs:4.92 time_left:369s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   194.9 .9800  3965 14092  .08307      38.11 89.85 1264            246415   5.46    .4594 81.99 5.256 9.9e-06  1530  5439   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1685      21.46 191.7      .2310         0                79477 5495 19531 3.555\n",
            "\n",
            "22:34:40 | time:23059s total_exs:1871124 total_steps:79527 epochs:4.92 time_left:356s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   298.6     1  4517 15499   .1642      90.25 74.38 1084            131072  6.382    .4595 86.12 5.234 9.9e-06  1329  4560   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1937      24.82 187.6      .2299         0                79527 5846 20059 3.431\n",
            "\n",
            "22:34:54 | time:23073s total_exs:1872352 total_steps:79577 epochs:4.93 time_left:341s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   211.9     1  4268 15278  .09446      38.09 87.92 1228            131072  6.901    .4594 65.84 5.239 9.9e-06  1260  4509   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1156      14.56 188.5      .2346         0                79577 5527 19787 3.58\n",
            "\n",
            "22:35:08 | time:23088s total_exs:1873632 total_steps:79627 epochs:4.93 time_left:325s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   201.4     1  4073 14277   .1086      42.26 89.73 1280            131072  6.119    .4594  68.5 5.232 9.9e-06  1365  4786   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1305      15.17 187.2      .2342         0                79627 5438 19063 3.505\n",
            "\n",
            "22:35:22 | time:23102s total_exs:1874776 total_steps:79677 epochs:4.93 time_left:311s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   223.8     1  4201 15002  .06119      40.19  81.7 1144            131072  6.122    .4595 80.58 5.277 9.9e-06  1409  5033   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1626      18.98 195.8      .2282         0                79677 5611 20035 3.571\n",
            "\n",
            "22:35:37 | time:23116s total_exs:1875968 total_steps:79727 epochs:4.94 time_left:296s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   247.4     1  4285 14961   .1233      67.68 83.23 1192            131072  5.926    .4595 80.52 5.271 9.9e-06  1347  4703   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1628      24.01 194.6      .2300         0                79727 5632 19663 3.491\n",
            "\n",
            "22:35:51 | time:23131s total_exs:1877100 total_steps:79777 epochs:4.94 time_left:282s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   269.6     1  4359 14932   .1431       77.1 77.55 1132            131072   6.68    .4595 79.33  5.21 9.9e-06  1326  4543   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1758      20.75  183      .2349         0                79777 5685 19475 3.426\n",
            "\n",
            "22:36:06 | time:23145s total_exs:1878272 total_steps:79827 epochs:4.94 time_left:268s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   243.5     1  4289 15076   .1280      60.49 82.39 1172            131072  7.154    .4595 65.31 5.184 9.9e-06  1218  4282   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1152      13.33 178.3      .2410         0                79827 5508 19358 3.515\n",
            "\n",
            "22:36:20 | time:23159s total_exs:1879400 total_steps:79877 epochs:4.95 time_left:254s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   249.2     1  4254 15048   .1144      60.63 79.81 1128            131072  6.271    .4595 86.59 5.282 9.9e-06  1373  4856   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1933      25.75 196.7      .2270         0                79877 5626 19904 3.538\n",
            "\n",
            "22:36:34 | time:23174s total_exs:1880428 total_steps:79927 epochs:4.95 time_left:241s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   336.5     1  4616 15844   .1770        112 70.57 1028            131072  6.746    .4596 81.07 5.219 9.9e-06  1220  4189   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1712       21.7 184.8      .2313         0                79927 5837 20033 3.432\n",
            "\n",
            "22:36:48 | time:23188s total_exs:1881632 total_steps:79977 epochs:4.95 time_left:227s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   213.1     1  4170 15005  .09053      39.95 86.64 1204            131072  6.586    .4595 72.94 5.228 9.9e-06  1333  4796   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1578      17.59 186.4      .2366         0                79977 5503 19801 3.598\n",
            "\n",
            "22:37:03 | time:23202s total_exs:1882840 total_steps:80027 epochs:4.95 time_left:212s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   234.9     1  4288 14896   .1250       57.4 83.93 1208            131072  6.348    .4596  68.7  5.23 9.9e-06  1284  4460   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1283      15.56 186.7      .2318         0                80027 5571 19356 3.474\n",
            "\n",
            "22:37:17 | time:23217s total_exs:1883944 total_steps:80077 epochs:4.96 time_left:198s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   276.5     1  4354 15153   .1594      79.34 76.85 1104            131072  6.624    .4596 78.88 5.227 9.9e-06  1249  4347   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1440      22.31 186.2      .2322         0                80077 5603 19500 3.481\n",
            "\n",
            "22:37:31 | time:23231s total_exs:1885120 total_steps:80127 epochs:4.96 time_left:184s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   233.7     1  4214 14896  .07908      54.54 83.15 1176            131072  6.311    .4596 75.92 5.249 9.9e-06  1311  4635   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1471      20.18 190.4      .2314         0                80127 5525 19531 3.535\n",
            "\n",
            "22:37:45 | time:23245s total_exs:1886264 total_steps:80177 epochs:4.96 time_left:170s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   226.1     1  4230 15039  .08304       41.2 81.35 1144            131072  6.547    .4596 78.59 5.222 9.9e-06  1322  4702   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1696      20.79 185.3      .2337         0                80177 5553 19741 3.556\n",
            "\n",
            "22:38:00 | time:23259s total_exs:1887276 total_steps:80227 epochs:4.97 time_left:157s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   329.9     1  4747 16358   .1868      95.35 69.75 1012            131072  6.582    .4597 81.45 5.215 9.9e-06  1214  4183   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1769      21.48 184.1      .2340         0                80227 5960 20542 3.447\n",
            "\n",
            "22:38:14 | time:23273s total_exs:1888572 total_steps:80277 epochs:4.97 time_left:141s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   195.1     1  4053 14792  .08796       38.7  94.6 1296            131072  7.071    .4596 62.71 5.206 9.9e-06  1278  4664   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1165      13.41 182.4      .2371         0                80277 5331 19456 3.65\n",
            "\n",
            "22:38:28 | time:23287s total_exs:1889904 total_steps:80327 epochs:4.97 time_left:125s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     191     1  3909 13961   .0976      44.22 95.14 1332            131072  5.966    .4597 68.38 5.231 9.9e-06  1407  5024   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1246      15.57  187      .2329         0                80327 5316 18985 3.572\n",
            "\n",
            "22:38:42 | time:23301s total_exs:1891084 total_steps:80377 epochs:4.98 time_left:110s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   216.6     1  4221 15068  .07881      37.72 84.25 1180            131072  6.469    .4596 77.91 5.216 9.9e-06  1337  4775   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1424      21.24 184.2      .2359         0                80377 5558 19843 3.57\n",
            "\n",
            "22:38:56 | time:23316s total_exs:1892316 total_steps:80427 epochs:4.98 time_left:95s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   230.6     1  4150 14529   .1201      62.22 86.26 1232            131072  6.196    .4597 79.82 5.237 9.9e-06  1334  4671   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1672      25.67 188.2      .2322         0                80427 5484 19201 3.501\n",
            "\n",
            "22:39:10 | time:23330s total_exs:1893516 total_steps:80477 epochs:4.98 time_left:80s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   226.8     1  4141 14598   .0775      54.32 84.61 1200            131072  5.943    .4597 74.95 5.213 9.9e-06  1365  4812   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1333      18.08 183.6      .2354         0                80477 5506 19410 3.526\n",
            "\n",
            "22:39:25 | time:23344s total_exs:1894564 total_steps:80527 epochs:4.99 time_left:67s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   305.5     1  4610 15989   .1718      85.54  72.7 1048            131072      7    .4597 81.83 5.226 9.9e-06  1159  4021   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1479      26.53  186      .2330         0                80527 5769 20010 3.469\n",
            "\n",
            "22:39:39 | time:23359s total_exs:1895724 total_steps:80577 epochs:4.99 time_left:53s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   287.1     1  4367 15239   .1681       98.9 80.96 1160            131072  6.733    .4597 72.88 5.244 9.9e-06  1280  4467   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "     .1526      17.71 189.4      .2321         0                80577 5647 19706 3.49\n",
            "\n",
            "22:39:53 | time:23372s total_exs:1896948 total_steps:80627 epochs:4.99 time_left:38s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     213     1  4049 14589  .07108      47.59 88.21 1224            131072  6.256    .4598 66.26 5.277 9.9e-06  1289  4645   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1258       13.6 195.8      .2299         0                80627 5338 19234 3.603\n",
            "\n",
            "22:40:08 | time:23387s total_exs:1898084 total_steps:80677 epochs:4.99 time_left:24s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     281     1  4278 14849   .1232       92.7 78.86 1136            131072  6.123    .4598 79.97 5.244 9.9e-06  1306  4534   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1620      22.48 189.3      .2311         0                80677 5584 19383 3.471\n",
            "\n",
            "22:40:22 | time:23401s total_exs:1899172 total_steps:80727 epochs:5.00 time_left:10s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   275.1     1  4488 15920   .1176      68.81 77.18 1088            131072  6.767    .4598 74.76 5.232 9.9e-06  1213  4301   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1461      19.03 187.2      .2315         0                80727 5701 20221 3.547\n",
            "\n",
            "22:40:31 | time:23411s total_exs:1900028 total_steps:80761 epochs:5.00 time_left:0s\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "     236     1  4216 14807   .1086      68.59 88.43  856            131072  6.224    .4598 78.11  5.21 9.9e-06  1407  4943   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "     .1449      22.22 183.1      .2370         0                80761 5623 19750 3.513\n",
            "\n",
            "22:40:31 | num_epochs completed:5.0 time elapsed:23410.72101521492s\n",
            "22:40:36 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "22:40:36 | Using CUDA\n",
            "22:40:36 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/model.dict\n",
            "22:40:36 | num words = 30051\n",
            "22:40:37 | Total parameters: 74,766,336 (74,766,336 trainable)\n",
            "22:40:37 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/model\n",
            "22:40:40 | creating task(s): fromfile:parlaiformat\n",
            "22:40:40 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data_valid.txt\n",
            "22:40:45 | running eval: valid\n",
            "22:44:00 | eval completed in 194.65s\n",
            "22:44:00 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   237.1  4259 44452   .1119      55.91 245.3 47532    .1426 75.36 5.233 9.9e-06  1311 13685   .1505      19.59 187.4   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .2331         0                80761 5570 58137\n",
            "\u001b[0m\n",
            "22:44:00 | creating task(s): fromfile:parlaiformat\n",
            "22:44:00 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data_test.txt\n",
            "22:44:05 | running eval: test\n",
            "22:47:17 | eval completed in 192.34s\n",
            "22:47:17 | \u001b[1mtest:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   241.6  4265 44895   .1130      59.74 246.9 47371    .1423 75.82 5.242 9.9e-06  1308 13775   .1488      20.02 189.1   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .2326         0                80761 5573 58670\n",
            "\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'clen': AverageMetric(237.1),\n",
              "  'ctpb': GlobalAverageMetric(4259),\n",
              "  'ctps': GlobalTimerMetric(4.445e+04),\n",
              "  'ctrunc': AverageMetric(0.1119),\n",
              "  'ctrunclen': AverageMetric(55.91),\n",
              "  'exps': GlobalTimerMetric(245.3),\n",
              "  'exs': SumMetric(4.753e+04),\n",
              "  'gpu_mem': GlobalAverageMetric(0.1426),\n",
              "  'llen': AverageMetric(75.36),\n",
              "  'loss': AverageMetric(5.233),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'ltpb': GlobalAverageMetric(1311),\n",
              "  'ltps': GlobalTimerMetric(1.369e+04),\n",
              "  'ltrunc': AverageMetric(0.1505),\n",
              "  'ltrunclen': AverageMetric(19.59),\n",
              "  'ppl': PPLMetric(187.4),\n",
              "  'token_acc': AverageMetric(0.2331),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(8.076e+04),\n",
              "  'tpb': GlobalAverageMetric(5570),\n",
              "  'tps': GlobalTimerMetric(5.814e+04)},\n",
              " {'clen': AverageMetric(241.6),\n",
              "  'ctpb': GlobalAverageMetric(4265),\n",
              "  'ctps': GlobalTimerMetric(4.49e+04),\n",
              "  'ctrunc': AverageMetric(0.113),\n",
              "  'ctrunclen': AverageMetric(59.74),\n",
              "  'exps': GlobalTimerMetric(246.9),\n",
              "  'exs': SumMetric(4.737e+04),\n",
              "  'gpu_mem': GlobalAverageMetric(0.1423),\n",
              "  'llen': AverageMetric(75.82),\n",
              "  'loss': AverageMetric(5.242),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'ltpb': GlobalAverageMetric(1308),\n",
              "  'ltps': GlobalTimerMetric(1.377e+04),\n",
              "  'ltrunc': AverageMetric(0.1488),\n",
              "  'ltrunclen': AverageMetric(20.02),\n",
              "  'ppl': PPLMetric(189.1),\n",
              "  'token_acc': AverageMetric(0.2326),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(8.076e+04),\n",
              "  'tpb': GlobalAverageMetric(5573),\n",
              "  'tps': GlobalTimerMetric(5.867e+04)})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_path = \"/content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/\"\n",
        "data_path = '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/'\n",
        "# !rm -R $model_path\n",
        "# !mkdir $model_path\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    model_file= f\"{model_path}model\",\n",
        "    task= 'fromfile:parlaiformat', \n",
        "    fromfile_datapath= f'{data_path}data',\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    batchsize=12,\n",
        "    \n",
        "    model= \"transformer/generator\",\n",
        "\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    learn_positional_embeddings=True,\n",
        "\n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    # use a small learning rate with ADAM optimizer\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    # early stopping on perplexity\n",
        "    validation_metric='ppl',\n",
        "\n",
        "    num_epochs = 5,\n",
        "    validation_every_n_epochs=2,\n",
        "\n",
        "    # depend on your gpu. If you have a V100, this is good\n",
        "    fp16=True, fp16_impl='mem_efficient',\n",
        "\n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "\n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQzMZxIOb5Rz"
      },
      "source": [
        "## DisplayModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHfzqgYp38vi",
        "outputId": "d5790bde-a0a3-4cfa-e711-36f992d3b66f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colabs/blender-models/trained_fr_reddit//model\n",
            "07:16:23 | \u001b[33mOverriding opt[\"datatype\"] to test (previously: train)\u001b[0m\n",
            "07:16:23 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "07:16:23 | \u001b[33mOverriding opt[\"beam_context_block_ngram\"] to 3 (previously: -1)\u001b[0m\n",
            "07:16:23 | \u001b[33mOverriding opt[\"beam_block_ngram\"] to 3 (previously: -1)\u001b[0m\n",
            "07:16:23 | \u001b[33mOverriding opt[\"beam_min_length\"] to 20 (previously: 1)\u001b[0m\n",
            "07:16:23 | \u001b[33mOverriding opt[\"beam_size\"] to 10 (previously: 1)\u001b[0m\n",
            "07:16:23 | \u001b[33mOverriding opt[\"inference\"] to topk (previously: greedy)\u001b[0m\n",
            "07:16:23 | \u001b[33mOverriding opt[\"topk\"] to 20 (previously: 10)\u001b[0m\n",
            "07:16:23 | \u001b[33mOverriding opt[\"temperature\"] to 0.5 (previously: 1.0)\u001b[0m\n",
            "07:16:23 | \u001b[33mOverriding opt[\"beam_length_penalty\"] to 0.8 (previously: 0.65)\u001b[0m\n",
            "07:16:23 | Using CUDA\n",
            "07:16:23 | loading dictionary from /content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/model.dict\n",
            "07:16:24 | num words = 30051\n",
            "07:16:25 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
            "07:16:36 | Total parameters: 74,766,336 (74,766,336 trainable)\n",
            "07:16:36 | Loading existing model params from /content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/model\n",
            "07:16:55 | creating task(s): fromfile:parlaiformat\n",
            "07:16:55 | Loading ParlAI text data: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data_test.txt\n",
            "07:16:59 | Opt:\n",
            "07:16:59 |     activation: gelu\n",
            "07:16:59 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "07:16:59 |     adam_eps: 1e-08\n",
            "07:16:59 |     add_p1_after_newln: False\n",
            "07:16:59 |     aggregate_micro: False\n",
            "07:16:59 |     allow_missing_init_opts: False\n",
            "07:16:59 |     attention_dropout: 0.0\n",
            "07:16:59 |     batchsize: 12\n",
            "07:16:59 |     beam_block_full_context: True\n",
            "07:16:59 |     beam_block_list_filename: None\n",
            "07:16:59 |     beam_block_ngram: 3\n",
            "07:16:59 |     beam_context_block_ngram: 3\n",
            "07:16:59 |     beam_delay: 30\n",
            "07:16:59 |     beam_length_penalty: 0.8\n",
            "07:16:59 |     beam_min_length: 20\n",
            "07:16:59 |     beam_size: 10\n",
            "07:16:59 |     betas: '[0.9, 0.999]'\n",
            "07:16:59 |     bpe_add_prefix_space: None\n",
            "07:16:59 |     bpe_debug: False\n",
            "07:16:59 |     bpe_dropout: None\n",
            "07:16:59 |     bpe_merge: None\n",
            "07:16:59 |     bpe_vocab: None\n",
            "07:16:59 |     checkpoint_activations: False\n",
            "07:16:59 |     compute_tokenized_bleu: False\n",
            "07:16:59 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "07:16:59 |     datatype: test\n",
            "07:16:59 |     delimiter: '\\n'\n",
            "07:16:59 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "07:16:59 |     dict_endtoken: __end__\n",
            "07:16:59 |     dict_file: /content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/model.dict\n",
            "07:16:59 |     dict_include_test: False\n",
            "07:16:59 |     dict_include_valid: False\n",
            "07:16:59 |     dict_initpath: None\n",
            "07:16:59 |     dict_language: english\n",
            "07:16:59 |     dict_loaded: True\n",
            "07:16:59 |     dict_lower: True\n",
            "07:16:59 |     dict_max_ngram_size: -1\n",
            "07:16:59 |     dict_maxexs: -1\n",
            "07:16:59 |     dict_maxtokens: -1\n",
            "07:16:59 |     dict_minfreq: 0\n",
            "07:16:59 |     dict_nulltoken: __null__\n",
            "07:16:59 |     dict_starttoken: __start__\n",
            "07:16:59 |     dict_textfields: text,labels\n",
            "07:16:59 |     dict_tokenizer: bpe\n",
            "07:16:59 |     dict_unktoken: __unk__\n",
            "07:16:59 |     display_add_fields: \n",
            "07:16:59 |     display_examples: False\n",
            "07:16:59 |     download_path: None\n",
            "07:16:59 |     dropout: 0.0\n",
            "07:16:59 |     dynamic_batching: full\n",
            "07:16:59 |     embedding_projection: random\n",
            "07:16:59 |     embedding_size: 512\n",
            "07:16:59 |     embedding_type: random\n",
            "07:16:59 |     embeddings_scale: True\n",
            "07:16:59 |     eval_batchsize: None\n",
            "07:16:59 |     eval_dynamic_batching: None\n",
            "07:16:59 |     evaltask: None\n",
            "07:16:59 |     ffn_size: 2048\n",
            "07:16:59 |     final_extra_opt: \n",
            "07:16:59 |     force_fp16_tokens: True\n",
            "07:16:59 |     fp16: True\n",
            "07:16:59 |     fp16_impl: mem_efficient\n",
            "07:16:59 |     fromfile_datapath: /content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data\n",
            "07:16:59 |     fromfile_datatype_extension: True\n",
            "07:16:59 |     gpu: -1\n",
            "07:16:59 |     gradient_clip: 0.1\n",
            "07:16:59 |     hide_labels: False\n",
            "07:16:59 |     history_add_global_end_token: None\n",
            "07:16:59 |     history_reversed: False\n",
            "07:16:59 |     history_size: -1\n",
            "07:16:59 |     image_cropsize: 224\n",
            "07:16:59 |     image_mode: raw\n",
            "07:16:59 |     image_size: 256\n",
            "07:16:59 |     inference: topk\n",
            "07:16:59 |     init_model: None\n",
            "07:16:59 |     init_opt: None\n",
            "07:16:59 |     interactive_mode: False\n",
            "07:16:59 |     invsqrt_lr_decay_gamma: -1\n",
            "07:16:59 |     is_debug: False\n",
            "07:16:59 |     label_truncate: 128\n",
            "07:16:59 |     learn_positional_embeddings: True\n",
            "07:16:59 |     learningrate: 1e-05\n",
            "07:16:59 |     log_every_n_secs: -1\n",
            "07:16:59 |     log_every_n_steps: 50\n",
            "07:16:59 |     log_keep_fields: all\n",
            "07:16:59 |     loglevel: info\n",
            "07:16:59 |     lr_scheduler: reduceonplateau\n",
            "07:16:59 |     lr_scheduler_decay: 0.5\n",
            "07:16:59 |     lr_scheduler_patience: 3\n",
            "07:16:59 |     max_train_steps: -1\n",
            "07:16:59 |     max_train_time: -1\n",
            "07:16:59 |     metrics: default\n",
            "07:16:59 |     model: transformer/generator\n",
            "07:16:59 |     model_file: /content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/model\n",
            "07:16:59 |     model_parallel: False\n",
            "07:16:59 |     momentum: 0\n",
            "07:16:59 |     multitask_weights: [1]\n",
            "07:16:59 |     mutators: None\n",
            "07:16:59 |     n_decoder_layers: -1\n",
            "07:16:59 |     n_encoder_layers: -1\n",
            "07:16:59 |     n_heads: 16\n",
            "07:16:59 |     n_layers: 8\n",
            "07:16:59 |     n_positions: 512\n",
            "07:16:59 |     n_segments: 0\n",
            "07:16:59 |     nesterov: True\n",
            "07:16:59 |     no_cuda: False\n",
            "07:16:59 |     num_epochs: 5.0\n",
            "07:16:59 |     num_examples: 20\n",
            "07:16:59 |     num_workers: 0\n",
            "07:16:59 |     nus: [0.7]\n",
            "07:16:59 |     optimizer: mem_eff_adam\n",
            "07:16:59 |     output_scaling: 1.0\n",
            "07:16:59 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/data', 'fromfile_datatype_extension': True, 'model_file': '/content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/model', 'datatype': 'test', 'num_examples': '20', 'skip_generation': False, 'beam_context_block_ngram': 3, 'beam_block_ngram': 3, 'beam_min_length': 20, 'beam_size': 10, 'inference': 'topk', 'topk': 20, 'temperature': 0.5, 'beam_length_penalty': 0.8}\"\n",
            "07:16:59 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "07:16:59 |     person_tokens: False\n",
            "07:16:59 |     rank_candidates: False\n",
            "07:16:59 |     relu_dropout: 0.0\n",
            "07:16:59 |     save_after_valid: False\n",
            "07:16:59 |     save_every_n_secs: -1\n",
            "07:16:59 |     save_format: conversations\n",
            "07:16:59 |     share_word_embeddings: True\n",
            "07:16:59 |     short_final_eval: False\n",
            "07:16:59 |     skip_generation: False\n",
            "07:16:59 |     special_tok_lst: None\n",
            "07:16:59 |     split_lines: False\n",
            "07:16:59 |     starttime: May17_14-32\n",
            "07:16:59 |     task: fromfile:parlaiformat\n",
            "07:16:59 |     temperature: 0.5\n",
            "07:16:59 |     tensorboard_log: False\n",
            "07:16:59 |     tensorboard_logdir: None\n",
            "07:16:59 |     text_truncate: 512\n",
            "07:16:59 |     topk: 20\n",
            "07:16:59 |     topp: 0.9\n",
            "07:16:59 |     truncate: -1\n",
            "07:16:59 |     update_freq: 1\n",
            "07:16:59 |     use_reply: label\n",
            "07:16:59 |     validation_cutoff: 1.0\n",
            "07:16:59 |     validation_every_n_epochs: 2.0\n",
            "07:16:59 |     validation_every_n_secs: -1\n",
            "07:16:59 |     validation_every_n_steps: -1\n",
            "07:16:59 |     validation_max_exs: -1\n",
            "07:16:59 |     validation_metric: ppl\n",
            "07:16:59 |     validation_metric_mode: None\n",
            "07:16:59 |     validation_patience: 10\n",
            "07:16:59 |     validation_share_agent: False\n",
            "07:16:59 |     variant: xlm\n",
            "07:16:59 |     verbose: False\n",
            "07:16:59 |     wandb_entity: None\n",
            "07:16:59 |     wandb_log: False\n",
            "07:16:59 |     wandb_name: None\n",
            "07:16:59 |     wandb_project: None\n",
            "07:16:59 |     warmup_rate: 0.0001\n",
            "07:16:59 |     warmup_updates: 100\n",
            "07:16:59 |     weight_decay: None\n",
            "07:16:59 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mPourquoi ne portent-elles pas plutôt plainte contre les vrais sites porno ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Parce que le porno est souvent bien meilleur en com' et a plus de répondant que la SNCF.\u001b[0;0m\n",
            "\u001b[0;95m     model: le problème c ' est que l ' on peut pas le même que les gens sont pas mal de la france .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mTrès beau tir, et quelle compassion. Je suis fier de nos gendarmes.\u001b[0;0m\n",
            "\u001b[1;94m    labels:  quelle compassion. Je suis fier de nos gendarmes.Ça manque d'exemple positifs ces dernier temps,quand ils merdent on se gêne pas pour le dire mais quand ils font les choses bien on considère qu'ils font juste leurs boulot.Du coup je partage ton avis.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un peu d ' accord avec toi , mais c ' est une bonne partie de la plupart des gens qui sont en france .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mdis leur de lire ce blog-borne.com/je te conseille ce livre même s'il n'est peut être pas bien ciblé mais quand même. Il n'est pas sur la théorie des logiciels libres mais apporte des réponses pratiques-informatiques-libres.fr/Tu peux télécharger des extraits gratuitement sur ce site\u001b[0;0m\n",
            "\u001b[1;94m    labels: haha je suis déjà un grand fan de Cyrille Borne ;) (qui a eu quelques péripéties recemment avec blog libre et tout le bazard j'ai pas trop compris ce qu'il s'était passé)EDIT : je suis allé voir ton deuxième lien, c'est ce qui se rapproche le plus de ce que j'avais en tête, ça a l'air très bien !\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est la question , mais je me suis pas si je ne suis pas d ' accord avec toi .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mCe texte m'inspire plusieurs réactions:- Extrêmement bien écrit, comme systématiquement pour Eolas. - Je compatis pour ce gars qui ne se rendait pas compte de ses actions. La prison n'est certainement pas la meilleure solution pour des délits de ce genre, je pense que la réhabilitation est clairement une meilleure solution. Peut-être des TIG au sein de la boutique qui est sa victime? Histoire de voir le boulot et l'effort qu'il a voulu ruiner par son acte? C'est plus intelligent...- Cela dit, j'ai une sainte horreur du vol, particulièrement dans des petites boutiques. Y'a de l'effort, de la réflexion, de l'investissement personnel et monétaire et parfois même de l'espoir dans chaque business, et voler réduit à néant tout cet effort. C'est pas \"juste de l'argent\", c'est tout ce qu'il y a derrière.- Pourquoi je dis ça? A cause de la phrase *\"Et ouais, si tu consommes pas, tu n’es rien, aujourd’hui.\"* Sans forcément parler de ce cas précis, est-ce que c'est moi ou est-ce qu'il vient de dire que le vol c'est à cause de la société de consommation?[Parce que si oui...]()\u001b[0;0m\n",
            "\u001b[1;94m    labels: Et ouais, si tu consommes pas, tu n’es rien, aujourd’hui.Tout le monde s'accroche sur cette phrase, enfin toi et les commentateurs du site.Cependant, personne pour dire que c'est faux. Dans le monde dans lequel vit ce jeune, c'est vrai. Quelles sont les références de ses amis, de sa communauté? Probablement l'argent, la pub, le bling. C'est assez effarant, mais apparamment c'est tout ce qu'on leur propose. Et même dans le discours des politiques, ce qui compte c'est \"la croissance\", \"le pouvoir d'achat\", etc. C'est pas mieux.On ne peut pas juger un individu sans tenir compte du milieu dans lequel il est et les influences externes qu'il subit. Et même si on fait partie de ceux qui pensent qu'on ne peut jamais excuser ce genre de comportement, il faut constater que sans comprendre les causes on ne saurait remédier aux effets. Il y a sans doute beaucoup de facteurs qui l'ont conduit à cette action, mais minimiser le rôle de l'argent et la consommation comme seul buts dans la vie revient à se voiler la face sur notre société.\u001b[0;0m\n",
            "\u001b[0;95m     model: si tu es un peu de temps , je n ' ai pas trop de faire des trucs qui ne sont pas le problème .\u001b[0;0m\n",
            "\u001b[0mAh,  je n'avais pas pris le temps de lire les commentaires. Mea culpa!Cela dit, il n'y a personne pour dire que c'est faux parce que c'est tout à fait vrai. Je dis juste que ca ne justifie pas le fait de voler. La manière dont l'article présentait ca semblait en faire une excuse pour le gosse, c'est surtout ca qui m'a fait tiquer. \u001b[0;0m\n",
            "\u001b[1;94m    labels: Perso je ne crois pas du tout au libre arbitre. Déformation professionnelle sans doute, mais j'ai conscience (haha autre grande question) que nous ne sommes que des machines moléculaires qui répondent aux stimulus externe par une réaction et des actions. Il y a une théorie en neuroscience que notre conscience ne fait que [remarquer avec un petit délai les décisions que nous avons prises](-end-of-knowing/?hp), autrement dit notre cerveau prend une décision et ne la contrôle pas. D'ailleurs, si on réfléchit bien, comment est-ce qu'un cerveau pourrait s'influencer lui-même?Il y a d'importantes répercutions de ce courant de pensée pour le système de justice, dont fait partie la remise en question de la peine ferme et irrévocable particulièrement pour les jeunes.\u001b[0;0m\n",
            "\u001b[0;95m     model: le problème c ' était pas la première fois que le cas de la france est que ça ne se trouve pas à l ' air de la même chose .\u001b[0;0m\n",
            "\u001b[0mDonc en fait ce gosse est un danger pour la société et le sera toujours puisqu'il n'en a pas conscience ? Quelle remise en question des peines fermes ? \u001b[0;0m\n",
            "\u001b[1;94m    labels: La peine ferme est un aveu d'impuissance pour un gamin de 13 ans. Ca veut dire qu'on estime qu'on ne sait pas quoi en faire et qu'il doit aller en prison. Parce que normalement, à cet âge là, on devrait pouvoir agir sur son esprit en développement et inculquer de meilleures bases, ou lui fournir d'autres horizons que le vol pour \"s'acheter des trucs\" pour s'épanouir. Et vraisemblablement il n'a aucune conscience de ce qu'il encourt, donc la peine ferme n'a aucun effet dissuasif sur cette catégorie de personnes. Et vraisemblablement il y aurait d'autres moyens de s'assurer qu'il ne vole pas, comme la surveillance ou bien l'inscription dans un autre programme.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne vois pas de ce que la même chose . c ’ est un peu de la plupart des gens qui ont un truc qui sont des gens en france .\u001b[0;0m\n",
            "\u001b[0mMais tu dis toi même qu'il n'y a pas de libre arbitre. Donc ce gosse n'a pas conscience de ce qu'il fait et il faut donc l'éloigner du reste de la population car il est dangereux. Si il n'a pas compris la première fois on ne peut plus prendre le risque de le laisser libre. Et pour les autres horizons, je ne vois même pas ce qu'il est possible de faire. Ce que l'avenir réserve à ce gamin c'est au mieux une formation pourrie puis un boulot précaire ou du chômage. On est dans un monde de compétition, pour lui c'est perdu et il en a très bien conscience. \u001b[0;0m\n",
            "\u001b[1;94m    labels: Attention, pas de libre arbitre n'implique pas une irresponsabilité, et encore moins une incapacité de l'apprentissage. Le cerveau est d'ailleurs une magnifique machine à apprendre.Pour l'avenir, certes c'est de toutes façons mal parti, mais avec une condamnation à la prison c'est quasiment désespéré.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis d ' accord avec toi , mais je ne suis pas le problème . je ne pense pas que ça . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mJe viens de finir un docu fascinant sur Arte sur l'histoire et la vie du mollah Omar d'Afghanistan. Comment il a grandi, ce qui l'a décidé de prendre le contrôle du pays et ses relations avec Ben Laden, et où il se cache maintenant. Je vais pas jusqu'à dire que ca réhabilite le personnage, mais ca en fait un portrait plus nuancé. Ça s'appelle le Dernier Calife d'Afghanistan, regardez-le en replay, ils évoquent évidemment la guerre contre les soviétiques et le culture pashtun, c'est hyper intéressant. Spoilers: Ils ne trouvent pas le mollah Omar à la fin.\u001b[0;0m\n",
            "\u001b[1;94m    labels:  Je vais pas jusqu'à dire que ca réhabilite le personnage, mais ca en fait un portrait plus nuancé.Comme presque tout ce dont traite une certaine partie des médias, il faut vraiment croiser les sources pour avoir une idée un peu plus juste de certains enjeux/personnages. Juste pour prendre un exemple, j'ai toujours eu une aversion envers Faurisson sans savoir pourquoi. Je détestais littéralement le mec non pas à cause ce que j'en savais à travers mes propres recherches mais de ce que j'en ai su à travers la télé. Il a fallu que je tombe sur un documentaire assez bien fait sur lui sur youtube, avec des entretiens et des séquences où on lui donne la parole (laquelle je n'avais jamais entendu). Ça m'a fait voir le personnage sous un autre jour. Je ne crois toujours pas que les chambres à gaz et le Shoah n'ont jamais existé comme il le prétend mais il m'a fait réfléchir sur la censure, sur l'Histoire et le révisionnisme en tant que forme de discours sur l'Histoire et les sciences en général mais également sur son pendant moral/juridique qu'est le négationnisme puisqu'on glisse du premier vers le second dés qu'un jugement moral/juridique est porté sur le premier. \u001b[0;0m\n",
            "\u001b[0;95m     model: j ' ai eu un peu de la tête de la même chose . . . c ' était un peu comme ça .\u001b[0;0m\n",
            "\u001b[0mOula, un avis nuancé sur un personnage que l'Opinion a érigé en monstre ? Attention, tu es en train de sombrer lentement entre les bras nauséabonds de la Bête immonde. Je commence vaguement à entendre un bruit de bottes au loin.Plus sérieusement, c'est clair que beaucoup de gens dont le nom sent le souffre ont en fait une histoire bien plus nuancée que ce qu'on croit. Ça n'en fait évidemment pas des enfants de choeur, et il faut se méfier, il y a des bourreaux dont le charisme et les réflexions qu'ils font naitre représentent un premier pas vers quelque chose de bien pire. Mais je pense qu'au final, il vaut mieux ne rien cacher et laisser la vérité l'emporter sur la propagande (fût-elle bien intentionnée).\u001b[0;0m\n",
            "\u001b[1;94m    labels: Et l'inverse, des personnages immaculés qui ne le sont pas exactement (mandela etc...) ... le pouvoir créé ses symboles positifs et négatifs comme ça lui chante, à nous de nous informer correctement.\u001b[0;0m\n",
            "\u001b[0;95m     model: j ' aurais pas le même si tu as de l ' impression que je suis une question de la même chose .\u001b[0;0m\n",
            "\u001b[0mOula, un avis nuancé sur un personnage que l'Opinion a érigé en saint ? Attention, tu es en train de sombrer lentement entre les bras condescendants du cynisme adolescent. Je commence vaguement à entendre de la musique industrielle post-punk à travers la porte fermée de ta chambre qui sent le renfermé.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Lol, je veux bien qu'on dise des conneries, mais de dire que l'Opinion est le grand ami de Mandela, il faut arrêter la drogue.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis d ' accord avec toi , je suis un peu comme un peu un coup de la loi .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mAbstinent depuis deux ans demain (alcool). Feelsgoodman.jpg \u001b[0;0m\n",
            "\u001b[1;94m    labels: Je suppose que tu dois être un ancien alcoolique, alors je te pose cette question : Quelle fierté en plus il y a à ne plus boire d'alcool du tout, comparé à boire de l'alcool sans excès ni habitude ?Je trouve que se contrôler et savoir boire une bonne bière, un bon vin ou un bon digestif montre bien plus de maîtrie de soi plutôt qu'une abstinence totale.\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne sais pas , je suis d ' accord avec toi . je vais à la question de la même chose .\u001b[0;0m\n",
            "\u001b[0mUn ancien alcoolique ça n'existe pas. Il y a les alcooliques, et les alcooliques abstinents comme moi.Boire juste un verre n'est pas une option pour moi. Si j'essaie, mon corps va immédiatement réclamer plus d'alcool, ça va monopoliser toutes mes pensées et mon énergie  jusqu'à ce que j'ai un autre verre. Je vais aussi en souffrir physiquement, ma tension va monter et je vais me mettre a trembler. Beaucoup d'alcooliques passent par une phase de déni ou ils pensent qu'ils peuvent contrôler la maladie, principalement parce qu'ils ont honte de la dénomination \"alcoolique\". C'est une étape importante de la guérison d'admettre qu'on est alcoolique et qu'il ne sert à rien de marchander. C'est pour cela que dans le cliché des réunions AA on se présente en disant \"Je m'appelle Leodagan et je suis alcoolique\".Je ne connais aucun alcoolique diagnostiqué qui soit capable de boire raisonnablement.Ce qui est source de fierté c'est d'être capable d'admettre qu'un retour a une consommation normale n'est pas possible. D'être capable de résister à la fois à l'énorme pression sociale et à son propre corps/cerveau qui négocie en permanence pour t'inciter à boire.Est-ce que tu conseillerais à un héroïnomane en convalescence de se faire \"juste un petit fix\"? Non. C'est pareil pour l'alcool. Pour nous, l'alcool est comme l’héroïne, il n'y a pas de demi-mesure ou de négociation possible. Si je bois, je vais recommencer à boire. Et je n'ai aucune envie de me retaper l'enfer qu'a été le sevrage. Un alcoolique, ce n'est pas juste une personne qui boit beaucoup, même si malheureusement c'est la perception qu'en ont beaucoup de gens. Tout comme être bipolaire ce n'est pas simplement avoir des sautes d'humeur.(Edit: Désolé pour les fautes ou si je ne suis pas clair, j'essaie de taper ça discrètement au boulot sur mon téléphone)   \u001b[0;0m\n",
            "\u001b[1;94m    labels: L'alcoolisme n'est pas une maladie, c'est une dépendance. La différence entre les deux c'est que la maladie, tu ne peux rien y faire et la dépendance est une question de volonté et de motivation. \"D'être capable de résister à la fois à l'énorme pression sociale et son propre corps/cerveau. \"Ton cerveau ne te réclame rien, il n'y a pas de capteurs pour l'alcool comme pour les autres drogues, c'est un empoisonnement. Après deux ans, tu n'as plus besoin d'alcool (tout dépends de la dépendance que tu avais), donc c'est toi qui a envie de boire. Se cacher derrière une fatalité est une méthode comme une autre, si tu te sens mieux et si ça t\"aide c'est le principal.Je connais des ex héroïnomanes qui en reprennent de temps en temps...  \u001b[0;0m\n",
            "\u001b[0;95m     model: je ne sais pas , mais je suis un peu de la politique de la vie de la même chose , c ' était une bonne idée .\u001b[0;0m\n",
            "\u001b[0mL'alcoolisme une putain de maladie. J'ai ete force d'arreter l'alcool (le moindre verre m'envoie à l'hopital) apres une longue periode de consommation quotidienne et c'est de loin la drogue la plus dure a arreter. Je continue de prendre des opiaces regulierement a cause de mes douleurs (et avouons le parce que j'aime ca) mais le sevrage est pour moi tres facile, meme apres une longue periode avec accoutumance. Mais ca depend vraiment des individus. J'ai de la chance a ce niveau.Quelque part au fond de moi j'ai toujours envie de boire et boire et boire, meme apres des mois de sobriete.\u001b[0;0m\n",
            "\u001b[1;94m    labels: C'est pour la bonne cause, bon courage. Il faut sûrement trouver d'autres motivations que l'obligation physique d'arrêter pour que d'une souffrance ça devienne quelque chose de positif. Moi quand j'ai arrêté je me suis vraiment demandé où était la magnifique joie dont tout le monde parle, la vie qui reprend, tout ça. En fait c'est un sacré boulot mais à force ça va mieux. \u001b[0;0m\n",
            "\u001b[0;95m     model: je suis un peu dans le monde , mais je n ’ ai pas du tout de la même chose . je suis pas que tu as de la plupart des gens qui sont des gens en france .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mLa plupart des français qui viennent au Québec sont plutôt snobs (ici, on dit «frais chiés») et affectent de ne pas comprendre quand on parle.Mais c’est simplement parce que le français lambda est naturellement con/beauf, exactement de la même façon que les amerloks sont cons (en fait, les français sont les amerloks d’Europe). On note le même phénomène ici, où les plus cons (ici, on dit «colon») n’hésitent pas à user d’anglicismes.Pour ce qui est de la pureté de la langue, le Québec n’a rien a apprendre de la France, avec ses anglicismes à la con, aussi dits par snobisme. Quant à la divergence du vocabulaire, c’est tout à fait normal lorsque les circonstances géographo-historiques coupent une langue en deux. À une époque, certes, nous avons usé de beaucoup d’anglicismes, mais nous nous en sommes débarrassés, principalement par haine de l’occupant rosbif.Quant à l’accent, en fait, nous parlons la langue de la cour du XVIIème siècle, celle-ci ayant été la langue commune de la Nouvelle-France des siècles avant que la IIIème République soit venue à bout des divers dialectes. On dira ce qu’on voudra, mais il faut dire que la France regorge d’accents aussi divergents du parisien jacté que l’est le joual. Ceci dit, le zigue qui lit les nouvelles à la télé a pratiquement le même accent à Paris qu’à Montréal, hormis peut-être un petit peu plus de cul-de-poulisme et de cîrcônfêxîsmê à Paris.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Certaines problèmes pour comprendre votre texte, pardonnez-moi. Qu'est-ce que sont \"con/beauf\" et \"amerloks\"? J'ai essayé d'utiliser Google pour traduire, mais ça vas pas...\u001b[0;0m\n",
            "\u001b[0;95m     model: l ' article est un peu plus que le monde est que les gens ne sont pas le même . . .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mJ'ai créé ma boite de développement informatique en France l'année dernière, grossière erreur.Sinon j'ai bossé dans un consulat français ! Ce qui est assez drôle aussi.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Pourquoi grossière erreur ? La conjoncture est pas bonne ? Parce que Xavier Niel crie partout que c'est le moment de faire des startups étouétou...\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est le même pas mal , ça se fait de l ' air d ' avoir un truc .\u001b[0;0m\n",
            "\u001b[0mFaut l'avouer c'est un beau bazar... Après c'est peut être le fait de ne pas être sur Paris mais l'encadrement lorsqu'il s'agit de la création d'une startup de développement est complètement dépassé.On est perdu pour nous définir un code APE, et quand c'est fait, on ne nous attribue même pas la bonne définition de notre activité. J'entends par là que, par exemple, alors que nous déclarons développer des \"services liés au réseautage social et développement de technologies et logiciels mobiles et bureautiques\" on découvre au fil de 50 coups de fils qu'on est répertoriés en tant que \"développeurs de sites webs pour particuliers et professionnels\"...Déjà la création est assez onéreuse (il faut penser à l'annonce dans le journal officiel, l'ouverture du compte en banque, les frais de carte, le capital, la mutuelle des employés, les frais de comptables et j'en passe...). Puis les taxes tombent rapidement, et tu vois l'argent sortir à une vitesse ahurissante dès le départ alors que t'as pas encore eu le temps de te relancer dans tes projets.Anyway, pour combler les problèmes financiers y'a deux solutions qu'on te proposera toujours : trouver des investisseurs ou aller tenter une pépinière d'entreprise (ou incubateur). Les investisseurs ça me chantait au début, tout en me doutant que j'aurais à faire à de sacrés bestiaux. Du coup je trouve une boite qui gère des services de comptables, de juristes... J'y ai exposé la situation de ma boite et tout ce qu'on comptait faire. Évidemment tout est absolument génial, en plus c'est cool je suis jeune (21 ans quand j'ai commencé, c'était l'année dernière), du coup je trouverai plein de mecs qui voudront et l'informatique c'est l'avenir blablabla... J'étais évidemment enchanté et pas plus tard qu'une semaine après j'ai 4 RDVs avec des gens qui seraient intéressés pour m'aider à lancer mes projets. Comme par hasard, tous sont jeunes, ont un CV bien rempli sur LinkedIN et ont tous bossé dans le même domaine que ma startup ! Ils sont portes-paroles d'une grosse boite et proposent des sommes à 5 zéros en échange de certaines part dans la boite. C'est normal me direz-vous, mais j'ai certainement pas fait ça pour qu'un mec blindé vienne imposer ses restrictions ou conditions à mes idées.Du coup c'est niet et on tente l'incubateur. Là tu passes devant un jury pour y exposer le tout... une nouvelle fois. Sauf que tu tombes face à des jurés d'une moyenne d'âge de 60 ans à qui tu parles d'informatique et d'internet, de \"services de réseautage social et développement mobile\" et tu te prends dans la tronche que \"ouais.. on entend ça tous les jours, c'est pas innovant\"...Du coup les aides, j'ai décidé d'abandonner, je veux rester maître de mon entreprise, et si les coups de main de l'Etat ne sont pas à ma portée eh bien soit... je me suis serré la ceinture et j'ai foutu toutes mes économies dans le compte de ma boîte. J'ai restreint l'embauche à des stagiaires et j'ai fait appel à des services extérieurs.Les temps sont difficiles mais j'arrive au bout d'une grosse réalisation et je regrette franchement pas de m'être autant battu jusqu'à présent...Et si rien ne fonctionne, ça aura quand même été une sacré expérience!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Bon eh bien ce n'était pas une si grossière erreur alors ! Je compte créer ma boîte dans quelques années, donc merci pour ton histoire :)\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne vois pas le mec , mais je ne sais pas que tu t ' es à la même si tu as des autres .\u001b[0;0m\n",
            "\u001b[0mNan ! Bien sûr! J'étais un peu fatigué quand j'ai écrit ce pathé.C'est une expérience plus qu'enrichissante et ça vaut \"tous les cours du monde\" selon moi. Même si tu te plante, tu rencontres tellement de gens et tu touches à tellement de choses différentes que c'est franchement passionant.Après si je peux me permettre un conseil : au début, vise l'auto-entrepreunariat ou la SAS. J'ai fait la grosse bêtise de faire une SARL (suite à de multiples conseils de gens \"bien calés\") pour au final rencontrer le RSI, ou le Régime Social des Indépendants. Soit la cotisation à la retraite, assurance maladie etc... N'importe quelle personne qui a à faire au RSI te le décrira comme un cancer et il n'aura pas tort. La façon dont ils calculent les sommes que tu leur doit est vraiment abérante.Par exemple 1 mois après la création de ma boîte, je reçois ma première lettre d'appel de cotisation et je vois qu'on me demande 7789€ pour le premier trimestre de l'année. Quand j'appelle pour demander comment c'est possible, on me répond -véridique- \"Ah mais monsieur je vois sur votre dossier que vous faites des réseaux sociaux et quand on voit Facebook et Google tout ça on estime que vous gagnez au minimum 10.000€/mois\". Ca fait un an que la bataille dure encore... :p\u001b[0;0m\n",
            "\u001b[1;94m    labels: Wow ... Ca fait peur ton truc :SOuais je vais commencer par l'auto-entrepenariat. Comme toi je suis dans le logiciel, j'ai un projet que je peaufine sur mon temps libre et une fois que je me sentirai pret, avec une situation un peu plus stable, je quitte mon job et je me lance. Je retiens donc: RSI = cancer, ca marche :P\u001b[0;0m\n",
            "\u001b[0;95m     model: je ne sais pas , mais je suis pas d ' accord avec les gens qui ne sont plus d ' être de la fin . mais je sais pas .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: french_reddit/data_test.txt- - -\u001b[0;0m\n",
            "\u001b[0mMAIS...67/100 des juifs israeliens sont pour une attaque.-briefs/2013/8/30/poll-67-percent-of-israeli-jews-support-us-attack-on-syriaantisemit news? false information?\u001b[0;0m\n",
            "\u001b[1;94m    labels: OK?Je vois pas trop le rapport.La plupart des français ne sont pas juifs israéliens donc la disparité entre l'article de reuters et celui de ton site douteux n'est pas vraiment une surprise.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis d ' accord avec toi . . je ne vois pas une bonne des gens qui ont des gens en france .\u001b[0;0m\n",
            "\u001b[0msi tu vois pas c est parce que tu es aveuglequel est le pourcentage de juif dans la politique francaise. tu vois pas pas le rapport ah oui c est vrai.c est le jewishnews. org que tu qualifies de site douteux? espece d antisemite.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Ok j'arrive pas à savoir si t'es complètement débile ou un mauvais troll.GG.\u001b[0;0m\n",
            "\u001b[0;95m     model: je suis d ' accord avec toi , je ne suis pas d ' être un peu de la fin .\u001b[0;0m\n",
            "\u001b[0mjparle pas aux antisemites c est tout.\u001b[0;0m\n",
            "\u001b[1;94m    labels:     se plaint du nombre de juifs dans la classe politique Française    traite les autres d'antisémites    okay.jpgNon mais en dehors de la proportion de juifs présente ou pas dans la politique française (ce dont je me contrefous royalement, au passage), il n'y a aucun rapport entre l'article de OP et le tiens, étant donné qu'ils parlent de deux populations complètement distinctes. Je vois pas ou tu va chercher une conclusion de mésinformation ou d'antisémitisme là dedans.\u001b[0;0m\n",
            "\u001b[0;95m     model: c ' est un peu de l ' air d ' être un peu un truc d ' avoir un autre .\u001b[0;0m\n"
          ]
        }
      ],
      "source": [
        "# model_path = \"trained_fr_reddit-small/\"\n",
        "# data_path = \"fr_reddit_dataset/\"\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/colabs/blender-models/trained_fr_reddit/\"\n",
        "data_path = '/content/drive/MyDrive/colabs/aliae-workspace/datasets/french_reddit/'\n",
        "print(f'{model_path}/model')\n",
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='fromfile:parlaiformat', \n",
        "    fromfile_datapath= f\"{data_path}data\",\n",
        "    fromfile_datatype_extension=True,\n",
        "\n",
        "    # model= \"transformer/generator\",\n",
        "    model_file= f\"{model_path}model\",\n",
        "    # init_model= \"zoo:blender/blender_400Mdistill/model\",\n",
        "    # dict_file= f\"zoo:blender/blender_400Mdistill/model.dict\",\n",
        "\n",
        "    datatype= \"test\",\n",
        "    # fromfile_datatype_extension= True,\n",
        "\n",
        "    num_examples=20,\n",
        "    skip_generation=False,\n",
        "\n",
        "    # inference = 'topk', # Generation algorithm. Choices: beam, topk, greedy, delayedbeam, nucleus\n",
        "    # temperature = 0.7, # Temperature to add during decoding. Default 1.0\n",
        "    # topk=30, # K used in Top K sampling\n",
        "    # beam_length_penalty=1.03 # Applies a length penalty. Set to 0 for no penalty. Default: 0.65.\n",
        "\n",
        "    beam_context_block_ngram= 3,\n",
        "    beam_block_ngram= 3, beam_min_length= 20, beam_size= 10,\n",
        "    inference =  'topk', topk=20, temperature = 0.5, beam_length_penalty=0.8\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "h89rB0oWNEBN",
        "iS4WNAkWkq4p",
        "Qm7b-GZHkuRg",
        "Kd2Wv9B08OI5"
      ],
      "machine_shape": "hm",
      "name": "training-transformer-french_reddit.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdYbJ0Ezs+souwV8/9psgk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}